<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>HYUNGI&#39;S TECH BLOG</title>
    <link>https://leehyungi0622.github.io/</link>
    
    <atom:link href="https://leehyungi0622.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Web developer(Front-end)</description>
    <pubDate>Tue, 02 May 2023 11:11:13 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>221016 Mapper class Bean 등록 Issue</title>
      <link>https://leehyungi0622.github.io/2022/10/16/202210/221016_spring_error/</link>
      <guid>https://leehyungi0622.github.io/2022/10/16/202210/221016_spring_error/</guid>
      <pubDate>Sun, 16 Oct 2022 07:01:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/221016_spring_bean_container.png&quot; alt=&quot;Spring bean container&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;우선 본 포스팅에서 다루고자 하는 에러가 어떤 상황에서 발생하게 되었는지에 대해서 간략하게 설명을 하고, 어떤 접근으로 해결하려고 했는지에 대해서 기술해보려고 한다.&lt;/p&gt;
&lt;p&gt;&lt;ins&gt;&lt;b&gt;프로젝트 진행&lt;/b&gt;&lt;/ins&gt;&lt;/p&gt;
&lt;p&gt;회사에서 진행중인 프로젝트에서 백엔드를 구성하면서 간단한 CRUD 처리의 경우에는 JPA를 사용하고, 복잡한 쿼리 사용의 경우에는 MyBatis를 사용하기로 했다. 여기서 말하는 복잡한 쿼리란 간단한 CRUD 이외의 복잡한 쿼리를 말한다.&lt;br&gt;그냥 이렇게 하라고 하니깐 하는 게 아니라 이렇게 구성을 하면 어떤 이점이 있는지에 대해서 간단하게 짚고 넘어가도록 하자.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;b&gt;Hibernate vs MyBatis&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;과거 EJB2로 개발을 하던 당시 Gavin king이라는 사람이 사용자 친화적이지 않은 자바 애플리케이션 개발 방식을 좀 더 사용자 친화적이게 만들고자 개발을 하게 된 것이 바로 그 유명한 Hibernate이다. 이 Hibernate가 점차 인기가 많아지자 자바 진영에서 Gavin king을 영입해서 자바 &lt;code&gt;ORM(Object Relational Mapping)&lt;/code&gt; 기술에 대한 표준 명세를 개발하도록 하였는데, 그것이 바로 &lt;code&gt;JPA(Java Persistent API)&lt;/code&gt;이다.&lt;/p&gt;
&lt;p&gt;JPA는 ORM을 사용하기 위한 인터페이스를 모아둔 것으로, 이를 구현한 프레임워크 중 대표적으로 Hibernate가 있고, Spring에서는 대부분 Hibernate를 사용하고 있다.&lt;br&gt;Spring에서는 JPA를 사용할 때 구현체들을 직접 다루지 않고, &lt;code&gt;구현체들을 좀 더 쉽게 사용하고자 추상화 시킨 Spring Data JPA라는 모듈을 시용하여 JPA 기술을 다룬다.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;그렇다면, JPA 하나만 사용해서 개발하면 만능일까? 찾아보니 대부분 단순한 CRUD의 처리를 하는 경우에는 JPA만을 사용해도 괜찮다고 한다. 하지만, 복잡한 통계나 정산관련 조회 쿼리가 포함된 경우, MyBatis로 처리하면 좀 더 개발자에게 편하다. 물론 JPA로도 복잡한 집계성 쿼리를 처리하는 것도 가능은 하지만 구현이 쉽지 않기 때문에 MyBatis를 사용하는 것이 낫다고 한다.&lt;/p&gt;
&lt;p&gt;따라서 JPA 또는 MyBatis만 사용하는 것이 아닌, 두 개를 적절히 조합해서 프로젝트를 진행하면 업무적으로 효율이 높아질 수 있다는 결론이 나온다.&lt;/p&gt;
&lt;br/&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;b&gt;MyBatis의 사용&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;MyBatis는 Mapper를 별도로 구성하며, SqlSession을 직접 사용하는 형태가 아닌 Mapper를 통해 처리한다. 기본적으로 Mapper를 사용하게 되면, Mapping 파일이 자동으로 Mapper의 단위가 되기 때문에 유지보수 및 관리에 용이하다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/221016_spring_bean_container.png" alt="Spring bean container"></div><br/><br/><p>우선 본 포스팅에서 다루고자 하는 에러가 어떤 상황에서 발생하게 되었는지에 대해서 간략하게 설명을 하고, 어떤 접근으로 해결하려고 했는지에 대해서 기술해보려고 한다.</p><p><ins><b>프로젝트 진행</b></ins></p><p>회사에서 진행중인 프로젝트에서 백엔드를 구성하면서 간단한 CRUD 처리의 경우에는 JPA를 사용하고, 복잡한 쿼리 사용의 경우에는 MyBatis를 사용하기로 했다. 여기서 말하는 복잡한 쿼리란 간단한 CRUD 이외의 복잡한 쿼리를 말한다.<br>그냥 이렇게 하라고 하니깐 하는 게 아니라 이렇게 구성을 하면 어떤 이점이 있는지에 대해서 간단하게 짚고 넘어가도록 하자.</p><ul><li><p><b>Hibernate vs MyBatis</b></p><p>과거 EJB2로 개발을 하던 당시 Gavin king이라는 사람이 사용자 친화적이지 않은 자바 애플리케이션 개발 방식을 좀 더 사용자 친화적이게 만들고자 개발을 하게 된 것이 바로 그 유명한 Hibernate이다. 이 Hibernate가 점차 인기가 많아지자 자바 진영에서 Gavin king을 영입해서 자바 <code>ORM(Object Relational Mapping)</code> 기술에 대한 표준 명세를 개발하도록 하였는데, 그것이 바로 <code>JPA(Java Persistent API)</code>이다.</p><p>JPA는 ORM을 사용하기 위한 인터페이스를 모아둔 것으로, 이를 구현한 프레임워크 중 대표적으로 Hibernate가 있고, Spring에서는 대부분 Hibernate를 사용하고 있다.<br>Spring에서는 JPA를 사용할 때 구현체들을 직접 다루지 않고, <code>구현체들을 좀 더 쉽게 사용하고자 추상화 시킨 Spring Data JPA라는 모듈을 시용하여 JPA 기술을 다룬다.</code></p><p>그렇다면, JPA 하나만 사용해서 개발하면 만능일까? 찾아보니 대부분 단순한 CRUD의 처리를 하는 경우에는 JPA만을 사용해도 괜찮다고 한다. 하지만, 복잡한 통계나 정산관련 조회 쿼리가 포함된 경우, MyBatis로 처리하면 좀 더 개발자에게 편하다. 물론 JPA로도 복잡한 집계성 쿼리를 처리하는 것도 가능은 하지만 구현이 쉽지 않기 때문에 MyBatis를 사용하는 것이 낫다고 한다.</p><p>따라서 JPA 또는 MyBatis만 사용하는 것이 아닌, 두 개를 적절히 조합해서 프로젝트를 진행하면 업무적으로 효율이 높아질 수 있다는 결론이 나온다.</p><br/></li><li><p><b>MyBatis의 사용</b></p><p>MyBatis는 Mapper를 별도로 구성하며, SqlSession을 직접 사용하는 형태가 아닌 Mapper를 통해 처리한다. 기본적으로 Mapper를 사용하게 되면, Mapping 파일이 자동으로 Mapper의 단위가 되기 때문에 유지보수 및 관리에 용이하다.</p></li></ul><br/><a id="more"></a><p><ins><b>문제상황</b></ins></p><p>  JPA와 MyBatis를 하나의 프로젝트에서 구성을 하면서 직면한 문제는 <code>@Mapper</code> annotation을 붙여서 정의한 Mapper class의 Bean 객체가 등록이 되지 않아 Service 클래스에서 해당 Mapper class의 Bean 객체의 의존성 주입시에 아래의 에러가 발생하였다.</p><p>  <code>&quot;Consider defining a bean of type &#39;&#123;Mapper class package detail&#125;&#39; in your configuration.&quot;</code></p><br/><p><ins><b>해결책</b></ins></p><p>  Mapper class에 달아주는 @Mapper annotation은 기본적으로 <code>@Component annotation 없이 클래스를 generate해준다.</code> @Component annotation은 기본적으로 Spring container에 Bean 객체를 등록해주기 위해 스캔해주는 과정을 위한 annotation이다. @Service와 @Repository, @Controller의 Custom annotation 정의를 보면, 모두 @Component annotation을 상속하고 있음을 알 수 있다.</p>  <br/><ul><li><p><ins><b>solution1) *.yml 파일에 mybatis에 대한 설정 추가하기</b></ins></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mybatis:</span><br><span class="line">  mapper-locations: classpath:&#123;구체 경로 지정&#125;&#x2F;mapper&#x2F;*.xml</span><br><span class="line">  type-aliases-package: ...(생략)...</span><br></pre></td></tr></table></figure><br/></li><li><p><ins><b>solution2) pom.xml 파일에서 dependency의 버전 간 호환성 문제</b></ins></p><p>mybatis-spring-boot-starter는 1.3.2, mybatis-spring을 1.3.2로 버전을 설정한다.<br><code>이 부분은 기존에 작업하고 있는 백엔드 프로젝트의 pom.xml의 버전 호환을 확인하여 수정할 필요가 있다.</code> (<code>현재 별도로 백엔드 프로젝트를 작업중...</code>) </p><p>일단 문제상황 재현을 위해 생성한 프로젝트의 pom.xml에서 아래와 같이 mybatis-spring-boot-starter(1.3.2), mybatis(3.4.6), mybatis-spring(1.3.2)로 버전을 맞춰서 다시 Maven clean 및 install을 해주니 정상적으로 Mapper class의 Bean이 등록되어 에러 메시지가 더 이상 나오지 않았다.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  <span class="string">&lt;dependency&gt;</span></span><br><span class="line"><span class="string">&lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;</span></span><br><span class="line"><span class="string">&lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;</span></span><br><span class="line"><span class="string">&lt;version&gt;1.3.2&lt;/version&gt;</span></span><br><span class="line"><span class="string">&lt;/dependency&gt;</span></span><br><span class="line"><span class="string">&lt;dependency&gt;</span></span><br><span class="line"><span class="string">&lt;groupId&gt;org.mybatis&lt;/groupId&gt;</span></span><br><span class="line"><span class="string">&lt;artifactId&gt;mybatis&lt;/artifactId&gt;</span></span><br><span class="line"><span class="string">&lt;version&gt;3.4.6&lt;/version&gt;</span></span><br><span class="line"><span class="string">&lt;/dependency&gt;</span></span><br><span class="line"><span class="string">&lt;dependency&gt;</span></span><br><span class="line"><span class="string">&lt;groupId&gt;org.mybatis&lt;/groupId&gt;</span></span><br><span class="line"><span class="string">&lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;</span></span><br><span class="line"><span class="string">&lt;version&gt;1.3.2&lt;/version&gt;</span></span><br><span class="line"><span class="string">&lt;/dependency&gt;</span></span><br></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Resolved-Error/">Resolved-Error</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Spring/">Spring</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/10/16/202210/221016_spring_error/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>221008 Apache NiFi 스터디 (작성중...)</title>
      <link>https://leehyungi0622.github.io/2022/10/08/202210/221008_apache_nifi_study/</link>
      <guid>https://leehyungi0622.github.io/2022/10/08/202210/221008_apache_nifi_study/</guid>
      <pubDate>Sat, 08 Oct 2022 05:27:00 GMT</pubDate>
      
        
        
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/221008_nifi_logo.jpeg&quot; alt=&quot;nifi&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 최근에 새롭게 접하게 된 ETL </description>
        
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/221008_nifi_logo.jpeg" alt="nifi"></div><br/><br/><p>이번 포스팅에서는 최근에 새롭게 접하게 된 ETL 툴인 Apache NiFi에 대해서 정리해보려고 한다.</p><h2 id="NiFi의-기술적-배경"><a href="#NiFi의-기술적-배경" class="headerlink" title="NiFi의 기술적 배경"></a><ins><b>NiFi의 기술적 배경</b></ins></h2><p>NiFi는 NAS라는 미국의 국가 안보국에서 Apache에 기부한 Dataflow 엔진으로, <code>ETL툴의 일종</code>이다.<br>NiFi는 과거에 NSA에 의해 개발되었다가 2014년 기술 전송 프로그램의 일부로, 오픈소스화된 나이아가라파일즈(NiagaraFiles)에 기반을 두고 있다.</p><p>NiFi는 분산환경에서 대량의 데이터를 수집, 처리한다. =&gt; FBP개념을 구현한 오픈소스 프로젝트<br>여기서 FBP란 사전에 Data Flow를 정의하고, 이를 지속적으로 유지하면서 데이터를 교환하는 프로그래밍 패러다임이다.<br>(<code>마치 Apache Airflow에서 DAG를 작성해서 전체적인 Task flow를 구성한 뒤에 반복적인 데이터 처리를 자동화 시키는 것과 같은 맥락이다</code>)</p><h2 id="NiFi의-필요성"><a href="#NiFi의-필요성" class="headerlink" title="NiFi의 필요성"></a><ins><b>NiFi의 필요성</b></ins></h2><p>그래서 왜 NiFi가 필요할까? 앞서 이미 정의했듯이 NiFi는 ETL툴의 일종으로, 클러스터로 구성이 되어있기 때문에 대량의 데이터를 분산시켜서 처리할 수 있다.<br>NiFi는 A 시스템에서 B 시스템으로 데이터를 이관하는 것을 손쉽게 할 수 있도록 도와주는 서비스 툴로, 데이터를 이관하는 중간에 데이터를 변형(정제)할 수 있다. 그 외에도 관리 및 모니터링이 가능하다.</p><h2 id="NiFi의-구성"><a href="#NiFi의-구성" class="headerlink" title="NiFi의 구성"></a><ins><b>NiFi의 구성</b></ins></h2><p>NiFi는 FlowFile, Processor, Connection, 이 세 가지로 구성이 되어있다.</p><ul><li><p><b>[FlowFile]</b></p><p>FlowFile은 NiFi가 인식하는 데이터 단위로, 속성과 내용이 Key/Value 혀태로 구성이 되어있다. Processor마다 이동시 복사본이 생성되어, 추적이 용이하다.</p><p><code>ref.</code> query를 작성할때 [table명] 대신에 FLOWFILE을 작성하게 되면, 파이프라인 상에서 흘러가는 파일 정보를 읽을 수 있다.</p></li><li><p><b>[Processor]</b></p><p>FlowFile을 수집, 변형 및 저장하는 기능이 있으며, 자주 사용되는 프로세서로는 <code>http, kafka, db, ftp</code>와 관련된 프로세서, 속성을 변경하는 <code>updateattribute</code>, 데이터를 합치는 <code>mergecontent</code>, 데이터를 분할하는 <code>split</code>, 데이터 타입을 변경하는 <code>convert</code>등이 있다. </p></li></ul><ul><li><p><b>[Connection]</b></p><p>각 Processor별로 연결해서 FLOWFILE을 전달하는 역할을 담당하고, FlowFile의 우선순위, 만료, 부하조절 기능도 제한하고 있다. </p></li></ul><h2 id="NiFi-아키텍쳐"><a href="#NiFi-아키텍쳐" class="headerlink" title="NiFi 아키텍쳐"></a><ins><b>NiFi 아키텍쳐</b></ins></h2><ul><li><p><b>[Web Server]</b></p><p>발생하는 이벤트를 모니터링, 소프트웨어를 시각적으로 제어하기 위해서 사용된다. (<code>HTTP기반 구성요소</code>)</p></li><li><p><b>[Flow controller]</b></p><p>NiFi 동작의 뇌 역할을 담당한다. <code>NiFi 확장기능의 실행을 통제</code>하고, 이를 위한 <code>자원 할당을 스케줄링</code>한다.</p></li><li><p><b>[Extensions]</b></p><p>NiFi가 다양한 종류의 시스템과 통신할 수 있게 하는 다양한 플러그인이다.</p></li><li><p><b>[FlowFile Repository]</b></p><p>NiFi가 현재 실행중인 FlowFile의 상태를 추적하고, 정비하기 위해 사용된다.</p></li><li><p><b>[Content Repository]</b></p><p>전송 대상의 데이터가 관리된다.</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/NiFi/">NiFi</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/NiFi/">NiFi</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/10/08/202210/221008_apache_nifi_study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220808 Apache Airflow</title>
      <link>https://leehyungi0622.github.io/2022/08/08/202208/220808_airflow_on_docker/</link>
      <guid>https://leehyungi0622.github.io/2022/08/08/202208/220808_airflow_on_docker/</guid>
      <pubDate>Mon, 08 Aug 2022 01:08:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220804_apache_airflow.png&quot; alt=&quot;Apache Airflow&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;h2 id=&quot;Configuration-살펴보기&quot;&gt;&lt;a href=&quot;#Configuration-살펴보기&quot; class=&quot;headerlink&quot; title=&quot;Configuration 살펴보기&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Configuration 살펴보기&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;더 많은 Airflow Worker가 필요하다면, 추가 machine에서 celery worker를 명령한다.&lt;/p&gt;
&lt;h3 id=&quot;Flower&quot;&gt;&lt;a href=&quot;#Flower&quot; class=&quot;headerlink&quot; title=&quot;Flower&quot;&gt;&lt;/a&gt;&lt;strong&gt;Flower&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Airflow workers를 dashboard를 통해서 모니터링하기 위한 툴이다. Celery Executor를 사용하면, Flower에 접속해서 Celery Executor의 Administrator와 Airflow worker를 대시보드를 통해 모니터링 할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;localhost:5555/dashboard&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;아래의 Worker dashboard를 살펴보면, Max concurrency 값이 16인 것으로 보아, 최대 16개의 Task를 동시에 실행하는 것이 가능하다. 이는 사용하는 PC의 리소스에 따라 줄일 수도 늘릴 수도 있다.&lt;/p&gt;
&lt;p&gt;다음으로 Queues 메뉴는 유용하게 사용될 수 있는데, 특정 Task를 특정 worker로 라우팅되도록 할 수도 있다.&lt;br&gt;예를들어 높은 리소스를 소비하는 Task가 존재하고, 현재 하나의 높은 리소스 Worker를 가진다고 가정하면, queue를 생성해서 queue를 Worker에 붙이고, 높은 리소스 소비 Task를 생성한 queue로 보내서 해당 Worker만 해당 작업을 실행할 수 있도록 queue를 지정해서 사용할 수 있다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220804_apache_airflow.png" alt="Apache Airflow"></div><br/><br/><h2 id="Configuration-살펴보기"><a href="#Configuration-살펴보기" class="headerlink" title="Configuration 살펴보기"></a><ins><b>Configuration 살펴보기</b></ins></h2><p>더 많은 Airflow Worker가 필요하다면, 추가 machine에서 celery worker를 명령한다.</p><h3 id="Flower"><a href="#Flower" class="headerlink" title="Flower"></a><strong>Flower</strong></h3><p>Airflow workers를 dashboard를 통해서 모니터링하기 위한 툴이다. Celery Executor를 사용하면, Flower에 접속해서 Celery Executor의 Administrator와 Airflow worker를 대시보드를 통해 모니터링 할 수 있다.</p><p><code>localhost:5555/dashboard</code></p><p>아래의 Worker dashboard를 살펴보면, Max concurrency 값이 16인 것으로 보아, 최대 16개의 Task를 동시에 실행하는 것이 가능하다. 이는 사용하는 PC의 리소스에 따라 줄일 수도 늘릴 수도 있다.</p><p>다음으로 Queues 메뉴는 유용하게 사용될 수 있는데, 특정 Task를 특정 worker로 라우팅되도록 할 수도 있다.<br>예를들어 높은 리소스를 소비하는 Task가 존재하고, 현재 하나의 높은 리소스 Worker를 가진다고 가정하면, queue를 생성해서 queue를 Worker에 붙이고, 높은 리소스 소비 Task를 생성한 queue로 보내서 해당 Worker만 해당 작업을 실행할 수 있도록 queue를 지정해서 사용할 수 있다.</p><a id="more"></a><div align="center">  <img src="/images/post_images/220808_flower_dashboard_monitor.png" alt="Apache Airflow Worker monitoring"></div><h2 id="DAG-예시-제거하기"><a href="#DAG-예시-제거하기" class="headerlink" title="DAG 예시 제거하기"></a><ins><b>DAG 예시 제거하기</b></ins></h2><p>Airflow instance를 깨끗하게 유지시키기 위해서 아래의 방법으로 DAG를 UI로부터 제거한다.</p><p>(1) docker-compose.yml 파일에서 <code>AIRFLOW__CORE__LOAD_EXAMPLES</code> 환경변수를 true에서 false로 변경 후 파일을 저장한다.</p><p>(2) 아래 명령을 통해서 Airflow를 재시작한다.<br>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$docker</span>-compose down &amp;&amp; docker-compose up -d</span><br></pre></td></tr></table></figure></p><h2 id="Flower-모니터링"><a href="#Flower-모니터링" class="headerlink" title="Flower 모니터링"></a><ins><b>Flower 모니터링</b></ins></h2><p>DAG를 Trigger한 후에 Flower를 통해 모니터링을 해보면, Active column을 통해 현재 DAG의 Task가 Celery worker에서 실행되고 있음을 확인할 수 있으며, status가 Processed를 거쳐 Succeeded로 완료가 되는 것을 확인할 수 있다.</p><p>Flower에서 Task를 모니터링할때 주의해서 봐야되는 부분은 <code>failure</code>와 <code>retried</code>부분으로, 이 부분에 대한 카운트가 있는지에 대해서 확인해야 한다. 이후에 Tasks 메뉴를 통해서 Task의 개별 UUID를 클릭해서 실행된 Task의 상세정보에 대해서 확인할 수 있다. 상세정보로는 Task가 현재 어떤 상태이고, 어느 Worker에서 실행이 되었는지에 대해서도 확인할 수 있다.</p><p>Task가 실행된 Worker는 queue에 따라 다른 Worker에서 실행할 수 있다.</p><h2 id="복수-개의-Queue가-필요한-이유"><a href="#복수-개의-Queue가-필요한-이유" class="headerlink" title="복수 개의 Queue가 필요한 이유"></a><ins><b>복수 개의 Queue가 필요한 이유</b></ins></h2><p>Airflow에서 queue(FIFO)는 trigger되기를 기다리는 Task들의 대기열로 볼 수 있다. Celery queue는 Broker와 Result Backend로 구성이 되어있고, Worker는 복수 개로 구성이 되어있다. 복수 개의 Worker는 각 각 5 CPUs, GPU, 1GPU등의 스펙을 가질 수 있는데, 이는 처리해야되는 Task에 따라서 Worker를 지정해서 처리해야한다.</p><p>만약 복잡한 연산처리가 필요한 경우에는 5CPUs Worker를 통해 Task가 실행되도록 해야하며, 머신러닝과 같은 처리를 위해서는 GPU의 성능이 좋은 Worker에서 처리를 해야한다. 그리고 그 이외의 경우에 일반적인 Task의 처리에서는 1CPU worker에서 처리되도록 분류해야한다. 이 경우에는 어떻게 분류를 해서 처리를 해야할까?</p><p>바로 Celery Queue에 세 개의 queue(high_cpu, ml_model, default)로 분류해서 high_cpu 큐는 5CPUs Worker에서 처리되는 Task를 쌓아서 처리하도록 하고, ml_model 큐는 GPU 성능이 좋은 Worker에서 처리가 되도록 Task를 쌓아둔다. 그리고 마지막으로 일반적인 Task를 쌓아두는 큐의 역할을 default queue가 담당하게 된다. </p><h2 id="Celery-worker-생성하기"><a href="#Celery-worker-생성하기" class="headerlink" title="Celery worker 생성하기"></a><ins><b>Celery worker 생성하기</b></ins></h2><p>Queue를 생성하기 전에, 우선 복수 개의 Celery Worker를 Apache Airflow 인스턴스에 생성해야 한다.<br>docker-compose.yml 파일에서 <code>airflow-worker</code> 서비스 항목을 복사해서 하나 더 생성을 한 다음에 아래 명령으로 서비스를 다시 시작한다.<br>서비스를 다시 시작하면, <code>localhost:5555</code> Flower web UI에서 두 개의 Worker가 생성이 된 것을 확인할 수 있다.</p><p>실제로 다수의 machine이 있을때, machine에서 아래의 명령을 실행함으로써 Celery worker로써 동작하도록 할 수 있다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$celery</span> worker</span><br></pre></td></tr></table></figure><h2 id="queue-생성하기"><a href="#queue-생성하기" class="headerlink" title="queue 생성하기"></a><ins><b>queue 생성하기</b></ins></h2><p>Airflow에서 queue를 생성하는 것은 간단하다. airflow-worker 서비스의 command 항목에서 celery worker를 해주는 경우, default queue를 통해 worker로 Task가 분배가 되지만, 아래와같이 worker 생성시에 command에 -q 옵션으로 Worker에 queue를 붙여주면, 특정 Task를 지정한 airflow worker에서 실행되도록 할 수 있다.</p>  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">airflow-worker-1:</span></span><br><span class="line"><span class="string">&lt;&lt;:</span> <span class="string">*airflow-common</span></span><br><span class="line"><span class="attr">command:</span> <span class="string">celery</span> <span class="string">worker</span> <span class="string">-q</span> <span class="string">high_cpu</span></span><br><span class="line"><span class="attr">healthcheck:</span></span><br><span class="line">  <span class="attr">test:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;CMD-SHELL&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;celery --app airflow.executors.celery_executor.app inspect ping -d &quot;celery@$$&#123;HOSTNAME&#125;&quot;&#x27;</span></span><br><span class="line">  <span class="attr">interval:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">timeout:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">retries:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">environment:</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="string">*airflow-common-env</span></span><br><span class="line">  <span class="comment"># Required to handle warm shutdown of the celery workers properly</span></span><br><span class="line">  <span class="comment"># See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation</span></span><br><span class="line">  <span class="attr">DUMB_INIT_SETSID:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line"><span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">depends_on:</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="string">*airflow-common-depends-on</span></span><br><span class="line">  <span class="attr">airflow-init:</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">service_completed_successfully</span></span><br></pre></td></tr></table></figure><h2 id="Task를-지정한-queue에-보내기"><a href="#Task를-지정한-queue에-보내기" class="headerlink" title="Task를 지정한 queue에 보내기"></a><ins><b>Task를 지정한 queue에 보내기</b></ins></h2><p>DAG에서 Operator로 Task를 작성할때, Operator의 인자로 아래와 같이 queue를 지정해주면, 지정한 queue로 해당 Task가 전송된다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">transform = BashOperator(</span><br><span class="line">  task_id=<span class="string">&#x27;transform&#x27;</span>,</span><br><span class="line">  queue=<span class="string">&#x27;high_cpu&#x27;</span>,</span><br><span class="line">  bash_command=<span class="string">&#x27;sleep 10&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Airflow-동시성을-위한-parameters"><a href="#Airflow-동시성을-위한-parameters" class="headerlink" title="Airflow 동시성을 위한 parameters"></a><ins><b>Airflow 동시성을 위한 parameters</b></ins></h2><p>병렬로 동시에 처리할 tasks와 DAG Runs를 정의하기 위해서는 아래의 parameters를 configuration settings에서 설정해줘야 한다.</p><h3 id="parallelism-AIRFLOW-CORE-PARALELISM"><a href="#parallelism-AIRFLOW-CORE-PARALELISM" class="headerlink" title="parallelism / AIRFLOW__CORE__PARALELISM"></a><strong>parallelism / AIRFLOW__CORE__PARALELISM</strong></h3><p>이 parameter는 Airflow의 Scheduler당 실행할 수 있는 task instances의 갯수에 대해서 정의한다. default로는 Scheduler당 최대 32개까지 동시에 실행이 가능하며, Scheduler의 수가 늘면 그 수가 배가 된다.</p><h3 id="max-active-tasks-per-dag-AIRFLOW-CORE-MAX-ACTIVE-TASKS-PER-DAG"><a href="#max-active-tasks-per-dag-AIRFLOW-CORE-MAX-ACTIVE-TASKS-PER-DAG" class="headerlink" title="max_active_tasks_per_dag / AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG"></a><strong>max_active_tasks_per_dag / AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG</strong></h3><p>이 parameter는 각 각의 DAG에서 동시(concurrency)에 실행할 수 있는 최대 task instance의 수를 정의한다. default로는 최대 16개의 tasks를 지정된 DAG에서 동시에 처리할 수 있다. (<code>모든 DAG 실행에서</code>)</p><h3 id="max-active-runs-per-dag-AIRFLOW-CORE-MAX-ACTIVE-RUNS-PER-DAG"><a href="#max-active-runs-per-dag-AIRFLOW-CORE-MAX-ACTIVE-RUNS-PER-DAG" class="headerlink" title="max_active_runs_per_dag / AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG"></a><strong>max_active_runs_per_dag / AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG</strong></h3><p>이 parameter는 DAG당 최대 활성시킬 수 있는 DAG의 수를 정의한다. default로는 최대 16개의 DAG를 동시에 실행할 수 있다.</p><h2 id="Sequential-executor에서의-SQLite의-특징"><a href="#Sequential-executor에서의-SQLite의-특징" class="headerlink" title="Sequential executor에서의 SQLite의 특징"></a><ins><b>Sequential executor에서의 SQLite의 특징</b></ins></h2><p>Sequential executor에서 사용되는 SQLite는 무제한으로 reader를 허용한다. 하지만 writer는 한 번에 한 명 밖에 허용하지 않는다. 이러한 이유로 SQLite는 복수의 Worker에서 병렬로 Tasks를 실행할 수 있는 Local executor와 Celery executor에서는 사용될 수 없다.</p><h2 id="Repetitive-Patterns"><a href="#Repetitive-Patterns" class="headerlink" title="Repetitive Patterns"></a><ins><b>Repetitive Patterns</b></ins></h2><p>만약 파일을 다운로드하는 Task 세 개가 주어지고, 세 개의 Task가 일괄적으로 Checking Files이라는 하나의 Task를 통해 파일 검사를 시행한다. 그 후에 세 개의 Task로 분리하여 다운로드 받은 파일들을 처리한다. 이런 구조로 되어있는 Task sequence에서는 다운로드하는 세 개의 Task를 그룹화해서 <code>Downloading Files</code>인 하나의 Task로 만들고, 다운로드 받은 파일을 처리하는 <code>Processing Files</code>도 하나의 Task로써 작성한다. </p><p>이는 DAG의 Task 구조를 쉽게 읽고 유지보수하기 위함이다. 이는 <code>SubDAGs</code>나 <code>TaskGroup</code>을 사용해서 구현할 수 있다.</p><p><code>SubDag를 사용해서 구현했을때에는 SubDag로 묶인 Task를 클릭해서 별도의 DAG 페이지에서 하위 Task 구성만 별도로 확인을 할 수 있었는데, TaskGroup을 사용해서 구현을 하면, 현재 DAG 구조에서 가시적으로 Group이 표시됨을 확인할 수 있었다.</code></p>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Airflow/">Airflow</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/Airflow/">Airflow</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/08/08/202208/220808_airflow_on_docker/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220807 Apache Airflow</title>
      <link>https://leehyungi0622.github.io/2022/08/07/202208/220807_airflow_on_docker/</link>
      <guid>https://leehyungi0622.github.io/2022/08/07/202208/220807_airflow_on_docker/</guid>
      <pubDate>Sun, 07 Aug 2022 00:35:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220804_apache_airflow.png&quot; alt=&quot;Apache Airflow&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;h2 id=&quot;Backfilling&quot;&gt;&lt;a href=&quot;#Backfilling&quot; class=&quot;headerlink&quot; title=&quot;Backfilling&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Backfilling&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;DAG를 처음 실행하게 되면, scheduler는 자동으로 non-triggered DagRuns을 시작 날짜(start_date)와 현재(now) 사이 시점에서 실행하게 된다.&lt;br&gt;catch up mechanism은 자동으로 non-triggered DagRun을 마지막으로 실행된 날짜와 현재 시간 사이에서 실행할 수 있도록 허용한다.&lt;/p&gt;
&lt;p&gt;예를들어, 만약에 DAG를 2일동안 중지시키고나서 DAG를 다시 시작했다면, 이 기간 동안 트리거되지 않은 DAG 실행에 해당하는 일부 DAG 실행이 발생합니다.&lt;br&gt;Backfilling mechanism은 historical DagRuns를 실행하도록 하는데, 예를들어 start date 이전의 기간에 DagRun을 실행할 수 있다.&lt;br&gt;방법은 Airflow DAG Backfill 명령을 실행하는 명령을 사용하면 된다. &lt;/p&gt;
&lt;p&gt;(&lt;code&gt;예를들어 01/03(start_date)부터 01/07(now)까지 DAG RUN을 실행했고, start_date 이전인 01/01부터 01/02 기간동안 DAG RUN을 실행하고자 한다면, Backfilling mechanism을 위한 명령을 사용하면 된다&lt;/code&gt;)&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;with&lt;/span&gt; DAG(&lt;span class=&quot;string&quot;&gt;&amp;#x27;my_dag&amp;#x27;&lt;/span&gt;, start_date=datetime(&lt;span class=&quot;number&quot;&gt;2022&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;), schedule_interval=&lt;span class=&quot;string&quot;&gt;&amp;#x27;@daily&amp;#x27;&lt;/span&gt;, catchup=&lt;span class=&quot;literal&quot;&gt;False&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; dag:&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;이렇게 catchup=False로 설정값을 바꿔주면, non-triggered DAG RUN이 실행되게 된다. 이 mechanism은 과거에 non-triggered DAG RUN을 자동으로 재실행할때 사용된다.&lt;/p&gt;
&lt;h2 id=&quot;Executor&quot;&gt;&lt;a href=&quot;#Executor&quot; class=&quot;headerlink&quot; title=&quot;Executor&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Executor&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;이전 포스팅에서도 다뤘던 내용이지만, Executor는 이름 자체는 Task를 실행할 것 같지만, Task를 실행하지 않는다. 단지 tasks를 시스템에서 어떻게 실행할 것인가에 대해 정의한다.&lt;/p&gt;
&lt;p&gt;Executor에는 다양한 종류가 있는데, local executors와 remote executors가 있다. &lt;code&gt;local executor&lt;/code&gt;는 여러 개의 task를 single machine에서 실행을 하고, &lt;code&gt;sequential executor&lt;/code&gt;는 single machine에서 한 번에 하나의 task를 실행할때 사용된다. &lt;/p&gt;
&lt;p&gt;remote executor에는 Celery executor가 있는데, tasks를 multiple machine, 그리고 salary cluster에서 실행한다. K8s executor는 multiple machine에서 K8s cluster의 multiple pods에서 multiple tasks를 실행한다.&lt;/p&gt;
&lt;p&gt;Executor의 변경은 Airflow의 환경설정 파일에서 executor parameter를 변경함으로써 적용할 수 있다. (&lt;code&gt;사용되는 executor에 따라 변경해야 되는 별도의 환경설정 요소가 있다&lt;/code&gt;)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220804_apache_airflow.png" alt="Apache Airflow"></div><br/><br/><h2 id="Backfilling"><a href="#Backfilling" class="headerlink" title="Backfilling"></a><ins><b>Backfilling</b></ins></h2><p>DAG를 처음 실행하게 되면, scheduler는 자동으로 non-triggered DagRuns을 시작 날짜(start_date)와 현재(now) 사이 시점에서 실행하게 된다.<br>catch up mechanism은 자동으로 non-triggered DagRun을 마지막으로 실행된 날짜와 현재 시간 사이에서 실행할 수 있도록 허용한다.</p><p>예를들어, 만약에 DAG를 2일동안 중지시키고나서 DAG를 다시 시작했다면, 이 기간 동안 트리거되지 않은 DAG 실행에 해당하는 일부 DAG 실행이 발생합니다.<br>Backfilling mechanism은 historical DagRuns를 실행하도록 하는데, 예를들어 start date 이전의 기간에 DagRun을 실행할 수 있다.<br>방법은 Airflow DAG Backfill 명령을 실행하는 명령을 사용하면 된다. </p><p>(<code>예를들어 01/03(start_date)부터 01/07(now)까지 DAG RUN을 실행했고, start_date 이전인 01/01부터 01/02 기간동안 DAG RUN을 실행하고자 한다면, Backfilling mechanism을 위한 명령을 사용하면 된다</code>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> DAG(<span class="string">&#x27;my_dag&#x27;</span>, start_date=datetime(<span class="number">2022</span>, <span class="number">1</span>, <span class="number">1</span>), schedule_interval=<span class="string">&#x27;@daily&#x27;</span>, catchup=<span class="literal">False</span>) <span class="keyword">as</span> dag:</span><br></pre></td></tr></table></figure><p>이렇게 catchup=False로 설정값을 바꿔주면, non-triggered DAG RUN이 실행되게 된다. 이 mechanism은 과거에 non-triggered DAG RUN을 자동으로 재실행할때 사용된다.</p><h2 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a><ins><b>Executor</b></ins></h2><p>이전 포스팅에서도 다뤘던 내용이지만, Executor는 이름 자체는 Task를 실행할 것 같지만, Task를 실행하지 않는다. 단지 tasks를 시스템에서 어떻게 실행할 것인가에 대해 정의한다.</p><p>Executor에는 다양한 종류가 있는데, local executors와 remote executors가 있다. <code>local executor</code>는 여러 개의 task를 single machine에서 실행을 하고, <code>sequential executor</code>는 single machine에서 한 번에 하나의 task를 실행할때 사용된다. </p><p>remote executor에는 Celery executor가 있는데, tasks를 multiple machine, 그리고 salary cluster에서 실행한다. K8s executor는 multiple machine에서 K8s cluster의 multiple pods에서 multiple tasks를 실행한다.</p><p>Executor의 변경은 Airflow의 환경설정 파일에서 executor parameter를 변경함으로써 적용할 수 있다. (<code>사용되는 executor에 따라 변경해야 되는 별도의 환경설정 요소가 있다</code>)</p><a id="more"></a><h3 id="Executor-configuration-확인"><a href="#Executor-configuration-확인" class="headerlink" title="Executor configuration 확인"></a><strong>Executor configuration 확인</strong></h3><p>  Airflow의 Executor의 configuration 파일을 확인하기 위해 docker container(scheduler)의 cfg 파일을 현 위치에 복사하도록 한다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$docker</span> cp materials_airflow-scheduler_1:/opt/airflow/airflow.cfg .</span><br></pre></td></tr></table></figure><p>  현재 Airflow를 docker를 사용해서 띄워서 사용하고 있고, Docker-compose 파일의 환경변수 정의를 보면, <code>AIRFLOW__CORE__EXECUTOR: CeleryExecutor</code>를 통해 CeleryExector가 사용되고 있음을 확인할 수 있다. 하지만, scheduler container에서 복사한 airflow.cfg 파일의 정의를 보면, SequentialExecutor가 정의되고 있음을 알 수 있다. </p><p>  이는 configuration 파일에 정의된 내용을 docker-compose.yml 파일에서 정의된 환경변수가 덮어쓰고 있다는 의미이다. 따라서 환경설정에 관한 내용은 docker-compose.yml 파일에서 정의할 수 있도록 해야한다.</p><h3 id="Sequential-Executor"><a href="#Sequential-Executor" class="headerlink" title="Sequential Executor"></a><strong>Sequential Executor</strong></h3><p>sequential executor는 Airflow를 수동으로 설치했을때 설정되는 default executor이다. Sequential Executor는 Web Server, Scheduler, SQLite로 구성되며, DAG가 실행될때, Scheduler는 한 번에 하나의 Task를 실행한다. (<code>동시에 여러 개의 Task 실행 불가능</code>)</p><p>따라서 매우 간단한 실험을 하거나, 디버깅을 하는 경우에만 사용이 되며, executor 설정에 대한 환경변수 부분만 수정해주면 사용할 수 있다.</p><h3 id="Local-Executor"><a href="#Local-Executor" class="headerlink" title="Local Executor"></a><strong>Local Executor</strong></h3><p>Local Executor는 Sequential Executor에서 한 단계 더 나아가서 단일 기계에서 여러 작업을 동시에 할 수 있도록 해준다. 그 외에도 데이터베이스가 Sequential Executor에서는 SQLite인것에 반해 Loal Executor에서는 Postgres 데이터베이스를 사용한다.</p><p>SequentialExecutor와 달리 Local Executor에서는 두 개이상의 Task에 대해 동시 실행이 가능하다.</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">executor=LocalExecutor</span><br><span class="line">sql_alchemy_conn=postgresql+psycopg2://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;/&lt;db&gt;</span><br></pre></td></tr></table></figure><p>단, 단일 머신에 한정해서 Task가 실행되기 때문에 확장성이 좋지 않으며, 리소스가 단일 기계로 제한된다는 단점이 있다.</p><h3 id="Celery-Executor"><a href="#Celery-Executor" class="headerlink" title="Celery Executor"></a><strong>Celery Executor</strong></h3><p>Celery Executor는 동시에 여러 컴퓨터에서 작업을 실행하기 위해 Celery 클러스터를 사용한다. </p><p>우선 간단한 구성을 살펴보면, Web server, Meta database(Postgres), Scheduler 그리고 추가적으로 Worker가 있는데, 이는 실제로 Task를 실행하는 것을 담당하는 기계인 Airflow의 Worker이다. 만약에 3명의 Airflow Worker가 있다면, 작업을 실행하는 3대의 machine이 있다는 것을 의미하고, 더 많은 작업을 실행하기 위해 더 많은 리소스가 필요한 경우에는 추가적으로 Airflow Worker를 추가하기만 하면 된다.</p><p>Celery Queue는 <code>Result Backend</code>와 <code>Broker</code>, 두 가지 구성요소로 구성되어 있는데, Result Backend에는 Airflow worker가 Task의 상태정보를 저장하는데 사용이 된다. 그리고 Broker는 Scheduler가 실행을 하기 위한 task를 보내면, worker가 task 실행을 위해 pull해서 사용하기 위한 queue의 역할을 한다.  </p><p>Scheduler가 Celery Queue의 Broker에 DAG의 Task를 전달하면, Worker 중 하나에서 해당 Task를 pull하게 된다. 그리고 Worker에서 작업이 완료되면, 작업 상태가 Result Backend에 저장이 된다. Result Backend는 Airflow의 Database와 같은 역할을 하며, 원하는 경우, 다른 데이터베이스를 사용할 수 있다. </p><p>Celery Executor를 사용할때에는 Celery Queue를 설치해야 되는데, 이는 Redis나 RabbitMQ가 될 수 있다.</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">executor&#x3D;CeleryExecutor</span><br><span class="line">sql_alchemy_conn&#x3D;postgresql+psycopg2:&#x2F;&#x2F;&lt;user&gt;:&lt;password&gt;@&lt;host&gt;&#x2F;&lt;db&gt;</span><br><span class="line">celery_result_backend&#x3D;postgresql+psycopg2:&#x2F;&#x2F;&lt;user&gt;:&lt;password&gt;@&lt;host&gt;&#x2F;&lt;db&gt;</span><br><span class="line">celery_broker_url&#x3D;redis:&#x2F;&#x2F;:@redis:6379&#x2F;0</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Airflow/">Airflow</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/Airflow/">Airflow</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/08/07/202208/220807_airflow_on_docker/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220805 Python 동시성 &amp; 병렬성 프로그래밍</title>
      <link>https://leehyungi0622.github.io/2022/08/05/202208/220805-python-study/</link>
      <guid>https://leehyungi0622.github.io/2022/08/05/202208/220805-python-study/</guid>
      <pubDate>Fri, 05 Aug 2022 05:28:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220806_python_multi_threading.png&quot; alt=&quot;파이썬의 멀티 스레딩(동시성)&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;h2 id=&quot;venv-명령어&quot;&gt;&lt;a href=&quot;#venv-명령어&quot; class=&quot;headerlink&quot; title=&quot;venv 명령어&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;venv 명령어&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;  &lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$python&lt;/span&gt; -m venv venv &lt;span class=&quot;comment&quot;&gt;# venv이름으로 가상환경 생성&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$source&lt;/span&gt; venv/bin/activate &lt;span class=&quot;comment&quot;&gt;# 가상환경 활성화 시키기&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$deactivate&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# 가상환경 나가기&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h2 id=&quot;pip-명령어&quot;&gt;&lt;a href=&quot;#pip-명령어&quot; class=&quot;headerlink&quot; title=&quot;pip 명령어&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;pip 명령어&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;  pip는 Python의 패키지 매니저로, 외부 패키지나 라이브러리, 프레임워크를 설치하고 관리할 수 있도록 도와준다.&lt;/p&gt;
  &lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$pip&lt;/span&gt; install pip --upgrade &lt;span class=&quot;comment&quot;&gt;# pip upgrade&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$pip&lt;/span&gt; install &lt;span class=&quot;string&quot;&gt;&amp;quot;package~=3.0.0&amp;quot;&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;#3.0.0 version의 패키지를 설치&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$pip&lt;/span&gt; install [package] &lt;span class=&quot;comment&quot;&gt;# package 설치&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$pip&lt;/span&gt; uninstall [package] &lt;span class=&quot;comment&quot;&gt;# package 삭제&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$pip&lt;/span&gt; --version &lt;span class=&quot;comment&quot;&gt;# 설치된 pip version을 확인할 수 있다.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$pip&lt;/span&gt; freeze &lt;span class=&quot;comment&quot;&gt;# 설치된 패키지를 확인할 수 있다.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;설치된-페키지를-text로-보내고-설치하기-협업&quot;&gt;&lt;a href=&quot;#설치된-페키지를-text로-보내고-설치하기-협업&quot; class=&quot;headerlink&quot; title=&quot;설치된 페키지를 text로 보내고 설치하기(협업)&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;설치된 페키지를 text로 보내고 설치하기(협업)&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;  &lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$pip&lt;/span&gt; freeze &amp;gt; requirements.txt &lt;span class=&quot;comment&quot;&gt;# requirements.txt 파일에 설치된 패키지 리스트를 파일로 뽑아내기&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$pip&lt;/span&gt; install -r requirements.txt &lt;span class=&quot;comment&quot;&gt;# requirements.txt파일에 기록된 패키지를 설치&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
  &lt;figure class=&quot;highlight txt&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# python version: 3.8.1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;autopep8==1.6.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;click==8.1.3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;flake8==5.0.4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;importlib-metadata==4.12.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;itsdangerous==2.1.2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Jinja2==3.1.2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MarkupSafe==2.1.1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mccabe==0.7.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pycodestyle==2.9.1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pyflakes==2.5.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;toml==0.10.2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Werkzeug==2.2.1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;zipp==3.8.1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;CPU-바운드-I-O-바운드-Blocking&quot;&gt;&lt;a href=&quot;#CPU-바운드-I-O-바운드-Blocking&quot; class=&quot;headerlink&quot; title=&quot;CPU 바운드, I/O 바운드, Blocking&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;CPU 바운드, I/O 바운드, Blocking&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;h3 id=&quot;바운드&quot;&gt;&lt;a href=&quot;#바운드&quot; class=&quot;headerlink&quot; title=&quot;바운드&quot;&gt;&lt;/a&gt;&lt;strong&gt;바운드&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;바운드란 장애물에 막혀서 실행이 되지 않는 상태를 말한다.&lt;/p&gt;
&lt;h3 id=&quot;CPU-바운드&quot;&gt;&lt;a href=&quot;#CPU-바운드&quot; class=&quot;headerlink&quot; title=&quot;CPU 바운드&quot;&gt;&lt;/a&gt;&lt;strong&gt;CPU 바운드&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;프로그램이 실행될 때 실행속도가 CPU 속도에 의해 제한되는 것을 말하며, 복잡한 수학 수식을 계산하는 경우, CPU의 연산 작업에 의해 프로그램이 실행될때 실행속도가 느려지거나 멈춰있는 되는 현상이 발생하게 되는데, 이를 CPU 바운드라고 한다.&lt;/p&gt;
&lt;h3 id=&quot;I-O-바운드&quot;&gt;&lt;a href=&quot;#I-O-바운드&quot; class=&quot;headerlink&quot; title=&quot;I/O 바운드&quot;&gt;&lt;/a&gt;&lt;strong&gt;I/O 바운드&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;프로그램이 실행될 때 실행속도가 I/O에 의해 제한되는 것을 말하며, 프로그램에서 사용자의 입력을 기다리기 위해 프로그램이 멈춰있는 경우가 발생하는데, 이를 I/0 바운드라고 한다. &lt;/p&gt;
&lt;h3 id=&quot;Network-I-O-바운드&quot;&gt;&lt;a href=&quot;#Network-I-O-바운드&quot; class=&quot;headerlink&quot; title=&quot;Network I/O 바운드&quot;&gt;&lt;/a&gt;&lt;strong&gt;Network I/O 바운드&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;사용자로부터 입력을 기다리기 위해 프로그램이 멈추는 것이 아닌, 외부 서버에 요청을 하여 응답을 기다리는 경우에도 프롤그램이 멈춰있는 현상이 발생하는데, 이를 Network I/O 바운드라고 한다. &lt;/p&gt;
&lt;h3 id=&quot;Blocking&quot;&gt;&lt;a href=&quot;#Blocking&quot; class=&quot;headerlink&quot; title=&quot;Blocking&quot;&gt;&lt;/a&gt;&lt;strong&gt;Blocking&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;바운드에 의해 코드가 멈추게 되는 현상이 일어나는 것을 블로킹이라고 한다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220806_python_multi_threading.png" alt="파이썬의 멀티 스레딩(동시성)"></div><br/><br/><h2 id="venv-명령어"><a href="#venv-명령어" class="headerlink" title="venv 명령어"></a><ins><b>venv 명령어</b></ins></h2>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$python</span> -m venv venv <span class="comment"># venv이름으로 가상환경 생성</span></span><br><span class="line"><span class="variable">$source</span> venv/bin/activate <span class="comment"># 가상환경 활성화 시키기</span></span><br><span class="line"><span class="variable">$deactivate</span> <span class="comment"># 가상환경 나가기</span></span><br></pre></td></tr></table></figure><h2 id="pip-명령어"><a href="#pip-명령어" class="headerlink" title="pip 명령어"></a><ins><b>pip 명령어</b></ins></h2><p>  pip는 Python의 패키지 매니저로, 외부 패키지나 라이브러리, 프레임워크를 설치하고 관리할 수 있도록 도와준다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$pip</span> install pip --upgrade <span class="comment"># pip upgrade</span></span><br><span class="line"><span class="variable">$pip</span> install <span class="string">&quot;package~=3.0.0&quot;</span> <span class="comment">#3.0.0 version의 패키지를 설치</span></span><br><span class="line"><span class="variable">$pip</span> install [package] <span class="comment"># package 설치</span></span><br><span class="line"><span class="variable">$pip</span> uninstall [package] <span class="comment"># package 삭제</span></span><br><span class="line"><span class="variable">$pip</span> --version <span class="comment"># 설치된 pip version을 확인할 수 있다.</span></span><br><span class="line"><span class="variable">$pip</span> freeze <span class="comment"># 설치된 패키지를 확인할 수 있다.</span></span><br></pre></td></tr></table></figure><h2 id="설치된-페키지를-text로-보내고-설치하기-협업"><a href="#설치된-페키지를-text로-보내고-설치하기-협업" class="headerlink" title="설치된 페키지를 text로 보내고 설치하기(협업)"></a><ins><b>설치된 페키지를 text로 보내고 설치하기(협업)</b></ins></h2>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$pip</span> freeze &gt; requirements.txt <span class="comment"># requirements.txt 파일에 설치된 패키지 리스트를 파일로 뽑아내기</span></span><br><span class="line"><span class="variable">$pip</span> install -r requirements.txt <span class="comment"># requirements.txt파일에 기록된 패키지를 설치</span></span><br></pre></td></tr></table></figure>  <figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># python version: 3.8.1</span><br><span class="line"></span><br><span class="line">autopep8==1.6.0</span><br><span class="line">click==8.1.3</span><br><span class="line">flake8==5.0.4</span><br><span class="line">importlib-metadata==4.12.0</span><br><span class="line">itsdangerous==2.1.2</span><br><span class="line">Jinja2==3.1.2</span><br><span class="line">MarkupSafe==2.1.1</span><br><span class="line">mccabe==0.7.0</span><br><span class="line">pycodestyle==2.9.1</span><br><span class="line">pyflakes==2.5.0</span><br><span class="line">toml==0.10.2</span><br><span class="line">Werkzeug==2.2.1</span><br><span class="line">zipp==3.8.1</span><br></pre></td></tr></table></figure><h2 id="CPU-바운드-I-O-바운드-Blocking"><a href="#CPU-바운드-I-O-바운드-Blocking" class="headerlink" title="CPU 바운드, I/O 바운드, Blocking"></a><ins><b>CPU 바운드, I/O 바운드, Blocking</b></ins></h2><h3 id="바운드"><a href="#바운드" class="headerlink" title="바운드"></a><strong>바운드</strong></h3><p>바운드란 장애물에 막혀서 실행이 되지 않는 상태를 말한다.</p><h3 id="CPU-바운드"><a href="#CPU-바운드" class="headerlink" title="CPU 바운드"></a><strong>CPU 바운드</strong></h3><p>프로그램이 실행될 때 실행속도가 CPU 속도에 의해 제한되는 것을 말하며, 복잡한 수학 수식을 계산하는 경우, CPU의 연산 작업에 의해 프로그램이 실행될때 실행속도가 느려지거나 멈춰있는 되는 현상이 발생하게 되는데, 이를 CPU 바운드라고 한다.</p><h3 id="I-O-바운드"><a href="#I-O-바운드" class="headerlink" title="I/O 바운드"></a><strong>I/O 바운드</strong></h3><p>프로그램이 실행될 때 실행속도가 I/O에 의해 제한되는 것을 말하며, 프로그램에서 사용자의 입력을 기다리기 위해 프로그램이 멈춰있는 경우가 발생하는데, 이를 I/0 바운드라고 한다. </p><h3 id="Network-I-O-바운드"><a href="#Network-I-O-바운드" class="headerlink" title="Network I/O 바운드"></a><strong>Network I/O 바운드</strong></h3><p>사용자로부터 입력을 기다리기 위해 프로그램이 멈추는 것이 아닌, 외부 서버에 요청을 하여 응답을 기다리는 경우에도 프롤그램이 멈춰있는 현상이 발생하는데, 이를 Network I/O 바운드라고 한다. </p><h3 id="Blocking"><a href="#Blocking" class="headerlink" title="Blocking"></a><strong>Blocking</strong></h3><p>바운드에 의해 코드가 멈추게 되는 현상이 일어나는 것을 블로킹이라고 한다.</p><a id="more"></a><h2 id="동기-및-비동기"><a href="#동기-및-비동기" class="headerlink" title="동기 및 비동기"></a><ins><b>동기 및 비동기</b></ins></h2><h3 id="동기-Sync"><a href="#동기-Sync" class="headerlink" title="동기(Sync)"></a><strong>동기(Sync)</strong></h3><p>코드가 동기적으로 동작한다는 의미는, 코드가 작성된 순서대로 실행된다는 것을 의미한다. </p><h3 id="비동기-Async"><a href="#비동기-Async" class="headerlink" title="비동기(Async)"></a><strong>비동기(Async)</strong></h3><p>코드가 비동기적으로 동작한다는 의미는, 코드가 반드시 작성된 순서 그대로 실행되지 않는 것을 의미한다.</p><h2 id="파이썬-코루틴과-비동기-함수"><a href="#파이썬-코루틴과-비동기-함수" class="headerlink" title="파이썬 코루틴과 비동기 함수"></a><ins><b>파이썬 코루틴과 비동기 함수</b></ins></h2><h3 id="루틴"><a href="#루틴" class="headerlink" title="루틴"></a><strong>루틴</strong></h3><p>루틴이란 일련의 명령으로, 코드의 흐름을 말한다.</p><h4 id="메인-루틴"><a href="#메인-루틴" class="headerlink" title="메인 루틴"></a><strong>메인 루틴</strong></h4><p>메인 루틴이란 프로그램의 메인 코드의 흐름을 말한다.</p><h4 id="서브-루틴"><a href="#서브-루틴" class="headerlink" title="서브 루틴"></a><strong>서브 루틴</strong></h4><p>서브 루틴은 하나의 진입점과 하나의 탈출점이 있는 루틴을 말한다.</p><p>메인 루틴을 보조하는 역할로, 함수나 메소드가 대표적이며, 별도의 스코프에 모여있다가 호출이 되었을 경우에 해당 스코프로 이동을 한 후에 return을 통해 원 호출 시점인 메인 루틴으로 돌아오게 된다.</p><h4 id="코루틴"><a href="#코루틴" class="headerlink" title="코루틴"></a><strong>코루틴</strong></h4><p>코루틴은 다양한 진입점과 다양한 탈출점이 있는 루틴을 말한다. 코루틴은 서브루틴과는 다르게 해당 로직들이 진행되는 중간에 멈춰서 특정위치로 돌아 갔다가 다시 코루틴 함수에서 진행되었던 원 위치로 돌아와서 나머지 로직을 수행한다. 아래의 비동기 처리코드를 보면, meeting 함수에서 진입점이 두 개(함수 인자, await 구문), 탈출점이 두 개(await 구문, return 구문)이 있음을 확인할 수 있다. </p><p>동기 코드에서는 코드가 순차적으로 실행되어야 되기 때문에 각 각의 meeting 함수의 처리가 모두 완료된 후에 순차적으로 함수가 실행되지만, 비동기 코드에서는 meeting 함수의 await 구문 실행에서 기다리지 않고, 바로 다음 함수 실행을 함으로써 실행시간이 단축된다. 1초 후 A 실행, A 실행 후 1초 후에 B 실행, B 실행 후 1 초후에 C 실행</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">meeting</span>(<span class="params">name, time</span>):</span></span><br><span class="line">  print(<span class="string">f&quot;Hi! <span class="subst">&#123;name&#125;</span>&quot;</span>)</span><br><span class="line">  <span class="keyword">await</span> asyncio.sleep(time)</span><br><span class="line">  print(<span class="string">f&quot;See you next time, <span class="subst">&#123;name&#125;</span>, (<span class="subst">&#123;time&#125;</span>초 만남)&quot;</span>)</span><br><span class="line">  print(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 만남 완료&quot;</span>)</span><br><span class="line">  <span class="keyword">return</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">  result = <span class="keyword">await</span> asyncio.gather(</span><br><span class="line">    meeting(<span class="string">&quot;A&quot;</span>, <span class="number">1</span>),</span><br><span class="line">    meeting(<span class="string">&quot;B&quot;</span>, <span class="number">2</span>),</span><br><span class="line">    meeting(<span class="string">&quot;C&quot;</span>, <span class="number">3</span>),</span><br><span class="line">  )</span><br><span class="line">  print(result) <span class="comment">#[1, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  start = time.time()</span><br><span class="line">  asyncio.run(main())</span><br><span class="line">  end = time.time()</span><br><span class="line">  print(end - start)</span><br></pre></td></tr></table></figure><h3 id="활용"><a href="#활용" class="headerlink" title="활용"></a><strong>활용</strong></h3><p>코루틴의 사용에 있어, asyncio 라이브러리를 사용할 수 있는데, async 키워드로 작성된 코루틴 함수를 아래와 같이 main 함수에서 asyncio.run(x) 메소드를 통해 실행할 수 있다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">hello_world</span>():</span></span><br><span class="line">  print(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="number">123</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># await 키워드의 경우, async 함수 내에서만 사용할 수 있기 때문에 main함수에서는 asyncio.run() 메소드를 통해 코루틴 함수를 실행한다.</span></span><br><span class="line">  <span class="comment"># await hello_world()  </span></span><br><span class="line">  asyncio.run(hello_world())</span><br></pre></td></tr></table></figure><h4 id="Fetcher-작성"><a href="#Fetcher-작성" class="headerlink" title="Fetcher 작성"></a><strong>Fetcher 작성</strong></h4><p>requests 라이브러리만을 사용하게 되면, URL에 요청을 보내고 요청에 대한 결과를 받은 후에 연결이 끊기기 때문에 지속적으로 연결상태를 유지하면서 response로부터 원하는 데이터를 얻기 위해 Session을 사용한다.</p><p>Session을 연결한 후에는 반드시 session 연결을 끊어주는 처리를 해줘야 하는데, <code>with 구문내</code>에서 처리를 함으로써 session 연결을 끊어주는 별도의 처리를 하지 않아도 된다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 일반 fetcher 코드 </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetcher</span>(<span class="params">session, url</span>):</span></span><br><span class="line">  <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">    <span class="keyword">return</span> response.text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">  urls = [<span class="string">&quot;https://naver.com&quot;</span>, <span class="string">&quot;https://google.com&quot;</span>, <span class="string">&quot;https://instagram.com&quot;</span>]*<span class="number">10</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> requests.Session() <span class="keyword">as</span> session:</span><br><span class="line">    result = [fetcher(session, url) <span class="keyword">for</span> url <span class="keyword">in</span> urls]</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  start = time.time()</span><br><span class="line">  main()</span><br><span class="line">  end = time.time()</span><br><span class="line">  print(end - start) <span class="comment"># 12</span></span><br></pre></td></tr></table></figure><p>코루틴으로 작성한 fetcher에서는 requests.Session()으로 작성된 session 호출을 <code>aiohttp.ClientSession()</code>으로 대체한다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코루틴 fetcher 코드 </span></span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">fetcher</span>(<span class="params">session, url</span>):</span></span><br><span class="line">  <span class="keyword">async</span> <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">await</span> response.text()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">  urls = [<span class="string">&quot;https://naver.com&quot;</span>, <span class="string">&quot;https://google.com&quot;</span>, <span class="string">&quot;https://instagram.com&quot;</span>]*<span class="number">10</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">    result = <span class="keyword">await</span> asyncio.gather(*[fetcher(session, url) <span class="keyword">for</span> url <span class="keyword">in</span> urls])</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  start = time.time()</span><br><span class="line">  asyncio.run(main())</span><br><span class="line">  end = time.time()</span><br><span class="line">  print(end - start) <span class="comment"># 4.8</span></span><br></pre></td></tr></table></figure><h2 id="컴퓨터-구조와-OS"><a href="#컴퓨터-구조와-OS" class="headerlink" title="컴퓨터 구조와 OS"></a><ins><b>컴퓨터 구조와 OS</b></ins></h2><h3 id="컴퓨터의-구성-요소"><a href="#컴퓨터의-구성-요소" class="headerlink" title="컴퓨터의 구성 요소"></a><strong>컴퓨터의 구성 요소</strong></h3><p>컴퓨터의 구성요소로는 명령어를 해석하여 실행하는 장치로 CPU가 있으며, 작업에 필요한 프로그램과 데이터를 저장하는 장소로써의 <code>주메모리</code>와 데이터를 일시적으로 혹은 영구적으로 저장하는 <code>보조 메모리</code>가 있다.<br>이 외에 키보드와 마우스와 같은 <code>입출력장치</code>가 있으며, CPU, 메모리, 입출력장치 사이를 연결하고 데이터를 주고 받는 역할을 해주는 <code>시스템 버스</code>가 있다.</p><h4 id="프로세싱"><a href="#프로세싱" class="headerlink" title="프로세싱"></a><strong>프로세싱</strong></h4><p>프로그램이 저장(HDD, SSD 저장장치(보조 메모리))되고, 사용자는 프로그램을 실행시키기 위해서 아이콘을 클릭해서 실행을 하게 되는데, 이 <code>프로그램이 실행된다는 의미는 해당 프로그램의 작성 코드들이 주메모리로 올라와서 작업이 진행되는 것을 의미</code>한다.<br>프로세스가 생성되면, CPU는 프로세스가 해야할 작업을 수행한다.</p><p>다시 정리하면, 프로세스란 실행을 위해 주메모리에 올라온 동적인 상태를 의미하며, 프로그램이란 저장장치에 저장된 정적인 상태를 의미한다.</p><h4 id="스레드"><a href="#스레드" class="headerlink" title="스레드"></a><strong>스레드</strong></h4><p>CPU가 처리하는 작업의 단위가 스레드인데, 스레드란 프로세스 내에서 실행되는 여러 작업의 단위를 말한다.</p><p>스레드가 한 개로 동작하면 싱글 스레드, 여러 개의 스레드가 동작하면 멀티 스레딩이라고 하며, 복수 개의 스레드를 사용하는 멀티 스레딩에서 스레드는 다수의 스레드끼리 메모리 공유와 통신이 가능하다. 이는 자원의 낭비를 막고 효율성을 향상시키기 위함이며,<br>한 스레드에 문제가 생기면 전체 프로세스에 영향을 미친다. </p><p>스레드의 종류로는 사용자 수준 스레드와 커널 수준의 스레드로 나뉘는데, 파이썬에서는 사용자 수준 스레드 선에서 스레드를 다룬다.</p><h2 id="파이썬-멀티-스레딩과-멀티-프로세싱"><a href="#파이썬-멀티-스레딩과-멀티-프로세싱" class="headerlink" title="파이썬 멀티 스레딩과 멀티 프로세싱"></a><ins><b>파이썬 멀티 스레딩과 멀티 프로세싱</b></ins></h2><h3 id="동시성-vs-병렬성"><a href="#동시성-vs-병렬성" class="headerlink" title="동시성 vs 병렬성"></a><strong>동시성 vs 병렬성</strong></h3><p>소프트웨어 공학에서 동시성(병행성)이란 Concurrency에 대한 번역이다. 그리고 병렬성이란 Parallelism의 의미를 가진다.<br>앞서 살펴본 코루틴으로 작성한 코드는 동시성(병행성)을 구현한 것이며, 그 차이에 대해서 살펴보자.</p><h4 id="동시성-Concurrency"><a href="#동시성-Concurrency" class="headerlink" title="동시성(Concurrency)?"></a><strong>동시성(Concurrency)?</strong></h4><p>동시성이란 한 번에 여러 작업을 동시에 다루는 것(switching을 하면서 작업을 다루는 것)을 의미한다. </p><p>동시성은 논리적 개념인데, 멀티 스레딩에서 사용이 되기도 하고, 싱글 스레드에서 사용이 되기도 한다. 또한 싱글 코어 뿐 아니라 멀티 코어에서도 각각의 코어가 동시성을 사용할 수 있다.</p><p>싱글 스레드에서 사용한 동시성이 바로 <code>asyncio를 사용한 프로그래밍</code>이다.그리고 코루틴 함수를 사용하지 않고, 스레드 자체가 함수들을 맡게 된다면, 멀티 스레딩에서 동시성을 지킬 수 있게 된다.</p><h4 id="병렬성-Parallelism"><a href="#병렬성-Parallelism" class="headerlink" title="병렬성(Parallelism)?"></a><strong>병렬성(Parallelism)?</strong></h4><p>한 번에 여러 작업을 병렬적으로 처리하는 것을 의미한다. (<code>at the same time</code>)그리고 이는 멀티 프로세싱과 멀티 스레딩을 가능하게 한다.</p><p>각 각의 작업이 분리된 CPU 코어에서 각 각 작업을 하게 되면, 멀티 코어 환경에서 병렬성 프로그래밍이 가능한 것이고, CPU 단위가 아닌, 하나의 프로세스 단위에서 생각해보면, 스레드가 여러 개 있어야지만 병렬성 프로그래밍이 가능한 것이다.<br>따라서 병렬성은 물리적 개념으로, 복수 개의 작업이 병렬로 수행되는 것을 의미한다.</p><p>또한 병렬성과 동시성은 동시에 공존할 수 있는데, 예를들어 100개의 요청이 있고 CPU 코어가 3개 있다고 가정하면, CPU 3개에서 병렬적으로 처리하는 병렬성을 유지하면서 다른 요청에 대한 처리를 switching하면서 처리하는, 동시성 또한 가질 수 있는 것이다.</p><p><code>하지만, 파이썬에서는 스레드로 병렬성을 구현할 수 없다. GIL Global Interpreter Lock이라는 개념이 있는데, 이로인해 구현될 수 없는 것이다. 따라서 파이썬에서는 멀티 스레드가 동시성으로 수행되어야 한다.</code> </p><h3 id="파이썬-멀티-스레딩"><a href="#파이썬-멀티-스레딩" class="headerlink" title="파이썬 멀티 스레딩"></a><strong>파이썬 멀티 스레딩</strong></h3><p>우선 Single thread에서의 작업처리에 대해 살펴보면, 아래의 urls의 각 각의 작업에 대해서 같은 thread에서 처리되고 있음을 확인할 수 있다.</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetcher</span>(<span class="params">session, url</span>):</span></span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;os.getpid()&#125;</span> process | <span class="subst">&#123;threading.get_ident()&#125;</span> url : <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">        <span class="keyword">return</span> response.text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    urls = [<span class="string">&quot;https://google.com&quot;</span>, <span class="string">&quot;https://apple.com&quot;</span>] * <span class="number">50</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> requests.Session() <span class="keyword">as</span> session:</span><br><span class="line">        result = [fetcher(session, url) <span class="keyword">for</span> url <span class="keyword">in</span> urls]</span><br><span class="line">        <span class="comment"># print(result) </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    main()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(end - start)  <span class="comment"># 19s</span></span><br></pre></td></tr></table></figure><p>  코루틴으로 작성한 fetcher 코드를 실행시켜보면, 같은 스레드에서 처리되고 있지만, 5초 정도 처리되는 시간이 단축됨을 확인할 수 있습니다.</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">fetcher</span>(<span class="params">session, url</span>):</span></span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;os.getpgid&#125;</span> process | <span class="subst">&#123;threading.get_ident()&#125;</span> url : <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">await</span> response.text()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    urls = [<span class="string">&quot;https://google.com&quot;</span>, <span class="string">&quot;https://apple.com&quot;</span>] * <span class="number">50</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        result = <span class="keyword">await</span> asyncio.gather(*[fetcher(session, url) <span class="keyword">for</span> url <span class="keyword">in</span> urls])</span><br><span class="line">        print(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    asyncio.run(main())</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(end - start)  <span class="comment"># 14.65s</span></span><br></pre></td></tr></table></figure><p>  만약에 aiohttp에서 제공하는 코루틴 함수가 없고, 이 상황에서 동기적 코드를 사용해서 동시성 프로그래밍을 하려면 어떻게 해야될까?<br>  바로 이 상황에서는 멀티 스레딩을 사용하면 된다.</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetcher</span>(<span class="params">params</span>):</span></span><br><span class="line">    session = params[<span class="number">0</span>]</span><br><span class="line">    url = params[<span class="number">1</span>]</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;os.getpid()&#125;</span> process | <span class="subst">&#123;threading.get_ident()&#125;</span> url : <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">        <span class="keyword">return</span> response.text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    urls = [<span class="string">&quot;https://google.com&quot;</span>, <span class="string">&quot;https://apple.com&quot;</span>] * <span class="number">50</span></span><br><span class="line">    <span class="comment"># max_workers: 실행할 스레드의 수</span></span><br><span class="line">    executor = ThreadPoolExecutor(max_workers=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">with</span> requests.Session() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="comment"># result = [fetcher(session, url) for url in urls]</span></span><br><span class="line">        <span class="comment"># print(result)</span></span><br><span class="line">        params = [(session, url) <span class="keyword">for</span> url <span class="keyword">in</span> urls]</span><br><span class="line">        results = <span class="built_in">list</span>(executor.<span class="built_in">map</span>(fetcher, params))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    main()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(end - start)  <span class="comment"># 3.4s</span></span><br></pre></td></tr></table></figure><p>  스레드를 사용하는 것보다는 코루틴을 사용하는 것을 권장한다. 그 이유는 스레드를 늘려서 작업을 처리하는 것에는 많은 연산과정이 추가되기 때문에 메모리 점유율이 많이 들어가기 때문이다.</p><h3 id="파이썬-멀티-프로세싱-GIL"><a href="#파이썬-멀티-프로세싱-GIL" class="headerlink" title="파이썬 멀티 프로세싱, GIL"></a><strong>파이썬 멀티 프로세싱, GIL</strong></h3><p>다른 프로그래밍 언어에서는 멀티 스레딩을 사용해서 병렬성 프로그래밍이 가능하고, 멀티 스레딩의 장점이자 단점은 메모리를 공유한다는 것이다. 그 이유는 멀티 스레딩에서는 스레드를 하나의 프로세스에서 여러 개로 만들어서 진행을 하게 되는데, 메모리를 공유하기 때문에 하나의 스레드에서 에러가 발생하면 다른 스레드에서도 에러가 발생하기 때문이다.<br>반면에 멀티 프로세싱에서는 각 각의 프로세스를 자식 프로세스로써 복제해서 진행을 하게 된다. </p><p>위의 이유로 인해 파이썬을 만든 개발자가 파이썬을 만들었을 때 <code>GIL(Global Interpreter Lock)</code>을 도입하게 되었는데, 이는 한 번에 1개의 스레드만 유지하는 락을 의미한다. GIL은 본질적으로 한 스레드가 다른 스레드를 차단해서 제어를 얻는 것을 막아준다.<br>이는 앞서 언급한 멀티 스레딩의 위험으로부터 보호하기 위함이다. 이러한 이유로 파이썬에서는 스레드로 병렬성 연산을 수행하지 못한다.<br>하지만, 파이썬의 멀티 스레딩은 동시성(Concurrency)를 사용해서 Network I/O bound 코드에서 유용하게 사용할 수 있지만, CPU bound에서는 GIL에 의해서 원하는 결과를 얻을 수 없다.</p><p>이 경우에 사용되는 것이 <code>멀티 프로세싱</code>인데, 프로세스를 여러 개 복제하고, 각 각의 프로세스들이 메모리 공유를 하지 않기 때문에 서로 소통을 하기 위해서 직렬화와 역직렬화 작업이 필요한데, 이런 비용이 멀티 스레딩을 사용했을때보다 크다.<br>만약에 이러한 단점을 감수해서라도 속도를 높이고 싶다면, 멀티 프로세싱을 사용한다. </p><p>CPU 연산 작업에 있어서 파이썬으로 멀티 스레딩으로 처리를 한다면, 이는 일반적으로 처리한 코드와 걸리는 시간은 별 차이가 없다. (<code>동시성 프로그래밍이 불필요한 케이스</code>)<br>이 경우에는 함수 하나 하나를 별도의 프로세싱으로 분리해서 병렬로 처리하는 것이 시간단축에 도움이 된다.</p><p>이는 <code>기존의 ThreadPoolExecutor를 ProcessPoolExecutor로 수정해서 실행</code>하면 되는데, 많은 시간을 단축할 수 있다.</p>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Python/">Python</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Python/">Python</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/08/05/202208/220805-python-study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220805 Apache Airflow</title>
      <link>https://leehyungi0622.github.io/2022/08/05/202208/220805_airflow_on_docker/</link>
      <guid>https://leehyungi0622.github.io/2022/08/05/202208/220805_airflow_on_docker/</guid>
      <pubDate>Fri, 05 Aug 2022 01:48:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220804_apache_airflow.png&quot; alt=&quot;Apache Airflow&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;h2 id=&quot;Apache-Airflow-UI-구성&quot;&gt;&lt;a href=&quot;#Apache-Airflow-UI-구성&quot; class=&quot;headerlink&quot; title=&quot;Apache Airflow UI 구성&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Apache Airflow UI 구성&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;Apache Airflow UI에서 DAGs 리스트를 보면, 현재 Apache Airflow에 포함되어있는 DAG의 목록이 출력된다. 리스트 중 하나의 아이템을 살펴보면, DAG의 이름의 좌측에 해당 DAG를 Pause/Unpause 할 수 있는 토글 버튼이 있고, 이름의 아래에는 data pipeline이나 팀 또는 기능별로 묶어서 관리할 수 있도록 태그가 표시되어있다. &lt;code&gt;(단, tag에 따라 permission을 부여할 수 없다)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;그 외에 Owner에 대한 정보와 Runs 칼럼에서는 &lt;code&gt;queued, success, running, failed&lt;/code&gt; 상태별로 현재 DAG이 실행 상태를 확인할 수 있다. Last Run 및 Next Run 항목을 통해서는 DAG가 언제 마지막으로 실행이 되었고, 그 다음 실행은 언제 되는지에 대해 확인할 수 있다.&lt;/p&gt;
&lt;p&gt;Recent Tasks에서는 총 15개의 상태 정보로 나뉘어 활성화된 DAG의 Task들의 실행 상태에 대해서 확인을 할 수 있다. (&lt;code&gt;none, removed, scheduled, queued, running, success, shutdown, ...&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Actions에서는 강제로 Trigger 시키거나 DAG를 삭제할 수 있는데, DAG를 삭제한다는 의미가 DAG 자체를 삭제하는 것이 아닌, Meta store에서 DAG RUN Instance를 삭제한다는 것을 의미한다.&lt;/p&gt;
&lt;h2 id=&quot;Apache-Airflow-DAG-item-상세보기&quot;&gt;&lt;a href=&quot;#Apache-Airflow-DAG-item-상세보기&quot; class=&quot;headerlink&quot; title=&quot;Apache Airflow DAG item 상세보기&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Apache Airflow DAG item 상세보기&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;DAG의 이름을 클릭하면, Grid를 통해 실행한 DAG들의 상태 정보에 대해서 모니터링 할 수 있으며, Graph View를 통해서는 DAG의 각 Tasks가 어떤 Tasks를 dependencies로 가지고 있는지에 대해서 구조적으로 확인을 할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Landing Times&lt;/code&gt;에서는 DAG에서의 모든 작업들이 scheduled 상태에서 completion으로 완료되는데 얼마나 걸렸는지에 대한 정보를 시각화된 그래프로 확인할 수 있다. 만약 시간이 오래걸린다면, 걸리는 시간을 줄일 수 있도록 별도의 대응이 필요하다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Calendar view&lt;/code&gt;에서는 각 각의 네모칸이 특정 DAG의 상태 다이어그램 정보를 종합해서 보여준다. 특정 DAG가 특정 날짜에 문제가 생긴다면, 빨간색으로 칸이 표시되며, 성공적으로 DAG가 시행이 되었다면 해당 일자에 초록색으로 포시된다. 점으로 표시된 칸은 얼마나 많은 다이어그램이 해당 날에 계획되어있는지에 대한 정보를 제공한다. 따라서 이러한 모니터링 정보를 기반으로 어떤 날에 데이터 파이프라인을 수정해서 문제를 해결해야되는지 알 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Gantt view&lt;/code&gt;에서는 DAG의 특정 Task가 완료되는데에 얼마나 시간이 걸렸는지와 pipeline에서 bottleneck이 발생했는지에 대한 전반적인 overview를 제공한다. 상대적으로 긴 직사각형은 그만큼 Task를 실행하는데 시간이 걸렸다는 의미이고, 오래걸린 작업에 대해서는 어떻게 하면 작업이 완료되는데 걸리는 시간을 단축시킬 수 있는지에 대한 고민이 필요하다. &lt;/p&gt;
&lt;p&gt;직사각형이 overlapping되었다는 것은 복수 개의 Task를 동시에 실행할 수 있다는 것을 의미한다. (&lt;code&gt;DAG parallelism&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Code view&lt;/code&gt;에서는 데이터 파이프라인의 코드에 접근할 수 있는데, 이를 통해서 적용한 수정사항이 DAG에 제대로 적용이 되었는지 확인을 할 수 있다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220804_apache_airflow.png" alt="Apache Airflow"></div><br/><br/><h2 id="Apache-Airflow-UI-구성"><a href="#Apache-Airflow-UI-구성" class="headerlink" title="Apache Airflow UI 구성"></a><ins><b>Apache Airflow UI 구성</b></ins></h2><p>Apache Airflow UI에서 DAGs 리스트를 보면, 현재 Apache Airflow에 포함되어있는 DAG의 목록이 출력된다. 리스트 중 하나의 아이템을 살펴보면, DAG의 이름의 좌측에 해당 DAG를 Pause/Unpause 할 수 있는 토글 버튼이 있고, 이름의 아래에는 data pipeline이나 팀 또는 기능별로 묶어서 관리할 수 있도록 태그가 표시되어있다. <code>(단, tag에 따라 permission을 부여할 수 없다)</code></p><p>그 외에 Owner에 대한 정보와 Runs 칼럼에서는 <code>queued, success, running, failed</code> 상태별로 현재 DAG이 실행 상태를 확인할 수 있다. Last Run 및 Next Run 항목을 통해서는 DAG가 언제 마지막으로 실행이 되었고, 그 다음 실행은 언제 되는지에 대해 확인할 수 있다.</p><p>Recent Tasks에서는 총 15개의 상태 정보로 나뉘어 활성화된 DAG의 Task들의 실행 상태에 대해서 확인을 할 수 있다. (<code>none, removed, scheduled, queued, running, success, shutdown, ...</code>)</p><p>Actions에서는 강제로 Trigger 시키거나 DAG를 삭제할 수 있는데, DAG를 삭제한다는 의미가 DAG 자체를 삭제하는 것이 아닌, Meta store에서 DAG RUN Instance를 삭제한다는 것을 의미한다.</p><h2 id="Apache-Airflow-DAG-item-상세보기"><a href="#Apache-Airflow-DAG-item-상세보기" class="headerlink" title="Apache Airflow DAG item 상세보기"></a><ins><b>Apache Airflow DAG item 상세보기</b></ins></h2><p>DAG의 이름을 클릭하면, Grid를 통해 실행한 DAG들의 상태 정보에 대해서 모니터링 할 수 있으며, Graph View를 통해서는 DAG의 각 Tasks가 어떤 Tasks를 dependencies로 가지고 있는지에 대해서 구조적으로 확인을 할 수 있다.</p><p><code>Landing Times</code>에서는 DAG에서의 모든 작업들이 scheduled 상태에서 completion으로 완료되는데 얼마나 걸렸는지에 대한 정보를 시각화된 그래프로 확인할 수 있다. 만약 시간이 오래걸린다면, 걸리는 시간을 줄일 수 있도록 별도의 대응이 필요하다.</p><p><code>Calendar view</code>에서는 각 각의 네모칸이 특정 DAG의 상태 다이어그램 정보를 종합해서 보여준다. 특정 DAG가 특정 날짜에 문제가 생긴다면, 빨간색으로 칸이 표시되며, 성공적으로 DAG가 시행이 되었다면 해당 일자에 초록색으로 포시된다. 점으로 표시된 칸은 얼마나 많은 다이어그램이 해당 날에 계획되어있는지에 대한 정보를 제공한다. 따라서 이러한 모니터링 정보를 기반으로 어떤 날에 데이터 파이프라인을 수정해서 문제를 해결해야되는지 알 수 있다.</p><p><code>Gantt view</code>에서는 DAG의 특정 Task가 완료되는데에 얼마나 시간이 걸렸는지와 pipeline에서 bottleneck이 발생했는지에 대한 전반적인 overview를 제공한다. 상대적으로 긴 직사각형은 그만큼 Task를 실행하는데 시간이 걸렸다는 의미이고, 오래걸린 작업에 대해서는 어떻게 하면 작업이 완료되는데 걸리는 시간을 단축시킬 수 있는지에 대한 고민이 필요하다. </p><p>직사각형이 overlapping되었다는 것은 복수 개의 Task를 동시에 실행할 수 있다는 것을 의미한다. (<code>DAG parallelism</code>)</p><p><code>Code view</code>에서는 데이터 파이프라인의 코드에 접근할 수 있는데, 이를 통해서 적용한 수정사항이 DAG에 제대로 적용이 되었는지 확인을 할 수 있다.</p><a id="more"></a><h2 id="첫-번째-데이터-파이프라인-작성-in-Airflow"><a href="#첫-번째-데이터-파이프라인-작성-in-Airflow" class="headerlink" title="첫 번째 데이터 파이프라인 작성 in Airflow"></a><ins><b>첫 번째 데이터 파이프라인 작성 in Airflow</b></ins></h2><p><code>(1)</code> Create table with Postgres separator </p><p>이 단계에서는 어떻게 SQL request를 Airflow에서 실행하는지 알 수 있다.</p><p><code>(2)</code> API 사용가능 유무 확인</p><p>이 단계에서는 API를 사용할 수 있는지에 대해 확인할 수 있으며, 특별한 종류의 operator를 사용하여, 다음 Task로 넘어가기 전에 특정 이벤트를 기다릴 수 있도록 설정할 수 있다. </p><p><code>(3)</code> User 정보 추출 -&gt; process_user -&gt; store_user</p><p>추출된 User 정보를 Airflow에서 가장 보편적인 operator를 사용해서 처리할 수 있다. (<code>User 정보를 Password 데이터 베이스에 저장</code>)</p><h2 id="DAG"><a href="#DAG" class="headerlink" title="DAG?"></a><ins><b>DAG?</b></ins></h2><p>DAG는 방향성 비순환 그래프로, Node는 Task를 나타내고, Edge는 각 Task간의 Dependency 나타낸다.</p><h3 id="DAG-Skeleton"><a href="#DAG-Skeleton" class="headerlink" title="DAG Skeleton"></a><strong>DAG Skeleton</strong></h3><p>  <code>(1)</code>  Import the DAG object</p><p>  <code>(2)</code>  Instantiate a the DAG object</p><p>  <code>(3)</code>  Define a unique dag id</p><p>  <code>(4)</code>  Define a start date</p><p>  <code>(5)</code>  Define a scheduled interval</p><p>  <code>(6)</code>  Define the catchup parameter</p><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이 파일이 DAG정보를 담고 있는 파일인지 알 수 있다.</span></span><br><span class="line"><span class="keyword">from</span> airflow <span class="keyword">import</span> DAG</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 첫 번째 parameter는 DAG ID로, unique identifier로 DAG의 이름이 된다. 이 ID는 Airflow instance의 모든 태그들에 대해 유일해야 한다.</span></span><br><span class="line"><span class="comment"># 두 번째 parameter는 start_date로, DAG가 언제 schedule이 시작되느지에 대한 날짜 정보를 넣는다.</span></span><br><span class="line"><span class="comment"># 세 번째 parameter는 schedule interval로, DAG가 trigger되는 빈도 수에 대해서 정의한다. (매일, 매 15분, 주에 1번 등...) @daily는 매일 자정을 의미한다.</span></span><br><span class="line"><span class="comment"># 네 번째 parameter는 catchup으로, false로 setting 하는 것이 좋다. catchup은 default로 true로 설정이 되어있는데, 이 의미는 현재와 start date 사이에</span></span><br><span class="line"><span class="comment"># DAG가 triggered되어있지 않으면, Airflow UI로부터 데이터 파이프라인의 scheduling을 시작하면, 해당 기간내(현재~마지막으로 DAG가 실행된 날짜 or 시작 날짜)의</span></span><br><span class="line"><span class="comment"># 모든 non-triggered DAG를 실행한다. 따라서 false로 설정함으로써 직접 세세하게 실행할 DAG를 컨트롤 할 수 있으며, 한 번에 많은 양의 DAG를 동시에 실행시키지 않아도 된다.</span></span><br><span class="line"><span class="keyword">with</span> DAG(<span class="string">&#x27;user_processing&#x27;</span>, start_date=datetime(<span class="number">2022</span>, <span class="number">1</span>, <span class="number">1</span>), schedule_interval=<span class="string">&#x27;@daily&#x27;</span>, catchup=<span class="literal">False</span>) <span class="keyword">as</span> dag:</span><br><span class="line">    <span class="literal">None</span></span><br></pre></td></tr></table></figure></code></pre><h2 id="Operator"><a href="#Operator" class="headerlink" title="Operator?"></a><ins><b>Operator?</b></ins></h2><p>  Operator는 데이터 파이프라인에서 하나의 Task를 정의한다. 따라서 하나의 Operator에는 두 개 이상의 Task가 위치하면 안된다.<br>  그 이유는 예를들어 Python Operator가 존재한다고 가정하고, Operator 내에 Cleaning Data와 Processing Data가 같이 존재한다면, Processing Data가 실패했을때 같은 Operator 내에 존재하는 두 개의 Task에 대해 모두 재시작을 해야한다. (<code>Processing Data에 대한 작업만 재시작해야 한다</code>)</p><p>  따라서 하나의 Operator에는 하나의 Task가 배치되도록 구성해야 한다. <code>PythonOperator(Cleaning Data) -&gt; PythonOperator(Processing Data)</code></p><h2 id="Providers"><a href="#Providers" class="headerlink" title="Providers?"></a><ins><b>Providers?</b></ins></h2><p>  Apache Airflow는 module화 되어 만들어졌다. </p><p>  처음 Apache Airflow를 시작했을때, 아래의 명령을 통해서 Airflow Core를 설치했다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Airflow Core</span></span><br><span class="line">$ pip install apache-airflow</span><br></pre></td></tr></table></figure><p>  만약에 AWS, Snowflake, Dbt, Databricks와 같이 사용되어야 한다면, 추가적으로 provider를 설치해야 한다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AWS</span></span><br><span class="line"><span class="variable">$pip</span> install apache-airflow-providers-amazon</span><br><span class="line"><span class="comment"># Snowflake</span></span><br><span class="line"><span class="variable">$pip</span> install apache-airflow-providers-snowflake</span><br><span class="line"><span class="comment"># Dbt</span></span><br><span class="line"><span class="variable">$pip</span> install apache-airflow-providers-dbt-cloud</span><br><span class="line"><span class="comment"># Databricks</span></span><br><span class="line"><span class="variable">$pip</span> install apache-airflow-providers-databricks</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=""></a><ins><b></b></ins></h2>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Airflow/">Airflow</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/Airflow/">Airflow</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/08/05/202208/220805_airflow_on_docker/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220804 Apache Airflow</title>
      <link>https://leehyungi0622.github.io/2022/08/04/202208/220804_airflow_on_docker/</link>
      <guid>https://leehyungi0622.github.io/2022/08/04/202208/220804_airflow_on_docker/</guid>
      <pubDate>Thu, 04 Aug 2022 05:40:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220804_apache_airflow.png&quot; alt=&quot;Apache Airflow&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 Apache Airflow의 기본 개념과 사용에 대해서 정리해보려고 한다.&lt;br&gt;이전에 AWS의 EventBridge라는 서비스를 사용해서 셍성한 Lambda 함수를 일정 주기의 시간동안 정기적으로 실행되도록 스케줄링해서 실습한 적이 있는데, Task를 좀 더 복잡한 구조로 스케줄링하기 위해서는 Apache Airflow를 활용하는 것이 좀 더 효율적이라고 오프라인 수업에서 배워서, 한 번 Apache Airflow를 사용해서 데이터 파이프라인을 구축해보고자 학습을 하게 되었다. 그리고 지원하고자 하는 기업에서 Apache Airflow를 업무에서 도입을 하여 사용하고 있기 때문에 좀 더 잘 이해하고 사용해보려고 한다.  &lt;/p&gt;
&lt;h2 id=&quot;Apache-Airflow를-사용하는-이유&quot;&gt;&lt;a href=&quot;#Apache-Airflow를-사용하는-이유&quot; class=&quot;headerlink&quot; title=&quot;Apache Airflow를 사용하는 이유&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Apache Airflow를 사용하는 이유&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;데이터 파이프라인 구축에 있어, 데이터를 추출하는 &lt;code&gt;Extract&lt;/code&gt;, 데이터를 적재하는 &lt;code&gt;Load&lt;/code&gt;, 데이터를 변환하는 &lt;code&gt;Transform&lt;/code&gt; 과정을 거친다. 데이터 추출은 API를 통해서 하기도 하며, Load는 Snowflake와 같은 data warehousing, data lake와 같은 Single platform을 제공하는 SaaS를 활용하기도 한다. 그리고 Transform은 Dbt와 같은 분석을 위한 데이터 웨어하우스에 적재된 데이터를 간단한 SELECT 문 작성을 통해 변환을 할 수도 있다.&lt;/p&gt;
&lt;p&gt;만약 Extract, Load, Transform 단계에서 예기치 못한 에러가 발생한다면, 그리고 데이터 파이프라인이 한 개가 아닌 100개 이상이라면 어떨까? &lt;/p&gt;
&lt;p&gt;관리하기가 많이 어려워진다. 이러한 이유로 인해 Airflow를 사용해서 종합적인 파이프라인의 관리가 필요하다.&lt;/p&gt;
&lt;h2 id=&quot;DAG-Directed-Acyclic-Graph&quot;&gt;&lt;a href=&quot;#DAG-Directed-Acyclic-Graph&quot; class=&quot;headerlink&quot; title=&quot;DAG(Directed Acyclic Graph)&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;DAG(Directed Acyclic Graph)&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;DAG는 &lt;code&gt;방향성 비순환 그래프&lt;/code&gt;로, Airfow의 주요 컨셉이며, 복수 개의 Task를 모아서 어떻게 실행이 되어야 하는지에 대한 종속성과 관계에 따라 구조화 시키는 것을 말한다.   &lt;/p&gt;
&lt;h2 id=&quot;Operator&quot;&gt;&lt;a href=&quot;#Operator&quot; class=&quot;headerlink&quot; title=&quot;Operator&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Operator&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;Operator는 Task이며, 실행이 되면 Task instance가 생성이 되며, Operator의 예로는 &lt;code&gt;Action operator&lt;/code&gt;, &lt;code&gt;Transfer operator&lt;/code&gt;, &lt;code&gt;Sensor operator&lt;/code&gt;가 있다. &lt;/p&gt;
&lt;p&gt;Action operator의 예로는 &lt;code&gt;Python operator&lt;/code&gt;, &lt;code&gt;Bash operator&lt;/code&gt;가 있는데, Python operator는 Python function을 실행시키며, Bash operator는 Bash command를 실행시키는 역할을 한다. 그리고 Transfer operator의 예로는 MySQL의 데이터를 RedShift로 이전하는 작업이 있으며, Sensor operator의 예로는 특정 이벤트가 발생하면 다음 Step으로 넘어가도록 하는 작업이 있다.&lt;/p&gt;
&lt;h2 id=&quot;Apache-Airflow의-요소&quot;&gt;&lt;a href=&quot;#Apache-Airflow의-요소&quot; class=&quot;headerlink&quot; title=&quot;Apache Airflow의 요소&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Apache Airflow의 요소&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;Apache Airflow에는 Webserver, Meta store, Scheduler, Executor, Queue, Worker가 있다. &lt;/p&gt;
&lt;p&gt;&lt;code&gt;Executor&lt;/code&gt;는 직접적으로 Task를 실행하지 않으며, K8S 클러스터는 &lt;code&gt;K8S Executor&lt;/code&gt;를 사용하고, Celery 클러스터는 &lt;code&gt;Celery Executor&lt;/code&gt;를 사용한다. 여기서 Celery는 multiple machine에서의 multiple tasks를 실행하기 위한 Python 프레임워크이다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Queue&lt;/code&gt;는 주어진 Task를 보장된 순서로 실행시키기 위해 존재한다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Worker&lt;/code&gt;는 Task가 효과적으로 실행될 수 있도록 하는 역할을 하며, Worker가 없다면, sub processes 혹은 K8S를 사용하는 경우, Path가 주어진다.&lt;/p&gt;
&lt;h2 id=&quot;Apache-Airflow의-Architecture&quot;&gt;&lt;a href=&quot;#Apache-Airflow의-Architecture&quot; class=&quot;headerlink&quot; title=&quot;Apache Airflow의 Architecture&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Apache Airflow의 Architecture&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;Apache Airflow의 Architecture로는 단일 노드로 구성된 &lt;code&gt;One Node Architecture&lt;/code&gt;, 그리고 복수 개의 노드들로 구성된 &lt;code&gt;Multi Node Architecture&lt;/code&gt;가 있다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220804_apache_airflow.png" alt="Apache Airflow"></div><br/><br/><p>이번 포스팅에서는 Apache Airflow의 기본 개념과 사용에 대해서 정리해보려고 한다.<br>이전에 AWS의 EventBridge라는 서비스를 사용해서 셍성한 Lambda 함수를 일정 주기의 시간동안 정기적으로 실행되도록 스케줄링해서 실습한 적이 있는데, Task를 좀 더 복잡한 구조로 스케줄링하기 위해서는 Apache Airflow를 활용하는 것이 좀 더 효율적이라고 오프라인 수업에서 배워서, 한 번 Apache Airflow를 사용해서 데이터 파이프라인을 구축해보고자 학습을 하게 되었다. 그리고 지원하고자 하는 기업에서 Apache Airflow를 업무에서 도입을 하여 사용하고 있기 때문에 좀 더 잘 이해하고 사용해보려고 한다.  </p><h2 id="Apache-Airflow를-사용하는-이유"><a href="#Apache-Airflow를-사용하는-이유" class="headerlink" title="Apache Airflow를 사용하는 이유"></a><ins><b>Apache Airflow를 사용하는 이유</b></ins></h2><p>데이터 파이프라인 구축에 있어, 데이터를 추출하는 <code>Extract</code>, 데이터를 적재하는 <code>Load</code>, 데이터를 변환하는 <code>Transform</code> 과정을 거친다. 데이터 추출은 API를 통해서 하기도 하며, Load는 Snowflake와 같은 data warehousing, data lake와 같은 Single platform을 제공하는 SaaS를 활용하기도 한다. 그리고 Transform은 Dbt와 같은 분석을 위한 데이터 웨어하우스에 적재된 데이터를 간단한 SELECT 문 작성을 통해 변환을 할 수도 있다.</p><p>만약 Extract, Load, Transform 단계에서 예기치 못한 에러가 발생한다면, 그리고 데이터 파이프라인이 한 개가 아닌 100개 이상이라면 어떨까? </p><p>관리하기가 많이 어려워진다. 이러한 이유로 인해 Airflow를 사용해서 종합적인 파이프라인의 관리가 필요하다.</p><h2 id="DAG-Directed-Acyclic-Graph"><a href="#DAG-Directed-Acyclic-Graph" class="headerlink" title="DAG(Directed Acyclic Graph)"></a><ins><b>DAG(Directed Acyclic Graph)</b></ins></h2><p>DAG는 <code>방향성 비순환 그래프</code>로, Airfow의 주요 컨셉이며, 복수 개의 Task를 모아서 어떻게 실행이 되어야 하는지에 대한 종속성과 관계에 따라 구조화 시키는 것을 말한다.   </p><h2 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a><ins><b>Operator</b></ins></h2><p>Operator는 Task이며, 실행이 되면 Task instance가 생성이 되며, Operator의 예로는 <code>Action operator</code>, <code>Transfer operator</code>, <code>Sensor operator</code>가 있다. </p><p>Action operator의 예로는 <code>Python operator</code>, <code>Bash operator</code>가 있는데, Python operator는 Python function을 실행시키며, Bash operator는 Bash command를 실행시키는 역할을 한다. 그리고 Transfer operator의 예로는 MySQL의 데이터를 RedShift로 이전하는 작업이 있으며, Sensor operator의 예로는 특정 이벤트가 발생하면 다음 Step으로 넘어가도록 하는 작업이 있다.</p><h2 id="Apache-Airflow의-요소"><a href="#Apache-Airflow의-요소" class="headerlink" title="Apache Airflow의 요소"></a><ins><b>Apache Airflow의 요소</b></ins></h2><p>Apache Airflow에는 Webserver, Meta store, Scheduler, Executor, Queue, Worker가 있다. </p><p><code>Executor</code>는 직접적으로 Task를 실행하지 않으며, K8S 클러스터는 <code>K8S Executor</code>를 사용하고, Celery 클러스터는 <code>Celery Executor</code>를 사용한다. 여기서 Celery는 multiple machine에서의 multiple tasks를 실행하기 위한 Python 프레임워크이다.</p><p><code>Queue</code>는 주어진 Task를 보장된 순서로 실행시키기 위해 존재한다.</p><p><code>Worker</code>는 Task가 효과적으로 실행될 수 있도록 하는 역할을 하며, Worker가 없다면, sub processes 혹은 K8S를 사용하는 경우, Path가 주어진다.</p><h2 id="Apache-Airflow의-Architecture"><a href="#Apache-Airflow의-Architecture" class="headerlink" title="Apache Airflow의 Architecture"></a><ins><b>Apache Airflow의 Architecture</b></ins></h2><p>Apache Airflow의 Architecture로는 단일 노드로 구성된 <code>One Node Architecture</code>, 그리고 복수 개의 노드들로 구성된 <code>Multi Node Architecture</code>가 있다.</p><a id="more"></a><p>우선 첫 번째로 단일 노드 아키텍처를 살펴보면, 하나의 노드에 <code>Web server</code>와 <code>Meta store</code>, <code>Scheduler</code>, <code>Executor</code>, <code>Queue</code>가 존재한다. 여기서 Meta store는 Airflow에 존재하는 서로 다른 component 사이에서 데이터를 교환할 수 있도록 한다. 또 다른 섹션에서 전체적인 흐름을 정리하겠지만, Scheduler는 사용자가 작성한 Dag 파일을 정기적으로 정해진 시간 간격으로 파싱해서 새로운 <code>Dag Run Object</code>를 Meta store에 생성되고, Task의 수에 따라 Task Instance가 이어서 생성이 된다. 그리고 Dag Run Object와 Task Instance는 상태 정보와 함께 관리된다. </p><p>앞서 언급한 객체와 인스턴스의 상태 정보들이 Meta store에 업데이트가 되면서 Apach Airflow의 UI가 업데이트되고, 이로써 사용자가 UI를 통해 Task의 상태 정보를 Tracking할 수 있게 되는 것이다.</p><div align="center">  <img src="/images/post_images/220804_one_node_architecture.jpg" alt="Apache Airflow One Node Architecture"></div><p>두 번째로 multi-node Architecture를 살펴보면, 위에서 살펴보았던 단일 노드 아키텍처와 달리 Meta store와 Queue를 별도의 노드로 분리하고, Queue에 쌓인 Task를 실행할 Worker node들이 복수 개 존재하고 있음을 알 수 있다. 이러한 복수 개의 노드로 구성하는 이유는 고가용성(HA)을 위함인데, Single point failure 이슈를 사전에 예방하고자 구성한다. </p><p>만약에 Load balancer를 사용해서 요청을 분산해서 처리하고자 한다면, 최소 두 개의 Web Server와 Scheduler가 필요하다.</p><div align="center">  <img src="/images/post_images/220804_multi_node_architecture.jpg" alt="Apache Airflow Multi Node Architecture"></div><h2 id="Apache-Airflow의-전체-흐름"><a href="#Apache-Airflow의-전체-흐름" class="headerlink" title="Apache Airflow의 전체 흐름"></a><ins><b>Apache Airflow의 전체 흐름</b></ins></h2><p>Apache Airflow의 Task와 데이터 파이프라인을 debug하기 위해서는 Task가 어떻게 실행되는지에 대한 전반적인 이해가 필요하다.</p><p>우선 단일 노드의 관점에서 살펴보면, 아래 그림과 같이 Web server, Meta store, Scheduler, Executor, Folder Dags가 존재한다. </p><p><code>(1)</code> 개발자가 <code>dag.py</code>파일을 작성해서 Folder Dags에 추가한다.</p><p><code>(2)</code> Scheduler는 정해진 시간동안 정기적으로 새로운 Dags 객체 생성을 위해 Folder Dags를 파싱한다.</p><p><code>(3)</code> Dag가 Trigger될 준비가 되었다면, Scheduler는 Dag Run Object를 Meta store에 생성한다. </p><p><code>(4)</code> Task의 갯수에 따라 Task Instance가 상태 정보와 함께 생성이 된다.</p><p><code>(5)</code> Task가 Trigger될 준비가 되었다면, Task Instance Object를 Executor로 보내게 되고, Task Instance의 상태는 <code>queued</code>상태로 변경된다.</p><p><code>(6)</code> Executor가 실행을 위해 Task instance를 Sub process나 Worker에 Task를 위치시키면, </p><p><code>(7)</code> Task Instance의 상태가 <code>queued</code>에서 <code>running</code>으로 변경된다. </p><p><code>(8)</code> Task가 완료되면, executor가 Task Instance Object의 상태를 databaser에 업데이트한다.</p><p><code>(9)</code> Scheduler가 Dag Run Object에 작업이 완료되었는지 확인을 하고, 작업이 완료되었다면, 상태가 success로 업데이트된다.</p><p><code>(10)</code> Web server는 Meta store를 참고해서 Apache Airflow의 UI를 업데이트하게 되고, Apache Airflow의 UI를 통해서 Task의 상태를 Tracking할 수 있게 된다.</p><div align="center">  <img src="/images/post_images/220804_airflow_overview.jpg" alt="Apache Airflow 전체 흐름"></div><h2 id="Apache-Airflow-설치"><a href="#Apache-Airflow-설치" class="headerlink" title="Apache Airflow 설치"></a><ins><b>Apache Airflow 설치</b></ins></h2><h2 id="꼭-알아야-할-중요한-내용-Apache"><a href="#꼭-알아야-할-중요한-내용-Apache" class="headerlink" title="(꼭 알아야 할 중요한 내용) Apache"></a><ins><b>(꼭 알아야 할 중요한 내용) Apache</b></ins></h2><p>(1) Airflow is an orchestrator, not a processing framework, process your gigabytes of data outside of Airflow (i.e. You have a Spark cluster, you use an operator to execute a Spark job, the data is processed in Spark).</p><p>(2) A DAG is a data pipeline, an Operator is a task.</p><p>(3) An Executor defines how your tasks are execute whereas a worker is a process executing your task</p><p>(4) The scheduler schedules your tasks, the web server serves the UI, the database stores the metadata of Airflow.</p><p>(5) airflow db init is the first command to execute to initialise Airflow</p><p>(6) If a task fails, check the logs by clicking on the task from the UI and “Logs”</p><p>(7) The Gantt view is super useful to sport bottlenecks and tasks are too long to execute</p><h2 id="Postgres-Database와-연결하기"><a href="#Postgres-Database와-연결하기" class="headerlink" title="Postgres Database와 연결하기"></a><ins><b>Postgres Database와 연결하기</b></ins></h2><p>Operator가 Postgres, MySQL, AWS, dbt 등의 외부 툴과 함께 상호작용을 해야 될 때에는 Apache Airflow의 [Admin]-[Connections]에서 새로운 Connection을 생성해줘야 한다.</p><h2 id="Task-생성하기"><a href="#Task-생성하기" class="headerlink" title="Task 생성하기"></a><ins><b>Task 생성하기</b></ins></h2><h3 id="1-Import-the-operator"><a href="#1-Import-the-operator" class="headerlink" title="(1) Import the operator"></a><strong>(1) Import the operator</strong></h3><h3 id="2-Define-the-task-id"><a href="#2-Define-the-task-id" class="headerlink" title="(2) Define the task id"></a><strong>(2) Define the task id</strong></h3><p>  Task ID는 같은 DAG 내에서 반드시 유일해야한다.</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> airflow.providers.postgres.operators.postgres <span class="keyword">import</span> PostgresOperator</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> DAG(<span class="string">&#x27;user_processing&#x27;</span>, start_date=datetime(<span class="number">2022</span>, <span class="number">1</span>, <span class="number">1</span>), schedule_interval=<span class="string">&#x27;@daily&#x27;</span>, catchup=<span class="literal">False</span>) <span class="keyword">as</span> dag:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># variable name과 task_id를 일치시켜놓는 것이 좋다.</span></span><br><span class="line">    <span class="comment"># task id는 같은 DAG 내에서 반드시 유일해야 한다.</span></span><br><span class="line">    <span class="comment"># database와 상호작용을 하기 위해서 데이터베이스와 연결을 해야하며,</span></span><br><span class="line">    create_table = PostgresOperator(</span><br><span class="line">        task_id=<span class="string">&#x27;create_table&#x27;</span>,</span><br><span class="line">        postgres_conn_id=<span class="string">&#x27;postgres&#x27;</span>,</span><br><span class="line">        sql=<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            CREATE TABLE IF NOT EXISTS users (</span></span><br><span class="line"><span class="string">                firstname TEXT NOT NULL,</span></span><br><span class="line"><span class="string">                lastname TEXT NOT NULL,</span></span><br><span class="line"><span class="string">                country TEXT NOT NULL,</span></span><br><span class="line"><span class="string">                username TEXT NOT NULL,</span></span><br><span class="line"><span class="string">                password TEXT NOT NULL,</span></span><br><span class="line"><span class="string">                email TEXT NOT NULL</span></span><br><span class="line"><span class="string">            )</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h2 id="DAG에-Task를-추가한-후-확인하기"><a href="#DAG에-Task를-추가한-후-확인하기" class="headerlink" title="DAG에 Task를 추가한 후 확인하기"></a><ins><b>DAG에 Task를 추가한 후 확인하기</b></ins></h2><p>Airflow의 DAG에 Task를 추가한 후에는 반드시 확인해야 한다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Airflow의 scheduler Name 확인</span></span><br><span class="line"><span class="variable">$docker</span>-compose ps </span><br><span class="line"><span class="variable">$docker</span> <span class="built_in">exec</span> -it materials_airflow-scheduler_1 /bin/bash</span><br><span class="line"><span class="comment"># airflow task 확인 </span></span><br><span class="line"><span class="variable">$airflow</span> tasks <span class="built_in">test</span> user_processing create_table 2022-08-06</span><br></pre></td></tr></table></figure><h2 id="Sensor-operator"><a href="#Sensor-operator" class="headerlink" title="Sensor operator"></a><ins><b>Sensor operator</b></ins></h2><p>Sensor는 Airflow의 Context로, 내가 원하는 무언가를 위해 기다려준다. 예를들어, 특정 위치에 파일이 생성되거나 SQL 테이블의 속성에 파일 정보가 기입되는 등의 작업들이 그 예시가 되며, Airflow에서는 수 많은 Sensor operator를 제공해준다.</p><p>기억해야 될 것은  두 가지 parameter, poke_interval(<code>default: 60s</code>)과 timeout(<code>default: 7days</code>)이 있다. poke_interval이 default 값으로 설정이 되어있으면, sensor는 다음 Task를 실행하기 전에 조건이 참이지 거짓인지 매 60초마다 확인을 한다. timeout은 무기한으로 poking하는 것을 허용하지 않기 위해 default 값으로 설정하는 경우, 최대 7일이라는 poking을 멈추고 종료하는 최대 시간을 갖습니다. (sensor operator는 <code>FAILED</code>나 <code>SKIPPED</code>로 표기됩니다.)</p><p>주어진 API가 유효한지에 대해 검사를 하기 위해 <code>HttpSensor</code> operator를 사용하며, 이외에 파일의 생성에 대해 감지하고 싶다면, <code>FileSensor</code>를, S3 bucket에 대한 감지가 필요하다면, <code>S3KeySensor</code>를 사용하면 된다. </p><h2 id="Hook"><a href="#Hook" class="headerlink" title="Hook"></a><ins><b>Hook</b></ins></h2><p>Airflow에서는 다양한 외부 툴과 손쉽게 상호작용 할 수 있도록 Hook이라는 개념이 존재한다. 예를들어 PostgresSQL 데이터베이스에 sql 쿼리를 처리할때, PostgresHook이라는 것이 PostgresOperator와 PostgresSQL 데이터베이스 사이에 존재하기 때문에 툴이나 서비스와 상호작용하기 위한 복잡한 부분이 추상화되어 개발자가 손쉽게 작업을 할 수 있도록 도와준다.</p><h3 id="PostgresHook"><a href="#PostgresHook" class="headerlink" title="PostgresHook"></a><strong>PostgresHook</strong></h3><p>PostgresSQL 데이터베이스의 테이블에 데이터를 저장하기 위해서는 PostgresHook이 필요하다.</p><h2 id="Dependency-추가"><a href="#Dependency-추가" class="headerlink" title="Dependency 추가"></a><ins><b>Dependency 추가</b></ins></h2><p>각 각의 개별 Task를 생성한 후에는 개별 Task에 대해 파일의 마지막에 dependencies를 정의해서 연결해줘야 한다. 아래 코드를 정의한 다음에 Apache Airflow의 DAG의 Graph 메뉴를 보면, 이전에 개별로 표시된 Task들이 화살표로 엮여서 표시된 것을 확인할 수 있다.</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create_table &gt;&gt; is_api_available &gt;&gt; extract_user &gt;&gt; process_user &gt;&gt; store_user</span><br></pre></td></tr></table></figure><h2 id="DAG-실행"><a href="#DAG-실행" class="headerlink" title="DAG 실행"></a><ins><b>DAG 실행</b></ins></h2><p>DAG 파일의 작성이 끝나면, DAG를 실행해서 모든 Task들이 정상적으로 실행이 되고, 완료되는지 확인하고, 아래 명령으로 Airflow worker container에 접속해서 테스트로 작성한 DAG에서 외부 API 호출을 통해 데이터를 긁어와서 CSV파일로 추출한 데이터 파일을 확인한다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$docker</span> <span class="built_in">exec</span> -it materials_airflow-worker_1 /bin/bash</span><br><span class="line"><span class="variable">$ls</span> /tmp/</span><br></pre></td></tr></table></figure><p>이제 PostgresSQL 데이터베이스에 csv 데이터가 잘 적재되었는지 아래의 명령을 통해 확인한다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$docker</span> <span class="built_in">exec</span> -it materials_postgres_1 /bin/bash</span><br><span class="line"><span class="variable">$psql</span> -Uairflow</span><br><span class="line">airflow=<span class="comment">#SELECT * FROM users;</span></span><br></pre></td></tr></table></figure><h2 id="DAG-스케줄링"><a href="#DAG-스케줄링" class="headerlink" title="DAG 스케줄링"></a><ins><b>DAG 스케줄링</b></ins></h2><p>우리가 DAG 객체를 정의할때, start_date와 schedule_interval을 정의해주는데, 아래의 규칙으로 DAG가 실행을 한다. (<code>end_date는 생략 가능</code>)</p><p> DAG는 <code>start_date/last_run + the schedule_interval</code> 이후에 트리거된다. 주의해야 될 것은 start_date에는 아무 것도 발생하지 않는다는 것이다.<br> 예를들어, start_date가 10:00AM 이고, schedule interval이 매 10분이라고 정의가 되었다면, 최초 10:00AM에는 아무 것도 발생하지 않고, 최초 10분 이후인 10:10AM에 DAG가 실행되어, data_interval_end가 10:10AM이 된다. 이후에는 data_interval_start이 10:10AM이 되고, data_interval_end가 10:20AM이 된다.</p>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Airflow/">Airflow</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/Airflow/">Airflow</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/08/04/202208/220804_airflow_on_docker/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220728 Burrow - Kafka consumer Lag 모니터링</title>
      <link>https://leehyungi0622.github.io/2022/07/28/202207/220728_datapipeline_study/</link>
      <guid>https://leehyungi0622.github.io/2022/07/28/202207/220728_datapipeline_study/</guid>
      <pubDate>Thu, 28 Jul 2022 06:51:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220728_kafka_burrow.png&quot; alt=&quot;Burrow&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 Kafka의 Consumer Lag를 모니터링할 때 필수적으로 사용되는 Burrow에 대해서 정리해보려고 한다.&lt;/p&gt;
&lt;p&gt;이번 포트폴리오에 추가 할 사이드 프로젝트로 Kafka를 활용해서 Kafka cluster를 구성하고, Kafka-client 라이브러리를 활용하여 Python으로 Producer 및 Consumer를 구성하였다. Consumer에서는 ELK 스택의 docker 컨테이너를 연결하여, Producer로부터 유입된 로그 데이터를 Logstash를 통해 Elasticsearch에 저자을 하고, 저장된 로그 데이터를 Kibana를 통해서 시각화하였다. &lt;/p&gt;
&lt;p&gt;Kafka consumer는 kafka-client 라이브러리를 활용해서 Java, Python등의 언어로 개발을 할 수 있는데, 생성한 KafkaConsumer 객체를 통해 현재 lag 정보를 가져올 수 있다.&lt;/p&gt;
&lt;p&gt;만약 lag을 실시간으로 모니터링하고 싶은 경우에는 Elasticsearch나 influxdb와 같은 곳으로 &lt;code&gt;Consumer lag metric&lt;/code&gt; 정보를 보낸 뒤에 Grafana 대시모드를 통해서 실시간으로 시각화하여 확인 할 수 있다.&lt;/p&gt;
&lt;p&gt;하지만 Consumer 단위에서 lag을 모니터링하는 것은 아주 위험하고, 운영요소가 많이 들어간다는 문제가 있다. 그 이유는 Consumer logic 단에서 lag을 수집하는 것은 Consumer 상태에 dependency가 걸리기 때문이다.&lt;br&gt;만약에 Consumer가 비정상적으로 종료되게 된다면, 더 이상 Consumer는 lag 정보를 보낼 수 없기 때문에 더 이상 lag을 측정할 수 없는 문제가 발생한다.  &lt;/p&gt;
&lt;p&gt;그리고 차후에 추가적으로 consumer가 개발될 때 마다 해당 consumer에 lag 정보를 특정 저장소에 저장할 수 있도록 로직을 개발해야되기 때문에 공수가 많이 든다. &lt;/p&gt;
&lt;p&gt;만약에 consumer lag을 수집할 수 없는 consumer라면, lag을 모니터링 할 수 없기 때문에 까다로워진다. &lt;/p&gt;
&lt;p&gt;이러한 이유로 인해 linkedIn에서는 Apache kafka와 함께 Kafka consumer lag을 효과적으로 모니터링 할 수 있도록 Burrow를 개발하였다.&lt;/p&gt;
&lt;p&gt;Burrow는 오픈 소스 프로젝트로, Go 언어로 개발이 되었으며, 현재 깃허브에 올라가 있다. 이 Burrow는 Kafka와는 독립적인 consumer의 lag을 모니터링하기 위한 애플리케이션이다.&lt;/p&gt;
&lt;h2 id=&quot;Burrow의-특징&quot;&gt;&lt;a href=&quot;#Burrow의-특징&quot; class=&quot;headerlink&quot; title=&quot;Burrow의 특징&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Burrow의 특징&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;h3 id=&quot;Multi-Kafka-cluster를-지원한다&quot;&gt;&lt;a href=&quot;#Multi-Kafka-cluster를-지원한다&quot; class=&quot;headerlink&quot; title=&quot;Multi-Kafka cluster를 지원한다.&quot;&gt;&lt;/a&gt;&lt;strong&gt;Multi-Kafka cluster를 지원한다.&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;  Kafka를 사용하는 기업에서는 보통 2개 이상의 Kafka cluster를 구성해서 사용하는데, 여러 개의 Kafka 클러스터를 구성하더라도 한 개의 Burrow만으로 모든 카프카 클러스터에 붙은 consumer의 lag을 모두 모니터링 할 수 있다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220728_kafka_burrow.png" alt="Burrow"></div><br/><br/><p>이번 포스팅에서는 Kafka의 Consumer Lag를 모니터링할 때 필수적으로 사용되는 Burrow에 대해서 정리해보려고 한다.</p><p>이번 포트폴리오에 추가 할 사이드 프로젝트로 Kafka를 활용해서 Kafka cluster를 구성하고, Kafka-client 라이브러리를 활용하여 Python으로 Producer 및 Consumer를 구성하였다. Consumer에서는 ELK 스택의 docker 컨테이너를 연결하여, Producer로부터 유입된 로그 데이터를 Logstash를 통해 Elasticsearch에 저자을 하고, 저장된 로그 데이터를 Kibana를 통해서 시각화하였다. </p><p>Kafka consumer는 kafka-client 라이브러리를 활용해서 Java, Python등의 언어로 개발을 할 수 있는데, 생성한 KafkaConsumer 객체를 통해 현재 lag 정보를 가져올 수 있다.</p><p>만약 lag을 실시간으로 모니터링하고 싶은 경우에는 Elasticsearch나 influxdb와 같은 곳으로 <code>Consumer lag metric</code> 정보를 보낸 뒤에 Grafana 대시모드를 통해서 실시간으로 시각화하여 확인 할 수 있다.</p><p>하지만 Consumer 단위에서 lag을 모니터링하는 것은 아주 위험하고, 운영요소가 많이 들어간다는 문제가 있다. 그 이유는 Consumer logic 단에서 lag을 수집하는 것은 Consumer 상태에 dependency가 걸리기 때문이다.<br>만약에 Consumer가 비정상적으로 종료되게 된다면, 더 이상 Consumer는 lag 정보를 보낼 수 없기 때문에 더 이상 lag을 측정할 수 없는 문제가 발생한다.  </p><p>그리고 차후에 추가적으로 consumer가 개발될 때 마다 해당 consumer에 lag 정보를 특정 저장소에 저장할 수 있도록 로직을 개발해야되기 때문에 공수가 많이 든다. </p><p>만약에 consumer lag을 수집할 수 없는 consumer라면, lag을 모니터링 할 수 없기 때문에 까다로워진다. </p><p>이러한 이유로 인해 linkedIn에서는 Apache kafka와 함께 Kafka consumer lag을 효과적으로 모니터링 할 수 있도록 Burrow를 개발하였다.</p><p>Burrow는 오픈 소스 프로젝트로, Go 언어로 개발이 되었으며, 현재 깃허브에 올라가 있다. 이 Burrow는 Kafka와는 독립적인 consumer의 lag을 모니터링하기 위한 애플리케이션이다.</p><h2 id="Burrow의-특징"><a href="#Burrow의-특징" class="headerlink" title="Burrow의 특징"></a><ins><b>Burrow의 특징</b></ins></h2><h3 id="Multi-Kafka-cluster를-지원한다"><a href="#Multi-Kafka-cluster를-지원한다" class="headerlink" title="Multi-Kafka cluster를 지원한다."></a><strong>Multi-Kafka cluster를 지원한다.</strong></h3><p>  Kafka를 사용하는 기업에서는 보통 2개 이상의 Kafka cluster를 구성해서 사용하는데, 여러 개의 Kafka 클러스터를 구성하더라도 한 개의 Burrow만으로 모든 카프카 클러스터에 붙은 consumer의 lag을 모두 모니터링 할 수 있다.</p><a id="more"></a><h3 id="Sliding-window를-통한-Consumer의-상태-확인"><a href="#Sliding-window를-통한-Consumer의-상태-확인" class="headerlink" title="Sliding window를 통한 Consumer의 상태 확인"></a><strong>Sliding window를 통한 Consumer의 상태 확인</strong></h3><p>  Burrow에서는 sliding window를 통해서 consumer의 상태 정보를 <code>ERROR</code>, <code>WARNING</code>, <code>OK</code>로 나눠서 확인을 한다. 만약 일시적으로 데이터의 양이 많아져서 consumer offset이 증가하면 <code>WARNING</code>으로 표기하며, 데이터의 양이 많아졌는데, consumer가 데이터를 가져가지 않으면 <code>ERROR</code>로 정의한다.(<code>consumer의 문제 확인</code>) </p><br/><h3 id="HTTP-API-제공"><a href="#HTTP-API-제공" class="headerlink" title="HTTP API 제공"></a><strong>HTTP API 제공</strong></h3><p>  위에서 정의한 정보들을 HTTP API를 통해서 확인할 수 있다. HTTP API를 통해 받은 response 데이터를 시계열 DB와 같은 곳에 저장하는 애플리케이션을 만들어서 활용할 수도 있다.</p><p>이러한 Burrow의 탄생 배경을 위해 <code>LinkedIn Engineering</code>블로그를 확인하도록 하자. </p><h2 id="Burrow-설치"><a href="#Burrow-설치" class="headerlink" title="Burrow 설치"></a><ins><b>Burrow 설치</b></ins></h2><p><strong>공식 문서 :</strong> <a href="https://github.com/linkedin/Burrow">https://github.com/linkedin/Burrow</a></p><h3 id="go-설치하기"><a href="#go-설치하기" class="headerlink" title="go 설치하기"></a><strong>go 설치하기</strong></h3><p>burrow는 go 언어로 개발된 애플리케이션이기 때문에 go를 우선적으로 설치해야 한다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$brew</span> install go</span><br></pre></td></tr></table></figure><h3 id="Burrow-exe-파일-생성하기"><a href="#Burrow-exe-파일-생성하기" class="headerlink" title="Burrow.exe 파일 생성하기"></a><strong>Burrow.exe 파일 생성하기</strong></h3><p>go 언어를 설치했다면, burrow GitHub repository를 clone해야한다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$git</span> <span class="built_in">clone</span> https://github.com/linkedin/Burrow.git</span><br></pre></td></tr></table></figure><p>이후에 clone 받은 directory로 이동해서 <code>$go mod tidy</code> 및 <code>$go install</code> 명령을 통해 /User/[사용자명]/go/bin 하위에 <code>Burrow.exe 파일이 생성</code>이 된다.</p><h3 id="설정-파일-수정하기"><a href="#설정-파일-수정하기" class="headerlink" title="설정 파일 수정하기"></a><strong>설정 파일 수정하기</strong></h3><p>Borrow.exe 파일 실행 전에 우선 설정 파일을 수정해야 한다. (<code>burrow.toml</code>- clone한 repo의 config 폴더 확인)</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[general]</span><br><span class="line"></span><br><span class="line">[logging]</span><br><span class="line">level=<span class="string">&quot;info&quot;</span></span><br><span class="line"></span><br><span class="line">[zookeeper]</span><br><span class="line">servers=[ <span class="string">&quot;localhost:2181&quot;</span>]</span><br><span class="line"></span><br><span class="line">[client-profile.local]</span><br><span class="line">client-id=<span class="string">&quot;burrow-local&quot;</span></span><br><span class="line">kafka-version=<span class="string">&quot;2.0.0&quot;</span></span><br><span class="line"></span><br><span class="line">[cluster.local]</span><br><span class="line">class-name=<span class="string">&quot;kafka&quot;</span></span><br><span class="line">servers=[ <span class="string">&quot;localhost:9092&quot;</span> ]</span><br><span class="line">client-profile=<span class="string">&quot;local&quot;</span></span><br><span class="line">topic-refresh=120</span><br><span class="line">offset-refresh=30</span><br><span class="line"></span><br><span class="line">[consumer.local]</span><br><span class="line">class-name=<span class="string">&quot;kafka&quot;</span></span><br><span class="line">cluster=<span class="string">&quot;local&quot;</span></span><br><span class="line">servers=[ <span class="string">&quot;localhost:9092&quot;</span> ]</span><br><span class="line">client-profile=<span class="string">&quot;local&quot;</span></span><br><span class="line">group-denylist=<span class="string">&quot;^(console-consumer-|python-kafka-consumer-|quick-).*$&quot;</span></span><br><span class="line">group-allowlist=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">[httpserver.default]</span><br><span class="line">address=<span class="string">&quot;:8000&quot;</span></span><br><span class="line"></span><br><span class="line">[storage.default]</span><br><span class="line">class-name=<span class="string">&quot;inmemory&quot;</span></span><br><span class="line">workers=20</span><br><span class="line">intervals=15</span><br><span class="line">expire-group=604800</span><br><span class="line">min-distance=1</span><br></pre></td></tr></table></figure><h3 id="Burrow-exe-파일-실행하기"><a href="#Burrow-exe-파일-실행하기" class="headerlink" title="Burrow.exe 파일 실행하기"></a><strong>Burrow.exe 파일 실행하기</strong></h3><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Burrow.exe 파일의 위치 경로를 지정한 후 수정한 burrow.toml 설정 파일을 실행하기 위해 아래의 명령을 실행한다.</span></span><br><span class="line">$/Users/[사용자명]/go/bin/Burrow --config-dir=/config</span><br></pre></td></tr></table></figure><p>Burrow 서비스 확인은 <code>localhost:8000/burrow/admin</code> URL을 통해서 확인하도록 한다. </p><p>Burrow EndPoint 정리 문서는 아래의 링크를 확인하도록 하자.</p><p><a href="https://github.com/linkedin/Burrow/wiki/HTTP-Endpoint">https://github.com/linkedin/Burrow/wiki/HTTP-Endpoint</a></p><h3 id="추가-Docker-컨테이너에-build된-Kafka-cluster-모니터링"><a href="#추가-Docker-컨테이너에-build된-Kafka-cluster-모니터링" class="headerlink" title="[추가] Docker 컨테이너에 build된 Kafka cluster 모니터링"></a><strong>[추가] Docker 컨테이너에 build된 Kafka cluster 모니터링</strong></h3><p>만약 Kafka cluster를 docker-compose.yml 파일에서 정의해서 구축했다면, 같은 파일 내에서 burrow에 대한 설정을 할 수 있습니다. 또한 burrow.toml 파일에서 server에 대한 정의도 docker-compose.yml에서 정의한 서비스의 hostname을 기반으로 작성할 수 있습니다.</p><h2 id="Burrow-Dashboard-설치"><a href="#Burrow-Dashboard-설치" class="headerlink" title="Burrow Dashboard 설치"></a><ins><b>Burrow Dashboard 설치</b></ins></h2><p>docker에서 Burrow dashboard를 구성하기 위해 필요한 이미지를 다운받습니다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$sudo</span> docker pull joway/burrow-dashboard</span><br></pre></td></tr></table></figure><p>아래의 method를 통해서 dashboard를 실행합니다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$sudo</span> docker run --network host -e BURROW_BACKEND=http://localhost:8000 -p 80:80 joway/burrow-dashboard:latest</span><br></pre></td></tr></table></figure><h2 id="Burrow-대시보드-만들기"><a href="#Burrow-대시보드-만들기" class="headerlink" title="Burrow 대시보드 만들기"></a><ins><b>Burrow 대시보드 만들기</b></ins></h2><p>Burrow endpoint url을 통해 health check나 offset의 변화에 대해서 모니터링을 할 수는 있지만, json 형태로 출력이 되기 때문에 시계열 형태의 그래프로 값의 변화 추이에 대해서 모니터링이 필요하다.</p><p>텔레그래프 -&gt; ES -&gt; Grafana 순으로 데이터를 전달하여 시각화하도록 구성한다. 따라서 텔레그래프와 ES, Grafana의 설치가 필요하다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$brew</span> install telegraf</span><br><span class="line"></span><br><span class="line"><span class="variable">$cd</span> /usr/<span class="built_in">local</span>/Cellar/telegraf/1.23.3/bin</span><br><span class="line"><span class="variable">$telegraf</span> config &gt; telegraf.conf</span><br></pre></td></tr></table></figure><p>telegraf.conf 파일에서 localhost 8086 포트에 대한 urls 정의 부분과 database 정의 부분에 대해 주석을 해제시켜준다.<br>아래의 설정은 telegraf에서 수집된 정보를 influxdb로 내보내기 위한 설정이다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">###############################################################################</span><br><span class="line">#                            OUTPUT PLUGINS                                   #</span><br><span class="line">###############################################################################</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[[inputs.burrow]]</span><br><span class="line">  servers &#x3D; [&quot;http:&#x2F;&#x2F;localhost:8000&quot;]</span><br><span class="line">  topics_exclude &#x3D; [ &quot;__consumer_offsets&quot; ]</span><br><span class="line">  groups_exclude &#x3D; [&quot;console-*&quot;]</span><br><span class="line">  &lt;!-- prefix에 대한 설정도 주석 제거 --&gt;</span><br><span class="line"></span><br><span class="line">[[outputs.elasticsearch]]</span><br><span class="line">  urls &#x3D; [ &quot;http:&#x2F;&#x2F;localhost:9200&quot; ] </span><br><span class="line">  timeout &#x3D; &quot;5s&quot;</span><br><span class="line">  enable_sniffer &#x3D; false</span><br><span class="line">  health_check_interval &#x3D; &quot;10s&quot;</span><br><span class="line">  index_name &#x3D; &quot;burrow-%Y.%m.%d&quot; </span><br><span class="line">  manage_template &#x3D; false</span><br><span class="line">  &lt;!-- elasticsearch username&#x2F;password 설정에 대한 부분도 주석 제거 --&gt;</span><br></pre></td></tr></table></figure><p><code>주석을 제거할 때 반드시 항목 이름에 대한 부분도 같이 제거를 해줘야 한다.</code></p><p>telegraf.conf 파일 수정이 되었다면, 이제 telegraf를 아래 명령으로 실행시켜주고, ES에서 누적된 burrow 데이터를 확인한다. </p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$telegraf</span> --config telegraf.conf</span><br></pre></td></tr></table></figure><p>grafana를 설치한다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$brew</span> install grafana</span><br></pre></td></tr></table></figure><p>grafana 시작</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$brew</span> tap homebrew/services</span><br><span class="line"><span class="variable">$brew</span> services start grafana</span><br></pre></td></tr></table></figure><p>이제 <code>localhost:3000</code>을 통해 Grafana 서비스 페이지로 접속할 수 있다.(admin/admin)<br>접속한 후에 새로운 데이터 소스로 elasticsearch를 추가해준다. (<code>index name 및 기타 정보 입력</code>)</p>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Data-Pipeline/">Data-Pipeline</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/AWS/">AWS</category>
      
      <category domain="https://leehyungi0622.github.io/tags/Kafka/">Kafka</category>
      
      <category domain="https://leehyungi0622.github.io/tags/Lag/">Lag</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/07/28/202207/220728_datapipeline_study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220721 SQL JOINS</title>
      <link>https://leehyungi0622.github.io/2022/07/21/202207/220721-sql-joins-study/</link>
      <guid>https://leehyungi0622.github.io/2022/07/21/202207/220721-sql-joins-study/</guid>
      <pubDate>Thu, 21 Jul 2022 04:48:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220721_sql_joins.png&quot; alt=&quot;SQL JOINS&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 SQL의 JOIN에 대해서 종합적으로 정리를 해보려고 한다. &lt;/p&gt;
&lt;h2 id=&quot;INNER-JOIN&quot;&gt;&lt;a href=&quot;#INNER-JOIN&quot; class=&quot;headerlink&quot; title=&quot;INNER JOIN&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;INNER JOIN&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;department에 속하는 employee name을 출력&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- INNER JOIN에서 INNER 생략 가능 &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- JOIN을 할때에는 ON 절에 작성하는 JOIN 조건의 column이름은 달라도 관계없다. Column의 값이 중요하다.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; e.emp_name, d.dept_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; employee e &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;span class=&quot;keyword&quot;&gt;JOIN&lt;/span&gt; department d &lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; e.dept_id &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; d.dept_id&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;OUTER-JOIN&quot;&gt;&lt;a href=&quot;#OUTER-JOIN&quot; class=&quot;headerlink&quot; title=&quot;OUTER JOIN&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;OUTER JOIN&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;h3 id=&quot;LEFT-JOIN&quot;&gt;&lt;a href=&quot;#LEFT-JOIN&quot; class=&quot;headerlink&quot; title=&quot;LEFT JOIN&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;LEFT JOIN&lt;/b&gt;&lt;/ins&gt;&lt;/h3&gt;&lt;p&gt;모든 employee 이름과 department 이름 출력&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- LEFT JOIN = INNER JOIN + ANY additional records from the LEFT TABLE.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; e.emp_name, d.dept_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; employee e&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;LEFT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;JOIN&lt;/span&gt; department d &lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; e.dept_id &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; d.dept_id;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;RIGHT-JOIN&quot;&gt;&lt;a href=&quot;#RIGHT-JOIN&quot; class=&quot;headerlink&quot; title=&quot;RIGHT JOIN&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;RIGHT JOIN&lt;/b&gt;&lt;/ins&gt;&lt;/h3&gt;&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- RIGHT JOIN = INNER JOIN + ANY additional records from the RIGHT TABLE.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; e.emp_name, d.dept_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; employee e&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;RIGHT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;JOIN&lt;/span&gt; department d &lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; e.dept_id &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; d.dept_id;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220721_sql_joins.png" alt="SQL JOINS"></div><br/><br/><p>이번 포스팅에서는 SQL의 JOIN에 대해서 종합적으로 정리를 해보려고 한다. </p><h2 id="INNER-JOIN"><a href="#INNER-JOIN" class="headerlink" title="INNER JOIN"></a><ins><b>INNER JOIN</b></ins></h2><p>department에 속하는 employee name을 출력</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- INNER JOIN에서 INNER 생략 가능 </span></span><br><span class="line"><span class="comment">-- JOIN을 할때에는 ON 절에 작성하는 JOIN 조건의 column이름은 달라도 관계없다. Column의 값이 중요하다.</span></span><br><span class="line"> <span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"> <span class="keyword">FROM</span> employee e </span><br><span class="line"> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> e.dept_id <span class="operator">=</span> d.dept_id</span><br></pre></td></tr></table></figure><h2 id="OUTER-JOIN"><a href="#OUTER-JOIN" class="headerlink" title="OUTER JOIN"></a><ins><b>OUTER JOIN</b></ins></h2><h3 id="LEFT-JOIN"><a href="#LEFT-JOIN" class="headerlink" title="LEFT JOIN"></a><ins><b>LEFT JOIN</b></ins></h3><p>모든 employee 이름과 department 이름 출력</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- LEFT JOIN = INNER JOIN + ANY additional records from the LEFT TABLE.</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> e.dept_id <span class="operator">=</span> d.dept_id;</span><br></pre></td></tr></table></figure><h3 id="RIGHT-JOIN"><a href="#RIGHT-JOIN" class="headerlink" title="RIGHT JOIN"></a><ins><b>RIGHT JOIN</b></ins></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- RIGHT JOIN = INNER JOIN + ANY additional records from the RIGHT TABLE.</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> e.dept_id <span class="operator">=</span> d.dept_id;</span><br></pre></td></tr></table></figure><a id="more"></a><p>모든 employee의 manager, department, project 정보 출력</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- PROJECT에 두 개 이상 참여하고 있는 employee의 경우에는 복수 emp_name으로 출력</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name, m.manager_name, p.project_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> e.dept_id <span class="operator">=</span> d.dept_id</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> manager m <span class="keyword">ON</span> m.manager_id <span class="operator">=</span> e.manager_id </span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> project p <span class="keyword">ON</span> p.team_member_id <span class="operator">=</span> e.emp_id;</span><br></pre></td></tr></table></figure><h3 id="FULL-OUTER-JOIN"><a href="#FULL-OUTER-JOIN" class="headerlink" title="FULL (OUTER) JOIN"></a><ins><b>FULL (OUTER) JOIN</b></ins></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- FULL JOIN = INNER JOIN + All remaining records from LEFT TABLE (returns null value for any columns fetch)</span></span><br><span class="line"><span class="comment">--                        + ALL remaining records from RIGHT TABLE (returns null value for any columns fetch)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> d.dept_id <span class="operator">=</span> e.dept_id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- INNER JOIN 결과가 출력되고 뒤이어 (department table을 기준으로 RIGHT OUTER JOIN한 결과 - INNER JOIN 결과) + (employee table을 기준으로 LEFT OUTER JOIN한 결과 - INNER JOIN 결과)가 붙어서 출력이 된다.</span></span><br></pre></td></tr></table></figure><h2 id="CROSS-JOIN"><a href="#CROSS-JOIN" class="headerlink" title="CROSS JOIN"></a><ins><b>CROSS JOIN</b></ins></h2><p>특정 기업 정보(기업명, 위치 등)만 가지고 있는 테이블이 있다고 가정하고, 해당 정보를 모든 employee에 출력해야한다면 어떻게 해야될까?<br>바로 CROSS JOIN을 사용하면 모든 employee 정보에 기업 정보들을 붙여서 출력할 수 있다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--CROSS JOIN은 cartesian product를 반환한다.</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e <span class="comment">-- 6 records</span></span><br><span class="line"><span class="keyword">CROSS</span> <span class="keyword">JOIN</span> department d; <span class="comment">-- 4 records</span></span><br><span class="line"><span class="comment">-- 6 records * 4records = 24 records</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name, c.company_name, c.location</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> e.dept_id <span class="operator">=</span> d.dept_id</span><br><span class="line"><span class="keyword">CROSS</span> <span class="keyword">JOIN</span> company c;</span><br></pre></td></tr></table></figure><h2 id="NATURAL-JOIN"><a href="#NATURAL-JOIN" class="headerlink" title="NATURAL JOIN"></a><ins><b>NATURAL JOIN</b></ins></h2><p>SELF JOIN은 INNER JOIN과 같다고 착각할 수 있지만, 그렇지 않다.<br>NATURAL JOIN은 INNER JOIN과 같이 특정 조인 조건을 주지 않아도 같은 COLUMN명을 가진 COLUMN을 서로 JOIN함으로써 INNER JOIN과 같은 결과를 낼 수 있다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">NATURAL</span> <span class="keyword">JOIN</span> department d;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> department RENAME <span class="keyword">COLUMN</span> dept_id <span class="keyword">TO</span> id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- COLUMN이 수정하면, 서로 일치하는 COLUMN이 없기 때문에 NATURAL JOIN이 아닌 CROSS JOIN과 같은 효과를 낸다. </span></span><br><span class="line"><span class="comment">-- NATURAL JOIN은 column name만을 비교해서 JOIN을 해주기 때문에 COLUMN명만 같고 값이 다른 경우에는 문제가 될 수 있기 때문에 권장되지 않는다.</span></span><br></pre></td></tr></table></figure><h2 id="SELF-JOIN"><a href="#SELF-JOIN" class="headerlink" title="SELF JOIN"></a><ins><b>SELF JOIN</b></ins></h2><p>부모 이름과 나이 정보를 그 자식의 이름과 나이 정보와 매칭해서 출력되도록 sql 쿼리를 작성하시오.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> family;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">child.name <span class="keyword">AS</span> child_name, </span><br><span class="line">child.age <span class="keyword">AS</span> child_age, </span><br><span class="line">parent.name <span class="keyword">AS</span> parent_name, </span><br><span class="line">parent.age <span class="keyword">AS</span> parent_age</span><br><span class="line"><span class="keyword">FROM</span> family <span class="keyword">AS</span> child</span><br><span class="line"><span class="keyword">JOIN</span> family <span class="keyword">AS</span> parent <span class="keyword">ON</span> child.parent_id <span class="operator">=</span> parent.member_id</span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT JOIN으로 수정하면, 부모 정보가 없는 child의 정보도 포함된 테이블 정보가 출력된다.</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/SQL/">SQL</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/TIL/">TIL</category>
      
      <category domain="https://leehyungi0622.github.io/tags/SQL/">SQL</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/07/21/202207/220721-sql-joins-study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220721 ANSI SQL &amp; NON ANSI SQL</title>
      <link>https://leehyungi0622.github.io/2022/07/21/202207/220720-sql-study/</link>
      <guid>https://leehyungi0622.github.io/2022/07/21/202207/220720-sql-study/</guid>
      <pubDate>Thu, 21 Jul 2022 04:45:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220721_ansi_non_ansi_sql.png&quot; alt=&quot;ANSI JOIN AND NON-ANSI JOIN&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 ANSI SQL과 NON ANSI SQL의 각기 다른 방식으로 JOIN 쿼리를 작성했을때의 차이점에 대해서 간단하게 포스팅하려고 한다.  &lt;/p&gt;
&lt;h2 id=&quot;ANSI-amp-NON-ANSI-SQL&quot;&gt;&lt;a href=&quot;#ANSI-amp-NON-ANSI-SQL&quot; class=&quot;headerlink&quot; title=&quot;ANSI &amp;amp; NON-ANSI SQL&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;ANSI &amp;amp; NON-ANSI SQL&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;표준 ANSI 방식의 JOIN 쿼리에서는 JOIN 키워드와 ON 절을 사용하여 두 테이블을 합치며, 필터 조건은 WHERE 절에 작성을 해준다.&lt;/p&gt;
&lt;p&gt;아래의 쿼리는 department name이 HR 부서인 employee의 이름과 부서 정보를 출력해주는 쿼리이다.  &lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- ANSI&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- JOIN 키워드를 사용해서 ON clause에서 조인되는 조건을 명시했다면 표준 ANSI 방식&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; e.emp_name, d.dept_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; employee e&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;INNER&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;JOIN&lt;/span&gt; department d &lt;span class=&quot;keyword&quot;&gt;on&lt;/span&gt; d.dept_id &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; e.dept_id&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;WHERE&lt;/span&gt; d.dept_name &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&amp;#x27;HR&amp;#x27;&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- INNER JOIN에서는 ON 절에 AND 필터 조건을 붙여서 작성해도 결과는 같다.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; e.emp_name, d.dept_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; employee e&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;INNER&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;JOIN&lt;/span&gt; department d &lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; d.dept_id &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; e.dept_id &lt;span class=&quot;keyword&quot;&gt;AND&lt;/span&gt; d.dept_name &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&amp;#x27;HR&amp;#x27;&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- 하지만, OUTER JOIN에서는 위와같이 ON 절에 필터조건을 거는 경우 문제가 될 수 있다.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- NON-ANSI&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- JOIN 키워드 대신 ,(comma)를 사용해서 조인 조건을 명시했다면 NON-ANSI 방식&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; e.emp_name, d.dept_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; employee e&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;,department d &lt;span class=&quot;keyword&quot;&gt;WHERE&lt;/span&gt; d.dept_id &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; e.dept_id &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;AND&lt;/span&gt; d.dept_name &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&amp;#x27;HR&amp;#x27;&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;아래의 쿼리는 모든 employee의 이름과 부서 정보를 출력해주는 쿼리이다.&lt;br&gt;모든 employee의 정보를 출력해주기 위해서는 department 테이블에 명기된 부서 정보와 matching되지 않은 employee의 정보도 출력해야 되기 때문에 OUTER JOIN을 해서 모든 employee의 정보를 출력해줘야 한다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220721_ansi_non_ansi_sql.png" alt="ANSI JOIN AND NON-ANSI JOIN"></div><br/><br/><p>이번 포스팅에서는 ANSI SQL과 NON ANSI SQL의 각기 다른 방식으로 JOIN 쿼리를 작성했을때의 차이점에 대해서 간단하게 포스팅하려고 한다.  </p><h2 id="ANSI-amp-NON-ANSI-SQL"><a href="#ANSI-amp-NON-ANSI-SQL" class="headerlink" title="ANSI &amp; NON-ANSI SQL"></a><ins><b>ANSI &amp; NON-ANSI SQL</b></ins></h2><p>표준 ANSI 방식의 JOIN 쿼리에서는 JOIN 키워드와 ON 절을 사용하여 두 테이블을 합치며, 필터 조건은 WHERE 절에 작성을 해준다.</p><p>아래의 쿼리는 department name이 HR 부서인 employee의 이름과 부서 정보를 출력해주는 쿼리이다.  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ANSI</span></span><br><span class="line"><span class="comment">-- JOIN 키워드를 사용해서 ON clause에서 조인되는 조건을 명시했다면 표준 ANSI 방식</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> department d <span class="keyword">on</span> d.dept_id <span class="operator">=</span> e.dept_id</span><br><span class="line"><span class="keyword">WHERE</span> d.dept_name <span class="operator">=</span> <span class="string">&#x27;HR&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- INNER JOIN에서는 ON 절에 AND 필터 조건을 붙여서 작성해도 결과는 같다.</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> d.dept_id <span class="operator">=</span> e.dept_id <span class="keyword">AND</span> d.dept_name <span class="operator">=</span> <span class="string">&#x27;HR&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 하지만, OUTER JOIN에서는 위와같이 ON 절에 필터조건을 거는 경우 문제가 될 수 있다.</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- NON-ANSI</span></span><br><span class="line"><span class="comment">-- JOIN 키워드 대신 ,(comma)를 사용해서 조인 조건을 명시했다면 NON-ANSI 방식</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line">,department d <span class="keyword">WHERE</span> d.dept_id <span class="operator">=</span> e.dept_id </span><br><span class="line"><span class="keyword">AND</span> d.dept_name <span class="operator">=</span> <span class="string">&#x27;HR&#x27;</span>;</span><br></pre></td></tr></table></figure><p>아래의 쿼리는 모든 employee의 이름과 부서 정보를 출력해주는 쿼리이다.<br>모든 employee의 정보를 출력해주기 위해서는 department 테이블에 명기된 부서 정보와 matching되지 않은 employee의 정보도 출력해야 되기 때문에 OUTER JOIN을 해서 모든 employee의 정보를 출력해줘야 한다.</p><a id="more"></a><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ANSI</span></span><br><span class="line"><span class="comment">-- JOIN 키워드를 사용해서 ON clause에서 조인되는 조건을 명시했다면 표준 ANSI 방식</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> department d <span class="keyword">on</span> d.dept_id <span class="operator">=</span> e.dept_id</span><br><span class="line"><span class="keyword">WHERE</span> d.dept_name <span class="operator">=</span> <span class="string">&#x27;HR&#x27;</span>;</span><br></pre></td></tr></table></figure><p>ANSI 방식의 SQL 쿼리에서 LEFT OUTER JOIN을 할때에는 OUTER를 생략하여 LEFT JOIN으로 작성할 수도 있다. 왼쪽 테이블을 기준으로 OUTER JOIN을 하기 때문에 오른쪽 부서 정보 테이블이 NULL인 경우도 모두 포함하여 모든 employee의 정보를 출력한다.</p><p>ANSI 방식의 JOIN에서는 간단하게 JOIN 키워드만 변경을 하면, FULL OUTER JOIN, LEFT OUTER JOIN, RIGHT OUTER JOIN 등을 손쉽게 할 수 있다.</p><p>하지만 NON-ANSI 방식의 쿼리문에서는 약간 까다로워지는 경우가 생긴다. NON-ANSI 방식으로 LEFT (OUTER) JOIN을 하기 위해서는 ORACLE DB에서 작성을 해야한다. ORACLE DB에서는 <code>(+)</code> symbol을 사용해서 OUTER JOIN 쿼리를 작성하게 되는데, (+) symbol은 조인하고자 하는 기준이 되는 열의 반대 열에 붙이도록 한다.<br>PostgreSQL에서는 NON-ANSI 방식을 사용해서 내부 조인은 가능하지만, 외부 조인은 할 수 없고, MS-SQL, MYSQL도 마찬가지다. (<code>단, ORACLE에서만 NON-ANSI 방식으로 OUTER JOIN이 가능하다</code>)  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- NON-ANSI / LEFT OUTER JOIN</span></span><br><span class="line"><span class="comment">-- 모든 employee를 출력하기 위해서 employee를 기준으로 OUTER JOIN을 해야되기 때문에 department의 조인 조건 절에 (+) symbol을 붙인다.</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line">,    department d</span><br><span class="line"><span class="keyword">WHERE</span> d.dept_id(<span class="operator">+</span>) <span class="operator">=</span> e.dept_id</span><br><span class="line"></span><br><span class="line"><span class="comment">-- RIGHT OUTER JOIN</span></span><br><span class="line"><span class="comment">-- 모든 department를 출력(Employee 정보가 NULL인 경우도 포함)</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line">,    department d</span><br><span class="line"><span class="keyword">WHERE</span> d.dept_id <span class="operator">=</span> e.dept_id(<span class="operator">+</span>)</span><br></pre></td></tr></table></figure><h2 id="ANSI-SQL이-NON-ANSI-SQL-보다-좋은-점"><a href="#ANSI-SQL이-NON-ANSI-SQL-보다-좋은-점" class="headerlink" title="ANSI SQL이 NON-ANSI SQL 보다 좋은 점"></a><ins><b>ANSI SQL이 NON-ANSI SQL 보다 좋은 점</b></ins></h2><h3 id="1-쿼리문이-더-짧고-간결해지며-가독성이-좋아지고-디버깅하기-쉬워진다"><a href="#1-쿼리문이-더-짧고-간결해지며-가독성이-좋아지고-디버깅하기-쉬워진다" class="headerlink" title="1. 쿼리문이 더 짧고 간결해지며, 가독성이 좋아지고 디버깅하기 쉬워진다."></a><strong>1. 쿼리문이 더 짧고 간결해지며, 가독성이 좋아지고 디버깅하기 쉬워진다.</strong></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ANSI </span></span><br><span class="line"><span class="comment">-- FROM 절의 테이블과 JOIN 키워드 우측의 테이블을 각 각 왼쪽 오른쪽 테이블로 정의한다.</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name, m.manager_name, p.project_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> department d <span class="keyword">on</span> d.dept_id <span class="operator">=</span> e.dept_id</span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> manager m <span class="keyword">on</span> m.manager_id <span class="operator">=</span> e.manager_id</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> projects p <span class="keyword">on</span> p.team_member_id <span class="operator">=</span> e.emp_id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- NON-ANSI (ORACLE)</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name, m.manager_name, p.project_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line">,    department d </span><br><span class="line">,    manager m</span><br><span class="line">,    projects p</span><br><span class="line"><span class="keyword">WHERE</span> d.dept_id (<span class="operator">+</span>) <span class="operator">=</span> e.dept_id</span><br><span class="line"><span class="keyword">AND</span> m.manager_id <span class="operator">=</span> e.manager_id (<span class="operator">+</span>)</span><br><span class="line"><span class="keyword">AND</span> p.team_member_id (<span class="operator">+</span>) <span class="operator">=</span> e.emp_id;</span><br></pre></td></tr></table></figure><h3 id="2-JOIN-조건과-FILTER-조건을-서로-분리할-수-있다-ON-절-JOIN-조건-FILTER-조건-WHERE-조건"><a href="#2-JOIN-조건과-FILTER-조건을-서로-분리할-수-있다-ON-절-JOIN-조건-FILTER-조건-WHERE-조건" class="headerlink" title="2. JOIN 조건과 FILTER 조건을 서로 분리할 수 있다.(ON 절 = JOIN 조건 / FILTER 조건 = WHERE 조건)"></a><strong>2. JOIN 조건과 FILTER 조건을 서로 분리할 수 있다.(ON 절 = JOIN 조건 / FILTER 조건 = WHERE 조건)</strong></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 부서명이 HR인 경우만 emp_name을 출력</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> e.dept_id <span class="operator">=</span> d.dept_id</span><br><span class="line"><span class="keyword">WHERE</span> d.dept_name <span class="operator">=</span> <span class="string">&#x27;HR&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 부서명이 HR인 경우만 dept_name을 표기하고, 나머지는 null로 채워서 출력</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> department d <span class="keyword">ON</span> e.dept_id <span class="operator">=</span> d.dept_id <span class="keyword">AND</span> d.dept_name <span class="operator">=</span> <span class="string">&#x27;HR&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="3-우연한-CROSS-JOINS을-피할-수-있다"><a href="#3-우연한-CROSS-JOINS을-피할-수-있다" class="headerlink" title="3. 우연한 CROSS JOINS을 피할 수 있다."></a><strong>3. 우연한 CROSS JOINS을 피할 수 있다.</strong></h3><p>두 개의 테이블을 특정 조인 조건 없이 합치는 경우, CROSS JOIN이 발생할 수 있다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- NON-ANSI (ORACLE)</span></span><br><span class="line"><span class="keyword">SELECT</span> e.emp_name, d.dept_name, m.manager_name, p.project_name</span><br><span class="line"><span class="keyword">FROM</span> employee e</span><br><span class="line">,    department d </span><br><span class="line">,    manager m</span><br><span class="line">,    projects p</span><br><span class="line"><span class="keyword">WHERE</span> d.dept_id (<span class="operator">+</span>) <span class="operator">=</span> e.dept_id</span><br><span class="line"><span class="comment">-- AND m.manager_id = e.manager_id (+)</span></span><br><span class="line"><span class="keyword">AND</span> p.team_member_id (<span class="operator">+</span>) <span class="operator">=</span> e.emp_id;</span><br></pre></td></tr></table></figure><p>ANSI SQL로 작성을 하는 경우에는 ON 절에서 조인 조건을 빼먹고 쓰지 않은 경우에 별도의 에러 메시지를 통해 문제를 해결 할 수 있다.</p><h3 id="4-ANSI는-모든-RDBMS와-시스템에서-범용적으로-사용-가능한다"><a href="#4-ANSI는-모든-RDBMS와-시스템에서-범용적으로-사용-가능한다" class="headerlink" title="4. ANSI는 모든 RDBMS와 시스템에서 범용적으로 사용 가능한다."></a><strong>4. ANSI는 모든 RDBMS와 시스템에서 범용적으로 사용 가능한다.</strong></h3>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/SQL/">SQL</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/TIL/">TIL</category>
      
      <category domain="https://leehyungi0622.github.io/tags/SQL/">SQL</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/07/21/202207/220720-sql-study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220703 데이터 파이프라인 구축 오프라인 수업 / 6주차</title>
      <link>https://leehyungi0622.github.io/2022/07/03/202207/220703_datapipeline_study/</link>
      <guid>https://leehyungi0622.github.io/2022/07/03/202207/220703_datapipeline_study/</guid>
      <pubDate>Sun, 03 Jul 2022 03:40:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220530_review.png&quot; alt=&quot;Review&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 여섯 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다.&lt;/p&gt;
&lt;p&gt;이번 여섯 번째 수업을 마지막으로 데이터 파이프라인 구축과 관련한 데이터 엔지니어링 수업이 마무리되었다. 이번 수업을 통해 정말 많은 것들을 배울 수 있었다. 특히 이전에는 클라우드 플랫폼을 활용해서 데이터 파이프라인을 구축하는 것이 전부라고 생각했었지만, 이번 수업을 듣고나서 &lt;code&gt;관리 및 운영의 관점&lt;/code&gt;에서 클라우드 플랫폼에서 제공하는 서비스들의 근간이 되는 &lt;code&gt;오픈 소스 프로젝트의 세부 동작원리에 대해서 이해하는 것이 더 중요&lt;/code&gt;하다는 것을 배웠다. 그리고 클라우드 플랫폼을 활용해서 데이터 파이프라인을 처음 구축했을때 그 다음 스탭으로 어떤 식으로 공부를 이어나갈지 감을 잡지 못했었는데, 이번 총 6번의 수업동안 (6주간 진행)앞으로 어떻게 더 공부를 해야되는지, 그리고 새로운 기술스택이 나왔을때 어떤식으로 학습을 이어나가야 되는지에 대해서 알게 되었다.&lt;br&gt;이번 마지막 수업을 마무리하며 강사님이 어느 데이터 파이프라인 구축에 있어, 어느 파이프라인 구성이 정답이고 그런 건 없다고 하셨다. 그리고 수업을 들으면서 느낀 것은 정말 하나의 파이프라인을 구성할때에도 많은 것들을 고려해야하며, 파이프라인의 각 구성 요소들의 특징들을 제대로 이해하고 있어야 비로소 효율적인 파이프라인을 구축할 수 있다는 것을 배웠다. 아무튼 이번 수업을 통해 좀 더 데이터 엔지니어의 업무 중 하나인 데이터 파이프라인 구축에 대해서 좀 더 심도있게 배울 수 있었던 것 같다. &lt;/p&gt;
&lt;p&gt;매주 한 번 강남역에 가서 세 시간씩 하루도 빠지지 않고 수업에 참여하고, 배운 내용을 블로그에 하나도 빠뜨리지 않고 기록하였다. 이런 나에게 칭찬을 하며, 마지막 수업시간에 배운 내용을 정리해보려고 한다.&lt;/p&gt;
&lt;h2 id=&quot;ElasticSearch&quot;&gt;&lt;a href=&quot;#ElasticSearch&quot; class=&quot;headerlink&quot; title=&quot;ElasticSearch&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;ElasticSearch&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;  ES를 NoSQL DB와 같은 저장소로써 사용을 하면서 저장된 데이터를 검색하는 용도로 사용된다. 그리고 주로 &lt;code&gt;모니터링이나 로깅과 같은 용도로 많이 사용&lt;/code&gt;된다. 메트릭 같은 것을 JSON에 같이 담아서 Kibana를 통해서 그래프로 그려주면, 프로메테우스와 같은 TS DB와 같은 성능은 내지 못하지만, 대시보드로 충분히 그려서 활용할 수 있다. 데이터가 적은 경우에는 앞에서 설명한 것과 같이 ElasticSearch에 저장된 데이터를 Kibana를 활용해서 시각화를 해줄 수 있지만, 이러한 메트릭 값들이 많아지면, TS(Time Series) 전용으로 담아두는 TS DB를 생성해서 관리한다.&lt;br&gt;  이처럼 ES는 검색엔진이지만, 다양한 용도로 사용이 되고 있다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220530_review.png" alt="Review"></div><br/><br/><p>이번 포스팅에서는 여섯 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다.</p><p>이번 여섯 번째 수업을 마지막으로 데이터 파이프라인 구축과 관련한 데이터 엔지니어링 수업이 마무리되었다. 이번 수업을 통해 정말 많은 것들을 배울 수 있었다. 특히 이전에는 클라우드 플랫폼을 활용해서 데이터 파이프라인을 구축하는 것이 전부라고 생각했었지만, 이번 수업을 듣고나서 <code>관리 및 운영의 관점</code>에서 클라우드 플랫폼에서 제공하는 서비스들의 근간이 되는 <code>오픈 소스 프로젝트의 세부 동작원리에 대해서 이해하는 것이 더 중요</code>하다는 것을 배웠다. 그리고 클라우드 플랫폼을 활용해서 데이터 파이프라인을 처음 구축했을때 그 다음 스탭으로 어떤 식으로 공부를 이어나갈지 감을 잡지 못했었는데, 이번 총 6번의 수업동안 (6주간 진행)앞으로 어떻게 더 공부를 해야되는지, 그리고 새로운 기술스택이 나왔을때 어떤식으로 학습을 이어나가야 되는지에 대해서 알게 되었다.<br>이번 마지막 수업을 마무리하며 강사님이 어느 데이터 파이프라인 구축에 있어, 어느 파이프라인 구성이 정답이고 그런 건 없다고 하셨다. 그리고 수업을 들으면서 느낀 것은 정말 하나의 파이프라인을 구성할때에도 많은 것들을 고려해야하며, 파이프라인의 각 구성 요소들의 특징들을 제대로 이해하고 있어야 비로소 효율적인 파이프라인을 구축할 수 있다는 것을 배웠다. 아무튼 이번 수업을 통해 좀 더 데이터 엔지니어의 업무 중 하나인 데이터 파이프라인 구축에 대해서 좀 더 심도있게 배울 수 있었던 것 같다. </p><p>매주 한 번 강남역에 가서 세 시간씩 하루도 빠지지 않고 수업에 참여하고, 배운 내용을 블로그에 하나도 빠뜨리지 않고 기록하였다. 이런 나에게 칭찬을 하며, 마지막 수업시간에 배운 내용을 정리해보려고 한다.</p><h2 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a><ins><b>ElasticSearch</b></ins></h2><p>  ES를 NoSQL DB와 같은 저장소로써 사용을 하면서 저장된 데이터를 검색하는 용도로 사용된다. 그리고 주로 <code>모니터링이나 로깅과 같은 용도로 많이 사용</code>된다. 메트릭 같은 것을 JSON에 같이 담아서 Kibana를 통해서 그래프로 그려주면, 프로메테우스와 같은 TS DB와 같은 성능은 내지 못하지만, 대시보드로 충분히 그려서 활용할 수 있다. 데이터가 적은 경우에는 앞에서 설명한 것과 같이 ElasticSearch에 저장된 데이터를 Kibana를 활용해서 시각화를 해줄 수 있지만, 이러한 메트릭 값들이 많아지면, TS(Time Series) 전용으로 담아두는 TS DB를 생성해서 관리한다.<br>  이처럼 ES는 검색엔진이지만, 다양한 용도로 사용이 되고 있다.</p><a id="more"></a><ul><li><h3 id="메트릭"><a href="#메트릭" class="headerlink" title="메트릭?"></a><strong>메트릭?</strong></h3><p><code>메트릭이란 타임스탬프와 보통 한 두 가지 숫자 값을 포함하는 이벤트</code>이다. 이 메트릭은 모든 메트릭의 행이 타임스탬프로 정렬된 메트릭 파일에 순차적으로 추가된다.<br>로그와는 달리 메트릭은 주기적으로 보내게 되며, 로그는 보통 무언가가 발생했을때 로그 파일에 추가되는 형식으로 동작한다.<br>메트릭은 종종 리소스 사용 모니터링, 데이터베이스 실행 메트릭 모니터링 등 소프트웨어나 하드웨어의 상태 모니터링 맥락에서 사용되는 것이 일반적이다.</p><p>Elastic은 솔루션의 모든 계층에서 메트릭 관리와 분석에 대한 사용자 경험을 증진하는 새로운 기능을 제공한다.  Matricbeat는 5.0의 새로운 기능 중 하나이며, 사용자가 머신이나 어플리케이션에서 ElasticSearch로 메트릭 데이터를 전달할 수도 있고, Kibana에서 바로 사용 가능한 대시보드를 제공한다. Kibana는 메트릭 같은 숫자 데이터를 다룰 수 있게 설계한 타임라인온 플러그인도 코어에 연동해서 사용할 수 있다. </p></li><li><h3 id="TSDB-Time-Series-Database"><a href="#TSDB-Time-Series-Database" class="headerlink" title="TSDB(Time Series Database)?"></a><strong>TSDB(Time Series Database)?</strong></h3><p>TSDB는 Time-Stamped Data라고도 불리며, 시간에 따라 저장된 데이터를 의미한다. 시계열 데이터는 동일한 소스로부터 시간이 지남에 따라 만들어진 데이터들로 구성이 되기 때문에 시간 경과에 따른 변화를 추적하는데 용이하다. </p><p><code>ex)</code> 어떤 집안의 온도들로부터 경제 지표, 환자의 심작 박동수나 회사의 주가, 서버로부터 기록되는 히스토리성 데이터, 센서 데이터</p><p>TSDB란 시계열 데이터를 처리하기 위해 최적화된 데이터베이스로, 빠르고 정확하게 실시간으로 쌓이는 대규모 데이터들을 처리할 수 있도록 고안되었다. TSDB는 데이터들과 시간이 함께 저장하는데, 이를 통해 시간의 흐름에 따라 데이터를 분석하기에 매우 용이하다. 오늘날 엄청나게 많은 데이터들이 수집이 되고 있고, 인공지능의 급격한 발달로 필요한 데이터들의 양이 급증하고 있다. 이러한 데이터들을 처리하기에는 관계형 데이터베이스와 NoSQL로는 한계가 있기 때문에 거의 끝없는 데이터들을 처리할 수 있는 TSDB, 시계열 데이터의 중요성이 대두되고 있다.</p><p><code>ref)</code> 자율주행과 같은 경우, 약 8시간 운전할 때마다 40TB의 데이터를 만들고 사용한다.</p></li></ul><h2 id="Amazon-QuickSight"><a href="#Amazon-QuickSight" class="headerlink" title="Amazon QuickSight"></a><ins><b>Amazon QuickSight</b></ins></h2><p>  Amazon QuickSight는 Amazon에서 제공하는 BI툴로, AWS 계정이 별도로 요구되지 않기 때문에 손쉽게 다른 사람들과 구성한 Dashboard를 공유할 수 있다. BI의 영역이 Engineer와 Scientist 간의 중첩 영역(협업 영역)으로 데이터 엔지니어가 데이터를 잘 정제해서 서빙을 해주면, 서빙된 데이터를 DA와 DS들이 붙어서 시각화 및 분석을 하기 때문이다.<br>  이러한 BI 툴은 규모가 점점 커지는 기업의 경우에는 FE개발자가 붙어서 전용 dashboard를 만들기도 한다고 한다. 이러한 FE 작업을 줄여주기 위해서 BI툴을 사용하기도 한다.<br>  그리고 AWS QuickSight의 최고 장점은 다양한 AWS의 여러 데이터 소스를 기반으로 BI툴을 활용할 수 있다.</p><p>  <code>(1) QuickSight demo</code> : <a href="https://democentral.learnquicksight.online/">https://democentral.learnquicksight.online/</a><br>  비 IT 부서에 차트로 시각화해서 정보를 제공할때 사용이 되며, 생성된 dashboard에 접근하기 위한 개별 계정을 부여할 수 있다.</p><p>  <code>(2) QuickSight workshop</code> : <a href="https://catalog.us-east-1.prod.workshops.aws/workshops/ac8dc849-3d95-43e3-b380-aa38f0dc31e4/en-US/author-workshop/1-build-your-first-dashboard">https://catalog.us-east-1.prod.workshops.aws/workshops/ac8dc849-3d95-43e3-b380-aa38f0dc31e4/en-US/author-workshop/1-build-your-first-dashboard</a></p><p>  S3의 데이터를 조회할때 자체 성능을 위해서 Partitioning이 필요하지만 엄청난 양의 데이터를 쌓을 수 있다.그리고 Amazon QuickSight는 Third party platform, On-premise DB 등 다양한 데이터 리소스를 연결해서 BI툴을 활용할 수 있다.<br>  <br/><br>  이번 실습에서는 간단하게 CloudFormation에서 랜덤한 로그 데이터를 생성해주고, 적재된 데이터를 기준으로 QuickSight에서 시각화를 해주었는데, 실제 업무에서는 데이터가 실시간으로 계속 쌓이고 있기 때문에 신규 스트리밍 데이터의 경우에는 Athena에서 계속 조회를 해서 가져와야한다. 따라서 별도로 배치를 돌려서 일률적으로 데이터를 업데이트해줘야한다. </p><p>  <code>참고)</code> 실제 hive, Presto에서도 MKCK repair update를 해서 메타 데이터를 업데이트해서 새로운 데이터를 가져오는데, QuickSight에서는 데이터 세트 메뉴에서 가져온 데이터를 조회하는 UI에서 <code>&quot;지금 새로 고침&quot;</code> 또는 <code>&quot;새로 고침 예약&quot;</code>을 통해 기존 데이터를 새로운 데이터로 업데이트해줄 수 있다.<br>  ES의 경우에는 새로운 데이터가 생기는 즉시 ES가 indexing을 통해 데이터를 업데이트해주기 때문에 실시간으로 Kibana를 통해 상시 업데이트된 시각화 정보를 보여줄 수 있다.<br>  하지만 BI 툴의 경우, 비즈니스 용도로 월별, 연도별로 데이터를 분류하여 시각화를 해주기 때문에 이러한 데이터 새로고침이 ES에 비해 좀 떨어지는 편이다.<br>  <br/></p><p>  QuickSight에서 업데이트된 데이터 세트를 refresh해주기 위해서 데이터 세트 메뉴에서 일정 생성을 통해서 <code>scheduling하는 기능을 추가</code>해줘야 한다. (<code>일정 기간 단위 실행</code>)<br>  이러한 QuickSight의 dashboard를 활용하여 CS(Customer Satisfaction)부서 내에 근무하는 직원이 현재 고객응대 현황을 그래프로 확인을 할 수도 있고, 개발자가 현재 고객들이 어떠한 문제로 연락을 하고 있는지 현황을 파악하는 용도로도 활용될 수 있다. 그리고 <code>QuickSight는 외부 서비스로써, 따로 조회만 가능한 계정을 발급</code>해 줄 수도 있다.<br>  실제 현업에서는 ES가 많이 사용되지만, QuickSight도 ES를 대체해서 사용할 수 있다.<br>  <br/></p><p>  그리고 대용량 데이터의 경우에는 Lambda로 데이터 변환작업을 하기 어려운 경우도 있기 때문에  Lambda를 Athena를 호출하는 용도로만 해서 작업을 할 수도 있고, Redshift에 데이터를 담아서 쿼리를 날리는 형태로 처리할 수도 있다.<br>  <br/></p><ul><li><h3 id="추가-공부"><a href="#추가-공부" class="headerlink" title="[추가 공부]"></a><strong>[추가 공부]</strong></h3><p>추가적으로 단일 파이프라인이 아닌, job들이 복잡해지는 경우에는 데이터가 쌓인 후에 특정 파이프라인을 실행하고, 해당 파이프라인의 처리가 성공한 경우에는 A 파이프라인으로, 실패한 경우에는 B 파이프라인으로 처리할 수 있도록 dependencies를 처리해야되는 경우도 생긴다.<br>이러한 복잡한 job에 대한 처리는 EventBridge로는 부족할 수 있기 때문에 Airflow나 아르고 플로우와 같은 스케쥴링 엔진을 적용해서 스케줄링 작업을 해줘야한다.</p><p><code>(참고)</code> SQS와 같은 큐 서비스를 사용해서 job을 분류해서 처리할 수도 있다.</p></li><li><h3 id="실습-Amazon-QuickSight"><a href="#실습-Amazon-QuickSight" class="headerlink" title="[실습] Amazon QuickSight"></a><strong>[실습] Amazon QuickSight</strong></h3><p>우선 Amazon QuickSight에 sample csv 파일을 import하여 x-axis, y-axis의 기준 칼럼을 변경해가면서 데이터를 그래프로 시각화해보았다.<br>Amazon QuickSight에서는 <code>이상 감지(Anormaly Insight)</code>를 지원하여, 그래프상에서 값이 갑자기 튀는 경우에 해당 부분에 대해 ML이 이를 분석하여 글로써 서술해준다. 그 외에 추가적으로 <code>Add forecast</code>를 사용하여 앞으로의 값 전망에 대해서 그래프상에서 확인할 수 있다. </p></li></ul><h2 id="마무리-통합-파이프라인-구축"><a href="#마무리-통합-파이프라인-구축" class="headerlink" title="마무리 통합 파이프라인 구축"></a><ins><b>마무리 통합 파이프라인 구축</b></ins></h2><p>  이번 통합 파이프라인 구축 실습에서는 여지까지 실습했을때 활용되었던 Amazon CloudFormation, Kinesis data stream, Kinesis data firehose, S3, Lambda를 종합해서 실습하였다.<br>  Kinesis data stream 부분을 직접 Kafka를 구축하거나, AWS에서 제공해주는 MSK를 사용하면 데이터 스트림 부분을 대체해서 구성할 수 있다. Kinesis client library를 이용해서 변환하는 로직을 추가해서 처리하거나 plugin을 활용해서 데이터를 변환할때에는 logstash를 연결해서 구성을 할 수도 있다.그리고 Firehose를 거치지 않고, 바로 ES쪽으로 데이터를 바로 적재하는 것이 가능하다.</p><p>  우선 전체적인 데이터 파이프라인의 flow는 우선 AWS의 CloudFormation을 통해 랜덤한 dummy data를 생성해주고, 생성된 데이터를 Kinesis data stream -&gt; Kinesis data firehose를 통해 S3 bucket에 데이터를 최종적으로 적재를 해준다. 적재된 데이터는 Athena를 통해서 쿼리를 사용해서 데이터를 필터하는 작업을 진행하도록 했다. (firehose에서 data transformation 옵션을 enable해서 생성해준 Lambda 함수를 넣어서 한 번 필터된 데이터를 S3에 저장할 수 있도록 구성하였다.(<code>Lambda에서 제공해주는 blueprint template code활용 - 커스텀해서 작성해보기</code>))</p><p>  <code>참고</code> : Lambda 함수를 설정할때 timeout에러가 발생하는 이유는 Firehose와 Lambda 함수의 time interval sync가 맞지 않아서이다. Lambda function에서 time setup에 대한 configuration을 수정해줘야한다.</p><p>  S3에 적재된 데이터를 Athena를 통해서 테이블을 생성해서 쿼리로 분석을 하고, 분석 결과 데이터를 QuickSight를 통해서 시각화 할 수 있다. S3에 적재된 데이터를 direct로 QuickSight와 연동하는 작업도 해보기.</p><h2 id="Q-amp-A-관련-내용-정리"><a href="#Q-amp-A-관련-내용-정리" class="headerlink" title="Q&amp;A 관련 내용 정리"></a><ins><b>Q&amp;A 관련 내용 정리</b></ins></h2><ul><li><p><strong>Q1. 데이터 엔지니어의 업무와 협업 방식</strong></p><p>회사 규모가 커질 수록 데이터 엔지니어나 데이터 플랫폼 엔지니어로 직무를 구분하기도 한다.<br>데이터 플랫폼 엔지니어의 경우에는 Presto나 AWS 사용하는 것을 좀 더 손쉽게 인프라를 구축해주는 작업을 하기도 하고, Presto나 Hive와 같은 인프라들을 관리해주는 플랫폼 개발자/엔지니어로 근무를 하기도 한다.그리고 데이터 엔지니어의 경우에는 SQL을 Presto나 Hive에서 돌려서 데이터를 뽑아내는 작업만을 하는 경우도 있다. </p><p>Spark job을 말아서 Airflow를 통해 스케줄링 작업을 해주고, CI/CD 배포하듯이 구축을 해주는 업무도 있다.</p><p>협업의 경우, 데이터 플랫폼 개발자의 경우에는 데이터 엔지니어들이 쓸 수 있는 플랫폼/툴을 개발하는 업무를 주로 하기 때문에 SW engineer와 DE가 서로 협업을 하는 경우도 있다. 그리고 다른 경우에는 DE가 DA나 DS와 같이 협업을 하는 경우도 있는데, DA/DS 분들이 특정 데이터들이 필요하다고 했을때, 분산되어있는 요구된 여러 데이터들을 spark를 사용해서 join 작업을 해주기도 한다. </p></li><li><p><strong>사이드 프로젝트</strong></p><p>활용할 수 있는 데이터 : 공공 데이터, Uber 데이터 활용하기(미국 정부에서 관리해주는 사이트 참고)</p><p>-&gt; 배웠던 것 복습한다는 의미로 여지까지 공부했던 파이프라인 구성요소들을 활용해서 직접 나만의 파이프라인을 만들어 보고, 구축한 후에는 데이터를 분석해보는 사이드 프로젝트 진행해보면 좋다.<br>파이프라인을 구축했을때 왜 A라는 기술을 사용했는지와 왜 이런식으로 구성했는지에 대한 이유를 설명해주는 것도 좋다.</p></li><li><p><strong>분석 단계의 전처리와 데이터 엔지니어링 단계에서의 전처리의 차이</strong></p><p>분석 단계의 전처리에서는 데이터가 잘못 들어가 있는 경우나, 널 값이 들어가 있는 경우에 대한 데이터 전처리 작업을 한다. 데이터 엔지니어링 단계의 전처리의 경우에도 분석가가 특정 데이터 전처리 요구를 하는 경우에는 파이프라인 상에 로직을 심어서 처리를 추가해줄 수도 있다. (<code>DA와의 협업</code>)<br>일반적으로 데이터 엔지니어링 단계에서의 전처리는 format이나 schema와 같은 변환작업을 주로 해주고, JSON, TEXT 데이터를 Parquet, ORC 포멧으로 변경하여 데이터 엔지니어링 관점에서 효율(속도/성능)을 위한 작업을 많이 해주게 된다.</p></li><li><p><strong>DM는 언제 구축해야되는지와 DW는 어느정도 규모일때 구축을 하는지</strong></p><p>만약에 인사팀 데이터들은 타 부서 사람들이 조회하면 안되기 때문에 이러한 경우에 별도의 DM을 구축해서 관리를 하기도 한다.<br>보통은 효율성을 위해서 빠르게 데이터를 조회하기 위해서 DW에 쌓인 데이터를 각 각의 개별  SCHEMA를 가진 DM으로 쌓아서 관리를 한다. (조직의 정책에 따라)</p><p>많은 경우에는 DW만 구축을 해주고, 새롭게 테이블만 구축해서 접근 권한만 다르게 부여해서 활용되는 경우도 많다.  </p></li><li><p><strong>VPC 활용 및 나누는 방법</strong></p><p>VPC는 네트워크나 보안 정책에 따라 구분된다. 이 부분은 데이터 엔지니어의 영역이 아닌, Cloud Architecture나 네트워크 엔지니어의 영역이다.<br>글로벌 기업의 경우에는 해외 각국에 지사가 나뉘어져 있기 때문에 국가별로 VPC로 나눠서 각 VPC를 peering해주기도 한다. (<code>회사 정책별로 상이</code>)</p></li></ul><h2 id="기업사례"><a href="#기업사례" class="headerlink" title="기업사례"></a><ins><b>기업사례</b></ins></h2><ul><li><h2 id="AWS를-활용하여-Lambda-Architecture-구축"><a href="#AWS를-활용하여-Lambda-Architecture-구축" class="headerlink" title="AWS를 활용하여 Lambda Architecture 구축"></a>AWS를 활용하여 Lambda Architecture 구축</h2><div align="center">  <img src="/images/post_images/220706_lambda-architecure-on-for-batch-aws.png" alt="Lambda Architecture 구축(AWS 공식 사이트 제공)"></div><p>이 Lambda Architecture는 오프라인 수업 초반에 배웠듯이 Batch 데이터 처리와 Streaming 데이터 처리의 장/단점을 서로 보완해주기 위해 등장한 모델이다.<br>위의 그림에서 보면, Batch 처리에서 S3에 데이터를 적재해주고, S3에 적재된 데이터를 metadata를 저장하고 있는 AWS Glue를 통해 Athena에서 데이터를 분석하고 있다. 또한 처음에 S3 bucket에 데이터가 적재되었을때, ETL 작업을 통해 또 다른 Batch View S3 Bucket에 데이터를 다시 저장하고 있다.</p><br/><p>Streaming data의 경우에는 Kinesis Stream을 통해서 데이터를 넘겨주고 있고, 넘겨진 데이터를 AWS Lambda를 통해 데이터 변환을 한 다음에 Kinesis Firehose를 통해 Kinesis Analytics로 이동해서 분석 환경을 구축하기도 하고 S3에 데이터를 적재해주기도 한다.</p><br/><p>중단부를 보면, Batch 처리를 통해 적재된 Batch View S3 bucket과 Kinesis stream을 통해 바로 넘겨진 데이터를 Amazon EMR을 통해 데이터를 통합해서 통합된 데이터를 Merged View S3 bucket에 저장을 하고 있다.</p><br/><ul><li><h3 id="AWS-Glue"><a href="#AWS-Glue" class="headerlink" title="AWS Glue"></a><strong>AWS Glue</strong></h3><p>Athena를 사용해서 테이블을 만들게 되면, 필드를 일일이 지정을 해줘야 했지만, Glue Crawler를 이용하면 데이터를 자동으로 parsing해서 테이블로 만들어준다. 그리고 AWS Glue에는 AWS Glue Catalog 기능이라고 해서 내가 가지고 있는 데이터가 어떤 field나 schema를 가지고 있는지, 전체 메타 정보를 가지고 있다.<br>AWS Glue Catalog에서 직접적으로 pySpark를 통해서 분석을 할 수도 있다. </p></li><li><h3 id="HDFS-vs-S3-사용"><a href="#HDFS-vs-S3-사용" class="headerlink" title="HDFS vs S3 사용"></a>HDFS vs S3 사용</h3><p>S3는 모든 AWS의 각종 서비스들을 연결시켜주는 anchor point의 역할을 해주기도 한다. 그리고 데이터 governance를 S3에 걸 수 있기 때문에 데이터의 저장용도로 S3를 주로 활용한다. (<code>S3는 AWS의 Killer App이다</code>)<br>REDSHIFT에 데이터를 쌓아도 되는데, S3에 쌓아놓고 REDSHIFT SPECTRUM에 저장을 할 수 있기 때문에 이러한 S3의 확장 가능성때문에 주로 사용이 된다. </p></li></ul><br/></li><li><h2 id="Uber-data-architecture"><a href="#Uber-data-architecture" class="headerlink" title="Uber data architecture"></a>Uber data architecture</h2><p>[참고] : <a href="https://eng.uber.com/uber-big-data-platform/">https://eng.uber.com/uber-big-data-platform/</a></p><p>우버에서 데이터 아키텍처 구조의 변천사에 대해서 살펴볼 수 있다. 각 아키텍처의 구조를 변경했을때 한계점에 대해서도 위의 페이지에서 명기를하고 있으며, 2015-2016년에 들어서 하둡 에코 시스템을 도입한 내용도 유익하기 때문에 한 번 읽어보기를 권장한다.</p><p>2017-현재까지 Kafka를 사용해서 데이터를 처리함으로써 이전 하둡 시스템을 사용했을때와 비교했을때 데이터 처리의 민첩성을 향상시켰다.(<code>이전 24h(end to end)에서 30min 미만 raw data, 1시간 미만으로 modeling되도록 개선</code>)<br>주로 Batch 처리를 하지 않고 Streaming 처리를 하는 이유는 latency를 개선하기 위해서이다. </p><br/><ul><li><h3 id="CDC-Change-Data-Capture"><a href="#CDC-Change-Data-Capture" class="headerlink" title="CDC (Change Data Capture)"></a>CDC (Change Data Capture)</h3><p>CDC를 활용해서 DB(RDBMS, Key-Val DB)상에서 변화가 감지되었을때 Kafka에 데이터를 먹일 수도 있다.</p><p>[참고] <a href="https://www.youtube.com/watch?v=T6PAcWtoHTo">https://www.youtube.com/watch?v=T6PAcWtoHTo</a></p></li></ul><br/></li><li><h2 id="Twitter"><a href="#Twitter" class="headerlink" title="Twitter"></a>Twitter</h2><p>[참고] : <a href="https://blog.twitter.com/engineering/en_us/topics/infrastructure/2021/processing-billions-of-events-in-real-time-at-twitter-">https://blog.twitter.com/engineering/en_us/topics/infrastructure/2021/processing-billions-of-events-in-real-time-at-twitter-</a></p><p>초기 old lambda architecture 도입부터 Twitter는 GCP를 많이 사용하기 때문에 GCP의 kinesis와 같은 서비스를 (Pub/Sub 구조) 활용해서 변경까지의 내용을 담고 있다.<br>아키텍처 구성의 변화를 통해 Latency가 많이 개선되었음을 확인할 수 있다.<br>(<code>Kinesis로 데이터를 받아서 S3에 저장한 다음에 Athena로 데이터를 조회하는 구조가 Twitter에서 GCP를 사용해서 새롭게 도입한 데이터 아키텍처 구성으로 볼 수 있다.(유사)</code>)</p><br/></li><li><h2 id="쏘카"><a href="#쏘카" class="headerlink" title="쏘카"></a>쏘카</h2><ul><li><p>국내에서 데이터 엔지니어링으로 유명한 기업</p></li><li><p>차량용 단말을 위한 IoT 파이프라인 구축 </p></li><li><p>AWS IoT Core, Amazon MSK, Amazon S3</p><br/><ul><li><strong>AWS IoT Core로의 전환</strong><br>쏘카에서는 기존에 Telemetrics Server가 차량 단말기로 HTTPS 호출을 통해 필요한 데이터들을 수집하는 구조로 되어있었지만, MQTT 프로토콜을 지원하는 브로커와 AWS IoT Core의 사용으로 대체를 하면서 좀 더 효율적인 데이터 파이프라인을 구성할 수 있었다.</li></ul><br/><ul><li><p><strong>Kinesis 사용</strong><br>(<code>아래 문제상황과 시도 그리고 해결과 관련된 내용은 쏘카 기술블로그에서 참고한 내용입니다</code>)<br>Kinesis는 좋고 편리한 서비스이지만 consumer가 많아질수로 기하급수적으로 서비스 사용하는 비용이 올라간다.<br>또한 Kinesis stream을 사용하는 프로젝트가 늘어날수록(Kinesis stream에서 받아서 처리하는 consumer가 늘어날수록) 파이프라인이 복잡해지고,<br>파이프라인을 관리하는 주체가 없는 상태에서 불 필요하게 많은 Consumer가 연결이 되면서 Kinesis stream에 많은 수의 Lambda 함수, 많은 Process들이 붙게 되고, 이로인해 결과적으로 Kinesis stream에 병목이 생기는 경우가 생겼다.(<code>문제상황인지</code>)<br>(<code>시도</code>) 더 많은 처리량을 위해 샤드(샤드 당 1초에 최대 2MB의 데이터 처리)를 늘리기도 하고, 향상된 팬 아웃 기능을 사용하여 상황 극복이 가능하다. 하지만 이는 서비스 사용 비용 증가와 직결되어 근본적인 문제 해결을 위한 해결책이 되지 못했다.<br>이러한 문제상황과 시도를 통해 MSK로 변경을 하여 좀 더 안정적인 새로운 파이프라인을 구성하였다.(<code>해결</code>)<br>실제로 EC2에 Kafka를 구축하여 운영할 수도 있지만 운영 비용을 줄이면서 Kafka를 사용하고자 MSK를 사용한다.</p><ul><li><h4 id="다른-관점에서의-문제해결-Q-amp-A"><a href="#다른-관점에서의-문제해결-Q-amp-A" class="headerlink" title="[다른 관점에서의 문제해결 - Q&amp;A]"></a>[다른 관점에서의 문제해결 - Q&amp;A]</h4><p>Topic을 분리해주는 것도 하나의 해결책이 될 수 있다. 예를들어 하나의 Topic으로 들어온 데이터 스트림에 3개의 consumer가 붙어있다면, 3개의 consumer가 각 각 나눠서 데이터를 가져가게 되는데, 들어가는 데이터가 배로 늘어나는 경우, consumer의 부담도 배로 늘어난다. (<code>1초에 1개의 데이터 -&gt; 1초에 10개의 데이터 전송</code>)<br>이러한 경우에는 토픽을 분리해서 트래픽을 복사하거나 하는 로직들을 고려해봐야 한다.</p></li></ul></li><li><p><strong>Kinesis Stream 병목</strong></p><p><strong>[참고]</strong> <a href="https://tech.socarcorp.kr/mobility/2022/01/06/socar-iot-pipeline-1.html">https://tech.socarcorp.kr/mobility/2022/01/06/socar-iot-pipeline-1.html</a></p><p>쏘카에서는 AWS IoT Core 서비스를 활용해서 데이터 아키텍처를 개선하였다. 쏘카 서비스 구조상 차량에서 서버쪽으로 계속 차량에서 수집한 정보를 넘겨주고, 서버쪽에서는 차량을 제어하기 위한 명령을 내려주는 구조로 되어있다. (단말과 서비스)<br>MQTT 프로토콜에서 지원하는 브로커를 사용해서 단말과 서버와의 통신을 하도록 구성을 하였다.</p></li></ul><br/><ul><li><h3 id="배운점"><a href="#배운점" class="headerlink" title="[배운점]"></a><strong>[배운점]</strong></h3><p>쏘카의 Kinesis stream 병목에 대한 글을 통해 많은 것을 배울 수 있었다. 나는 AWS에서 제공해주는 서비스에는 streaming 데이터 처리에 있어 별다른 문제가 없을 것이라고 생각을 했는데, 실제 shard 수와 각 shard마다 감당할 수 있는 데이터의 양이 한정되어 있기 때문에 설정해준 shard 수와 실시간으로 전송되는 데이터의 수에 따라 병목현상이 일어날 수도 있다는 것을 배웠다.<br>이러한 병목현상을 해결하기 위해 기존 Kinesis를 KMS(Kafka의 완전 관리형 서비스)로 전환을 하여 별도의 모니터링을 통한 shard 수 변경없이, Kafka의 완전 관리형 서비스인 Amazon MSK를 사용해서 문제 상황을 해결할 수 있다는 점도 배울 수 있었다.<br>그리고 RaspberryPi를 활용해서 실제 센서의 데이터를 MQTT라는 메시지 프로토콜에서 지원하는 브로커(<code>mosquitto</code>)를 사용해서 AWS IoT Core 서비스(<code>관리형 메시지 브로커 서비스</code>)로 보내도록 구성을 해봤었는데, MQTT 프로토콜에서 지원해주는 브로커를 거치지 않고도 AWS IoT Core를 통해 직통으로 IoT 단말기로부터 데이터를 받아서 처리할 수 있다니, 한 번 방법을 찾아보고 변경된 구조로 데이터 파이프라인 구조를 재구성해봐야겠다.<br>그리고 나는 IoT 기기로부터 센서 데이터를 받아서 처리하는 실습을 개인적으로 했기 때문에 안정성까지 고려하여 클러스터링을 지원하는 브로커인지 고려하지 않았지만, 찾아보니 내가 사용한 mosquitto 브로커의 경우에는 클러스터링을 지원하지 않고, HiveMQ와 같은 브로커만 클러스터링을 지원한다고 한다.<br>또 HiveMQ의 경우에는 AWS나 Azure와 같은 Cloud provider에서 동작을 잘 하고, auto-discovery, distributed masterless architecture를 지원하기 때문에 다음에 좀 더 안정성을 요구하는 상황에서 데이터 파이프라인을 구축할때에는 HiveMQ를 활용해봐야겠다.<br>(<code>좀 더 찾아보기 - mosquitto가 clustering을 지원하는지에 대해 좀 더 찾아보자</code>)</p><br/></li></ul></li></ul></li><li><h2 id="넷마블"><a href="#넷마블" class="headerlink" title="넷마블"></a>넷마블</h2><p>아키텍처보다 공부할만한 내용이 많기 때문에 읽어보면 도움이 된다. (<code>데이터 파이프라인의 기본 원리와 원칙 및 분산환경에 대한 내용</code>)</p><ul><li><p>데이터 파이프라인 기본 원리와 원칙은 시간이 지나도 유효해야 한다.</p><p><a href="https://netmarble.engineering/data-pipeline-design-principles-a/?fbclid=IwAR3NakVT3JM5eEDKnaT6zCoFVN5fq9XwbG8XvzQqU8p9WoW7p460TSzec2U">https://netmarble.engineering/data-pipeline-design-principles-a/?fbclid=IwAR3NakVT3JM5eEDKnaT6zCoFVN5fq9XwbG8XvzQqU8p9WoW7p460TSzec2U</a><br><a href="https://netmarble.engineering/data-pipeline-design-principles-b/">https://netmarble.engineering/data-pipeline-design-principles-b/</a></p></li></ul></li><li><h2 id="카카오"><a href="#카카오" class="headerlink" title="카카오"></a>카카오</h2><ul><li><p>데이터 엔지니어링 ?<br><a href="https://tech.kakao.com/2020/11/30/kakao-data-engineering/">https://tech.kakao.com/2020/11/30/kakao-data-engineering/</a></p></li><li><p><code>대량의 스트림 데이터를 실시간으로 분류하기 (Elasticsearch percolator)</code><br><a href="https://tv.kakao.com/channel/3693125/cliplink/423590245">https://tv.kakao.com/channel/3693125/cliplink/423590245</a></p><p><code>(자료)</code><br><a href="https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636524938152">https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636524938152</a></p><ul><li>분류할 데이터나 필터가 많은데 빠르게 분류하고 싶은 경우</li><li>데이터 기반 실시간 알람(문자열 매칭)</li><li>특정 조건에 만족하는 중고 제품이 올라오면 알람 </li></ul></li><li><p><code>Druid@Kakao</code><br><a href="https://tv.kakao.com/channel/3693125/cliplink/423590646">https://tv.kakao.com/channel/3693125/cliplink/423590646</a></p><p><code>(자료)</code><br><a href="https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636526360840">https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636526360840</a></p><ul><li>Druid 도입 사례 및 Multi-Tenant 클러스터 소개</li><li>데이터 실시간 처리 관련 문제를 해결하고 싶은 경우</li></ul></li><li><p><code>카카오 공용 하둡 운영 사례</code><br><a href="https://tv.kakao.com/channel/3693125/cliplink/423590637">https://tv.kakao.com/channel/3693125/cliplink/423590637</a></p><p><code>(자료)</code><br><a href="https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636526054873">https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636526054873</a></p></li><li><p><code>카카오의 전사 리소스 모니터링 시스템</code></p><p>카카오의 전사 리소스 모니터링 시스템은 크게 숫자 데이터를 모니터링하는 STAT(<code>KEMI-STATS</code>), 로그 문자 데이터를 모니터링하는 LOG(<code>KEMI-LOG</code>)로 분류하여 구성이 되어있으며, <code>STAT METRIC 데이터의 경우에는</code> 여러 데이터 리소스들을 카프카를 통해서 받아서 <code>SAMZA(Metric Calculator)</code>와 같은 실시간 처리 엔진을 붙여서 전처리를 해주고, OPEN TSDB라는 곳에 적재를 한 다음에 분석을 하고 있다.<br><code>LOG 데이터의 경우에는</code> fluentd로 수집을 하고, fluentd로 다시 모아서 Lambda Architecture로 구성을 한 파이프라인에 로그 데이터를 흘려서 보내주고 있다. 실시간으로 카프카로 넘겨지는 데이터를 알람과 전처리를 위해서 스톰을 배치하고 있고, 최근에는 Flink로 대체해서 구성을 하고 있다고 한다.</p><p><a href="https://tech.kakao.com/2016/08/25/kemi/">https://tech.kakao.com/2016/08/25/kemi/</a></p></li></ul></li><li><h2 id="네이버"><a href="#네이버" class="headerlink" title="네이버"></a>네이버</h2><p>네이버 플레이스(장소 검색 <code>ex.</code>맛집)에서는 플레이스 데이터 플랫폼을 구축하고 있다.<br>(<code>어뷰즈 검출 사례</code>)</p><ul><li><p>데이터 명세에 대한 내용 포함(format, schema)</p></li><li><p>protobuf는 GRPC 바이트 인코딩해서 속도가 매우 빠르고, GRPC가 사용하고 있는 직렬화 방식을 사용하고 있기 때문에 참고해보기<br><a href="https://medium.com/naver-place-dev/%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-%EC%96%B4%EB%B7%B0%EC%A6%88-%EA%B2%80%EC%B6%9C-%EC%82%AC%EB%A1%80-e9caa31511dc">https://medium.com/naver-place-dev/%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-%EC%96%B4%EB%B7%B0%EC%A6%88-%EA%B2%80%EC%B6%9C-%EC%82%AC%EB%A1%80-e9caa31511dc</a></p></li><li><p>CDC(<code>Change Data Capture</code>)툴이 DBMS에서 적재된 데이터에서 변경된 데이터만 캡처해서 Kafka와 같은 실시간 데이터 스트림 처리하는 곳으로 넘겨서 데이터를 추출해서 처리해주는 역할을 해준다.</p></li></ul></li><li><h2 id="토스"><a href="#토스" class="headerlink" title="토스"></a>토스</h2><ul><li>토스 데이터의 흐름과 활용<br><a href="https://toss.im/slash-21/sessions/2-1">https://toss.im/slash-21/sessions/2-1</a></li></ul><p>(<code>자료</code>)<br><a href="https://static.toss.im/slash21/pdf/%5B%ED%86%A0%EC%8A%A4_SLASH%2021%5D%20%ED%86%A0%EC%8A%A4%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98%20%ED%9D%90%EB%A6%84%EA%B3%BC%20%ED%99%9C%EC%9A%A9_%EC%9C%A0%EA%B2%B0.pdf">https://static.toss.im/slash21/pdf/%5B%ED%86%A0%EC%8A%A4_SLASH%2021%5D%20%ED%86%A0%EC%8A%A4%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98%20%ED%9D%90%EB%A6%84%EA%B3%BC%20%ED%99%9C%EC%9A%A9_%EC%9C%A0%EA%B2%B0.pdf</a></p><ul><li>Sqoop(SQL to Hadoop): Hadoop과 RDB간 데이터를 전송할 수 있는 오픈소스로, RDB의 특정 테이블 또는 쿼리 결과를 HDFS로 쉽게 옮길 수 있다.</li></ul></li><li><h2 id="우아한-형제들"><a href="#우아한-형제들" class="headerlink" title="우아한 형제들"></a>우아한 형제들</h2><ul><li><p>데이터 분석 플랫폼, AWS 이관기<br><a href="https://www.youtube.com/watch?v=rH_xJBoRuZE">https://www.youtube.com/watch?v=rH_xJBoRuZE</a></p><ul><li>AWS EMR에서 온프레미스로 전환</li><li>AWS Athena, Trino 비교 및 Trino 사용기</li><li>Graviton2 (ARM 기반의 EC2 인스턴스) 사용기</li><li>Zeppelin 사용기 (대화형 분석/시각화 도구)</li></ul></li><li><p>로그 데이터로 유저 이해하기<br><a href="https://techblog.woowahan.com/2536/">https://techblog.woowahan.com/2536/</a></p></li><li><p>실시간 인덱싱을 위한 Elasticsearch 구조를 찾아서<br><a href="https://techblog.woowahan.com/7425/">https://techblog.woowahan.com/7425/</a></p></li></ul></li><li><h2 id="쿠팡"><a href="#쿠팡" class="headerlink" title="쿠팡"></a>쿠팡</h2><p>Big Data Platform: Evolving from start-up to big tech company<br><a href="https://medium.com/coupang-engineering/evolving-the-coupang-data-platform-308e305a9c45">https://medium.com/coupang-engineering/evolving-the-coupang-data-platform-308e305a9c45</a></p></li><li><h2 id="왓차"><a href="#왓차" class="headerlink" title="왓차"></a>왓차</h2><p>단일 클라우드 플랫폼이 아닌 다양한 클라우드 플랫폼을 같이 사용해서 데이터 파이프라인을 구성<br><a href="https://medium.com/watcha/%EB%A9%80%ED%8B%B0%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A1%9C%EA%B7%B8-%EB%B6%84%EC%84%9D-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0-8c5f671df559">https://medium.com/watcha/%EB%A9%80%ED%8B%B0%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A1%9C%EA%B7%B8-%EB%B6%84%EC%84%9D-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0-8c5f671df559</a></p></li><li><h2 id="줌-인터넷"><a href="#줌-인터넷" class="headerlink" title="줌 인터넷"></a>줌 인터넷</h2><p>검색 데이터 서빙 플랫폼 구축<br>file:///Users/hyungilee/Desktop/Data%20engineering/Pipeline/Learning%20spoons/220703_Day6.pdf</p></li></ul><h2 id="AWS-IoT-Core"><a href="#AWS-IoT-Core" class="headerlink" title="AWS IoT Core"></a><ins><b>AWS IoT Core</b></ins></h2><ul><li>관리형 서비스</li><li>MQTT, HTTPS, LoRaWAN등의 프로토콜을 지원</li><li>기기간 메시지를 AWS 서비스에 라우팅</li><li>IoT 전용 데이터 스트림 역할을 해준다.</li></ul><h2 id="Q-amp-A-Section"><a href="#Q-amp-A-Section" class="headerlink" title="Q&amp;A Section"></a><ins><b>Q&amp;A Section</b></ins></h2><ul><li><p><strong>Queue 서비스(MQ/SQS)와 스트리밍 서비스(Kinesis/Kafka)의 선택시 주요 포인트</strong></p><p>-&gt; SQS를 사용하는 경우는 확장성을 고려해야한다. SQS는 단순 큐로써, 스케줄링을 하거나 작업을 순차적으로 처리해야되는 경우와 같이 간단한 경우에는 SQS를 사용해도 괜찮다.(<code>초당 3천개 정도는 커버 가능</code>) 하지만 빅데이터를 처리해줘야되는 경우에는 Kinesis를 고려해야하고, 더 확장성 있는 구조를 고려해야된다면, Kafka를 고려해줘야 될 수도 있다. Kafka + lambda 구조로 사용하는 경우는 괜찮지만, 더 많은 consumer가 붙어서 처리되는 경우에는 요금부하가 생기기 때문에 이 경우에는 lambda 대신에 Kafka connector, Kafka streams를 사용해서 DocumentDB로 넣어주는 것이 권장된다고 한다.</p><p>스트리밍 데이터를 여러 컨슈머 그룹으로 묶여서 처리를 해야되는 경우와 데이터가 일정기간동안 저장 가능해서 replayable하다는 장점을 가져와야 되는 경우에는 Queue 서비스 보다는 스트리밍 서비스를 선택해서 사용하는 것이 권장된다. 그리고 들어온 데이터가 순서를 보장해야되는 경우에는 Queue 서비스인 SQS를 사용해서 처리해주는 것이 좋다. (<code>Kafka를 사용해서 단일 브로커로 구성해서 데이터 순서를 보장해서 처리를 해줄 수도 있다</code>) 그리고 로그 데이터는 SQS를 사용할 필요는 없다고 한다.</p></li><li><p>초당 3000개 이상의 사용자의 API 호출 로그를 Kinesis로 보내고, Lambda를 이용해서 DocumentDB(NoSQL)로 넣는 구조를 하고 있는데, Kinesis의 write 성능 제한으로 peak hour에 reject 되는 경우가 있어서 On-failure destination SQS(<code>실패한 경우에 SQS로 넣어주는 것을 고려</code>)를 지정해서 사용하는 것을 고려중. </p></li></ul>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Data-Pipeline/">Data-Pipeline</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/AWS/">AWS</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/07/03/202207/220703_datapipeline_study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220626 데이터 파이프라인 구축 오프라인 수업 / 5주차</title>
      <link>https://leehyungi0622.github.io/2022/06/26/202206/220626_datapipeline_study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/26/202206/220626_datapipeline_study/</guid>
      <pubDate>Sun, 26 Jun 2022 05:20:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220530_review.png&quot; alt=&quot;Review&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 다섯 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. &lt;/p&gt;
&lt;h2 id=&quot;Flink&quot;&gt;&lt;a href=&quot;#Flink&quot; class=&quot;headerlink&quot; title=&quot;Flink&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Flink&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;우선 실습에 앞서 간단하게 Flink에 대해서 정리를 해보자. Flink는 분산 데이터 처리 프레임워크 (Processing unbounded data)로, &lt;code&gt;Kafka와 같은 MQ에 Flink를 붙여서 처리&lt;/code&gt;를 할 수 있다. 이외에도 MQ에 Kinesis Firehose를 붙이고, Lambda를 붙여서 custom한 형태의 데이터로 추출을 할 수도 있고, 데이터를 암호화하거나 특정 format(Parquet, ORC)으로 변환을 해서 추출을 할 수도 있다.&lt;br&gt;또 Logstash를 붙여서 데이터를 간단하게 처리해서 넘겨줄 수 있다. (whitelist / filter plugin을 통해 처리) 커스텀하게 데이터를 모아주거나 분석을 하는 경우, 예를들어 GPS 신호를 계속 보내서 사용자들이 이 데이터를 1분동안 aggregation해서 어느 지역에 사람이 많은지 분석하는 작업은 logstash로 분석을 하는 것이 불가능하다. 그리고 각 각의 logstash로 나눠서 분산처리를 하는 것은 가능하지만, 각 각의 logstash가 서로 데이터를 공유하지는 못하기 때문에(&lt;code&gt;클러스터 내의 노드로써 존재하지 않기 때문에&lt;/code&gt;) 복잡한 스트림 처리나 프로세싱, 분석이 필요할때 Flink를 사용한다.&lt;/p&gt;
&lt;p&gt;그리고 &lt;code&gt;Flink, Storm을 개발할때에는 DAG를 많이 사용&lt;/code&gt;한다. Kafka에서 받아온 실시간 데이터를 키별로 분류를 하는 작업에서도 사용이 될 수 있는데, 게임 데이터를 분석한다고 가정했을때 각 캐릭터의 직업에 따라 분류를 해서 각 직업별로 행동 패턴을 분석하고자 할때 각 각의 Dataflow를 머릿속으로 구조화시킨 다음에 붙여주는 작업을 해야한다.&lt;br&gt;&lt;code&gt;source operator(Kafka) -&amp;gt; keyBy operator -&amp;gt; aggregation operator -&amp;gt; HDFS sink&lt;/code&gt; Flink와 같은 데이터 플로우 프로그래밍이 필요한 어플리케이션은 operator와 같은 요소를 하나 하나 개발을 한 다음에 연결을 해주는 방식으로 개발을 하게 된다.&lt;/p&gt;
&lt;p&gt;DB ETL작업을 할때 DB에서 주기적으로 데이터를 select해서 Flink내부에서 처리를 하고자 할 때에도 source operator를 사용한다. (&lt;code&gt;Flink 외부에서 데이터를 긁어오는 부분을 source operator&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;중간에서 데이터를 변환해주는 부분을 Transformation Operator라고 한다. 그리고 처리 결과를 밖으로 빼내는 부분을 Sink Operator라고 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h3 id=&quot;Flink-parallelism&quot;&gt;&lt;a href=&quot;#Flink-parallelism&quot; class=&quot;headerlink&quot; title=&quot;Flink parallelism&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Flink parallelism&lt;/b&gt;&lt;/ins&gt;&lt;/h3&gt;&lt;p&gt;실제 flink로 어플리케이션을 개발하게 되면, 수십개의 TM가 생성이 된다.&lt;br&gt;개발을 할때 각 각의 operator의 갯수를 parallism을 통해 정의할 수 있다.&lt;br&gt;Kafka(3) -&amp;gt; Map(10) -&amp;gt; Sink(3) =&amp;gt; 16개의 operator가 TM의 각 각의 노드에 분산이 되어 처리된다. 이와같은 특징으로, Fluentd와 logstash와 같은 agent 기반의 데이터 파이프라인과 비교되어 사용된다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220530_review.png" alt="Review"></div><br/><br/><p>이번 포스팅에서는 다섯 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. </p><h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a><ins><b>Flink</b></ins></h2><p>우선 실습에 앞서 간단하게 Flink에 대해서 정리를 해보자. Flink는 분산 데이터 처리 프레임워크 (Processing unbounded data)로, <code>Kafka와 같은 MQ에 Flink를 붙여서 처리</code>를 할 수 있다. 이외에도 MQ에 Kinesis Firehose를 붙이고, Lambda를 붙여서 custom한 형태의 데이터로 추출을 할 수도 있고, 데이터를 암호화하거나 특정 format(Parquet, ORC)으로 변환을 해서 추출을 할 수도 있다.<br>또 Logstash를 붙여서 데이터를 간단하게 처리해서 넘겨줄 수 있다. (whitelist / filter plugin을 통해 처리) 커스텀하게 데이터를 모아주거나 분석을 하는 경우, 예를들어 GPS 신호를 계속 보내서 사용자들이 이 데이터를 1분동안 aggregation해서 어느 지역에 사람이 많은지 분석하는 작업은 logstash로 분석을 하는 것이 불가능하다. 그리고 각 각의 logstash로 나눠서 분산처리를 하는 것은 가능하지만, 각 각의 logstash가 서로 데이터를 공유하지는 못하기 때문에(<code>클러스터 내의 노드로써 존재하지 않기 때문에</code>) 복잡한 스트림 처리나 프로세싱, 분석이 필요할때 Flink를 사용한다.</p><p>그리고 <code>Flink, Storm을 개발할때에는 DAG를 많이 사용</code>한다. Kafka에서 받아온 실시간 데이터를 키별로 분류를 하는 작업에서도 사용이 될 수 있는데, 게임 데이터를 분석한다고 가정했을때 각 캐릭터의 직업에 따라 분류를 해서 각 직업별로 행동 패턴을 분석하고자 할때 각 각의 Dataflow를 머릿속으로 구조화시킨 다음에 붙여주는 작업을 해야한다.<br><code>source operator(Kafka) -&gt; keyBy operator -&gt; aggregation operator -&gt; HDFS sink</code> Flink와 같은 데이터 플로우 프로그래밍이 필요한 어플리케이션은 operator와 같은 요소를 하나 하나 개발을 한 다음에 연결을 해주는 방식으로 개발을 하게 된다.</p><p>DB ETL작업을 할때 DB에서 주기적으로 데이터를 select해서 Flink내부에서 처리를 하고자 할 때에도 source operator를 사용한다. (<code>Flink 외부에서 데이터를 긁어오는 부분을 source operator</code>)</p><p>중간에서 데이터를 변환해주는 부분을 Transformation Operator라고 한다. 그리고 처리 결과를 밖으로 빼내는 부분을 Sink Operator라고 한다.</p><ul><li><h3 id="Flink-parallelism"><a href="#Flink-parallelism" class="headerlink" title="Flink parallelism"></a><ins><b>Flink parallelism</b></ins></h3><p>실제 flink로 어플리케이션을 개발하게 되면, 수십개의 TM가 생성이 된다.<br>개발을 할때 각 각의 operator의 갯수를 parallism을 통해 정의할 수 있다.<br>Kafka(3) -&gt; Map(10) -&gt; Sink(3) =&gt; 16개의 operator가 TM의 각 각의 노드에 분산이 되어 처리된다. 이와같은 특징으로, Fluentd와 logstash와 같은 agent 기반의 데이터 파이프라인과 비교되어 사용된다.</p><a id="more"></a><div align="center">  <img src="/images/post_images/220627_flink_structure.jpg" alt="Flink parallelism"></div><ul><li><h4 id="Code-구현"><a href="#Code-구현" class="headerlink" title="Code 구현"></a><strong>Code 구현</strong></h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Source operator 구현</span></span><br><span class="line">DataStream&lt;String&gt; lines = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;());</span><br><span class="line"><span class="comment">// Transformation operator 구현</span></span><br><span class="line">DataStream&lt;Log&gt; logs = lines.map((line) -&gt; parse(line));</span><br><span class="line"><span class="comment">// Transformation operator 구현 (위의 Transformation 결과 변수를 또 다시 변형)</span></span><br><span class="line">DataStream&lt;Statistics&gt; stats = logs.keyBy(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">                                  <span class="comment">// 10초마다 데이터를 끊어서 처리</span></span><br><span class="line">                                   .timeWindow(Time.second(<span class="number">10</span>))</span><br><span class="line">                                   .apply(<span class="keyword">new</span> MyWindowAggregationFunc());</span><br><span class="line"></span><br><span class="line">stats.addSink(<span class="keyword">new</span> CustomSink(path));</span><br></pre></td></tr></table></figure></li><li><h4 id="Operator-Chaining"><a href="#Operator-Chaining" class="headerlink" title="Operator Chaining"></a><strong>Operator Chaining</strong></h4><p>각 각의 노드들에 분산되서 처리되서 처리되는 것 보다는 서로 연관있는 operator나 데이터를 주고 받지 않고 바로 바로 처리해도 되는 요소의 경우에는 operator chain을 해서 효율성을 높일 수 있다.</p><p><a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/</a></p></li></ul></li></ul><h2 id="실습-Flink-cluster"><a href="#실습-Flink-cluster" class="headerlink" title="[실습] Flink cluster"></a><ins><b>[실습] Flink cluster</b></ins></h2><p>  이번 Flink 실습에서는 EC2 instance에서 Flink를 설치하고, 웹 상에서 Flink cluster를 관리할 수 있다. 따라서 default로 설정이 되어있는 SSH(21) 포트 외에 추가적으로 8081 port를 열어줘야한다. (Inbound rule 수정)</p><ul><li><h3 id="Flink-설치"><a href="#Flink-설치" class="headerlink" title="Flink 설치"></a><ins><b>Flink 설치</b></ins></h3><p><a href="https://www.apache.org/dyn/closer.lua/flink/flink-1.14.5/flink-1.14.5-bin-scala_2.12.tgz">https://www.apache.org/dyn/closer.lua/flink/flink-1.14.5/flink-1.14.5-bin-scala_2.12.tgz</a></p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$wget</span> https://dlcdn.apache.org/flink/flink-1.14.5/flink-1.14.5-bin-scala_2.12.tgz</span><br><span class="line"></span><br><span class="line"><span class="variable">$tar</span> -zxvf flink-1.14.5-bin-scala_2.12.tgz</span><br><span class="line"></span><br><span class="line"><span class="variable">$cd</span> flink-1.14.5-bin-scala_2.12</span><br><span class="line"></span><br><span class="line"><span class="variable">$sudo</span> apt install openjdk-11-jdk</span><br></pre></td></tr></table></figure><p>실습에서는 하나의 EC2 instance 내에 JM, TM을 각 각 하나씩 두지만, 실제로는 (<code>EC2, Kubernetes, YARN과 같은 컨테이너 환경에서 개별로 존재</code>)JM가 따로, TM도 서로 다른 컨테이너안에서 개별 노드로 존재한다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 설치한 flink 디렉토리 안의 bin 폴더</span></span><br><span class="line">$./bin/start-cluster.sh</span><br></pre></td></tr></table></figure></li><li><h3 id="Flink에-실행할-Application-import"><a href="#Flink에-실행할-Application-import" class="headerlink" title="Flink에 실행할 Application import"></a><ins><b>Flink에 실행할 Application import</b></ins></h3><p>Java로 코드를 작성한 다음에 *.jar 파일로 추출을 해서 브라우저 UI 상에서 Job을 제출하거나 터미널상에서 실행을 할 수도 있다. </p><p><code>Browser UI상에서 Job submit</code></p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Entry Class : org.apache.flink.streaming.examples.multiple.SocketMultiple</span><br><span class="line">Parallelism: 1 <span class="comment">#TM를 1개</span></span><br><span class="line">Program Arguments: --hostname localhost --port 9090</span><br></pre></td></tr></table></figure><p><code>Terminal 상에서 Job submit</code></p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$./flink-1.14.5/bin/flink run [실행할 job의 *.jar 파일] --hostname localhost --port 9090</span><br><span class="line"></span><br><span class="line"><span class="comment"># 또는 원격지 (PC)에서 생성한 EC2 인스턴스의 주소를 통해 Flink로 코드를 실행할 수도 있다.</span></span><br></pre></td></tr></table></figure><p>EC2 Instance에서는 flink에서 Listen할 data source operator의 역할을 해주기 위해서 9090 포트를 열어서 데이터를 입력해줘야 한다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$nc</span> -l 9090</span><br></pre></td></tr></table></figure><p>EC2 instance에서 9090 포트를 열어준 다음에 문자를 입력해주게 되면, 처리결과를 아래와 같이 터미널상에서 log 파일을 직접 확인하거나, Apache Flink Dashboard의 TM의 항목을 통해 처리 결과를 확인해줄 수 있다. (<code>Stdout</code>)</p><p>다시 Flink에서의 데이터 처리 흐름을 살펴보면, Socket에 붙어서 텍스트를 입력을 해주게 되면 Flink Application에 source로 데이터가 들어가게 되고, Transformation operator(Map function)을 통해 데이터가 정제되서 최종적으로 Sink operator(Stdout)을 통해 출력을 해주게 된다.</p></li><li><h3 id="Apache-Flink-Dashboard"><a href="#Apache-Flink-Dashboard" class="headerlink" title="Apache Flink Dashboard"></a><ins><b>Apache Flink Dashboard</b></ins></h3><p>Apache Flink Dashboard에서 Overview 메뉴를 통해 현재 실행중인 Job이 몇 개인지 확인할 수 있고, JM, TM의 상세 정보(Total Process Memory/Total Flink Memory/TM의 heap 메모리 영역)를 확인할 수 있다.<br>JM의 Running job 항목을 클릭해보면, operator에 대한 정보를 확인할 수 있는데, 클릭하면 <code>BackPressure/Idle/Busy</code> 정보(%)를 확인할 수 있는데 이 부분에 대한 확인이 중요하다.</p><div align="center">  <img src="/images/post_images/220627_flink_operator_backpressure_busy_status.jpg" alt="Flink operator backpressure & busy status"></div><ul><li>Flink와 관련된 서비스를 제공하는 third party 서비스사에서 제공하는 샘플 코드도 한 번 보기(코드 참고해서 직접 커스텀 어플리케이션 만들어보기)</li></ul></li></ul><h2 id="시각화-및-분석-툴"><a href="#시각화-및-분석-툴" class="headerlink" title="시각화 및 분석 툴"></a><ins><b>시각화 및 분석 툴</b></ins></h2><p>  이번 파트는 많이 유용할 것 같다. 지금 프로젝트로 정리하고 있는 데이터 파이프라인 중에 Amazon EMR로 데이터를 정제하고, DW, DM까지만 데이터를 적재하는 부분까지만 했었는데, 이번 5주차 수업을 통해서 그 다음 파이프라인에서는 어떤식으로 데이터를 분석하고 시각화할지에 대해서 아이디어를 얻을 수 있었던 것 같다.</p><ul><li><h3 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a><ins><b>Elasticsearch</b></ins></h3><ul><li><p>Apache Lucene를 기반으로 한 오픈소스 분산 검색 엔진이다.</p></li><li><h4 id="루씬-Lucene"><a href="#루씬-Lucene" class="headerlink" title="루씬(Lucene)"></a><strong>루씬(Lucene)</strong></h4><p>Lucene이란 자바로 만들어진 고성능 정보 검색(IR: Information Retrieval)라이브러리로, 여기서 IR이란 문서를 검색하거나, 문서의 내용을 검색하거나 문서와 관련된 메타 정보를 검색하는 과정을 의미한다.<br>그리고 Lucene은 파일 검색이나 웹 문서 수집, 웹 검색 등에 바로 사용할 수 있는 어플리케이션이 아닌, <code>검색 기능을 갖고 있는 어플리케이션을 개발할 때 사용할 수 있는 라이브러리</code>이다.</p></li><li><p>JSON 기반의 문서를 저장하고 검색할 수 있다. (<code>이러한 이유로 로그와 같은 JSON형태의 데이터를 쉽게 저장할 수 있는 데이터베이스, NoSQL와 같이 사용이 되고 있다</code>)</p></li><li><p>로그를 수집하고 조회하는 용도(로깅 &amp; 모니터링)/ 통합 로그 분석 및 보안관련 이벤트 분석에 사용되고 있다.</p></li><li><p>실시간 집계 및 데이터 분석 용도로도 사용되고 있다.</p></li><li><p>ML 기능도 추가가 되어, ML 엔진으로도 활용되고 있다.</p></li></ul><p>Elasticsearch는 온프레미스 환경에서 로컬로 서버를 구축할 수도 있고, AWS상에서만 효율적으로 활용할때에는 QuickSight를 사용하기도 한다.<br>그리고 Elasticsearch는 데이터 검색 엔진으로 나왔지만, 최근에는 short term 데이터 저장소로도 활용이 되고 있다. 시각화 툴로써도 기능이 좋지만, 전문 BI 툴에 비하면 부족한 면이 있지만, 엔지니어에게 있어서 기능적인 면은 충분하다.(<code>분석가, 데이터 사이언티스트에게 있어서는 부족한 부분이 있다</code>)<br>그 이유는 엔지니어에게 있어서는 로그 데이터를 뽑아서 관련된 정보를 보거나, 그래프로 그리는 툴정도로만 활용이 되기 때문이다.<br>그리고 ElasticSearch는 Server side 어플리케이션으로, UI가 없기 때문에 Kibana가 붙어서 시각화 처리를 해준다.</p><ul><li><h4 id="데이터-베이스에서-Query를-통한-처리와-Elasticsearch에서의-처리의-차이"><a href="#데이터-베이스에서-Query를-통한-처리와-Elasticsearch에서의-처리의-차이" class="headerlink" title="데이터 베이스에서 Query를 통한 처리와 Elasticsearch에서의 처리의 차이"></a><strong>데이터 베이스에서 Query를 통한 처리와 Elasticsearch에서의 처리의 차이</strong></h4><p>실제 DB에 일일이 Query를 작성해서 원하는 데이터를 추출 할 수는 있지만, 그에 대한 한계점이 명확하기 때문에 ES를 사용한다. 대표적인 한계점으로는 데이터베이스의 query 연산은 느리고 부하가 높으며, 공백이나 띄어쓰기를 포함한 exact 연산을 기반으로 한다는 단점이 있다.</p></li><li><h4 id="ES의-특징"><a href="#ES의-특징" class="headerlink" title="ES의 특징"></a><strong>ES의 특징</strong></h4><ul><li>준실시간 검색 엔진(Full text Search)</li><li>클러스터링을 통한 안정성 강화와 부하 분산</li><li>다양한 형태의 문서도 동적으로 인데싱되어 검색이 가능하다.(Schemaless)<br><code>-&gt; JSON 기반으로 문서를 제공하기 때문에 NoSQL로 분류를 하기도 한다.</code> </li><li>REST API로 검색을 할 수 있기 때문에 별도의 드라이버나 라이브러리 없이 연동이 가능하다.</li><li>역색인(Inverted Index) : 검색어가 포함된 모든 문서를 조회하는 것이 아닌, <code>해당 검색어가 포함된 모든 문서의 위치를 알 수 있다.</code> 미리 검색을 통한 indexing을 해주기 때문에 검색에 대한 성능이 좋다.</li><li>확장성과 가용성이 좋다.</li></ul></li><li><h4 id="RDBMS와-Elasticsearch의-차이점"><a href="#RDBMS와-Elasticsearch의-차이점" class="headerlink" title="RDBMS와 Elasticsearch의 차이점"></a><strong>RDBMS와 Elasticsearch의 차이점</strong></h4><p>Elasticsearch는 단기 저장소로도 사용이 된다고 했는데, RDBMS와는 아래와같은 차이점이 있다. 아래의 도표에서 Elasticsearch의 Type은 최신 버전에서 개념적으로 사라지고 있다.<br><code>Index는</code> 데이터 저장 공간으로, 검색의 범위가 될 수 있다.(Query에서 테이블과 같은 역할, <code>server-log-2022.06.27</code> 또는 <code>server.log-2022.06.27</code>로 날짜 또는 앞의 데이터 타입에 따라 데이터를 구분하여 읽을 수 있게 해준다)그리고 <code>Shard는</code> primary와 replica로 구분이 되는데, 문서를 저장하는 단위 중 하나로, 하나의 인덱스가 여러 물리적인 단위인 샤드로 나뉘어서 분산되어 저장이 된다. 그리고 이 Shard 단위로 replica가 된다.<br><code>Document</code>는 데이터의 최소 단위(row)로, 다수의 필드로 구성이 되어 있으며, 기본적으로 JSON format으로 제공된다.<br><code>Field</code>는 문서를 구성하는 column으로, 동적 데이터 타입이다. (성능을 높이기 위해서는 타입을 static하게 assign해주는 것이 좋다)<br>하나의 Document는 특정 Index의 특정 Shard에 속한 데이터 조각으로, JSON의 Key-value 형태로 Mapping이 되어있다.</p><div align="center">  <img src="/images/post_images/220627_rdbms_elasticsearch.png" alt="Elasticsearch와 RDBMS 비교"></div></li><li><h4 id="ES-node의-역할"><a href="#ES-node의-역할" class="headerlink" title="ES node의 역할"></a><strong>ES node의 역할</strong></h4><p>Elasticsearch에는 여러 종류의 노드들이 존재한다. </p><div align="center">  <img src="/images/post_images/220629_es_cluster_node_structure.jpg" alt="Elasticsearch의 클러스터 내 노드의 구성"></div><ul><li><p><strong>[Master node]</strong></p><p>마스터 노드는 클러스터의 상태와 각종 메타데이터를 관리하는 역할을 한다.</p><br/></li><li><p><strong>[Data node]</strong></p><p>Document를 저장하고 조회하는 역할을 한다. 이 Data 노드에 여러가지 role을 적용할 수 있다. (<code>한 노드가 다양한 노드의 역할을 수행할 수도 있다.</code>)<br>데이터 저장</p></li></ul><br/><ul><li><p><strong>[Ingest node]</strong></p><p>실제로는 Master node와 Data node만 존재해도 되지만, Document에 저장이 되기 전에 전처리 작업이 필요한 경우에는 Ingest node가 필요하다. (<code>Data node에 Ingest node의 역할을 부여</code>) </p><br/></li><li><p><strong>[Coordinate node]</strong></p><p>요청을 데이터 노드로 전달하고, 다시 데이터 노드로부터 결과를 취합하는 역할을 하는 노드이다.</p><br/></li><li><p><strong>[이외의 다양한 노드의 roles]</strong></p><p>data_hot, data_warm, data_cold, ml(<code>GPU가 붙어있는 노드</code>), etc…</p><br/></li><li><p><strong>[이외의 다양한 노드의 roles]</strong></p><p>Elasticsearch를 운영/관리할때에는 JVM의 heap을 얼마나 설정하느냐와 Shard 수 및 Shard 당 용량 그리고 Replica 수, 클러스터 노드 수, OS 메모리 용량 등 고려되어야할 부분이 많다. 구체적인 내용은 아래의 노트 필기를 확인해보고, Elasticsearch를 직접 서버에 올려서 구성할때 아래의 조건들을 직접 적용시켜가면서 운영해봐야겠다.</p><div align="center">  <img src="/images/post_images/220629_elasticsearch_operation.jpg" alt="Elasticsearch의 운영과 관리"></div> <br/></li><li><p><strong>[상황예시]</strong></p><p>데이터 노드를 구성할때에는 데이터 노드의 종류인 hot, warm, cold 중에서 최신/조회 수가 많은 데이터들을 담아두는 hot node를 구성한다.(<code>SSD</code>) 1주일내에 들어온 데이터가 조회 수가 많은 데이터라면, hot node에 담아두고, 1주일이 지나면 자동으로 warm data node(<code>HDD</code>)로 migration할 수 있도록 해줄 수 있다.<br>이와같이 장비와의 호환성을 맞춰서 각 각의 데이터 노드를 구성할 수 있다. </p></li></ul></li></ul></li><li><h3 id="ELK-Stack"><a href="#ELK-Stack" class="headerlink" title="ELK Stack"></a><ins><b>ELK Stack</b></ins></h3><p>ELK stack은 Elasticsearch(데이터 저장 및 검색 엔진 / Short term 데이터 저장소로도 활용), Logstash(데이터 수집 및 전처리), Kibana(데이터 시각화)로 컴포넌트들이 구성이 되어있다. 그리고 ELK를 구성하는 각 컴포넌트들은 Elastic 기업에서 진행하고 있는 In-house 프로젝트들로써 각 컴포넌트들 간의 호환성이 좋다.</p><p><code>ref.</code> EFK(Elasticsearch Fluentd Kibana) 스택을 사용할 수도 있다.</p></li><li><h3 id="Kibana"><a href="#Kibana" class="headerlink" title="Kibana"></a><ins><b>Kibana</b></ins></h3><p>Kibana는 Elasticsearch에 붙여서 데이터를 검색하고, 시각화해주는 UI dashboard이다.<br>일일이 사용자가 API 요청을 Elasticsearch의 데이터 노드들에 날려서 데이터를 검색할 수 없기 때문에 Kibana가 이러한 번거로움을 개선하여, <code>ES와 사용자 간의 인터페이스 역할</code>을 해준다.<br>시각화는 다양한 시각화 도구를 제공(막대, 원형, 표, 히스토그램, 히트맵 등)해준다.   </p></li></ul><h2 id="Amazon-OpenSearch-Service"><a href="#Amazon-OpenSearch-Service" class="headerlink" title="Amazon OpenSearch Service"></a><ins><b>Amazon OpenSearch Service</b></ins></h2><pre><code>Amazon OpenSearch Service는 AWS의 Elasticsearch 관리형 서비스로, 오픈소스 라이센스의 문제로 인해 AWS 측에서는 Elasticsearch를 fork하여 별도의 프로젝트로 관리 및 서비스하고 있다고 한다.v7.10까지는 기능이 모두 동일하고, OpenSearch 초기 버전은 Elasticsearch와 거의 동일하다. </code></pre><ul><li><h3 id="Elasticsearch-amp-Kibana-설치-운영"><a href="#Elasticsearch-amp-Kibana-설치-운영" class="headerlink" title="Elasticsearch &amp; Kibana 설치/운영"></a><strong>Elasticsearch &amp; Kibana 설치/운영</strong></h3><p>Window, mac, Linux 로컬 환경에서 설치를 하고 테스트를 할 수도 있지만, docker container 기반으로 Elasticsearch와 Kibana를 설치해서 테스트를 할 수도 있다.</p><ul><li><h4 id="Elasticsearch-설치-in-local"><a href="#Elasticsearch-설치-in-local" class="headerlink" title="[Elasticsearch 설치 in local]"></a><strong>[Elasticsearch 설치 in local]</strong></h4><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html</a></p></li><li><h4 id="Elasticsearch와-Kibana를-docker에-설치"><a href="#Elasticsearch와-Kibana를-docker에-설치" class="headerlink" title="[Elasticsearch와 Kibana를 docker에 설치]"></a><strong>[Elasticsearch와 Kibana를 docker에 설치]</strong></h4><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</a></p></li></ul></li></ul><h2 id="실습-Amazon-Opensearch-서비스"><a href="#실습-Amazon-Opensearch-서비스" class="headerlink" title="[실습] Amazon Opensearch 서비스"></a><ins><b>[실습] Amazon Opensearch 서비스</b></ins></h2><p>  [Create domain]으로 새로운 도메인을 생성해준다. 배포 유형은 실습이기 때문에 개발 및 테스트 유형을 선택해준다. 그리고 버전은 Elasticsearch와 Kibana의 환경을 실습하기 위해서 Elasticsearch 7.10 버전을 선택하도록 한다.<br>  가용영역에 대한 설정은 가용성을 높여서 사용할 필요가 없기 때문에 1-AZ로 설정하고, 인스턴스 유형은 t3.small.search, 노드 수는 3, EBS/SSD/10GB로 선택을 한다. 또한 추가적으로 네트워크는 public access, Advanced cluster settings에서는 Max clause count의 값을 default로 1024로 설정해준다.<br>  (Master user에 대해서도 IAM 옵션이 아닌 사용자 계정을 만들고, 접근 정책도 <code>Only use fine-grained access control</code>로 설정하도록 한다) </p><ul><li><h3 id="Elasticsearch-1"><a href="#Elasticsearch-1" class="headerlink" title="Elasticsearch"></a><strong>Elasticsearch</strong></h3><p>Elasticsearch의 장점인 JDBC나 ODBC 별도의 client library 상관없이 Kibana 자체적으로 제공하는 기능을 사용해서 web 상에서 직접 query를 보내서 API 테스트해 볼 수 있다.</p><ul><li><p><strong>인덱스 생성</strong></p><p>데이터 베이스 생성과 같이 분산된 document를 쌓을 공간을 만드는 것을 말한다. (<code>앞에서는 쌓을 공간을 만드는 것을 index, 데이터를 ES에 넣는 행위를 indexing이라고 한다</code>) </p></li><li><p><strong>[실습]</strong></p><p>Kibana link를 통해 Elasticsearch에 들어가서 sample로 제공해주는 로그와 eCommerce 데이터를 선택해주면 생성한 Elasticsearch로 데이터를 밀어넣어준다. Dashboard 메뉴에서는 밀어넣은 데이터에 대한 dashboard 하위 메뉴가 생성이 되고, 클릭하면 미리 생성되어있는 dashboard graph를 볼 수 있다.<br><code>이러한 시각화 그래프는 Discovery에서 해당 그래프의 데이터가 json 형태로 쌓여있기 때문에 이 데이터를 기반으로 그래프를 그려준다.</code> 그리고 이 Discovery 메뉴에서는 column 이름을 기준으로 해서 데이터를 필터할 수도 있다.<br>추가적으로 그래프를 그릴때는 <code>Visualize</code>메뉴를 통해 시각화하고자하는 데이터 dashboard를 선택해서 x-axis, y-axis를 선택해서 그리면 된다.<br>좌측 메뉴중에 Management 하위의 Dev Tool을 활용하면, query를 보내서 데이터를 검색할 수 있다. </p><ul><li><p><strong>Index 생성</strong></p><p>Index를 생성한다는 것은 앞서 배웠듯이 데이터를 검색하는 범위를 새롭게 생성한다는 의미이다.</p></li><li><p><strong>개발자의 Kibana 활용</strong></p><p>개발자들은 주로 Pi chart나 추세를 보여주는 라인 그래프를 주로 활용한다.</p></li><li><p><strong>API 테스트</strong></p><p>Dev Tools에서 RestAPI를 지원하기 때문에 다른 서비스와 연계시켜서 검색하는 서비스를 만들어낼 수 있다. </p></li><li><p><strong>사용자 정의 인덱스 생성</strong></p><p>Dev Tools에서 아래와 같이 인덱스를 생성 할 수 있다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PUT &#x2F;my-index-1</span><br><span class="line">PUT &#x2F;my-index-2</span><br><span class="line">PUT &#x2F;my-index-3</span><br></pre></td></tr></table></figure><p>Stack Management / Index pattern / Create index pattern을 통해 새롭게 생성한 인덱스(<code>Document가 쌓이는 저장 공간</code>)를 확인할 수 있다. 이 인덱스를 my-index*와 같은 패턴을 정의해서 생성할 수 있다<br>서비스별로 로그를 분류할때에도 위의 방법이 활용되는데, service-2022.07.01과 같이 날짜별로 인덱스를 만들어서 데이터를 쌓을 수 있다. 각 각의 날짜별로 분류된 서비스의 로그 데이터는 <code>service-*</code>와 같은 특정 패턴으로 정의를 해서 생성을 할 수 있으며, 이렇게 정의된 패턴을 통해 Discover 메뉴에서 전체 서비스 로그에 대해 조건 검색을 할 수도 있다.<br>schemaless이기 때문에 좀 더 유연하게 데이터를 검색할 수 있다.</p></li><li><p><strong>Logstash / Flink —&gt; ES</strong></p><p>Flink나 Logstash를 활용해서 ES에 데이터를 넣어주면, 데이터가 계속해서 쌓이게 되고, 쌓인 데이터로 ES에서 데이터 분석을 하거나 시각화를 할 수 있다. </p></li></ul></li></ul></li></ul><h2 id="BI-tool"><a href="#BI-tool" class="headerlink" title="BI tool"></a><strong>BI tool</strong></h2><p>  Amazon Quicksight는 일종의 BI(Business Intelligence) 툴로서, <code>Kibana보다는 좀 더 계산된 기능을 지원</code>하고, 서버 분석을 해서 데이터를 긁어오는 서버와 UI가 합쳐진 형태를 BI툴이라고 한다.<br>  BI 툴은 Data-Driven 형태로 비즈니스 관점에서 시간대별로 데이터를 보여주고, 다양한 관점의 자료를 시각화하여 더 나은 비즈니스 의사 결정을 내릴 수 있도록 도와주는 tool이다.<br>  <code>Kibana는 BI 툴이라고 하지 않으며, 단지 데이터를 시각화해주는 기능적인 면에서는 비슷하다고 볼 수 있다.</code> 대표적인 BI 솔루션으로는 Tableau, MS Power BI, MicroStrategy, Redash 등이 있다.</p><p>  Kibana를 들어가보면, Opensearch에서 샘플로 제공해주는 데이터들을 넣어서 dashboard를 생성해볼 수 있다.</p><p>  사용예로는 보고서, 온라인 분석 처리, 데이터 마이닝, 시각화등에 사용되며, SQL, Hadoop, Hive를 통해서 SQL로 쿼리를 해보는 것과 달리 쉽게 UI상에서의 조작을 통해 데이터를 손쉽게 분석할 수 있게 도와준다.</p><p>  <code>Ref.</code> Data mining : 통계적으로 전처리를 하여 데이터 간의 아직 알려지지 않은 관련성이나 경향 등을 분석하는 방법이다. 서로다른 데이터 소스의 데이터를 결합해서 새로운 경향을 도출해내는 방법으로 이해할 수 있다.</p><ul><li><h3 id="Amazon-Quicksight"><a href="#Amazon-Quicksight" class="headerlink" title="Amazon Quicksight"></a><strong>Amazon Quicksight</strong></h3><ul><li><p> Cloud native serverless BI 서비스이다.</p></li><li><p>데이터를 기반으로 시각화 dashboard를 생성하고 공유할 수 있다. (AWS 계정 불필요)</p></li><li><p>별도의 인프라 관리 없이 S3에 있는 페타바이트 규모의 데이터에 쿼리를 하고 시각화 할 수 있다.</p></li><li><p>다양한 데이터 소스(AWS/Third party cloud/On-premise 데이터)를 연결할 수 있다.</p></li><li><p>AWS 계정없이 사용할 수 있기 때문에 데이터 분석/시각화를 하는 타 부서에서 별도의 서비스로써 활용을 할 수 있다.</p></li><li><p>SPICE(Super-fast, Parallel, In-memory, Calculation, Engine)을 이용한 캐시로 조회 성능이 향상 되었다.</p></li><li><p>ML Insight 자체를 제공하기 때문에 ML 기반의 이상 탐지와 ML 기반의 예측을 할 수도 있다. 또한 자동 서술 기능으로 dashboard의 결과를 텍스트로 작성해준다고 한다.</p></li><li><p>Amazon SageMaker 통합을 통해서 복잡한 데이터 파이프라인 없이 ML 모델을 통합할 수 있다.</p></li><li><p>다양한 암호화 기능을 제공한다.(다양한 데이터 표준에 대한 Compliance / HIPAA, HITRUST CSF, GDPR, SOC, PCI, ISO 27001, FedRAMP(High))</p></li><li><h4 id="다양한-데이터-소스-제공"><a href="#다양한-데이터-소스-제공" class="headerlink" title="다양한 데이터 소스 제공"></a><strong>다양한 데이터 소스 제공</strong></h4><p>Quicksight에서는 아래의 다양한 서비스로부터 데이터 소스를 받아서 활용을 할 수도 있다.</p><ul><li>File</li><li>S3</li><li>Athena</li><li>Redshift</li><li>RDS</li><li>Aurora</li><li>MySQL</li><li>PostgreSQL</li><li>ORACLE</li><li>Salesforce</li><li>Presto</li><li>Snowflake</li></ul></li></ul></li></ul><h2 id="5주차-수업-후기"><a href="#5주차-수업-후기" class="headerlink" title="5주차 수업 후기"></a><ins><b>5주차 수업 후기</b></ins></h2><p>  이번 5주차 수업에서는 Flink에 대한 실습과 Elasticsearch의 운용과 관리에 대한 내용을 포함한 전반적인 내용들에 대해서 학습을 하였다. 사실 이전에 온라인 강의로 데이터 파이프라인 구축에 대한 수업을 들었을때에는 각 각의 비슷 비슷한 기능을 하는 구성요소가 왜 존재를 하는지, 그리고 단순히 AWS OpenSearch 서비스를 띄워서 Elasticsearch의 기능을 사용만 했었기 때문에 만약에 ElasticSearch를 별도의 서버에 구축해서 운영 및 관리를 할때에는 어떤식으로 해야되는지에 대해서는 전혀 알지 못했었다. 그런데 이번 수업을 통해서 Elasticsearch 클러스터 내의 각 노드들은 어떤식으로 구성이 되는지 구체적으로 알 수 있었다.(<code>내용에 대해 반복학습이 필요할 것 같다</code>) </p><h2 id="6주차-수업-내용"><a href="#6주차-수업-내용" class="headerlink" title="6주차 수업 내용"></a><ins><b>6주차 수업 내용</b></ins></h2><ul><li>Amazon QuickSight에 대한 이론 및 실습</li><li>마무리 통합 파이프라인 구축</li><li>기업사례에 대한 공유</li></ul>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Data-Pipeline/">Data-Pipeline</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/AWS/">AWS</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/26/202206/220626_datapipeline_study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220623 Terraform study</title>
      <link>https://leehyungi0622.github.io/2022/06/23/202206/220623-terraform-study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/23/202206/220623-terraform-study/</guid>
      <pubDate>Thu, 23 Jun 2022 05:23:00 GMT</pubDate>
      
        
        
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220614_terraform.png&quot; alt=&quot;Terraform&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 실제 프로젝트에서 AWS </description>
        
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220614_terraform.png" alt="Terraform"></div><br/><br/><p>이번 포스팅에서는 실제 프로젝트에서 AWS Topology를 Terraform 코드로 작성(IaC)해보면서 전체적인 프로젝트의 공통적인 구조를 어떻게 작성을 할 것인가에 대해 방향을 잡아가면서 정리한 내용을 작성해보려고 한다.</p><h2 id="데이터-파이프라인-구축에-있어-AWS-network-topology-구성"><a href="#데이터-파이프라인-구축에-있어-AWS-network-topology-구성" class="headerlink" title="데이터 파이프라인 구축에 있어, AWS network topology 구성"></a><ins><b>데이터 파이프라인 구축에 있어, AWS network topology 구성</b></ins></h2>  <div align="center">    <img src="/images/post_images/220623_basic_aws_network_topology.jpg" alt="기본 AWS 네트워크 토폴로지 구성">  </div><p>  우선 AWS의 서비스를 활용해서 데이터 파이프라인을 구축함에 있어, 전체적인 AWS network topology의 구성은 중요하다. 실습을 했을때는 default VPC, Subnet에 각 각의 AWS 서비스를 활용해서 network topology를 구성을 했지만, 실무에서는 default vpc와 subnet을 사용하지 않는 것을 권장한다고 한다.<br>  실제 업무에서는 Production에서 각 각의 서비스별로나 네트워크 보안 정책에 따라서 VPC를 나눠서 관리하고, 세팅하는 게 일반적이라고 한다. (<code>DE 현직자 오프라인 수업을 통해 배웠다.</code>)<br>  AWS 서비스 중에서 완전 관리형 서비스가 아닌 서비스들은 VPC를 타지만, 완전 관리형 서비스의 경우에는 VPC가 아닌 별도의 AWS Zone(AWS 전용 네트워크 라인)을 탄다고 한다.<br>  그래서 이번 프로젝트 진행에 있어, 각 프로젝트별로 하나의 AWS Network Topology가 나오기 때문에 위의 구성과 같이 전체적으로 구성을 해보려고 한다.</p>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Terraform/">Terraform</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Terraform/">Terraform</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/23/202206/220623-terraform-study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220619 데이터 파이프라인 구축 오프라인 수업 / 4주차</title>
      <link>https://leehyungi0622.github.io/2022/06/19/202206/220619_datapipeline_study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/19/202206/220619_datapipeline_study/</guid>
      <pubDate>Sun, 19 Jun 2022 04:34:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220530_review.png&quot; alt=&quot;Review&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 네 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. &lt;/p&gt;
&lt;h2 id=&quot;Kafka-실습환경-구축-및-Single-consumer-실습&quot;&gt;&lt;a href=&quot;#Kafka-실습환경-구축-및-Single-consumer-실습&quot; class=&quot;headerlink&quot; title=&quot;Kafka 실습환경 구축 및 Single consumer 실습&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Kafka 실습환경 구축 및 Single consumer 실습&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;  &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/post_images/220620_single_consumer_practice.jpg&quot; alt=&quot;Kafka 실습환경 구축 및 Single consumer 실습&quot;&gt;
  &lt;/div&gt;

&lt;p&gt;  이전에 Kafka를 실습했을때는 생성한 EC2 인스턴스에 apache kafka 압축파일을 다운받고, 압축을 풀고, zookeeper와 kafka server를 시작하고, Topic을 partitions와 replication-factor 옵션의 값을 1로 생성하고 bootstrap-server 옵션은 localhost의 9092번 포트로해서 설정하였다. (&lt;code&gt;kafka server: 9092, zookeeper: 2181&lt;/code&gt;) 이때 실습을 했을때는 세부 옵션에서 partitions이 뭔지 replication-factor가 뭔지 구체적으로 알지 못했었는데, &lt;code&gt;이제는 ISR 그룹이 뭔지 partition과 replication-factor이 뭔지 이해를 한 상태이기 때문에 좀 더 체계적으로 실습을 할 수 있는 것 같다.&lt;/code&gt; &lt;/p&gt;
&lt;p&gt;  그리고 이번 실습에서는 zookeeper와 kafka server를 하나의 터미널에서 명령어로 일일이 실행시키지 않고, zookeeper와 kafka host server, producer, consumer를 각각의 container로 구동을 시키기 때문에 구조적으로 좀 더 확장성 있는 것 같다. (&lt;code&gt;docker image를 이용해서 컨테이너 서비스로 구동시키기 때문에 좀 더 빠르게 환경 구성을 할 수 있는 것 같다. 이래서 컨테이너로 서버를 띄우고 관리하는 것 같다.&lt;/code&gt;)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220530_review.png" alt="Review"></div><br/><br/><p>이번 포스팅에서는 네 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. </p><h2 id="Kafka-실습환경-구축-및-Single-consumer-실습"><a href="#Kafka-실습환경-구축-및-Single-consumer-실습" class="headerlink" title="Kafka 실습환경 구축 및 Single consumer 실습"></a><ins><b>Kafka 실습환경 구축 및 Single consumer 실습</b></ins></h2>  <div align="center">    <img src="/images/post_images/220620_single_consumer_practice.jpg" alt="Kafka 실습환경 구축 및 Single consumer 실습">  </div><p>  이전에 Kafka를 실습했을때는 생성한 EC2 인스턴스에 apache kafka 압축파일을 다운받고, 압축을 풀고, zookeeper와 kafka server를 시작하고, Topic을 partitions와 replication-factor 옵션의 값을 1로 생성하고 bootstrap-server 옵션은 localhost의 9092번 포트로해서 설정하였다. (<code>kafka server: 9092, zookeeper: 2181</code>) 이때 실습을 했을때는 세부 옵션에서 partitions이 뭔지 replication-factor가 뭔지 구체적으로 알지 못했었는데, <code>이제는 ISR 그룹이 뭔지 partition과 replication-factor이 뭔지 이해를 한 상태이기 때문에 좀 더 체계적으로 실습을 할 수 있는 것 같다.</code> </p><p>  그리고 이번 실습에서는 zookeeper와 kafka server를 하나의 터미널에서 명령어로 일일이 실행시키지 않고, zookeeper와 kafka host server, producer, consumer를 각각의 container로 구동을 시키기 때문에 구조적으로 좀 더 확장성 있는 것 같다. (<code>docker image를 이용해서 컨테이너 서비스로 구동시키기 때문에 좀 더 빠르게 환경 구성을 할 수 있는 것 같다. 이래서 컨테이너로 서버를 띄우고 관리하는 것 같다.</code>)</p>  <a id="more"></a><ul><li><h3 id="실습전-Setting"><a href="#실습전-Setting" class="headerlink" title="[실습전 Setting]"></a><strong>[실습전 Setting]</strong></h3><p>이번 docker-compose.yml 파일 안에 zk1<del>zk3, kafka1</del>kafka3, producer와 consumer에 대한 service를 미리 지정해서 docker image로부터 지정한 환경변수를 setting해서 구성하였다.<br>(<code>Kafka와 Zookeeper는 각 각 3대이상 설치가 되어있어야 한다.</code>)</p><ul><li><h4 id="Q1-Producer와-Consumer-서비스-부분은-restart-on-failure-cluster-zookeeper-부분은-restart-always로-설정되어있는데-그-이유는"><a href="#Q1-Producer와-Consumer-서비스-부분은-restart-on-failure-cluster-zookeeper-부분은-restart-always로-설정되어있는데-그-이유는" class="headerlink" title="Q1. Producer와 Consumer 서비스 부분은 restart: on-failure, cluster / zookeeper 부분은 restart: always로 설정되어있는데 그 이유는?"></a><strong>Q1.</strong> Producer와 Consumer 서비스 부분은 <code>restart: on-failure</code>, cluster / zookeeper 부분은 <code>restart: always</code>로 설정되어있는데 그 이유는?</h4>docker-compose.yml 파일을 보던 중 이 restart policy에 대해서 의문이 생겼었다. 이전에 docker에 대해서 학습을 했었기 때문에 어떤 역할을 하는지는 알았지만, 왜 두 개를 다르게 설정을 했지? 라는 궁금증이 생겨서 오프라인 수업때 강사님께 여쭤보았다. 어떻게 보면, 좀 이상할 수 있는 질문이긴 했지만, 모르니깐 배우는 거고, 질문하는 거라고 생각한다. <del>비싼 돈을 내고 듣는 수업이니 많이 배워야한다.</del> <br/></li><li><h4 id="A-Q1"><a href="#A-Q1" class="headerlink" title="A(Q1)"></a><strong>A(Q1)</strong></h4>producer와 consumer는 ssh로 연결해서 사용할 PC의 개념이기 때문에 restart:always로 설정하게 되면, 인스턴스를 껐을때 항시 컨테이너가 재시작이 되버린다. 그래서 <code>restart: on-failure</code>로 해서 에러로 인한 exit이 발생하였을 경우에만 재시작되도록 설정한 것이다. 그리고 Kafka cluster는 항시 구동이 된 상태에서 에러로 인해 중지가 되면, 다시 자동으로 구동시켜야되기 때문에 <code>restart: always</code>정책으로 restart를 설정해줘야 한다.<br>이렇게 설명을 듣고 보니, 깔끔하게 이해가 되었다.    <br/><br/></li></ul></li><li><h3 id="Single-consumer-실습"><a href="#Single-consumer-실습" class="headerlink" title="[Single consumer 실습]"></a><strong>[Single consumer 실습]</strong></h3><p>실습을 위한 환경 구성에 대해 준비가 끝나고 이제 Single consumer에 대한 실습을 하였다. 현재 실습 환경에 대해서 간단하게 살펴보면, ZK 3대, Kafka 3대, Producer와 Consumer host가 각 각 1대로 구성이 되어있다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># producer_host에 bash shell로 ssh 연결</span></span><br><span class="line"><span class="variable">$docker</span> <span class="built_in">exec</span> -it producer_host /bin/bash</span><br><span class="line"><span class="comment"># docker container로 kafka 서비스를 setup할때, confluentinc/cp-kafka 이미지를 사용하였는데, Apache kafka를 위한 docker이미지이다.</span></span><br><span class="line"><span class="comment"># kafka-console-producer는 Apache Kafka 내 명령이다.</span></span><br><span class="line"><span class="variable">$kafka</span>-console-producer --bootstrap-server kafka1:9091 --topic ft</span><br><span class="line"><span class="comment"># --bootstrap-server 옵션은 실제 명령으로 Kafka에 붙을때는 bootstrap server라고 해서, Kafka에서 노출하고 있는 host의 port로 붙어서 데이터를 쏴주게 된다.</span></span><br><span class="line"><span class="comment"># kafka host로 producer를 붙여줌과 동시에 topic을 생성해줄 수 있다.(--topic 옵션)</span></span><br></pre></td></tr></table></figure><p>위에서는 producer에서 생성된 데이터를 ft topic에 전송하고, kafka1:9091에 전달해서 처리를 하고 있지만, kafka1 host는 클러스터 내의 host들 중에서 하나로 클러스터링 되어있기 때문에 다른 Kafka host들에 데이터를 분산하여 처리한다.<br>위의 명령을 하면, &gt; prompt로 Kafka cluster에 있는 Kafka host들로 데이터를 쏴줄 수 있는 환경이 Setting된 것이다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 생성된 kafka topic의 리스트를 확인 (모든 kafka host에서 확인 가능)</span></span><br><span class="line"> <span class="variable">$kafka</span>-topics --bootstrap-server kafka1:9091 --list</span><br><span class="line"></span><br><span class="line"> <span class="comment"># </span></span><br><span class="line"> <span class="variable">$kafka</span>-topics --bootstrap-server kafka1:9091 --topic ft --describe</span><br><span class="line"> <span class="comment"># Topic: ftPartitionCount: 1  ReplicationFactor: 1  Configs:</span></span><br><span class="line"><span class="comment"># Topic: ftPartition: 0      Leader: 1Replicas: 1  Isr: 1</span></span><br></pre></td></tr></table></figure><p>위에서 Kafka cluster의 구성을 자세하게 살펴보기 위해 명령으로 확인을 할 수 있는데, 내용을 자세히 살펴보면 별도로 kafka-topic에 대해 <code>partition</code>이나 <code>replication-factor</code>을 설정하지 않았기 때문에 default로, PartitionCount: 1로, partition은 하나(partition 0), ReplicationFactor도 하나(리더만), replicas값이 1인 것은 리더만 1번째 브로커에 할당이 되었기 때문이다. (producer관점에서 acks=1과 동일한 결과(<code>프로듀서가 리더 파티션에 메시지를 전송하고, 리더로부터 ack를 기다린다. 단, 리더로부터 데이터를 복제해서 갖고 있는 팔로워들에게 까지의 전달은 확인하지 않는다</code>))그리고 마지막으로 Isr은 1개로, partition이 하나이기 때문에 ISR 그룹도 하나 존재한다.<br>파티션의 갯수는 브로커의 갯수만큼 나눠줘야 안정적으로 처리가 가능하다.</p><br/><p>그 다음으로 producer에서 topic을 통해 kafka cluster의 kafka host(broker)로 데이터를 전달을 하고, 전달한 데이터를 consumer에서 뽑아내기 위한 setting이 필요하다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apache kafka 내의 kafka-console-consumer 명령으로, consumer를 kafka host에 붙인다.</span></span><br><span class="line"><span class="comment"># 여기서 producer에서 데이터를 쏠때 지정했던 kafka host와 다른 host를 지정해도 데이터를 정상적으로 받아올 수 있다. 그 이유는 kafka host1, 2, 3이 클러스터링 되어있기 때문이다.</span></span><br><span class="line"><span class="comment"># topic은 producer에서 생성한 topic을 바라보게 만든다.</span></span><br><span class="line"><span class="variable">$kafka</span>-console-consumer --bootstrap-server kafka2:9092 --topic ft</span><br></pre></td></tr></table></figure><p>위의 구성이 끝나면 producer에서 전송한 텍스트 메시지를 consumer에서 받아서 화면에 출력하게 된다. 지금은 간단하게 consumer console 명령으로 붙여서 실습을 했지만, <code>Logstash나 Flink와 같은 오픈 소스 툴들을 양단에 붙여서 데이터 스트리밍 실시간 처리를 할 수 있다.</code></p><ul><li><h4 id="실무-팁"><a href="#실무-팁" class="headerlink" title="[실무 팁]"></a><strong>[실무 팁]</strong></h4>실제 업무시에는 Kafka의 앞단에 Load balancer를 붙여서 Kafka host를 하나로 묶어주기도 한다.</li></ul><br/></li><li><h3 id="Multi-consumer-실습"><a href="#Multi-consumer-실습" class="headerlink" title="[Multi consumer 실습]"></a><strong>[Multi consumer 실습]</strong></h3><p>이번 실습에서는 위의 Single consumer 실습과 전반적인 구성은 같지만, consumer_host가 2개 존재한다. 실습은 consumer_host를 각기 다른 consumer group으로 지정해서 데이터가 어떤식으로 받아오는지에 대해 자세히 살펴본다.<br>우선 topic을 커스텀해서 생성할 것이기 때문에 아래와 같이 <code>--replication-factor</code>와 <code>--partitions</code> 옵션을 지정해서 커스텀해준다.</p><p>partition이 하나인데, consumer가 2개이면, consumer 한 대는 놀기 때문에 파티션을 2개로 주는 것이 좋다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이번 multi consumer 실습에서는 topic을 자동생성하지 않을 것이기 때문에 우선적으로 topic을 생성해준다.</span></span><br><span class="line"><span class="comment"># topic은 --replication-factor와 --partitions 옵션으로 각 각 2를 준다.</span></span><br><span class="line"><span class="comment"># replication-factor를 2를 주었기 때문에 리더 하나와 팔로우 하나로 ISR 그룹이 구성이 된다.</span></span><br><span class="line"><span class="comment"># 그리고 partition은 2개로 구성되도록 설정하고 생성한다.</span></span><br><span class="line"><span class="variable">$kafka</span>-topics --bootstrap-server kafka1:9091 --replication-factor 2 --partitions 2 --topic st --create</span><br><span class="line"></span><br><span class="line"><span class="variable">$kafka</span>-topics --bootstrap-server kafka1:9091 --list <span class="comment"># 생성된 topic 리스트 확인</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$kafka</span>-topics --bootstrap-server kafka1:9091 --topic st --describe</span><br><span class="line"><span class="comment"># topic의 파티션은 위에서 지정해준 것 같이 총 2개 (partition 2, 3) 생성되었다.</span></span><br><span class="line"><span class="comment"># 그리고 t-0, t-1 각 각의 파티션 Leader는 2번째와 3번째 브로커에 각 각 할당이 되었고,</span></span><br><span class="line"><span class="comment"># 리더가 2번 브로커에 할당된 친구(t-0)는 Replicas 값이 2(리더가 2번 브로커), 3(팔로워가 3번 브로커) </span></span><br><span class="line"><span class="comment"># 리더가 3번 브로커에 할당된 친구(t-1)는 Replicas 값이 3(리더가 3번 브로커), 1(팔로워가 1번 브로커)에 배치었다.</span></span><br><span class="line"><span class="comment"># Isr 그룹은 리더와 팔로워가 배치된 브로커의 위치를 (리더, 팔로워)순으로 표기한 것이다. </span></span><br><span class="line">Topic: stPartitionCount: 2ReplicationFactor: 2Configs:</span><br><span class="line">Topic: stPartition: 0Leader: 2Replicas: 2,3Isr: 2,3</span><br><span class="line">Topic: stPartition: 1Leader: 3Replicas: 3,1Isr: 3,1</span><br></pre></td></tr></table></figure><div align="center">  <img src="/images/post_images/220620_partition_replication_factor.jpg" alt="Kafka cluster의 Partition과 Replication factor 설정"></div><p>위와같이 하나의 파티션은 replication-factor의 수 만큼 복제되어 분산되어있는 것을 확인할 수 있다.</p><ul><li><h4 id="Consumer-group-구성하기"><a href="#Consumer-group-구성하기" class="headerlink" title="Consumer group 구성하기"></a><strong>Consumer group 구성하기</strong></h4></li></ul><div align="center">  <img src="/images/post_images/220620_multi_consumer_structure.jpg" alt="Kafka multi consumer 실습 구조"></div><p>  Consumer group은 <code>group-1</code> 이름으로 두 개의 consumer_host를 묶어서 구성하고, group별로 offset을 달리해서 데이터를 뽑아내는 것을 확인하기 위해 별도의 consumer_host를 하나 더 생성해서 별도의 <code>group-2</code> customer group을 생성하도록 한다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$docker</span> <span class="built_in">exec</span> -it consumer_host1 /bin/bash</span><br><span class="line"><span class="variable">$kafka</span>-console-consumer --bootstrap-server kafka1:9091 --topic st --group group-1</span><br><span class="line"></span><br><span class="line"><span class="variable">$docker</span> <span class="built_in">exec</span> -it consumer_host2 /bin/bash</span><br><span class="line"><span class="variable">$kafka</span>-console-consumer --bootstrap-server kafka1:9091 --topic st --group group-1</span><br><span class="line"></span><br><span class="line"><span class="variable">$docker</span> <span class="built_in">exec</span> -it consumer_host3 /bin/bash</span><br><span class="line"><span class="variable">$kafka</span>-console-consumer --bootstrap-server kafka1:9091 --topic st --group group-2</span><br></pre></td></tr></table></figure><ul><li><h4 id="기존-컨테이너-서비스가-유지된-상태에서-새로운-컨테이너-서비스-추가하기"><a href="#기존-컨테이너-서비스가-유지된-상태에서-새로운-컨테이너-서비스-추가하기" class="headerlink" title="기존 컨테이너 서비스가 유지된 상태에서 새로운 컨테이너 서비스 추가하기"></a><strong>기존 컨테이너 서비스가 유지된 상태에서 새로운 컨테이너 서비스 추가하기</strong></h4><p>원래 이전 docker-compose.yml에는 consumer_host3가 없었는데, 추가하려고 한다. 현재 docker container 서비스들이 실행되는 것은 유지된 상태에서 변경된 docker container를 추가해서 적용하려고 하는데, 어떻게 해야할까?</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$docker</span>-compose up --build --force-recreate -d</span><br><span class="line"><span class="comment"># --build : 컨테이너를 시작하기 전에 이미지를 빌드한다. </span></span><br><span class="line"><span class="comment"># --force-recreate : 컨테이널르 재생성한다. docker-compose up을 하면, 변경된 사항을 적용하여 컨테이너를 재생성하지만, up을 했을때에도 변경이 적용이 안되는 경우, 이 옵션을 적용시킨다.</span></span><br></pre></td></tr></table></figure><p>우선 docker-compose.yml 파일을 수정하고, 위의 명령을 하면, 아래와 같이 새롭게 추가된 consumer_host3가 새롭게 생성되는 것을 확인할 수 있다.</p><div align="center">  <img src="/images/post_images/220620_docker_add_new_producer.png" alt="새로운 consumer 추가"></div><p>새롭게 consumer_host3을 추가해서 group-2의 consumer_host로 배정했다.</p><div align="center">  <img src="/images/post_images/220620_multi_consumer_group_practice.png" alt="multi-consumer 구성 실습"></div><p>실습한 결과, 같은 group 내의 consumer_host는 producer로부터 전송되는 메시지 데이터를 분산해서 처리하고 있음을 알 수 있었고, 새롭게 추가한 group-2의 consumer_host는 단일 호스트이기 때문에 producer로부터 전송되는 데이터가 전부 단일 호스트로 들어오고 있음을 확인할 수 있었다.</p></li></ul><br/><ul><li><h4 id="Q-amp-A-1"><a href="#Q-amp-A-1" class="headerlink" title="[Q&amp;A(1)]"></a><strong>[Q&amp;A(1)]</strong></h4><p>partition이 2개인 상태에서 consumer를 한 개 더 늘려서 총 3대가 되면, consumer에서 나눠서 처리하는가에 대한 질문이 있었는데, partition이 2개이기 때문에 consumer host 2대에서 나눠갖고 한 대는 놀게 되는 구조가 된다.<br>따라서 partition의 수가 충분하다면 consumer를 늘려서 나눠갖는 것이 좋다.</p></li><li><h4 id="Q-amp-A-2"><a href="#Q-amp-A-2" class="headerlink" title="[Q&amp;A(2)]"></a><strong>[Q&amp;A(2)]</strong></h4><p>순서를 보장해야되는 특별한 경우에는 단일 broker 내에서는 순서가 보장이 되기 때문에 partition을 하나로 사용하는 경우도 있다. 하지만 데이터의 처리상의 안전성을 위해서는 기본적으로 partition 2개 이상을 사용하는 것이 좋다.</p></li><li><h4 id="실무-팁-1"><a href="#실무-팁-1" class="headerlink" title="(실무 팁)"></a><strong>(실무 팁)</strong></h4><p>Broker를 두면 좋은 이유는 A부서가 기존에 Broker로부터 데이터를 받아서 전처리를 하고 있었는데, 신규 사업때문에 B부서가 생겨난 경우, 새롭게 데이터를 받아올 필요가 있을때 이 Broker에 Consumer_host를 붙여서 새로운 데이터셋을 받아오면 되기 때문에 매우 유용하다.<br>그리고 A부서에서 broker에 물리고 있는 consumer group의 consumer_host가 producer로부터 오는 데이터를 감당할 수 없다면, partition과 consumer_host를 늘려서 빠르게 병렬처리해서 처리하도록 하면 된다. 단, Kafka에서는 partition을 늘릴 수는 있어도 줄일 수는 없다. 따라서 partition을 늘리는 경우에는 신중하게 생각하고 늘리도록 해야한다. Partition을 늘리는 경우는 브로커 한 대에서 얼마나 처리할 수 있는가에 대한 고려가 필요하다.<br>MSK를 사용하면 통계된 데이터가 나오는데, 통계된 데이터를 기반으로 의사결정을 하면 된다.<br>만약에 데이터를 기존 consumer에서 처리를 못하는 경우에 우선 partition을 늘리고, consumer를 늘리도록 하는 것이 맞다. 그리고 좀 드문 경우인데 producer가 생성하는 데이터의 양 만큼 Partition을 늘리는 경우이다. (<code>현재 producer로부터 오는 데이터를 Partition(Broker)에서 견디지 못하는 경우</code>)</p></li></ul></li></ul><h2 id="Amazon-Kinesis"><a href="#Amazon-Kinesis" class="headerlink" title="Amazon Kinesis"></a><ins><b>Amazon Kinesis</b></ins></h2><p>   Amazon에서 만든 Kafka와 비슷한 서비스이다. Amazon kinesis 서비스는 내부적으로 Amazon Kinesis Data Analytics, Amazon Kinesis Data Streams, Amazon Kinesis Firehose, video stream이 있다. </p><p>   카프카를 대체할 수 있으며, 저지연 스트리밍을 위한 서비스이다. EC2, Client, Agent, 사용자가 개발한 코드에서 생산된 데이터를 받고, 이를 다른 서비스에서 소비할 수 있도록 처리해준다.<br>   <code>Kafka의 Partition</code>과 같은 것이 <code>Kinesis의 shard</code>이다. 그리고 Data retention은 24시간에서 1년까지 가능하고, 데이터 보존기간이 길어질수록 가격이 비싸진다.<br>   Kafka는 partition을 늘리면 다시 줄일 수 없지만, Kinesis는 reshard가 가능하다.(<code>split &amp; merge</code>)</p><ul><li><p><strong>Data Stream</strong>: 데이터 스트림을 캡처, 처리 및 저장(<code>Kafka와 대응</code>)</p></li><li><p><strong>Data Firehose</strong>: 데이터 스트림을 AWS 데이터 스토어로 저장(<code>S3, Redshift에 저장, Fluentd, Logstash와 대체가능</code>)</p></li><li><p><strong>Data Analytics</strong>: SQL 또는 Apache Flink로 데이터 스트림 분석</p></li><li><p><strong>Video Stream</strong>: 비디오 스트림을 캡처, 처리 및 저장 </p><p>실시간으로 비디오 및 데이터 스트림을 손쉽게 수집, 처리 및 분석할 수 있는 <code>완전 관리형 서비스</code>이다. <code>(별도로 spec에 대한 설정이 필요가 없다)</code></p></li><li><h3 id="구조"><a href="#구조" class="headerlink" title="구조"></a><strong>구조</strong></h3><p><strong>(STEP1)</strong> Input (Producer와 동일 레벨) Kinesis producer SDK, Kinesis producer Library(KPL), Kinesis agent<br><strong>(STEP2)</strong> <code>Amazon Kinesis Data Streams</code><br><strong>(STEP3)</strong> Amazon Kinesis Data Analytics, Spark on EMR, Amazon EC2, AWS Lambda<br><code>Output</code> - (Consumer와 동일 레벨) Kinesis consumer SDK, Kinesis Client Library(KCL), Kinesis Connector Library, <code>Third party(Spark, Flume, Kafka connect, Flink)</code>, Kinesis Firehose, AWS Lambda 등으로 전달을 할 수 있으며, Kinesis Data Stream으로부터 받은 데이터를 AWS Lambda에서 특정 시간 주기로 함수를 실행시켜서 데이터가 처리될 수 있게 할 수 있다.<br><strong>(STEP4)</strong> 최종적으로 BI툴을 활용해서 consumer로부터 받은 데이터를 시각화시켜서 처리할 수 있다.</p></li><li><h3 id="Amazon-Kinesis-Data-Firehose"><a href="#Amazon-Kinesis-Data-Firehose" class="headerlink" title="Amazon Kinesis Data Firehose"></a><ins><b>Amazon Kinesis Data Firehose</b></ins></h3><p>Data Firehose를 사용해서 데이터를 transformation하는 것이 가능하다. (Format conversion(Parquet, ORC), Encryption)<br>만약에 데이터를 custom하고자 할때는 Lambda를 붙여서 DataStream으로부터 넘어온 데이터를 Lambda로 밀어넣고 처리된 데이터를 Data Firehose로 넘겨주도록 처리하면 된다.</p><ul><li><h4 id="연결-서비스"><a href="#연결-서비스" class="headerlink" title="[연결 서비스]"></a><strong>[연결 서비스]</strong></h4>Data Firehose의 뒷단에는 Amazon S3, Amazon Redshift, Amazon Elasticsearch, Splunk 등의 서비스에 연결시켜서 처리된 데이터를 BI 툴에 연결시켜서 분석 및 시각화를 할 수 있다.</li></ul></li><li><h3 id="Amazon-Kinesis-Data-Analytics"><a href="#Amazon-Kinesis-Data-Analytics" class="headerlink" title="Amazon Kinesis Data Analytics"></a><ins><b>Amazon Kinesis Data Analytics</b></ins></h3><p>input과 output 사이에 붙여서 SQL과 Flink를 붙일 수 있도록 해주는 Amazon의 서비스이다.<br>(Input -&gt; Amazon Kinesis Data Streams -&gt; <code>Amazon Kinesis Data Analytics</code> -&gt; Amazon Lambda -&gt; Amazon DynamoDB -&gt; Output의 구성과 같이 Data Stream으로부터 들어온 데이터를 Data Analytics로 붙여서 실시간으로 들어온 데이터를 분석할 수 있다)<br>실시간으로 데이터를 처리하기 때문에 대용량의 로그 데이터를 처리(분석)하는데 매우 빠른 시간내에 처리할 수 있으며, 문제를 실시간으로 발견 및 대응을 할 수 있기 때문에 고가용성은 물론 우수한 고객 경험을 줄 수 있다. (<code>Netflix 사용 사례</code>)</p></li></ul><h2 id="Amazon-Kinesis-실습"><a href="#Amazon-Kinesis-실습" class="headerlink" title="Amazon Kinesis 실습"></a><ins><b>Amazon Kinesis 실습</b></ins></h2><ul><li><h3 id="실습-구조"><a href="#실습-구조" class="headerlink" title="실습 구조"></a><ins><b>실습 구조</b></ins></h3><p>STEP1. Amazon Kinesis Data Generator (<code>랜덤 로그 데이터 생성</code>)<br>STEP2. Kinesis Data Stream<br>STEP3. Kinesis Firehose<br>STEP4. S3</p></li><li><h3 id="CloudFormation"><a href="#CloudFormation" class="headerlink" title="CloudFormation"></a><ins><b>CloudFormation</b></ins></h3><p>CloudFormation을 활용하면 Data Generator환경을 생성할 수 있다.그리고 IaC(Infrastructure as Code)의 일환이다.<br>CloudFormation으로 랜덤한 로그 데이터를 생성하는 producer를 구성하기 위해서는 Template이 필요하다.</p><ul><li><h4 id="Amazon-Congnito"><a href="#Amazon-Congnito" class="headerlink" title="Amazon Congnito"></a><strong>Amazon Congnito</strong></h4><p>Amazon Cognito는 웹 및 모바일 앱을 위한 Authentication, User management의 기능을 제공한다. 사용자는 thrid party 웹 페이지를 통하거나 직접적으로 사용자 이름과 비밀번호를 입력해서 로그인을 할 수 있다.<br>cognito는 user pools과 identity pools 두 가지 메인 컴포넌트가 있다. user pools는 사용자 디렉토리로, 앱에서의 회원가입과 로그인에 대한 옵션을 사용자에게 제공한다.<br>identity pools는 다른 AWS 서비스들에 접근하는 권한을 사용자들에게 부여할 수 있도록 허용을 해주며, user pools와 identity pools를 같이 혹은 나눠서 사용할 수 있다.</p></li></ul><p>실습을 위해 주어진 congnito-setup.json의 내용을 보니, 해당 json파일은 인증된 사용자에게 cognito의 identity pool에서의 역할을 할당하고, 사용자가 Kinesis Data Generator tool을 사용할 수 있도록 해주는 설정들이 담겨져있는 파일이다. (<code>+Lambda 함수와 Cognito를 붙여주는 작업</code>)<br>해당 파일을 template으로써 지정해서 stack을 생성하면 된다.</p><p>생성을 해주게 되면, Output 탭에서 링크가 생성된 것을 볼 수 있는데, <code>template으로부터 생성된 Kinesis Data Generator tools를 사용할 수 있는 링크</code>이다.</p><p>이제 username과 password를 입력해서 들어가면, UI상으로 Region, Stream/Delivery stream을 선택해서 생성된 랜덤한 로그 데이터를 쏴줄 수 있다.</p></li><li><h3 id="Kinesis-stream-서비스-생성"><a href="#Kinesis-stream-서비스-생성" class="headerlink" title="Kinesis stream 서비스 생성"></a><ins><b>Kinesis stream 서비스 생성</b></ins></h3><p>이제 Kinesis Data Generator를 통해 생성된 랜덤 sensor 데이털르 쏴 줄 target인 Kinesis data streams와 Kinesis data firehose 생성을 한다. </p><ul><li><h4 id="Kinesis-streams"><a href="#Kinesis-streams" class="headerlink" title="Kinesis streams"></a><strong>Kinesis streams</strong></h4><p>샤드 계산기를 통해서 적정 샤드의 수를 계산해서 프로비저닝된 샤드의 갯수를 설정하고 Kinesis streams를 생성해주면 된다. (<code>기본 데이터 retention 시간은 24시간이다</code>)</p></li><li><h4 id="Kinesis-delivery-stream"><a href="#Kinesis-delivery-stream" class="headerlink" title="Kinesis delivery stream"></a><strong>Kinesis delivery stream</strong></h4><p>source는 생성한 data streams으로 하고, destination은 S3를 선택하고 생성해주면 된다. 추가적으로 firehose에서는 생성한 lambda function을 붙여서 데이터를 transformation할 수 있다. 이외에 옵션으로 버퍼 사이즈나 버퍼 인터벌 시간을 조절 할 수 있는데, 버퍼 인터벌 사이즈를 줄여서 반응시간을 빠르게 할 수 있다.</p></li></ul></li></ul><hr><h2 id="Kappa-Architecture-The-concentration-in-stream-processing-스트림-프로세싱-심화"><a href="#Kappa-Architecture-The-concentration-in-stream-processing-스트림-프로세싱-심화" class="headerlink" title="Kappa Architecture : The concentration in stream processing(스트림 프로세싱 심화)"></a><ins><b>Kappa Architecture : The concentration in stream processing(스트림 프로세싱 심화)</b></ins></h2><p>  Kappa Architecture에서는 배치와 스트림 프로세스를 모두 실시간으로<br>  처리된다. (<code>Lambda Architecture에서 배치 레이어가 없어진 형태</code> - <code>파이프라인의 구조 단순화</code>)<br>  현대에 와서는 컴퓨터 리소스와 컴퓨팅 기술, 스트림처리 엔진에 대한 기술의 발달로 모든 처리(배치, 스트림 프로세스)를 실시간 스트림으로 처리하는 것이 가능해졌다.<br>  이에 따라 개발/운영 이중화에 대한 부하가 줄어들게 되고, 이는 Kafka를 개발한 Jay Kreps에 의해 제안되었다.</p><ul><li><h3 id="디즈니-Disney"><a href="#디즈니-Disney" class="headerlink" title="디즈니(Disney)"></a><ins><b>디즈니(Disney)</b></ins></h3><p>디즈니라고 하면 애니메이션만 만드는 회사라고만 알고 있었는데, 데이터를 많이 사용해서 앞서나가는 아키텍처를 구축하고 있는 기업이라고 한다.</p><p>수업에서는 Lambda Architecture에 대한 Disney의 생각이라는 section으로 설명을 해주셨는데, 아래와 같이 디즈니에서는 Lambda Architecture에 대한 생각을 가지고 있다고 한다.</p><ul><li><h4 id="Duplicate-Code"><a href="#Duplicate-Code" class="headerlink" title="Duplicate Code"></a><strong>Duplicate Code</strong></h4><p>Deplicated code, Lambda Architecture는 구조상 보면, 스트림 처리와 배치 처리하는 구간이 나눠져 있기 때문에 <code>서로 다른 두 코드에 대한 관리를 위한 개발팀과 유닛 테스트가 필요</code>하다. 따라서 처리상 하나만 바뀌어도 모두 전파해야하고, 릴리즈 또한 연동이 되어야한다. </p></li><li><h4 id="Data-Quality"><a href="#Data-Quality" class="headerlink" title="Data Quality"></a><strong>Data Quality</strong></h4><p>Batch와 Speed layer 간의 알고리즘이 일치하는지에 대한 확인과 입증이 필요하다. </p></li><li><h4 id="Added-Complexity"><a href="#Added-Complexity" class="headerlink" title="Added Complexity"></a><strong>Added Complexity</strong></h4><p>추가적인 복잡도에 대한 고려가 필요하다. 두 파이프라인에서의 데이터 중 어떤 데이터를 언제 읽을지에 대해 고려해야되며, 배치 잡의 딜레이되는 경우에 대해서도 고려해야한다.</p></li><li><h4 id="Two-Distributed-System"><a href="#Two-Distributed-System" class="headerlink" title="Two Distributed System"></a><strong>Two Distributed System</strong></h4><p>하나의 데이터 처리에 대해서 두 개의 파이프라인으로 나뉘어서 같은 데이터에 대해 처리하므로, 두 배의 인프라를 구성해야 하고, 모니터링과 로깅도 각 각 나눠서 해야되기 때문에 2배의 리소스가 든다.</p></li></ul></li><li><h3 id="Single-pipeline-for-streaming-and-batch-consumer"><a href="#Single-pipeline-for-streaming-and-batch-consumer" class="headerlink" title="Single pipeline for streaming and batch consumer"></a><ins><b>Single pipeline for streaming and batch consumer</b></ins></h3><p>유입되는 Data resource가 단일 Real-Time Layer을 통해 좀 더 민첩하게 처리해야 할 데이터는 ms 단위로 처리하고, 덜 민첩하게 처리해도 되는 데이터에 대해서는 Real-Time Layer를 통해 들어온 데이터를 담고 있는 Storage를 통해 min/hr 단위로 배치처리를 하게 된다.</p></li><li><h3 id="스트림-프로세싱의-심화"><a href="#스트림-프로세싱의-심화" class="headerlink" title="스트림 프로세싱의 심화"></a><ins><b>스트림 프로세싱의 심화</b></ins></h3><p>여지까지 Kafka와 Kinesis를 이용해서 데이터를 전달하고 적재하는 부분까지는 했고, Lambda나 Kinesis Firehose에서 정형화된 데이터의 transformation을 경험했다.</p><p>이번 시간에는 스트림 프로세싱에서 <code>처리의 관점</code>에서 데이터를 어떻게 transformation할 것인지에 대해 고도화시킬 수 있는 방법에 대해서 심도있게 다룬다. 오픈소스나 클라우드 서비스에서 제공되는 기능 이외에 커스텀해서 구성할 수 있는 방법들이 많기 때문에 이 부분에 대해서도 배우게 된다.</p><br/></li><li><h3 id="Apache-Flink-stream-engine"><a href="#Apache-Flink-stream-engine" class="headerlink" title="Apache Flink (stream engine)"></a><ins><b>Apache Flink (stream engine)</b></ins></h3><div align="center">  <img src="/images/post_images/220623_apache_flink.png" alt="Apache Flink"></div><p>분산 스트림 처리 프레임워크이며, Spark streaming와 대응된다. Spark streaming은 Flink와 비교했을때 성능면에서 많이 차이가 나지만, 국내에서 Spark 사용자들이 많기 때문에 Spark로 배치처리를 하다가 Streaming이 필요한 경우에 기존 기술셋을 사용해서 기존에 사용하던 언어를 사용하기 위해 Spark streaming을 도입한느 경우가 많다. 하지만 Flink가 기능이나 생태계를 보면, Spark보다 Flink가 더 스티리밍 데이터(<code>데이터의 제한이 없는 무한한 데이터</code>) 처리에 최적화가 되어있다.</p><p>Flink를 도입해서 사용하고 있는 기업은 Alibaba, Tencent, Uber, AWS, Lyft, SKT, Kakao, NAVER, Toss, Coupang, 하이퍼커넥트 등이 있다.<br>Flink는 Native streaming 방식(<code>건 by 건으로 처리하는 방식</code>-Performance 좋음)을 채택하고 있으며, micro batch 방식(<code>작은 단위(5초, 1분 단위)로 배치처리를 함으로써 마치 실시간 스트리밍 처리를 하는 것과 같은 효과를 주는 방식</code>)을 사용하고 있는 Spark와 대비된다.<br>그리고 Flink는 High Performance, low latency, exactly-once라는 특징을 가지고 있지만, 처리 속도는 빠르다는 장점을 가지고 있다. 그리고 <code>Flink는 Java(86%)와 Scala(10.1%), Python(2.4%)로 만들어진 프로젝트</code>이다.<br>그리고 Flink에서 사용할 수 있는 언어로는 Java, Python, Scala로 개발을 할 수 있다.</p><br/></li><li><h4 id="Apache-Flink의-특징"><a href="#Apache-Flink의-특징" class="headerlink" title="[Apache Flink의 특징]"></a><strong>[Apache Flink의 특징]</strong></h4><ul><li><p>Flink를 사용해서 스트림 및 배치 처리를 둘 다 할 수 있다. (통합 데이터 처리 엔진)</p></li><li><p>High Performance(Native Stream, Low Latency, High Throughput)<br><strong>-&gt;</strong> Spark streaming처럼 micro/mini batch 구조가 아닌, 완전 스트림 최적화 방식을 사용하고 있다.</p></li><li><p>Fault tolerance</p><div align="center">  <img src="/images/post_images/220623_flink_checkpoint.svg" alt="Apache Flink의 checkpoint"></div><p><strong>-&gt;</strong> Flink의 핵심인 Checkpoint를 통해 Exactly-once를 지원한다. Checkpoint는 분산 체크포인팅 및 분사 스냅샷 기술의 일종으로, 작업 중간 중간에 스냅샷을 찍어서 장애 발생시 스냅샷을 한 상태로 되돌릴 수 있다. 내부적으로는 <code>Chandy-Lamport</code>라는 알고리즘을 개선한 알고리즘이 적용되어있다고 한다.(<code>나중에 찾아서 읽어보기</code>)<br>   <a href="https://en.wikipedia.org/wiki/Chandy%E2%80%93Lamport_algorithm">https://en.wikipedia.org/wiki/Chandy%E2%80%93Lamport_algorithm</a></p></li><li><p>Message guarantees로 <code>Exactly-once</code>를 지원한다.</p></li><li><p>DataStream API, DataSet API(legacy), Table API를 지원하기 때문에 All rounder로 볼 수 있다.</p></li><li><p>Batch 처리를 위한 별도의 Batch runtime mode가 제공되기 때문에 배치 처리도 가능</p></li><li><p>env.setRuntimeMode(RuntimeExecutionMode.BATCH);<br><strong>[참고]</strong>: <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution_mode/">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution_mode/</a></p></li></ul></li><li><h4 id="Flink의-구조"><a href="#Flink의-구조" class="headerlink" title="[Flink의 구조]"></a><strong>[Flink의 구조]</strong></h4><div align="center">  <img src="/images/post_images/220623_flink_architecture.png" alt="Flink Architecture"></div><p>Flink Architecture는 크게 Job Manager는 Master 노드의 역할로, Task Manage(Worker)의 작업 스케줄링을 하는 역할과 Streaming 데이터의 각 구간에 Checkpointing을 하는 역할과 복구를 하는 역할을 한다.<br>Task Manager는 실제 일을 하는 Worker로, 분산된 Operator들은 TM로 분산 처리된다.<br>위의 구성도를 보면, Job Manager에서 Dataflow Graph를 통해 각 각의 Operator들의 순서도를 관리한다. Dataflow Graph의 각 각의 STEP의 작업을 TaskManager(Worker)에 분산을 시키게 된다.  </p><div align="center">  <img src="/images/post_images/220623_flink_job_graph_task_node.jpeg" alt="Flink allocation of parallel subtasks"></div></li><li><h4 id="Apache-Flink의-활용"><a href="#Apache-Flink의-활용" class="headerlink" title="[Apache Flink의 활용]"></a><strong>[Apache Flink의 활용]</strong></h4><p>Apache Flink는 <code>단순 수집과 전달에서부터 합계, 평균 계산과 같은 집계와 패턴에 기반한 예측 분석 및 데이터 형식을 변환하고 다른 데이터 소스와 결합(join)등의 작업이 가능</code>하다. 그리고 Flink는 streaming 엔진이지만 배치 처리가 가능하다는 장점을 가지고 있다. 또한 케이블 기능도 하기 때문에 데이터 베이스에서 하나의 테이블을 가져와서 스트리밍 데이터와 조인하는 기능도 제공한다.</p></li><li><h4 id="경쟁-프로젝트"><a href="#경쟁-프로젝트" class="headerlink" title="[경쟁 프로젝트]"></a><strong>[경쟁 프로젝트]</strong></h4><p>앞서 살펴본 Spark streaming, Storm(<code>현재는 잘 사용되지 않음</code>), Kafka streams, Samza(<code>현재는 잘 사용되지 않음</code>)</p></li><li><h4 id="배포"><a href="#배포" class="headerlink" title="[배포]"></a><strong>[배포]</strong></h4><p>Flink는 주로 Standalone, Kubernetes(<code>YARN Resource에 배포하지 않고 Kubernetes에 배포해서 백엔드 개발자와 같은 인프라를 사용하는 경우도 많다</code>), Hadoop YARN에 배포가 되어 사용이 된다. 그리고 Kubernetes는 Spark에서보다 더 잘 지원하고 있다.</p></li><li><h4 id="내부-동작방식"><a href="#내부-동작방식" class="headerlink" title="[내부 동작방식]"></a><strong>[내부 동작방식]</strong></h4><p>Flink를 사용해서 HDFS와 같은 FileSytem으로부터 데이터를 뒷단으로 넘겨서 처리를 할 수 있으며, Kafka로부터 스트림 데이터를 읽어서 처리를 하거나 주기적으로 JDBC와 같은 DB에서 데이터를 읽어서 처리를 할 수 있다.<br>그 외에도 Socket에서 실시간으로 데이터를 읽어와서 처리를 할 수 있다. (<code>Source</code>)<br>이렇게 Source로 받아온 데이터를 File의 형태로 출력, Elasticsearch로 데이터를 올리거나, HBase, 다시 또 다른 Kafka의 Topic으로 받아서 처리한 데이터를 쏴 줄 수 있다.</p></li><li><h4 id="사용-예시"><a href="#사용-예시" class="headerlink" title="[사용 예시]"></a><strong>[사용 예시]</strong></h4><p>신규 기술이 나오면, 기업에서는 바로 도입을 하지 않고, 다른 큰 기업에서 도입해서 안정성과 효율성에 대한 인정을 받았을때 도입을 한다.<br>이 Flink라는 기술도 <code>우버(Uber)</code>에서 스트리밍 분석 플랫폼인 AthenaX에 Flink를 사용함으로써 알려졌고, <code>알리바바(Alibaba)</code>는 Flink를 기반으로 한 Blink를 개발하여, 자체 실시간 검색 숝위를 최적화하였다고 한다. 그리고 데이터가 폭증하면서 배치 처리와 스트림 처리 모두 Flink를 사용하고 있다고 한다.<br><code>AWS</code>의 스트림 프로세싱을 위한 완전 관리형 클라우드 서비스인 Kinesis Data Analytics도 내부적으로 Flink를 사용하고 있다고 한다.</p><p>앞에서 이미 정리를 했지만, Flink는 단순 데이터 전달 및 저장만 하는 것이 아닌, 자체적으로 컴퓨팅을 통해 연산 및 분석 결과만 다른 곳으로 전달하는 역할도 가지고 있다.<br>(<code>연산/분석을 통해 실시간 검색어 순위에 대한 정보도 출력할 수 있으며, ML 라이브러리를 통해 처리를 할 수도 있다</code>)</p><br/></li><li><h4 id="Flink에서의-Time"><a href="#Flink에서의-Time" class="headerlink" title="[Flink에서의 Time]"></a><strong>[Flink에서의 Time]</strong></h4><div align="center">  <img src="/images/post_images/220623_flink_event_processing_time.svg" alt="Flink에서의 시간 개념"></div><p>Flink에서는 세 가지 유형의 시간개념으로 구분을 한다.</p><ul><li><strong>Event Time</strong> : 데이터가 실제로 생성되는 시간 (Event producer)으로, 데이터가 실제로 생성되었을때를 말한다. (센서로부터 센서 데이터가 생성되었을때)</li><li><strong>Ingestion Time</strong> : 데이터가 Flink job으로 유입된 시간 (Flink Data)을 말한다.</li><li><strong>Processing Time</strong> : 데이터가 특정 operator에서 처리된 시간<br>이렇게 구분하는 이유는 Operator마다 시간기준으로 정확한 연산을 해야 할 필요성이 있기 때문이다.<br/></li></ul></li><li><h4 id="Flink의-Task-amp-Operator-chain"><a href="#Flink의-Task-amp-Operator-chain" class="headerlink" title="[Flink의 Task &amp; Operator chain]"></a><strong>[Flink의 Task &amp; Operator chain]</strong></h4><div align="center">  <img src="/images/post_images/220623_task_operator_chaining.png" alt="Flink의 Task& Operator chaining"></div><p>Flink에서 로직을 복잡하게 작성을 하다보면, 효율성을 위해서 연관된 각 처리과정을 Chaining하는 경우도 많다. </p></li><li><h4 id="Flink의-Sink-Parallelism"><a href="#Flink의-Sink-Parallelism" class="headerlink" title="[Flink의 Sink Parallelism]"></a><strong>[Flink의 Sink Parallelism]</strong></h4><p><code>Task slot: 6</code>, <code>Sink parallelism: 1</code></p><div align="center">  <img src="/images/post_images/220623_flink_sink_parallelism.png" alt="Flink의 Sink Parallelism"></div><p>기본적으로 Flink는 다른 작업의 하위 작업인 경우에도 하위 작업이 슬롯을 공유할 수 있도록 허용한다. 결과적으로 하나의 slot은 전체 파이프라인의 job에 대해 저장을 할 수 있다.<br>Task slot은 정적인 개념으로, Task Manager 참조하여 동시 실행 기능을 가지고 있다.<br>Task Manager들의 내부의 Task slot이 각기다른 Thread로 구성이 되며, Task Manager들은 Job Manager에 의해 관리가 된다.</p><br/></li><li><h4 id="Windows"><a href="#Windows" class="headerlink" title="[Windows]"></a><strong>[Windows]</strong></h4><p><a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/</a><br>Flink에는 다양한 방식으로 데이터를 나눠서 처리하는 방법이 있는데, Windows란 개념으로 데이터를 잘라서 처리를 한다.<br>Windows를 사용해서 데이터를 분석하는 예로 <code>10초에 한 번 평균을 내야되는 연산을 하는 경우</code>나 <code>사용자가 접속한 순간부터 나간 순간까지의 이벤트를 모아서 한 번에 처리하는 경우</code>가 있다.</p><p>여기서 윈도우란 Spark streaming의 micro batch의 개념과 다르다. 5초 단위로 들어오는 데이터들을 처리해야될 경우에 Window size를 5초 단위로 나눠서 처리를 할 수 있으며(<code>Tumbling Windows</code>), 각 각의 Window의 구간이 겹쳐서(데이터가 중복) 처리하는 것은 <code>Sliding Windows</code>, 정확히 시간을 기준으로 구간을 나누기 애매한 경우(사용자의 접속부터 종료 시점까지의 이벤트 일괄처리)에는 각 session별로 데이터 구간을 나눠서 처리하는 <code>Session Windows</code>의 개념으로 데이터를 잘라서 처리한다.<br>만약에 특정 시간 구분 없이 전체 데이터를 처리하는 경우는 <code>Global Windows</code>개념으로 데이터를 일괄 처리하도록 한다.</p><p>Flink에서는 Tumbling Windows, Sliding Windows, Session Windows 이 세가지를 지원한다.</p><p>ex) 실제 사용자가 들어와서 세션내에서 어떤 작업을 했는지 취합하고 검색을 할때 주로 Session Window를 사용한다.</p><br/></li></ul><h2 id="Dataflow-Programming"><a href="#Dataflow-Programming" class="headerlink" title="Dataflow Programming"></a><ins><b>Dataflow Programming</b></ins></h2>  <div align="center">    <img src="/images/post_images/220623_dataflow_programming.png" alt="DAG">  </div><ul><li><p>스트림 프레임워크 엔진을 개발할때 중요하게 보는 것이 Dataflow Programming이다.</p></li><li><p>DAG(Directed Acyclic Graphs):유향 비순환 그래프(방향성은 있는데 순환하지 않는 형태), Airflow에서도 많이 접하게 되는 개념 </p></li><li><p>각 각의 데이터 플로우 그래프를 만들어서 서로 연결시켜서 하나의 그래프로 만들어주는 것이 <code>Dataflow Programming</code>이다.</p></li><li><h3 id="Dataflow-Programming의-세-가지-Operator"><a href="#Dataflow-Programming의-세-가지-Operator" class="headerlink" title="[Dataflow Programming의 세 가지 Operator]"></a><strong>[Dataflow Programming의 세 가지 Operator]</strong></h3><ul><li><strong>Source Operator</strong> : 그래프 상에서 제일 처음인 부분, Kafka와 같이 외부에서 데이터를 받아오는 부분 (입력이 없는 연산자 - root node (<code>a operator</code>)) <ul><li>Custom source</li><li>Apache Kafka</li><li>AWS Kinesis Streams</li><li>RabbitMQ</li><li>Twitter Stream API</li><li>Google PubSub</li><li>Collections</li><li>Files</li><li>Sockets</li></ul></li><li><strong>Transformation Operator</strong> : 변환/연산 등의 연산자 (<code>b, d operator</code>)<ul><li>Map : 사용자가 정의한 변환 코드를 통과시켜서 하나의 이벤트를 출력</li><li>FlatMap : map과 유사하지만 각 요소에 대해 0개 이상의 출력을 생성하는 것이 가능하다. (문자열을 split해서 처리 여러 요소로 나눠서 처리)</li><li>Filter : 특정 조건에 따라 pass or drop하는 필터 기능</li><li>KeyBy : 특정 옵션(키)에 따라 스트림을 파티션별로 분리</li><li>Union : 두 개 이상의 동일한 타입의 스트림을 병합</li><li>Reduce : 키별로 나뉘어진 데이터 스트림을 합쳐주는 역할</li></ul></li><li><strong>Sink Operator</strong> : 그래프상에서 제일 마지막 출력부 (leaf node - <code>c, f operator</code>)<ul><li>Custom sink</li><li>Elasticsearch</li><li>Kafka producer</li><li>Cassandra</li><li>AWS Kinesis Streams</li><li>File</li><li>Socket</li><li>Standard output</li><li>Redis</li></ul></li></ul></li><li><p>실제 프로그래밍을 할때에는 Source operator 부분을 개발한다. 예를들어 Kafka를 활용해서 데이터를 입력받는 부분을 우선적으로 만들고(프레임워크에서 제공), Kafka에서 들어온 데이터를 transform해주는 Transformation operator를 만든다. 그리고 최종적으로 변환된 데이터를 다른 저장소나 목적지로 쏴주는 Sink operator를 만들어서 연결시켜준다.<br>이와 같은 일련의 과정을 DAG라고 하며, Flink 어플리케이션에서는 이러한 일련의 과정을 만드는 것이다.</p></li><li><p>ETL과 비슷한 과정</p></li><li><p>Flink는 분산 스트리밍 엔진이기 때문에 <code>source -&gt; map -&gt; key By()/Window()/Apply()</code> 구조의 각 STEP을 여러 노드로 분산해서 처리를 할 수 있도록 할 수 있다.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Input Source</span></span><br><span class="line">DataStream&lt;String&gt; lines = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Transformation</span></span><br><span class="line">DataStream&lt;Log&gt; logs = lines.map((line) -&gt; parse(line));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Transformation</span></span><br><span class="line">DataStream&lt;Statistics&gt; stats = logs.keyBy(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">                                   .timeWindow(Time.second(<span class="number">10</span>))</span><br><span class="line">                                   .apply(<span class="keyword">new</span> MyWindowAggregationFunc());</span><br><span class="line"><span class="comment">// Output Sink</span></span><br><span class="line">stats.addSink(<span class="keyword">new</span> CustomSink(path))</span><br></pre></td></tr></table></figure></li><li><p>예를들어 Kafka에서 1, 2, 3, 4라는 숫자 데이터를 받아오면, Flink 클러스터의 노드가 2개 있다고 가정을 했을때, 받아온 데이터 중에서 1, 2를 A라는 노드에서, 3, 4를 B라는 노드에서 처리하도록 분배할 수 있다.</p></li><li><p>이렇게 분배된 데이터들은, map()을 통해 제곱된 수로 변환이 되고, 홀/짝수로 분류를 하는 일종의 MapReduce와 같은 과정을 거쳐서 홀수로 분류된 데이터는 또 다시 Sink를 통해 또 다른 Kafka의 odd topic으로 날리고, 짝수로 분류된 데이터는 even topic으로 날리도록 처리를 할 수 있다.</p></li><li><p>Flink와 Logstash를 비교해보면, Logstash는 각 각의 Logstash에서의 작업을 하지만 다른 Logstash 간에는 소통을 할 수 없다. 하지만 Flink는 하나의 클러스터 이기 때문에 각 각의 처리 노드들끼리 서로 소통을 할 수 있다.</p></li></ul><h2 id="실무팁"><a href="#실무팁" class="headerlink" title="(실무팁)"></a><ins><b>(실무팁)</b></ins></h2><p>  Kafka의 Partition이 3개 있고, consumer가 4개 있는 상황에서는 consumer 하나가 잉여가 된다. 만약에 Flink로 어플리케이션을 만든 다음에 복잡한 데이터를 처리할때에는 source node가 10대 이상이 되어야 되기 때문에 이 경우에 consumer를 10대 이상으로 늘려주게 되면, 실제로는 Partition이 3개이기때문에 consumer 3개에서만 데이터를 받아서 처리를 해주게 된다.<br>  이 경우에는 Flink가 클러스터링 되어있기 때문에 데이터를 받지 못하는 나머지 consumer에게도 실제 데이터를 받아오는 consumer로부터 분배받을 수 있도록 구성할 수 있다. (<code>Kafka 자체에서는 데이터를 나눠줄 수 없는 consumer이지만, Flink 내부의 operator가 consumer들에게 데이터를 분배할 수 있도록 해줄 수 있다</code>)</p><h2 id="수업내용-외-질문-Q1-VPC와-Subnet-구성에-대한-질문"><a href="#수업내용-외-질문-Q1-VPC와-Subnet-구성에-대한-질문" class="headerlink" title="(수업내용 외 질문)Q1. VPC와 Subnet 구성에 대한 질문"></a><ins><b>(수업내용 외 질문)Q1. VPC와 Subnet 구성에 대한 질문</b></ins></h2><p>  내가 수업시간에 한 질문은 다른 AWS service topology를 살펴보면, VPC와 Subnet으로 구분된 것들이 많은데, 실제 실무에서는 어떤식으로 사용되고 있나였다.</p><p>  <strong>A</strong> : 실습에서는 default vpc와 subnet에서 AWS resource가 생성이 되는데, 실무에서 default vpc와 subnet을 사용하지 않는 것이 권장된다고 한다. 실제 업무에서는 Production에서 각 각의 서비스별로나 네트워크 보안 정책에 따라서 VPC를 나눠서 관리를 하고, 세팅을 하는 것이 옳은 방법이라고 한다.<br>  AWS 서비스 중에서 완전 관리형 서비스가 아닌 서비스들은 VPC을 타지만, 완전 관리형 서비스의 경우에는 VPC가 아닌 별도의 AWS Zone(AWS전용 네트워크 라인)을 탄다.</p>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Data-Pipeline/">Data-Pipeline</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/AWS/">AWS</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/19/202206/220619_datapipeline_study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220618 데이터 파이프라인 구축 오프라인 수업 / 4주차 오프라인 수업 전 준비</title>
      <link>https://leehyungi0622.github.io/2022/06/18/202206/220618_datapipeline_study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/18/202206/220618_datapipeline_study/</guid>
      <pubDate>Sat, 18 Jun 2022 14:42:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220604_preparation.jpeg&quot; alt=&quot;Preparation&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 내일 있을 데이터 파이프라인 구축 관련 수업에서 실습할 내용들에 대해 미리 실습을 해보고, 개념적인 부분을 좀 다져보려고 한다.&lt;br&gt;내일 수업에서는 Apache Kafka를 실습하는데, Kafka cluster의 각 Broker와 Zookeeper, Producer와 Consumer를 전부 Docker container로 띄우고 관리한다. 이미 Docker에 대해 공부했고, Kubernetes에 대해서 추가적으로 공부를 하는 중이라 이번 수업을 통해 Kafka의 각 구성요소들을 Docker container에 띄워서 관리하면 많은 도움이 될 것 같다.&lt;br&gt;그리고 여지까지 했던 실습들과 비교했을때 상대적으로 구조가 복잡하기 때문에 미리 Kafka의 개념적인 내용을 머릿속에 그리면서 실습하게 될 전체적인 구조에 대해서 파악하고 가야 할 필요성을 느꼈다. &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220604_preparation.jpeg" alt="Preparation"></div><br/><br/><p>이번 포스팅에서는 내일 있을 데이터 파이프라인 구축 관련 수업에서 실습할 내용들에 대해 미리 실습을 해보고, 개념적인 부분을 좀 다져보려고 한다.<br>내일 수업에서는 Apache Kafka를 실습하는데, Kafka cluster의 각 Broker와 Zookeeper, Producer와 Consumer를 전부 Docker container로 띄우고 관리한다. 이미 Docker에 대해 공부했고, Kubernetes에 대해서 추가적으로 공부를 하는 중이라 이번 수업을 통해 Kafka의 각 구성요소들을 Docker container에 띄워서 관리하면 많은 도움이 될 것 같다.<br>그리고 여지까지 했던 실습들과 비교했을때 상대적으로 구조가 복잡하기 때문에 미리 Kafka의 개념적인 내용을 머릿속에 그리면서 실습하게 될 전체적인 구조에 대해서 파악하고 가야 할 필요성을 느꼈다. </p><a id="more"></a><h2 id="Apache-Kafka"><a href="#Apache-Kafka" class="headerlink" title="Apache Kafka"></a><ins><b>Apache Kafka</b></ins></h2><p>  우선 실습은 EC2 인스턴스 (Ubuntu)를 하나 띄워서 인스턴스 내부에서 진행하게 된다.</p><p>  <strong>(1)</strong> Kafka cluster의 각 Broker, Zookeeper, 그리고 Producer와 Consumer를 Docker로 띄워서 관리를 할 것이기 때문에 우선적으로 <code>docker와 docker-compose를 설치</code>한다.<br>  <strong>(2) consumer 구성에 대한 docker-compose.yml 파일</strong><br>  dockedr-compose.yml파일은 multi consumer와 single consumer로 나뉘어져있는데, 이름 그대로 multi consumer의 docker-compose.yml에는 producer_host가 하나, consumer_host가 1, 2로 두 개 설정되어있다. 그리고 single consumer의 docker-compose.yml에는 producer_host 하나와 consumer_host 하나가 설정되어있다.<br>  이 차이점에 유의해서 실습하도록 하자. 앞서 두 파일의 차이점에 대해서 살펴보았으니, 이제 두 파일의 공통점에 대해서 살펴보면, 두 파일은 공통적으로 zookeeper에 대한 container service 3개, kafka에 대한 container service 3개, kafka_manager에 대한 container service 1개, 그런데 zookeeper가 Kafka cluster에 대해 관리를 해준다고 알고 있었는데, kafka manager는 뭐지? 찾아보니, <code>Kafka manager는 CMAK로, 브라우저상에서 Kafka 클러스터와 Topic(Partitions, Replication Factor)을 손쉽게 추가하고 관리</code>할 수 있게 해주는 것 같다.<br>  <br/></p><ul><li><h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a><ins><b>Zookeeper</b></ins></h3><p>zookeeper는 항상 재시작 되도록 설정이 되어있으며, port는 <code>2181, 2182, 2183번</code>으로 설정되어있고, 환경변수로는 SERVER_ID, CLIENT_PORT, TICK_TIME, INIT_LIMIT, SYNC_LIMT, ZOOKEEPER_SERVERS(zk1, zk2, zk3:2888:3888)에 대한 정보를 담고 있다.<br>위의 각 각의 환경변수의 의미에 대해서 한 번 찾아보고 정리를 해보자.</p><p><a href="https://docs.docker.com/config/containers/start-containers-automatically/#:~:text=Docker%20provides%20restart%20policies%20to,process%20managers%20to%20start%20containers">https://docs.docker.com/config/containers/start-containers-automatically/#:~:text=Docker%20provides%20restart%20policies%20to,process%20managers%20to%20start%20containers</a>.</p><p><code>restart:always</code>설정은 컨테이너가 항상 실행되도록 한다. 컨테이너가 중지되면 즉시 다시 시작되며, 직접적인 중단(docker stop)된 경우에는 컨테이너를 다시 시작하지 않고, Docker daemon이 재시작되거나 컨테이너 자체가 수동으로 재시작되면 다시 시작된다. (<code>오류로 인한 중단인 경우에 컨테이너가 다시 시작</code>)<br>반면에 producer_host와 consumer_host의 경우에는 <code>restart: on-failure</code>로 설정이 되어있다.<br>이 옵션은 error(non-zero exit code)때문에 exit이 되거나 docker daemon이 컨테이너를 재시작하는 시도의 횟수에 대해 <code>:maz-retries</code>옵션으로 제한한 경우 재시작되도록 할때 사용된다.</p><p><code>restart:always</code>와 <code>restart: on-failure</code>의 차이는 전자는 컨테이너가 오류로 인해 중지되는 경우에 한해 컨테이너가 재시작되고, 후자는 컨테이너가 오류로 인해 exit되는 경우와 Docker daemon의 컨테이너 재시작 시도 횟수에 대한 제한이 걸려있는 경우에 컨테이너가 재시작되도록 설정할때 사용된다.</p><p>(<code>docker 데몬은 docker를 이용하기 위해서 반드시 실행중이어야 된다. 하지만 모종의 이유로 인해 데몬이 꺼지거나 서버 재부팅시 데몬이 실행되도록 해놓지 않았다면, 이용하던 컨테이너들이 제공하던 서비스를 전부 이용할 수 없게 되는 경우가 생긴다.</code>)</p><ul><li><h4 id="Docker-데몬-실행"><a href="#Docker-데몬-실행" class="headerlink" title="Docker 데몬 실행"></a><strong>Docker 데몬 실행</strong></h4><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dockerd이 foreground 형태로 실행이 된다.</span></span><br><span class="line"><span class="variable">$dockerd</span></span><br><span class="line"><span class="comment"># dockerd이 background 형태로 실행이 된다.</span></span><br><span class="line"><span class="variable">$dockerd</span> &amp;</span><br><span class="line"><span class="comment"># service혹은 systemctl 명령을 통해 docker 데몬 실행 가능</span></span><br><span class="line"><span class="variable">$service</span> docker start (stop|restart)</span><br><span class="line"><span class="variable">$systemctl</span> start docker</span><br></pre></td></tr></table></figure>일반적으로 production workload에 <code>always, on-failure, unless-stopped</code> 이 세가지 옵션 중에 하나가 사용되며, Docker container는 장기간 실행되는 백그라운드 서비스에 자주 사용되기 때문에 일반적으로 문제가 발생하면 다시 시작되기를 원한다. </li></ul><p>producer_host와 consumer_host의 <code>tty:true</code> 옵션에 대해서 찾아보기<br>tty는 terminal로, linux환경에서 docker container에 연결해서 command를 실행할때 필요한 옵션이다.<br>이러한 이유로 producer_host와 consumer_host에만 tty:true option이 추가되있다.</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Data-Pipeline/">Data-Pipeline</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/AWS/">AWS</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/18/202206/220618_datapipeline_study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220618 Terraform study</title>
      <link>https://leehyungi0622.github.io/2022/06/18/202206/220618-terraform-study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/18/202206/220618-terraform-study/</guid>
      <pubDate>Sat, 18 Jun 2022 04:40:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220614_terraform.png&quot; alt=&quot;Terraform&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 Terraform에서 생성한 AWS Resource를 수정하는 부분에 대한 내용부터 시작해보려고 한다.&lt;br&gt;Terraform은 손쉽게 AWS Infrastructure를 구성할 수 있도록 도와줌과 동시에 코드를 통해 전체적인 AWS Infrastructure의 구조를 파악할 수 있다.&lt;/p&gt;
&lt;h2 id=&quot;Terraform의-동작-방식&quot;&gt;&lt;a href=&quot;#Terraform의-동작-방식&quot; class=&quot;headerlink&quot; title=&quot;Terraform의 동작 방식&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Terraform의 동작 방식&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;  만약 실행하고자 하는 *.tf 파일이 이전에 실행을 한 다음에 별도의 수정을 하지 않았다면, &lt;code&gt;$terraform apply&lt;/code&gt;명령을 실행하여도 아무런 변화가 일어나지 않는다. 그 이유는 terraform은 기본적으로 &lt;code&gt;$terraform plan&lt;/code&gt;명령을 통해 확인할 수 있는 변화된 내용이 있어야 &lt;code&gt;$terraform apply&lt;/code&gt;명령을 통해 새롭게 인프라를 구축할 수 있다.&lt;/p&gt;
&lt;p&gt;  만약 기존 리소스에서 tag에 대한 정보만 추가한 다음에 다시 실행하면, 기존에 생성되었던 리소스는 유지된 상태에서 resource name (Name)이 추가가 된다.&lt;/p&gt;
&lt;h2 id=&quot;Delete-resources&quot;&gt;&lt;a href=&quot;#Delete-resources&quot; class=&quot;headerlink&quot; title=&quot;Delete resources&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Delete resources&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;  기존에 *.tf파일을 통해 생성하였던 리소스들을 삭제하려면, &lt;code&gt;$terraform destroy&lt;/code&gt; 명령을 치면 된다. &lt;/p&gt;
  &lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$terraform&lt;/span&gt; destroy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;  만약 기졵의 .tf 코드에서 일부 리소스를 주석처리하게 되면, apply를 했을때 주석처리된 리소스를 제거한다. (&lt;code&gt;일부 리소스 제거&lt;/code&gt;)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220614_terraform.png" alt="Terraform"></div><br/><br/><p>이번 포스팅에서는 Terraform에서 생성한 AWS Resource를 수정하는 부분에 대한 내용부터 시작해보려고 한다.<br>Terraform은 손쉽게 AWS Infrastructure를 구성할 수 있도록 도와줌과 동시에 코드를 통해 전체적인 AWS Infrastructure의 구조를 파악할 수 있다.</p><h2 id="Terraform의-동작-방식"><a href="#Terraform의-동작-방식" class="headerlink" title="Terraform의 동작 방식"></a><ins><b>Terraform의 동작 방식</b></ins></h2><p>  만약 실행하고자 하는 *.tf 파일이 이전에 실행을 한 다음에 별도의 수정을 하지 않았다면, <code>$terraform apply</code>명령을 실행하여도 아무런 변화가 일어나지 않는다. 그 이유는 terraform은 기본적으로 <code>$terraform plan</code>명령을 통해 확인할 수 있는 변화된 내용이 있어야 <code>$terraform apply</code>명령을 통해 새롭게 인프라를 구축할 수 있다.</p><p>  만약 기존 리소스에서 tag에 대한 정보만 추가한 다음에 다시 실행하면, 기존에 생성되었던 리소스는 유지된 상태에서 resource name (Name)이 추가가 된다.</p><h2 id="Delete-resources"><a href="#Delete-resources" class="headerlink" title="Delete resources"></a><ins><b>Delete resources</b></ins></h2><p>  기존에 *.tf파일을 통해 생성하였던 리소스들을 삭제하려면, <code>$terraform destroy</code> 명령을 치면 된다. </p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$terraform</span> destroy</span><br></pre></td></tr></table></figure><p>  만약 기졵의 .tf 코드에서 일부 리소스를 주석처리하게 되면, apply를 했을때 주석처리된 리소스를 제거한다. (<code>일부 리소스 제거</code>)</p><a id="more"></a><h2 id="Reference-resources"><a href="#Reference-resources" class="headerlink" title="Reference resources"></a><ins><b>Reference resources</b></ins></h2><p>  우선 실습을 위해 VPC를 구성하고, VPC 내에 별도의 서브넷을 구성하도록 한다. VPC는 Virtual Private Cloud의 약어로 격리된 Private Network 영역이다. AWS 사용자가 정의한 가상 네트워크로써 cidr 블록 방식으로 IP대역대를 설정한다. VPC의 대표 구성요소는 아래와 같다.</p><p>  <code>ref.</code> *.tf 코드에서 선언 순서는 문제가 되지 않는다. 그 이유는 Terraform이 Subnet이 VPC에 속해있다는 것을 파악하고 코드를 실행하기 때문에 순서가 뒤바뀌어도 VPC를 우선 생성하고, Subnet을 생성한다.</p><ul><li><h3 id="VPC-구성"><a href="#VPC-구성" class="headerlink" title="VPC 구성"></a><strong>VPC 구성</strong></h3><div align="center">  <img src="/images/post_images/220618_vpc_subnet_structure.jpg" alt="VPC & Subnet"></div><ul><li><h3 id="Internet-Gateway-IGW"><a href="#Internet-Gateway-IGW" class="headerlink" title="Internet Gateway(IGW)"></a><strong>Internet Gateway(IGW)</strong></h3><p>vpc 내부의 public subnet 상의 인스턴스들과 외부 인터넷 간의 통신을 위해 vpc에 연결하는 게이트웨이이다. </p></li><li><h3 id="NAT-Gateway-NGW"><a href="#NAT-Gateway-NGW" class="headerlink" title="NAT Gateway(NGW)"></a><strong>NAT Gateway(NGW)</strong></h3><p>nat는 네트워크 주소를 변환하는 장치로, ngw는 vpc 내부의 private subnet의 인스턴스들과 인터넷/AWS 서비스에 연결하는 게이트웨이이다. 실제 <code>구성은 public subnet에 위치하며, Elastic IP를 할당한 상태로 구성</code>된다.<br>고정 할당된 Elastic IP를 route tableㅇ르 통해 연결시킨다.<br>여기서 말하는 Routing Table이란 네트워크 트래픽을 전달할 위치를 결정하는데 사용되는 라우팅 규칙들의 집합이다.<br>(<code>subnet - subnet</code>, <code>subnet - gateway</code>간의 통신을 결정한다) </p></li><li><h3 id="Security-Gruop"><a href="#Security-Gruop" class="headerlink" title="Security Gruop"></a><strong>Security Gruop</strong></h3><p>Network ACL과 함께 VPC의 보안 장치 중 하나이며, Security Group은 인스턴스 단위의 보안계층이다. 인스턴스에 대한 인바운드/아웃바운드 트래픽을 제어하는 가상 방화벽 역할을 한다.<br>Network ACL과 달리 stateful하다.</p></li><li><h3 id="Network-ACL"><a href="#Network-ACL" class="headerlink" title="Network ACL"></a><strong>Network ACL</strong></h3><p>security group과 함께 VPC의 보안 장치 중의 하나로, 1개 이상의 subnet과 외부 트래픽을 제어할 수 있다. 즉, subnet 단위의 보안 계층이며, stateless라는 특징을 가진다.</p></li></ul></li><li><h3 id="aws-vpc-구성"><a href="#aws-vpc-구성" class="headerlink" title="[aws_vpc] 구성"></a><strong>[aws_vpc] 구성</strong></h3><p><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># VPC 내에 Subnet을 구성한다.</span><br><span class="line"># VPC(Virtual Private Cloud) - Private Isolated Network</span><br><span class="line"># first-vpc는 지정한 리소스의 이름으로, 해당 리소스를 참조할때 사용된다. (terraform내에서 참조)</span><br><span class="line"></span><br><span class="line">resource &quot;aws_vpc&quot; &quot;first-vpc&quot; &#123;</span><br><span class="line">  cidr_block &#x3D; &quot;10.0.0.0&#x2F;16&quot;</span><br><span class="line">  tags &#x3D; &#123;</span><br><span class="line">    Name &#x3D; &quot;production&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><h3 id="aws-subnet"><a href="#aws-subnet" class="headerlink" title="[aws_subnet]"></a><strong>[aws_subnet]</strong></h3><p><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/subnet">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/subnet</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Subnet 생성</span><br><span class="line">resource &quot;aws_subnet&quot; &quot;subnet-1&quot; &#123;</span><br><span class="line">  # reference the vpc created </span><br><span class="line">  vpc_id     &#x3D; aws_vpc.first-vpc.id</span><br><span class="line">  cidr_block &#x3D; &quot;10.0.1.0&#x2F;24&quot;</span><br><span class="line"></span><br><span class="line">  tags &#x3D; &#123;</span><br><span class="line">    Name &#x3D; &quot;prod-subnet&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>최종적으로 AWS에서 VPC - VPC dashboard를 살펴보면, default VPC와 production VPC 가 생성된 것을 확인할 수 있다.<br>그리고 Virtual private cloud의 하위에 Subnets 메뉴를 통해서 VPC와 함께 새로 생성된 prod-subnet도 생성된 것을 확인할 수 있다.</p></li></ul><h2 id="Terraform-project-files"><a href="#Terraform-project-files" class="headerlink" title="Terraform project files"></a><ins><b>Terraform project files</b></ins></h2><ul><li><p><strong>.terraform/plugins/…</strong></p><p>.terraform/plugins/… 하위에는 <code>terraform init</code>명령을 했을때 작성한 *.tf 파일을 읽고, 설치가 필요한 platform의 plugin을 다운받는다. 다운받음과 동시에 해당 configuration 정보가 하위 폴더에 저장한다.</p></li><li><p><strong>terraform.tfstate</strong>(<code>매우 중요한 파일)</code></p><p>모든 Terraform의 상태정보를 담고 있는 파일이다. 우리가 *.tf 파일에 작성했던 AWS의 리소스의 정보 일부를 수정하게 되면, Terraform이 이를 감지해서 apply시 변경사항을 반영해주는데, 이러한 상태정보를 저장해서 담고 있는 파일이 바로 terraform.tfstate 파일이다.<br>*.tf 파일의 내용을 수정하고, tfstate 파일의 내용을 보면, 수정된 내용이 반영된 것을 확인할 수 있다.</p></li></ul><h2 id="Practice-Project"><a href="#Practice-Project" class="headerlink" title="Practice Project"></a><ins><b>Practice Project</b></ins></h2><p>  여지까지 배운 내용을 기반으로 간단한 연습 프로젝트를 진행해본다.<br>  내용은 EC2 인스턴스를 Custom Subnet 영역에 Deploy하고, Public IP를 할당해서 SSH 연결뿐만 아니라 웹 서버를 구동시킬 수 있도록 구성해본다.<br>  <div align="center"><br>    <img src="/images/post_images/220618_practice_project_network_topology.jpg" alt="Practice Project AWS Network Topology"><br>  </div></p><ul><li><h3 id="STEP1-VPC-생성하기"><a href="#STEP1-VPC-생성하기" class="headerlink" title="[STEP1] VPC 생성하기"></a><strong>[STEP1] VPC 생성하기</strong></h3><p>이름이 prod-vpc인 VPC를 생성한다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">resource &quot;aws_vpc&quot; &quot;prod-vpc&quot; &#123;</span><br><span class="line">  cidr_block &#x3D; &quot;10.0.0.0&#x2F;16&quot;</span><br><span class="line">  tags &#x3D; &#123;</span><br><span class="line">    Name &#x3D; &quot;production&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CIDR(Classless Inter-Domain Routing)은 클래스가 없는 도메인간 라우팅하는 기법이다.<br>클래스가 없다는 뜻은 네트워크 구분을 별도의 클래스로 하지 않는 다는 의미로, Class는 사이더가 나오기전 사용했던 네트워크 구분 체계이다. 사이더가 나오면서 Class 체계보다 더 유연하게 IP 주소를 여러 네트워크 영역으로 나눌 수 있게 되었다.</p><p>CIDR는 위의 Infra-Domain과 같이 각 네트워크 대역을 구분 짓고, Inter-Domain과 같이 구분된 네트워크간 통신을 위한 주소 체계라고 이해하면 된다. </p></li><li><h3 id="STEP2-Internal-Gateway-생성하기"><a href="#STEP2-Internal-Gateway-생성하기" class="headerlink" title="[STEP2] Internal Gateway 생성하기"></a><strong>[STEP2] Internal Gateway 생성하기</strong></h3><p>traffic을 인터넷 밖(outbound)으로 보내고, public ip를 서버에 할당하기 위함이다. </p><p><code>ref.</code><br><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/internet_gateway">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/internet_gateway</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># IGW(Internet Gateway) 생성</span><br><span class="line"># VPC</span><br><span class="line">resource &quot;aws_internet_gateway&quot; &quot;gw&quot; &#123;</span><br><span class="line">  vpc_id &#x3D; aws_vpc.prod-vpc.id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><h3 id="STEP3-Custom-Route-Table-생성하기"><a href="#STEP3-Custom-Route-Table-생성하기" class="headerlink" title="[STEP3] Custom Route Table 생성하기"></a><strong>[STEP3] Custom Route Table 생성하기</strong></h3><p>VPC 내부에서 외부로 Routing 시켜줄때 참조할 Route table을 생성한다.</p><p><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Route table 생성</span><br><span class="line">resource &quot;aws_route_table&quot; &quot;prod-route-table&quot; &#123;</span><br><span class="line">  # Route table이 적용되는 vpc영역에 대한 지정</span><br><span class="line">  vpc_id &#x3D; aws_vpc.prod-vpc.id</span><br><span class="line"></span><br><span class="line">  # IPv4에 대한 Route table setup</span><br><span class="line">  route &#123;</span><br><span class="line">    # cidr_block address를 IGW로 보내고, </span><br><span class="line">    cidr_block &#x3D; &quot;0.0.0.0&#x2F;0&quot; </span><br><span class="line">    # 0.0.0.0&#x2F;0 send all traffic before ip will route to IGW. </span><br><span class="line">    # 모든 트래픽이 aws IGW로 전송되기 때문에 gaqteway_id에 대한 설정이 필요하다.</span><br><span class="line">    gateway_id &#x3D; aws_internet_gateway.gw.id</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  # IPv6에 대한 Route table setup</span><br><span class="line">  route &#123;</span><br><span class="line">    # 실제로는 IPv6를 사용하지 않지만 All traffic이 동일한 IGW로 routing되도록 설정한다.</span><br><span class="line">    ipv6_cidr_block        &#x3D; &quot;::&#x2F;0&quot;</span><br><span class="line">    gateway_id &#x3D; aws_internet_gateway.gw.id</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  tags &#x3D; &#123;</span><br><span class="line">    Name &#x3D; &quot;Prod&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><h3 id="STEP4-Subnet-생성하기"><a href="#STEP4-Subnet-생성하기" class="headerlink" title="[STEP4] Subnet 생성하기"></a><strong>[STEP4] Subnet 생성하기</strong></h3><p>Subnet을 생성할때 Subnet이 속한 vpc에 대한 id 지정을 해줘야 한다. 그리고 cidr_block으로 네트워크 영역을 구분하고, AZ에 대한 설정과 tag 정보를 넣어준다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Subnet 생성</span><br><span class="line"># AZ(Data centre에 대한 setup)</span><br><span class="line"># 생성한 Subnet을 상단에서 설정한 Route table에 넣어준다.</span><br><span class="line">resource &quot;aws_subnet&quot; &quot;subnet-1&quot; &#123;</span><br><span class="line">  vpc_id &#x3D; aws_vpc.prod-vpc.id</span><br><span class="line">  cidr_block &#x3D; &quot;10.0.1.0&#x2F;24&quot;</span><br><span class="line">  availability_zone &#x3D; &quot;ap-northeast-2a&quot;</span><br><span class="line">  tags &#x3D; &#123;</span><br><span class="line">    Name &#x3D; &quot;prod-subnet&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><h3 id="STEP5-Route-Table에-Subnet-연관시키기"><a href="#STEP5-Route-Table에-Subnet-연관시키기" class="headerlink" title="[STEP5] Route Table에 Subnet 연관시키기"></a><strong>[STEP5] Route Table에 Subnet 연관시키기</strong></h3><p><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table_association">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table_association</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Associate subnet with route table</span><br><span class="line">resource &quot;aws_route_table_association&quot; &quot;a&quot; &#123;</span><br><span class="line">subnet_id      &#x3D; aws_subnet.subnet-1.id</span><br><span class="line">route_table_id &#x3D; aws_route_table.prod-route-table.id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><h3 id="STEP6-Security-group-생성-22-80-443-포트-허용-하기"><a href="#STEP6-Security-group-생성-22-80-443-포트-허용-하기" class="headerlink" title="[STEP6] Security group 생성(22, 80, 443 포트 허용)하기"></a><strong>[STEP6] Security group 생성(22, 80, 443 포트 허용)하기</strong></h3><p>일반적으로 네트워크 트래픽은 Ingress와 egress으로 구분된다. Ingress는 외부로부터 서버 내부로 유입되는 네트워크 트래픽(Inbound), egress는 서버 내부에서 외부로 나가는 트래픽(Outbound)를 말한다.<br>여기서 Ingress와 egress는 컨테이너 환경에서 과거 네트워크에서 사용되었던 정의가 포함이 되면서 데이터 경로와 컨테이너 환경에서 Ingress/egress라는 용어가 사용되고 있다. </p><p>Ingress :  클러스터 내 서비스에 대한 외부에서의 접근(일반적으로 HTTP)를 관리하는 API 객체로, 로드 밸런싱, SSL 종료, 이름 기반 가상 호스팅 을 제공할 수 있다.</p></li></ul><p>  <a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group</a></p><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># create security group </span><br><span class="line">resource &quot;aws_security_group&quot; &quot;allow_web&quot; &#123;</span><br><span class="line">  name        &#x3D; &quot;allow_web_traffic&quot;</span><br><span class="line">  description &#x3D; &quot;Allow Web traffic&quot;</span><br><span class="line">  vpc_id      &#x3D; aws_vpc.prod-vpc.id</span><br><span class="line"></span><br><span class="line">  # Port의 range setup</span><br><span class="line">  ingress &#123;</span><br><span class="line">    description      &#x3D; &quot;HTTPS&quot;</span><br><span class="line">    from_port        &#x3D; 443</span><br><span class="line">    to_port          &#x3D; 443</span><br><span class="line">    protocol         &#x3D; &quot;tcp&quot;</span><br><span class="line">    cidr_blocks      &#x3D; [&quot;0.0.0.0&#x2F;0&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">  ingress &#123;</span><br><span class="line">    description      &#x3D; &quot;HTTP&quot;</span><br><span class="line">    from_port        &#x3D; 80</span><br><span class="line">    to_port          &#x3D; 80</span><br><span class="line">    protocol         &#x3D; &quot;tcp&quot;</span><br><span class="line">    cidr_blocks      &#x3D; [&quot;0.0.0.0&#x2F;0&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">  ingress &#123;</span><br><span class="line">    description      &#x3D; &quot;SSH&quot;</span><br><span class="line">    from_port        &#x3D; 22</span><br><span class="line">    to_port          &#x3D; 22</span><br><span class="line">    protocol         &#x3D; &quot;tcp&quot;</span><br><span class="line">    cidr_blocks      &#x3D; [&quot;0.0.0.0&#x2F;0&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">  # egrass policy </span><br><span class="line">  # protocol: -1 (any protocol)</span><br><span class="line">  egress &#123;</span><br><span class="line">    from_port        &#x3D; 0</span><br><span class="line">    to_port          &#x3D; 0</span><br><span class="line">    protocol         &#x3D; &quot;-1&quot;</span><br><span class="line">    cidr_blocks      &#x3D; [&quot;0.0.0.0&#x2F;0&quot;]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  tags &#x3D; &#123;</span><br><span class="line">    Name &#x3D; &quot;allow_web&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></code></pre><ul><li><h3 id="STEP7-STEP4에서-생성한-Subnet의-IP-주소로-Network-Interface-생성하기"><a href="#STEP7-STEP4에서-생성한-Subnet의-IP-주소로-Network-Interface-생성하기" class="headerlink" title="[STEP7] STEP4에서 생성한 Subnet의 IP 주소로 Network Interface 생성하기"></a><strong>[STEP7] STEP4에서 생성한 Subnet의 IP 주소로 Network Interface 생성하기</strong></h3><p>Network Interface의 정의에서 attachement section은 나중에 EC2 Instance를 provisioning할때 대체해서 처리할 것이다. </p><p><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/network_interface">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/network_interface</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Internet interface</span><br><span class="line"># subnet 내의 ip를 private_ips의 ip로 지정한다.(any)</span><br><span class="line"># 이제 s</span><br><span class="line">resource &quot;aws_network_interface&quot; &quot;web-server-nic&quot; &#123;</span><br><span class="line">  subnet_id       &#x3D; aws_subnet.subnet-1.id</span><br><span class="line">  private_ips     &#x3D;  [&quot;10.0.1.50&quot;]</span><br><span class="line">  security_groups &#x3D; [aws_security_group.allow_web.id]</span><br><span class="line"></span><br><span class="line"># EC2 Instance의 provisioning section에서 대체해서 처리한다.</span><br><span class="line">#   attachment &#123;</span><br><span class="line">#     instance     &#x3D; aws_instance.test.id</span><br><span class="line">#     device_index &#x3D; 1</span><br><span class="line">#   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><h3 id="STEP8-STEP7에서-생성된-Network-Interface에-Elastic-IP-할당하기"><a href="#STEP8-STEP7에서-생성된-Network-Interface에-Elastic-IP-할당하기" class="headerlink" title="[STEP8] STEP7에서 생성된 Network Interface에 Elastic IP 할당하기"></a><strong>[STEP8] STEP7에서 생성된 Network Interface에 Elastic IP 할당하기</strong></h3><p><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eip">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eip</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 위에서 생성한 NI에 EIP적용를 적용한다.</span><br><span class="line">resource &quot;aws_eip&quot; &quot;one&quot; &#123;</span><br><span class="line">  vpc                       &#x3D; true</span><br><span class="line">  network_interface         &#x3D; aws_network_interface.web-server-nic.id</span><br><span class="line">  associate_with_private_ip &#x3D; &quot;10.0.1.50&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AWS EIP는 IGW의 deployment에 의존적이다. 따라서 실제 EIP를 deploy하기 위해서는 IGW가 우선적으로 deploy되어야한다. (<code>선 IGW deploy, 후 EIP deploy</code>)</p><p>공식 문서를 보면, EIP는 IGW가 associate하기 전에 존재해야된다고 명기하고 있다. 이를 위해 EIP의 <code>depends_on</code> 속성을 사용하여, 명시적으로 IGW에 대한 의존성을 명시해줘야 한다. (<code>전체 객체에 대한 참조이므로 의존성에 대해 명시할때 특정 id가 아닌 객체로써 명시해준다</code>)</p></li></ul><ul><li><h3 id="STEP9-Ubuntu-server-생성-및-Apache2-설치-및-Enable하기"><a href="#STEP9-Ubuntu-server-생성-및-Apache2-설치-및-Enable하기" class="headerlink" title="[STEP9] Ubuntu server 생성 및 Apache2 설치 및 Enable하기"></a><strong>[STEP9] Ubuntu server 생성 및 Apache2 설치 및 Enable하기</strong></h3><p><code>생성되는 EC2 Instance의 AZ와 Instance가 속해있는 Subnet의 AZ는 반드시 일치</code>시켜줘야 한다. AZ가 같다는 것은 같은 Data Centre라는 것을 의미하기 때문에 반드시 AZ은 일치시켜서 관리하도록 한다.</p><p>모든 EC2 인스턴스를 배포하기 위해서는 NIC에 대한 정의가 필요하다. 앞서 NIC에서 attachement section에 대해 생략한 부분을 Instance를 정의할때 network_interface에 대한 block 부분에 정의하도록 한다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Server</span><br><span class="line"># device에 접근하기 위해서 key-pair에 대한 이름을 정의해줘야 한다.</span><br><span class="line">resource &quot;aws_instance&quot; &quot;web-server-instance&quot; &#123;</span><br><span class="line">  ami &#x3D; &quot;ami-058165de3b7202099&quot;</span><br><span class="line">  instance_type &#x3D; &quot;t2.micro&quot;</span><br><span class="line">  availability_zone &#x3D; &quot;ap-northeast-2a&quot;</span><br><span class="line">  key_name &#x3D; &quot;class-hglee-seoul&quot;</span><br><span class="line">  network_interface &#123;</span><br><span class="line">    device_index &#x3D; 0</span><br><span class="line">    network_interface_id &#x3D; aws_network_interface.web-server-nic.id</span><br><span class="line">  &#125;</span><br><span class="line">  user_data &#x3D; &lt;&lt;-EOF</span><br><span class="line">              #!&#x2F;bin&#x2F;bash</span><br><span class="line">              sudo apt update -y</span><br><span class="line">              sudo apt install apache2 -y</span><br><span class="line">              sudo systemctl start apache2</span><br><span class="line">              sudo bash -c &#39;echo your very first web server &gt; &#x2F;var&#x2F;www&#x2F;html&#x2F;index.html&#39; </span><br><span class="line">              EOF</span><br><span class="line">  tags &#x3D; &#123;</span><br><span class="line">    Name &#x3D; &quot;web-server&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Terraform/">Terraform</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Terraform/">Terraform</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/18/202206/220618-terraform-study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220616 Kubernetes 스터디 4일차</title>
      <link>https://leehyungi0622.github.io/2022/06/16/202206/220616_kubernetes_study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/16/202206/220616_kubernetes_study/</guid>
      <pubDate>Thu, 16 Jun 2022 09:30:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220606_kubernetes.png&quot; alt=&quot;Kubernetes&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;h2 id=&quot;ClusterIP-Service-실습&quot;&gt;&lt;a href=&quot;#ClusterIP-Service-실습&quot; class=&quot;headerlink&quot; title=&quot;ClusterIP Service 실습&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;ClusterIP Service 실습&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;ClusterIP Service를 생성하고, 지정한 selector가 app:pod를 가지는 pod와 연결을 하였다. 그런 다음에 연결된 app:pod label을 가지는 Pod를 삭제하고, 다시 재생성을 한다. 이렇게 되면, 실제 pod에 할당되어있던 IP는 동적으로 할당이 되어 변경이 되지만, ClusterIP Service는 재생성이 되지 않아, 동일한 Service IP로 Pod에 접근이 가능하다.&lt;/p&gt;
&lt;h2 id=&quot;NodePort-Servce-실습&quot;&gt;&lt;a href=&quot;#NodePort-Servce-실습&quot; class=&quot;headerlink&quot; title=&quot;NodePort Servce 실습&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;NodePort Servce 실습&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;실제 Dashboard의 디스커버리 및 로드 밸런싱 하위의 서비스 항목을 가보면, 내부 엔드 포인트가 두 개 생성된 것을 확인할 수 있는데, 상위에 있는 9000번 포트가 클러스터 내에서 접근할때 사용되는 포트이고, 30000번 포트가 각 각의 노드들에 동일하게 할당된 포트 번호이다. 이 포트를 통해 외부에서 클러스터 내부의 Service를 통해 하위 Pod들에 트래픽을 보낼 수 있다.&lt;br&gt;실습에서는 외부 터미널에서 curl 명령을 통해 서비스에 접근하여 각 pod에 대한 hostname이 출력되는 것을 확인하였다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220606_kubernetes.png" alt="Kubernetes"></div><br/><br/><h2 id="ClusterIP-Service-실습"><a href="#ClusterIP-Service-실습" class="headerlink" title="ClusterIP Service 실습"></a><ins><b>ClusterIP Service 실습</b></ins></h2><p>ClusterIP Service를 생성하고, 지정한 selector가 app:pod를 가지는 pod와 연결을 하였다. 그런 다음에 연결된 app:pod label을 가지는 Pod를 삭제하고, 다시 재생성을 한다. 이렇게 되면, 실제 pod에 할당되어있던 IP는 동적으로 할당이 되어 변경이 되지만, ClusterIP Service는 재생성이 되지 않아, 동일한 Service IP로 Pod에 접근이 가능하다.</p><h2 id="NodePort-Servce-실습"><a href="#NodePort-Servce-실습" class="headerlink" title="NodePort Servce 실습"></a><ins><b>NodePort Servce 실습</b></ins></h2><p>실제 Dashboard의 디스커버리 및 로드 밸런싱 하위의 서비스 항목을 가보면, 내부 엔드 포인트가 두 개 생성된 것을 확인할 수 있는데, 상위에 있는 9000번 포트가 클러스터 내에서 접근할때 사용되는 포트이고, 30000번 포트가 각 각의 노드들에 동일하게 할당된 포트 번호이다. 이 포트를 통해 외부에서 클러스터 내부의 Service를 통해 하위 Pod들에 트래픽을 보낼 수 있다.<br>실습에서는 외부 터미널에서 curl 명령을 통해 서비스에 접근하여 각 pod에 대한 hostname이 출력되는 것을 확인하였다.</p><a id="more"></a><h2 id="LoadBalancer-Servce-실습"><a href="#LoadBalancer-Servce-실습" class="headerlink" title="LoadBalancer Servce 실습"></a><ins><b>LoadBalancer Servce 실습</b></ins></h2><p>LoadBalancer service의 경우에는 이전에 이론에서 배웠듯이 외부로 연결되는 IP가 생성되기 위해서는 플러그인이 설치가 되어있어야 한다.</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$kubectl</span> get service svc-2</span><br></pre></td></tr></table></figure><p>위 명령을 통해 확인해보면, EXTERNAL-IP가 <none>으로 표기된 것을 확인할 수 있다.</p><h2 id="Object-Volume"><a href="#Object-Volume" class="headerlink" title="Object - Volume"></a><ins><b>Object - Volume</b></ins></h2><p>  이전에 Kubernetes의 전체 구조를 살펴볼때 pod에 문제가 생겨서 재생성되면, 내부 데이터가 다 날라가기 때문에 pod의 데이터를 별도의 volume에 mount시켜서 저장을 하고 관리해야 한다는 부분에 대해서 배웠다. 이번 세션에서는 이 Volume에 대해서 좀 더 구체적으로 이론적인 부분을 정리해보려고 한다.</p><p>  Volume의 종류로는 emptyDir, hostPath, PVC/PV가 있다.</p><ul><li><h3 id="1-emptyDir"><a href="#1-emptyDir" class="headerlink" title="(1) emptyDir"></a><strong>(1) emptyDir</strong></h3><p>emptyDir은 Pod 생성시 Pod의 내부에 만들어지고, Pod가 삭제되면 사라진다. 그래서 <code>일시적 사용 목적으로 사용</code>이 되며, 복수의 컨테이너 간에 파일을 공유할 때 사용이 된다.<br>만약 Web Server를 서비스하고 있는 Container1과 Backend server를 서비스하는 Conatiner2가 있다고 가정하고, 두 서버간에 파일을 공유해야되는 경우, 두 컨테이너를 하나의 볼륨에 연결시켜놓으면 손쉽게 파일을 공유할 수 있다.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-volume-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container1</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">tmkube/init</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">empty-dir</span></span><br><span class="line">          <span class="comment"># container가 mountPath 경로로 마운트하겠다는 의미이다.</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/mount1</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container2</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">tmkube/init</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">empty-dir</span></span><br><span class="line">          <span class="comment"># 만약 경로가 잘못되어도 name에서 empty-dir을 가르키고 있기 때문에 </span></span><br><span class="line">          <span class="comment"># empty-dir volume으로 mount가 된다.</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/mount2</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">empty-dir</span></span><br><span class="line">        <span class="attr">emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure></li><li><h3 id="2-hostPath"><a href="#2-hostPath" class="headerlink" title="(2) hostPath"></a><strong>(2) hostPath</strong></h3><p>hostPath는 <code>Pod의 데이터를 저장하기 위한 용도가 아닌, Node에 있는 데이터를 Pod에서 쓰기 위한 용도</code>이다.<br>hostPath에서 host란 Pad가 올라가있는 Node를 의미한다.</p><p>전체 구조는 Node내부에 Pod가 있고, Pod가 Node에 존재하는 Volume에 연결되어있는 구조이다. 그리고 Node 내의 Volume은 Pod의 유/무와 관계없이 사라지지 않는다.</p><ul><li><h4 id="문제상황"><a href="#문제상황" class="headerlink" title="문제상황"></a><strong>문제상황</strong></h4><p>Node1에 Pod1, Pod2가 있고, Volume에 두 Pod가 연결되어있는 상황에서 Pod2에 문제가 생겨서 재생성되었을때, 스케줄러에 이해서 Node2에 재생성이 되거나 Node1에 장애가 발생을 해서 통으로 다른 Node로 이전을 해야되는 상황이 생겼을때, 기존 Node1에 있는 Volume에 연결되지 못한다.<br>이러한 상황에서 해결방법은 Node가 추가될때마다 항상 동일한 경로에 마운트 시켜주는 것이다. (Node1의 Volume(<code>/node-v1</code>), Node2의 Volume(<code>/node-v1</code>))</p></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-volume-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">tmkube/init</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-path</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/mount1</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-path</span></span><br><span class="line">      <span class="attr">hostPath:</span></span><br><span class="line">        <span class="comment"># 사전에 해당 경로에 생성을 해둬야 한다.</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/node-v</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">Directory</span></span><br></pre></td></tr></table></figure><ul><li><h4 id="사용사례"><a href="#사용사례" class="headerlink" title="사용사례"></a><strong>사용사례</strong></h4><p>각각의 노드는 자기자신을 위해서 사용되는 파일이 있다. (시스템 파일, 기타 설정 파일들)<br>Pod는 자신이 속해있는 host(Node)의 데이터를 읽거나 써야될때 hostPath 유형의 Volume을 사용한다.  </p></li></ul></li><li><h3 id="3-PVC-Persistence-Volume-Claim-PV-Persistence-Volume"><a href="#3-PVC-Persistence-Volume-Claim-PV-Persistence-Volume" class="headerlink" title="(3) PVC(Persistence Volume Claim) / PV(Persistence Volume)"></a><strong>(3) PVC(Persistence Volume Claim) / PV(Persistence Volume)</strong></h3><p>PVC/PV의 목적은 Pod에 영속성있는 Volume을 제공하기 위함이다.</p><p>우선 PVC/PV의 흐름을 살펴보면, 우선 <code>(1)admin이 PV 정의에 대해서 생성</code>을 하면, <code>(2)사용자가 PVC를 생성</code>한다. 그런 다음에 <code>(3)K8S가 PVC의 내용에 맞는 적절한 Volume에 연결</code>을 시켜준다.<br>Pod를 생성할때 사용자가 생성한 PVC를 사용하면 된다.</p><p>흐름도는 <code>Pod -&gt; PVC -&gt; PV (복수) -&gt; Volume</code>인데, Pod가 PV에 직접 연결이 되지 않고, PVC를 통해서 연결되는 이유는 k8s는 volume영역에 대해 사용자 영역과 관리자 영역으로 나뉘기 때문이다.<br>여기서 사용자란 Pod에 서비스를 만들고 관리/배포를 담당하는 업무를 하며, 관리자는 PV를 작성한다.<br><code>Volume</code>에는 Local과 Remote 원격 Volume이 있는데, 원격 Volume에는 AWS, Git, NFS(Network File System), Volume을 직접 만들고 서비스도 가능한 Sorage OS가 있다.</p><p> <code>(Pod ~ PVC 영역이 사용자 영역이며, PV ~ Volume이 관리자 영역이다)</code></p><p><code>pod.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-volume-3</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">tmkube/init</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pvc-pv</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/volume</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pvc-pv</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="comment"># 이 claimName이 PVC 정의 파일의 metadata name과 연결된다.</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">pvc-01</span></span><br></pre></td></tr></table></figure><p><code>pvc.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-01</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span> <span class="comment"># 읽기/쓰기 모드가 되고,</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1G</span> <span class="comment"># 용량이 1G인 Volume이 필요(용량 할당)</span></span><br><span class="line">  <span class="comment"># 현재 생성되어 있는 PV들 중 선택이 된다. &quot;&quot;(필수)    </span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><code>pv.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-01</span></span><br><span class="line">    <span class="comment"># 앞서 k8s가 PVC 내용에 맞는 적절한 Volume에 연결을 해준다고 했는데,</span></span><br><span class="line">    <span class="comment"># 바로 아래 spec의 capacity와 accessModes의 내용이 근거가 된다!!</span></span><br><span class="line">    <span class="comment"># 설정이 되면, PVC에서 작성한 요청에 맞는 Volume에 연결이 된다.  </span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1G</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span> <span class="comment"># 읽기/쓰기 모드가 되고,</span></span><br><span class="line">  <span class="comment"># 실제로는 많이 사용되지 않는다. </span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/node-v</span></span><br><span class="line">    <span class="comment"># 아래의 내용은 PV에 연결되는 Pod들은 node들은 node1(label)</span></span><br><span class="line">    <span class="comment"># 노드 위에서만 무조건적으로 생성된다는 의미이다.</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">node</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">node1</span>]&#125;</span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1G</span> <span class="comment"># 용량이 1G인 Volume이 필요(용량 할당)</span></span><br><span class="line">  <span class="comment"># 현재 생성되어 있는 PV들 중 선택이 된다. &quot;&quot;(필수)    </span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>PV를 전문적으로 관리하는 관리자가 필요한 이유는 아래와같이 세부적으로 PV에 대한 작성을 해야되며, PV를 만들어두면, User는 사용을 위해서 PVC를 생성하는 패턴으로 작업을 하기 때문이다.</p><p>아래 내용은 remote volume 지정시, spec의 하위에 정의한다.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">nfs:</span></span><br><span class="line">  <span class="attr">service:</span></span><br><span class="line">  <span class="attr">path:</span></span><br><span class="line"><span class="attr">iscsi:</span></span><br><span class="line">  <span class="attr">targetPortal:</span></span><br><span class="line">  <span class="attr">iqn:</span></span><br><span class="line">  <span class="attr">lun:</span></span><br><span class="line">  <span class="attr">fsType:</span></span><br><span class="line">  <span class="attr">readOnly:</span></span><br><span class="line">  <span class="attr">chapAuthSession:</span></span><br><span class="line"><span class="attr">gitRepo:</span></span><br><span class="line">  <span class="attr">repository:</span></span><br><span class="line">  <span class="attr">revision:</span></span><br><span class="line">  <span class="attr">directory:</span></span><br></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Docker-K8s/">Docker&amp;K8s</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Docker/">Docker</category>
      
      <category domain="https://leehyungi0622.github.io/tags/Kubernetes/">Kubernetes</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/16/202206/220616_kubernetes_study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220614 Terraform study</title>
      <link>https://leehyungi0622.github.io/2022/06/14/202206/220614-terraform-study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/14/202206/220614-terraform-study/</guid>
      <pubDate>Tue, 14 Jun 2022 12:09:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220614_terraform.png&quot; alt=&quot;Terraform&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 Terraform에 대해 공부한 내용을 정리해두려고 한다. 지금 Docker나 Kubernetes도 데이터 파이프라인 구축시 별도의 서버를 컨테이너로 띄우고 관리하기 위해서 공부를 하고 있지만, 결국에는 반복을 통해서 익숙해지는 것이 최고이기 때문에 Terraform도 빠르게 개념을 정리하고 어떻게 AWS 인프라를 코드로 작성해서 간편하게 관리할 수 있는지에 주안점을 두고 학습을 해보려고 한다.&lt;br&gt;Terraform을 학습하게 된 계기는 우선 AWS의 서비스로 데이터 파이프라인을 구축할때 웹 페이지나 AWS CLI 상에서 계속 반복적인 작업을 통해 각 컴포넌트를 생성하는데 있어, 비효율적인 것 같다는 생각에서 비롯되었다.&lt;br&gt;그러던 중 현재 오프라인으로 DE 현직자에게 AWS 클라우드 환경에서 데이터 파이프라인 구축하는 방법에 대해서 수업을 듣고 있는데, Terraform과 같은 IaC(Infrastructure as Code)로 관리를 하게 되면, 좀 더 수월하게 데이터 파이프라인의 AWS 인프라를 빠르게 구축할 수 있다고 해서 개인적으로 공부를 시작하게 되었다.    &lt;/p&gt;
&lt;h2 id=&quot;Terraform&quot;&gt;&lt;a href=&quot;#Terraform&quot; class=&quot;headerlink&quot; title=&quot;Terraform ? &quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Terraform ? &lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;  Terraform은 IaC로, 인프라를 코드로 정의함으로써, 손쉽게 인프라 전반을 관리할 수 있도록 해주는 툴이다. &lt;/p&gt;
&lt;h2 id=&quot;Homebrew를-사용해서-Terraform-설치&quot;&gt;&lt;a href=&quot;#Homebrew를-사용해서-Terraform-설치&quot; class=&quot;headerlink&quot; title=&quot;Homebrew를 사용해서 Terraform 설치&quot;&gt;&lt;/a&gt;&lt;ins&gt;&lt;b&gt;Homebrew를 사용해서 Terraform 설치&lt;/b&gt;&lt;/ins&gt;&lt;/h2&gt;&lt;p&gt;  package management tool인 homebrew를 사용해서 Terraform을 설치할 수 있다. &lt;/p&gt;
&lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$brew&lt;/span&gt; install terraform&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable&quot;&gt;$terraform&lt;/span&gt; -v &lt;span class=&quot;comment&quot;&gt;# Terraform v1.2.2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220614_terraform.png" alt="Terraform"></div><br/><br/><p>이번 포스팅에서는 Terraform에 대해 공부한 내용을 정리해두려고 한다. 지금 Docker나 Kubernetes도 데이터 파이프라인 구축시 별도의 서버를 컨테이너로 띄우고 관리하기 위해서 공부를 하고 있지만, 결국에는 반복을 통해서 익숙해지는 것이 최고이기 때문에 Terraform도 빠르게 개념을 정리하고 어떻게 AWS 인프라를 코드로 작성해서 간편하게 관리할 수 있는지에 주안점을 두고 학습을 해보려고 한다.<br>Terraform을 학습하게 된 계기는 우선 AWS의 서비스로 데이터 파이프라인을 구축할때 웹 페이지나 AWS CLI 상에서 계속 반복적인 작업을 통해 각 컴포넌트를 생성하는데 있어, 비효율적인 것 같다는 생각에서 비롯되었다.<br>그러던 중 현재 오프라인으로 DE 현직자에게 AWS 클라우드 환경에서 데이터 파이프라인 구축하는 방법에 대해서 수업을 듣고 있는데, Terraform과 같은 IaC(Infrastructure as Code)로 관리를 하게 되면, 좀 더 수월하게 데이터 파이프라인의 AWS 인프라를 빠르게 구축할 수 있다고 해서 개인적으로 공부를 시작하게 되었다.    </p><h2 id="Terraform"><a href="#Terraform" class="headerlink" title="Terraform ? "></a><ins><b>Terraform ? </b></ins></h2><p>  Terraform은 IaC로, 인프라를 코드로 정의함으로써, 손쉽게 인프라 전반을 관리할 수 있도록 해주는 툴이다. </p><h2 id="Homebrew를-사용해서-Terraform-설치"><a href="#Homebrew를-사용해서-Terraform-설치" class="headerlink" title="Homebrew를 사용해서 Terraform 설치"></a><ins><b>Homebrew를 사용해서 Terraform 설치</b></ins></h2><p>  package management tool인 homebrew를 사용해서 Terraform을 설치할 수 있다. </p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$brew</span> install terraform</span><br><span class="line"><span class="variable">$terraform</span> -v <span class="comment"># Terraform v1.2.2</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Terraform-SETUP-in-VSCode"><a href="#Terraform-SETUP-in-VSCode" class="headerlink" title="Terraform SETUP in VSCode"></a><ins><b>Terraform SETUP in VSCode</b></ins></h2><p>  Terraform을 만든 HashiCorp의 VSCode - Terraform extension을 설치한다.</p><ul><li>자동완성, 자동 포맷팅 등의 다양한 기능들을 지원한다.</li></ul><h2 id="Terraform-실습"><a href="#Terraform-실습" class="headerlink" title="Terraform 실습"></a><ins><b>Terraform 실습</b></ins></h2><p>  Terraform 파일은 HashiCorp configuration language로 작성이 되며, 파일 확장자는 *.tf이다.</p><p>  Terraform은 다양한 provider를 제공한다. 따라서 terraform과 연동하고자하는 provider의 plug-in을 다운받아서 설치해야한다.<br>  플러그인 다운은 Terraform configuration 파일에서 선언해서 필요한 provider의 플러그인을 다운받도록 할 수 있다.</p><p>  아래 링크는 내가 데이터 파이프라인을 구축할때 AWS 클라우드 플랫폼을 사용할 것이기 때문에 AWS Provider에 대한 Terraform 공식 사이트 documentation이다.</p><p>  <a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs">https://registry.terraform.io/providers/hashicorp/aws/latest/docs</a></p><p>  나는 현재 <code>Terraform v1.2.2</code>를 사용하고 있기 때문에 Terraform 0.13 이상 버전과 호환되는 공식문서의 내용을 참고한다.</p><p>  <code>main.tf</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">terraform &#123;</span><br><span class="line">  required_providers &#123;</span><br><span class="line">    aws &#x3D; &#123;</span><br><span class="line">      source  &#x3D; &quot;hashicorp&#x2F;aws&quot;</span><br><span class="line">      version &#x3D; &quot;~&gt; 3.0&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Configure the AWS Provider</span><br><span class="line"># Seoul - (ap-northeast-2)</span><br><span class="line">provider &quot;aws&quot; &#123;</span><br><span class="line">  region &#x3D; &quot;ap-northeast-2&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 권장되지 않는 방식</span><br><span class="line">provider &quot;aws&quot; &#123;</span><br><span class="line">  region &#x3D; &quot;ap-northeast-2&quot;</span><br><span class="line">  access_key &#x3D; &quot;[access_key]&quot;</span><br><span class="line">  secret_key &#x3D; &quot;[secret_key]&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 예시) Create a VPC</span><br><span class="line">resource &quot;aws_vpc&quot; &quot;example&quot; &#123;</span><br><span class="line">  cidr_block &#x3D; &quot;10.0.0.0&#x2F;16&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h3 id="Authentication-amp-Configuration"><a href="#Authentication-amp-Configuration" class="headerlink" title="Authentication &amp; Configuration"></a><ins><b>Authentication &amp; Configuration</b></ins></h3><p>공식 사이트에 따르면, Provider configuratiopn에서 Hard-coded Credentials 방식은 Terraform configuration에서 권장되지 않는다고 한다.<br><code>(만약 AWS의 access_key나 secret_key에 대한 정보를 provider &quot;aws&quot;에 함께 기입을 했을 경우에는 반드시 VCS(Version Control System)에 업로드되지 않도록 주의해햐 한다.)</code><br>CodeBuild나 ECS를 사용하고 있고, IAM Task Role에 대해 정의를 하고 있다면, Terraform에서 해당 컨테이너의 Task Role을 활용할 수 있다고 한다. 또는 <code>Users/사용자명/.aws</code> 하위에 conf와 credential 파일이 있는데, 해당 path를 적용해서 하는 방법도 있다.<br>그런데 적용을 해봤는데, 적절하지 않는 argument라고 나와서 이 부분도 다시 document를 보고 해봐야겠다.</p></li></ul><h2 id="실습1-Terraform으로-AWS-EC2-인스턴스-생성하기"><a href="#실습1-Terraform으로-AWS-EC2-인스턴스-생성하기" class="headerlink" title="실습1) Terraform으로 AWS EC2 인스턴스 생성하기"></a><ins><b>실습1) Terraform으로 AWS EC2 인스턴스 생성하기</b></ins></h2><p>  EC2 인스턴스를 생성하기 위해서 resource 뒤에 생성할 resource 이름을 적게 되는데, 항상 위에 첨부한 공식 사이트에서 검색을 통해서 참고하도록 한다.</p><p>  <code>ec2_instance.tf</code><br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">terraform &#123;</span><br><span class="line">required_providers &#123;</span><br><span class="line">    aws &#x3D; &#123;</span><br><span class="line">      source  &#x3D; &quot;hashicorp&#x2F;aws&quot;</span><br><span class="line">      version &#x3D; &quot;~&gt; 3.0&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"># Configure the AWS Provider</span><br><span class="line"># Seoul - ap-northeast-2</span><br><span class="line">provider &quot;aws&quot; &#123;</span><br><span class="line">  region &#x3D; &quot;ap-northeast-2&quot;</span><br><span class="line">  access_key &#x3D; &quot;[access_key]&quot;</span><br><span class="line">  secret_key &#x3D; &quot;[secret_key]&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># [참고] provider의 resource 생성하기</span><br><span class="line">resource &quot;&lt;provider&gt;_&lt;resource_type&gt;&quot; &quot;name&quot; &#123;</span><br><span class="line">  config options......</span><br><span class="line">  key &#x3D; value&quot;</span><br><span class="line">  key2 &#x3D; &quot;another value&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource &quot;aws_instance&quot; &quot;my-first-terraform-server&quot; &#123;</span><br><span class="line">  # ami의 id는 EC2 인스턴스를 생성할때 ami-* 로 시작하는 부분을 복사해서 넣어주면 된다.</span><br><span class="line">  ami           &#x3D; &quot;ami-058165de3b7202099&quot;</span><br><span class="line">  instance_type &#x3D; &quot;t3.micro&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>  우선 위에서 작성한 *.tf 파일을 실행하기 전에 가장 먼저 <code>$terraform init</code>을 해줘야 한다.<br>  terraform init 명령을 통해 현재 위치한 디렉토리 내의 *.tf파일의 설정을 스캔하고, *.tf 파일의 내용에서 정의한 provider에 대한 내용 또한 스캔한다.<br>  이를 통해 정의한 aws과 관련되서 필요한 플러그인을 aws api와의 상호작용을 통해 다운로드받는다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$terraform</span> init</span><br><span class="line"><span class="comment"># Initializing provider plugins...</span></span><br><span class="line"><span class="comment"># - Finding hashicorp/aws versions matching &quot;~&gt; 3.0&quot;...</span></span><br><span class="line"><span class="comment"># - Installing hashicorp/aws v3.75.2...</span></span><br><span class="line"><span class="comment"># - Installed hashicorp/aws v3.75.2 (signed by HashiCorp)</span></span><br></pre></td></tr></table></figure><p>  그 다음으로 살펴 볼 명령은 <code>$terraform plan</code>인데, 이 명령은 선택사항인데, 내가 code로 작성한 infrastructure를 실제로 적용하기 전에 변경사항을 체크해볼 수 있는 명령이다.<br>  terraform plan 명령을 하기 전에 terraform init 명령이 선행되어야 한다. 초록색으로 표기(+)되는 리소스는 새로 추가되는 리소스이며, 빨간색으로 표기(-)되는 리소스는 제거된 리소스, 주황색으로 표기(~)되는 리소스는 수정된 리소스로 표기된다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$terraform</span> plan</span><br></pre></td></tr></table></figure><p>  이제 최종적으로 진행될 작업을 확인한 뒤에는 <code>$terraform apply</code>명령을 통해 실행을 시킨다.</p>  <figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$terraform</span> apply --auto-approve</span><br></pre></td></tr></table></figure><p>  최종적으로 AWS EC2 Instance Dashboard를 통해서 인스턴스가 생성된 것을 확인할 수 있었다. 인스턴스 세부 옵션을 넣는 부분도 확인해봐야겠다.<br>  만약 작성한 *.tf파일을 수정없이 다시 terraform apply를 하게 되면, 실제 AWS Infrastructure와 적용하고자 하는 *.tf파일의 내용을 비교해서 변화가 없으면 실행을 하지 않는다.</p><p>  이렇게 간단하게 코드로 작성해서 EC2 인스턴스를 생성하니, AWS 웹 페이지에서 번거롭게 마우스로 클릭하는 일을 하지 않아도 되서 좋은 것 같다.</p>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Terraform/">Terraform</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Terraform/">Terraform</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/14/202206/220614-terraform-study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220612 데이터 파이프라인 구축 오프라인 수업 / 3주차</title>
      <link>https://leehyungi0622.github.io/2022/06/12/202206/220612_datapipeline_study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/12/202206/220612_datapipeline_study/</guid>
      <pubDate>Sun, 12 Jun 2022 04:53:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220530_review.png&quot; alt=&quot;Review&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에서는 세 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. 참고로 첫 번째와 두 번째 수업때도 너무 유익한 내용들이 많았는데, 이번 시간이 정말 너무 유익하고 좋았다.&lt;br&gt;아마도 이전에 인터넷 강의로 수강을 했을 때 아쉬웠던 부분이 많았는데, 이번에 개별적으로 현직자 분께 오프라인으로 직접 수업을 들으니, 궁금했던 부분이 많이 해소되기도 했고, 강사님이 수업에 필요한 여러 자료나 실제 회사에서 업무했을 때 필요한 부분에 대해 설명을 잘 해주셔서 그런 것 같다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220530_review.png" alt="Review"></div><br/><br/><p>이번 포스팅에서는 세 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. 참고로 첫 번째와 두 번째 수업때도 너무 유익한 내용들이 많았는데, 이번 시간이 정말 너무 유익하고 좋았다.<br>아마도 이전에 인터넷 강의로 수강을 했을 때 아쉬웠던 부분이 많았는데, 이번에 개별적으로 현직자 분께 오프라인으로 직접 수업을 들으니, 궁금했던 부분이 많이 해소되기도 했고, 강사님이 수업에 필요한 여러 자료나 실제 회사에서 업무했을 때 필요한 부분에 대해 설명을 잘 해주셔서 그런 것 같다.</p><a id="more"></a><p>이번 3주차 수업에서는 수업 한 시간 전에 미리 강의장에 도착해서 어떤 식으로 데이터 엔지니어 포트폴리오의 프로젝트를 구성해야되는지 과거에는 K사에서 근무하셨고, 현재는 N사에서 데이터 엔지니어로 근무하시는 강사님께 조언을 구했는데, 너무 감사하게도 세세하게 답변을 해주셔서 이 부분에 대해서도 한 번 정리를 해보려고 한다.</p><h2 id="포트폴리오-프로젝트-준비"><a href="#포트폴리오-프로젝트-준비" class="headerlink" title="포트폴리오 프로젝트 준비"></a><ins><b>포트폴리오 프로젝트 준비</b></ins></h2><p>이번 3주차 수업 시작전에 강사님께 미리 데이터 엔지니어 포트폴리오는 어떤 식으로 준비를 해야되는지 여쭤보았다. 내가 이전에 생각했던 것은 FE/BE는 하나의 프로젝트당 하나의 코드로 깃허브 Repository에 정리를 하면 되는데, 데이터 엔지니어는 AWS와 같은 클라우드 서비스를 이용해서 데이터 파이프라인을 구축하고, 내부적으로 일부 코드를 작성하기 때문에 문서 형태로 정리를 하면 되는 것인가? 라는 생각을 했었다.<br>그런데 강사님이 <code>&quot;데이터 엔지니어도 FE/BE 프로젝트와 다를 것이 없고, 단지 데이터 엔지니어를 위한 프로젝트는 FE/BE처럼 하나의 코드로는 나오지 않을 것이다.&quot;</code>라는 답변을 해주셨다.<br>덫붙여서 1주차 강의때 더 알면 좋은 내용들 중에 <code>IaC(Infrastructure as Code)로 인프라구축을 코드로 작성</code>해서, 프로젝트를 구성하고 있는 인프라를 손 쉽게 구축할 수 있게 하는 부분을 해보고, 해당 코드를 프로젝트에 넣으면 좋을 것 같다는 말씀도 해주셨다.<br>아마 FE/BE 사이드 프로젝트와 별반 다르지 않다는 말씀이 이런 코드들을 GitHub 레포지토리에 정리해서 올리고, 데이터를 정제할때나 Event Driven Architecture로 구성했을때 작성했던 코드들도 GitHub에 프로젝트 단위로 묶어서 올리면 되기 때문에 그렇게 말씀해주신 것 같다.(<code>Markdown으로 문서화는 필수</code>)</p><p>IaC로 인프라 구축을 코드로 작성했을 때, Terraform에 대해 언급을 해주셨는데, 이전에 Vagrant로 스크립트를 작성해서 VM의 구성을 손쉽게 하는 것을 해보았는데, 이것과 같은 맥락인 것 같았다. (<code>찾아보니, Vagrant와 Terraform은 둘 다 HashiCorp에서 파생된 프로젝트라고 한다. Vagrant는 개발 환경을 관리하는데 주안점을 둔 툴이고, Terraform은 인프라를 구축하는데 주안점을 둔 툴이라는 차이가 있다.</code>) </p><p>AWS로 구축한 인프라는 HCL(Haship Configuration Language, *.tf)으로 작성해서 GitHub에 코드를 올리도록 하자.</p><p>실제 업무를 할때나 사이드 프로젝트를 할때에도 우아하게 파이프라인을 구축할 필요 없이 Lambda와 triggering하는 요소들을 잘 조합해도 효과적으로 공수를 덜 들이고 인프라를 구축할 수 있다.<br>간단한 처리는 Lambda와 EventBridge만을 사용해서 처리를 할 수 있다.  </p><h2 id="Athena의-사용"><a href="#Athena의-사용" class="headerlink" title="Athena의 사용"></a><ins><b>Athena의 사용</b></ins></h2><p>Athena 엔진(Version2)은 Presto(0.217)라는 오픈소스를 기반으로 만들어졌다. 관련된 함수, 연산자, 표현식에 대한 자세한 내용은 Presto documentation에서 확인할 수 있다.(Kakao ) </p><p>Athena는 Presto 및 Trino의 함수와 기능의 전부는 아니지만, 일부 지원을 한다. 최근에는 Presto에서 Trino로 바뀌었다.</p><p>Athena를 사용할때 직접 Athena에 접속을 해서 브라우저에서 쿼리를 날리거나 하는 작업을 하기도 하지만, 현업에서는 파이프라인을 구축할때, Athena를 원격지의 데이터베이스 개념으로 다뤄서 쿼리를 보내는 형태로 작업을 한다.(<code>한번 감싼 형태로 Athena를 활용</code>) </p><p>이전에 BI툴인 Tableau에서 Amazon Athena의 데이터와 연결해서 실습해본적이 있는데, 이렇게 사용한다는 의미인 것 같다. (<code>Tableau에서 Athena JDBC 드라이버를 설치해서 사용, 자동화시에 사용</code>)</p><p>Presto와 Trino는 파일들을 Parsing해서 테이블의 스키마를 생성해서 데이터를 조회한다거나 하는 작업을 가능하게 해주는 엔진이다. </p><h2 id="Output이-여러개-파일로-나오는-것을-하나의-파일로-통합"><a href="#Output이-여러개-파일로-나오는-것을-하나의-파일로-통합" class="headerlink" title="Output이 여러개 파일로 나오는 것을 하나의 파일로 통합"></a><ins><b>Output이 여러개 파일로 나오는 것을 하나의 파일로 통합</b></ins></h2><p>이전 실습을 할때 CTAS query의 결과로 S3에 저장된 데이터가 여러개로 생성(<code>Athena가 Presto 엔진을 기반으로, 분산된 되서 처리를 해주기 때문에 여러개의 파일로 생성</code>)이 되었었는데, 아래와 같이 <code>bucketed_by=ARRAY[&#39;time&#39;], bucket_count=1</code> time 필드를 기준으로 한 번 만 실행되도록 설정해서 CTAS 쿼리를 날려주면, 한 개의 파일로 추출이 된다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> new_table2 <span class="keyword">WITH</span> (external_location<span class="operator">=</span>‘s3:<span class="operator">/</span><span class="operator">/</span>hg<span class="operator">-</span>data<span class="operator">-</span>bucket<span class="operator">/</span>ctas<span class="operator">-</span>bucketing<span class="operator">/</span><span class="string">&#x27;, </span></span><br><span class="line"><span class="string">format=&#x27;</span>PARQUET<span class="string">&#x27;,parquet_compression=&#x27;</span>SNAPPY<span class="string">&#x27;,bucketed_by=ARRAY[&#x27;</span><span class="type">time</span><span class="string">&#x27;], </span></span><br><span class="line"><span class="string">  bucket_count=1) </span></span><br><span class="line"><span class="string">AS SELECT time, remote_ip FROM &quot;class&quot;.&quot;hg-data-table&quot;</span></span><br></pre></td></tr></table></figure><p>PARQUET 데이터 포맷으로 하고, 압축방식은 SNAPPY로 한다.(SNAPPY 압축방식은 네트워크 통신비용을 줄이기 위해 많이 사용된다)</p><p>그리고 결과로 S3에 적재된 파일의 데이터가 제대로 들어갔는지 확인하려면 생성된 파일을 체크하고, [작업] - <code>&quot;S3 Select&quot;</code>를 사용한 쿼리를 선택해서 data type을 parquet로 변경하고 쿼리를 날려서 결과 데이터를 확인해볼 수 있다. (parquet 타입의 데이터의 경우에는 다운받아서 확인하려면 별도의 프로그램을 다운받아야 한다) 이렇게 하면 별도로 Athena로 연결해서 데이터를 확인하지 않아도 된다.</p><h2 id="Hadoop-Spark를-Athena로-대체"><a href="#Hadoop-Spark를-Athena로-대체" class="headerlink" title="Hadoop, Spark를 Athena로 대체"></a><ins><b>Hadoop, Spark를 Athena로 대체</b></ins></h2><p>ETL과정에서 Transform 과정에서 Hadoop이나 Spark를 사용하지 않아도 Athena를 사용해서 대체 가능하다. (<code>time format의 포멧을 변경하거나, 파일의 포맷을 바꾸는 쿼리 옵션도 있기 때문에 Athena에서 쿼리를 작성하면 별도로 Hadoop이나 Spark를 사용하지 않고 데이터 전처리가 가능</code>)</p><h2 id="데이터-저장-포맷-CSV-TSV-JSON-Parquet-ORC"><a href="#데이터-저장-포맷-CSV-TSV-JSON-Parquet-ORC" class="headerlink" title="데이터 저장 포맷(CSV, TSV, JSON, Parquet, ORC)"></a><ins><b>데이터 저장 포맷(CSV, TSV, JSON, Parquet, ORC)</b></ins></h2><p>CSV는 Comma Separatored Values로 comma로 구분된 데이터 포멧을 말한다. 그리고 TSV는 Tab Separated Values로, Tab으로 각 칼럼 데이터가 구분된 데이터 포맷을 말한다.<br>CSV, TSV, JSON은 사람이 읽기에는 용이하나, 압축률이나 데이터를 읽어들이는 속도가 느리기 때문에 빅데이터를 보관 및 저장을 할때는 Parquet나 ORC를 주로 사용한다.<br>Parquet와 ORC는 column oriented data(columnar)로, 칼럼 기반 저장 포멧이다. 따라서 압축률이 높고, spark, hadoop에서 많이 사용(Parquet)되며, Hive에 특화(ORC)되어있다.</p><h2 id="S3-객체의-스토리지-클래스-변경"><a href="#S3-객체의-스토리지-클래스-변경" class="headerlink" title="S3 객체의 스토리지 클래스 변경"></a><ins><b>S3 객체의 스토리지 클래스 변경</b></ins></h2><p>S3에 보관된 객체의 스토리지 클래스를 변경할 수 있다. 이는 데이터의 사용빈도에 따라서 다르게 해서 적용해야 자원을 좀 더 효율적으로 사용할 수 있다.</p><p>그 예시로, <code>Glacier Deep Archive</code>가 있는데, <code>일년에 한 번 엑세스하는 오래된 아카이브 데이터</code>가 있으며, 매우 저렴하게 이용할 수  있다. 저렴하지만, 해당 스토리지 클래스 객체를 검색하게 되면, 몇 분 내지 몇 시간의 검색 시간이 소요된다는 단점이 있다.<br><code>Standard</code>는 <code>자주 엑세스하는 데이터로, 한 달에 한 번 이상 접근하는 데이터</code>의 경우에 적용되는 스토리지 클래스이다. 가격이 다른 스토리지 클래스에 비해 비싸다는 단점이 있다.</p><h2 id="S3의-수명-주기와-복제-규칙"><a href="#S3의-수명-주기와-복제-규칙" class="headerlink" title="S3의 수명 주기와 복제 규칙"></a><ins><b>S3의 수명 주기와 복제 규칙</b></ins></h2><p>앞에서는 저장된 데이터 객체의 특성에 따라서 직접 객체의 스토리지 클래스를 변경하였는데, 또 다른 방법으로는 S3 bucket에 수명 주기 규칙이나 복제 규칙등을 지정해주는 것이다.<br>(AWS S3 버킷 선택해서 내부 버킷으로 들어와서 <code>[관리]탭 선택</code> - <code>수명 주기 규칙 및 복제 규칙 생성 및 관리</code>)<br>이렇게 규칙을 생성해주면, S3 버킷 내의 특정 디렉토리에 계속해서 데이터가 쌓이게 된다고 가정했을때 필터의 접두사로 <code>long-</code>으로 설정해주고, 해당 디렉토리에 쌓인 데이터의 객체의 현재 버전 전환으로 특정 클래스를 지정하고 경과 시간을 입력해주면, log-으로 시작하는 디렉토리에 쌓인 데이터 객체가 지정한 기간 이후에 지정된 스토리지 클래스로 전환이 된다. 이외에도 특정 기간이 지나면 해당 데이터가 삭제되도록 할 수도 있다.</p><p>복제 규칙의 경우에는 일정 기간이 지나면 암호화를 하거나 다른 버킷으로 옮기거나 다른 계정의 버킷으로 옮기는 것이 가능하도록 해준다.<br>본사에 한 달 동안 데이터 분석 처리를 하고, 계열사나 고객사로 데이터를 넘기는 경우, 복제 규칙을 활용할 수 있다.</p><p>위와같이 자동화 설정이 가능하다.</p><h2 id="AWS-Lambda"><a href="#AWS-Lambda" class="headerlink" title="AWS Lambda"></a><ins><b>AWS Lambda</b></ins></h2><p>FaS(Function as Service)로 Serverless computing이라고 한다. CPU나 메모리는 신경쓰지 않고, 오직 코드만 작성하는데 집중할 수 있다.(<code>Infra의 provisioning 없이 내가 선택한 언어로 코드만 업로드해서 사용</code>)</p><p>코드는 직접 브라우저에서 작성을 하거나 S3에 업로드된 코드를 import해서 사용할 수 있는 방식으로 되어있다. 초당 수십만개의 데이터를 요청 처리하는 것이 가능하기 때문에 경우에 따라서 이벤트가 올때마다 서버(Spring boot와 같은)에서 처리를 하도록 처리하지 않고, 간단한 코드처리의 경우에는 Lambda에 올려서 처리하도록 할 수 있다. 특정 API 요청이 오거나 Action, Event 가 왔을때 넘길 수 있거나 회원가입이나 간단한 조회의 경우 람다함수를 사용할 수 있다.<br>비용 청구는 코드가 실행되는 시간(ms)단위로 기준으로 청구된다. (<code>길게 연산이 요구되는 처리에는 적합하지 않다</code>)</p><p>다양한 언어를 지원해주며, 연산을 하거나 데이터 처리를 하고 결과만 넘기는 처리(stateless)를 주로 하기도 하지만, 요즘에는 storage를 직접 붙이거나해서 로컬에 데이터를 저장할 수 있도록 변하고 있다.</p><p>용도와 가격에 맞게 EC2로 서버를 올릴 것인지 ECS(Elastic Container Service)를 사용해서 서버를 올릴 것인지 선택을 해서, Filebeat를 올릴 것인지 고려를 해서 AWS를 활용하는 것이 좋다.(<code>이 부분은 프로젝트 구성할때 한 번 고려를 해봐야겠다.</code>)</p><p>Lambda를 호출(트리거)할 수 있는 것은 API Gateway, S3, SQS, EventBridge가 있다. </p><p><code>시나리오1</code>) S3에 데이터가 들어오면 람다가 돌면서 데이터를 처리한 다음에 다른 서비스로 넘길 수 있다.</p><p><code>시나리오2</code>) SQS(Simple Queue Service)에 이벤트가 쌓이면 Lambda에서 처리를 할 수 있게 할 수 있다.<br>예를들어, 마케팅을 위해서 여러 다수의 고객들에게 이메일을 보낼때, 사용자 데이터를 뿌려서 큐에 집어넣고, Lambda로 호출해서 큐의 정보를 뽑아서 처리하는 경우에도 활용될 수 있다.</p><ul><li><h3 id="Lambda-함수-생성"><a href="#Lambda-함수-생성" class="headerlink" title="[Lambda 함수 생성]"></a><strong>[Lambda 함수 생성]</strong></h3><p>람다 함수를 생성할때에는 새로 code editor를 사용해서 생성을 하거나 블루프린트(자주 사용되는 template을 AWS에서 제공)로 생성을 할 수 있다.</p><p>함수 이름과 사용할 언어의 런타임을 선택해주고, 내부 코드 소스에서 editor에 직접 코드를 입력하거나 S3에 업로드된 code를 담은 zip 파일 패키지를 업로드할 수 있다.<br>생성된 코드는 별도로 테스트를 할 수 있도록 AWS Lambda에서 테스트 전용 탭을 제공한다. 그리고 작성한 코드에서 아래와 같이 환경변수를 호출해서 사용하는 경우도 있는데, 이 환경변수는 생성한 Lambda 함수의 “구성”탭에서 하위의 “환경변수”를 선택하면 환경변수를 추가할 수 있는데, 여기에 추가된 환경변수를 참조한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 예시</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment">#boto3는 python에서 AWS　서비스를 사용할때 많이 사용되는 라이브러리이다.</span></span><br><span class="line"><span class="keyword">import</span> boto3</span><br><span class="line"></span><br><span class="line">SOURCE_DATABASE = os.getenv(<span class="string">&#x27;SOURCE_DATABASE&#x27;</span>)</span><br><span class="line"><span class="comment"># AWS Athena 서비스 접근</span></span><br><span class="line">boto3.client(<span class="string">&#x27;athena&#x27;</span>)</span><br><span class="line"><span class="comment"># AWS S3 서비스에 접근</span></span><br><span class="line">Bucket = boto3.resource(<span class="string">&#x27;s3&#x27;</span>).Bucket(<span class="string">&#x27;hg-project-data&#x27;</span>)</span><br><span class="line">Bucket.put_object(Key=file_name, Body=log)</span><br></pre></td></tr></table></figure><p>추가적으로 Trigger해주는 서비스들 중에서 CodeCommit이 있는데, 이는 깃에서 코드를 commit했을때 자동으로 CI/CD가 돌도록 구성하는 부분과 같이 CodeCommit의 서비스가 구동이 되면 생성한 Lambda함수가 실행되도록 설정할 수 있다.</p></li></ul><h2 id="Amazon-EventBridge"><a href="#Amazon-EventBridge" class="headerlink" title="Amazon EventBridge"></a><ins><b>Amazon EventBridge</b></ins></h2><ul><li><p>서버리스 이벤트 버스이다. 여기서 이벤트 버스란, 수많은 이벤트들을 받아서 라우팅하거나 필터링, 트리거링 할 수 있도록 도와주는 서비스이다.</p></li><li><p> EDA(Event-Driven Architecture) 구축을 간편하게 처리할 수 있도록 해준다.</p></li><li><p>AWS 서비스에서 생성되는 다양한 이벤트들을 가져올 수 있다.</p></li><li><p>특정 규칙을 지정하고 규칙에 맞을때, AWS의 다른 서비스를 호출할 수 있도록 해준다.</p></li><li><p>리눅스의 cron 기능과 같이 일정 시간마다 AWS 서비스를 호출시켜주는 스케줄러의 기능을 제공한다. </p><p>주문이 왔을때 Evnet를 EventBridge로 발생시키고, 발생시킨 EventBridge가 하위의 각 각의 서비스(청구서, 지불 등의 서비스들)을 호출한다. 이전에는 API 호출을 해서 일일이 트리거 시켜줘야했는데 EDA에서는 EventBridge를 사용해서 Loose coupling을 할 수 있고, copepipe라인에 들어온 이벤트를 이벤트 브릿지가 처리해서 람다함수를 호출하게도 할 수 있다.</p><p>Apach Airflow를 사용해서 dag를 생성해서 일정 시간에 실행되도록 만들 수 있다. </p></li></ul><h2 id="실습-1-Lambda-EventBridge-통합-실습"><a href="#실습-1-Lambda-EventBridge-통합-실습" class="headerlink" title="실습 1) Lambda + EventBridge 통합 실습"></a><ins><b>실습 1) Lambda + EventBridge 통합 실습</b></ins></h2>  <div align="center">    <img src="/images/post_images/220614_lambda_eventbridge.jpg" alt="Lambda와 EventBridge를 활용한 파이프라인 실습">  </div><ul><li><p><code>log-generator</code>　Lambda function을 생성하는데, s3 버킷에 지정한 디렉토리를 생성하고, timestamp, device_name, metric 값을 생성해서 해당 log값을 내용으로 하는 파일을 버킷에 저장되도록 코드를 구성한다.<br>반환되는 값은 statusCode와 body의 내용으로 한다.</p><p>Access denied 에러가 발생하는 이유는 IAM 권한이 없어서다.<br>log-generator 코드상에서 결과 파일을 S3에 저장하는 부분이 있는데, 이 부분<br>때문에 에러가 발생된다.<br>AWS에서는 whitelist 정책에 의해 허가된 서비스 간에서 서로 통신이 가능하다. 따라서 현재 실습에서는 Lambda에서 S3에 접근을 할 수 있도록 해야한다. (<code>최소권한의 원칙 - Lambda함수에서 업로드를 하는 기능만 하기 때문에 S3서비스에 대해 put에 대한 권한만 부여해야 한다.</code>)<br>권한에 대한 부여는 생성된 람다 함수의 <code>&quot;구성&quot;탭에서 하위메뉴의 &quot;권한&quot;을 선택</code>하고, 실행역할에 역할 이름 링크를 타고 들어가서 추가 권한을 부여해주면 된다. (<code>&quot;권한추가&quot; - &quot;정책연결&quot; - AmazonS3FullAccess 권한 추가</code>) </p><p>그리고 구성 부분에서 timeout 및 메모리의 사용량에 대해서도 편집할 수 있다.<br>Lambda 함수를 작성하고 반드시 Deploy를 해주도록 하자.<br>Deploy후에 테스트를 해주면 결과적으로 S3에 코드에서 생성한 임의의 디렉토리의 하위에 파일이 생성된 것을 확인할 수 있다.</p></li><li><p><code>EventBridge</code> Trigger 추가는 우선 EventBridge에 대한 개별설정을 한 다음에 연결되도록 하는 것이 좋다. (EvnetBridge에 대한 개념 설명은 별도의 세션에 정리를 하였다(<code>참고</code>))<br>EvnetBridge에서 <code>&quot;규칙 생성&quot; - 이름 입력 및 규칙 유형을 &quot;일정&quot;으로 선택</code>하고 일정한 빈도로 실행(1분에 한 개씩)할 것이기 때문에 빈도 값과 단위를 선택해서 EventBridge의 규칙을 생성해주면 된다. 그 다음에는 AWS 서비스를 선택하고, Lambda function을 선택해서 이전에 생성한 log-generator lambda function을 선택한다.</p><p>이렇게 되면, EventBridge에 의해 Lambda 함수가 실행되어, S3에 파일이 하나씩 떨어지는 것을 확인할 수 있다. (<code>IoT 기기에서 센서 로그를 찍어내는 것과 같은 상황 구현</code>)</p></li><li><p>log-generator에 의해서 생성되는 JSON 포맷의 데이터에 맞게 Athena table의 schema를 생성해줘야 한다. [Create] - S3 Bucket data을 선택하고, 테이블 명과 데이터 베이스를 선택해준 다음에 <code>Dataset</code>을 현재 1분에 한 개씩 로그 데이터가 쌓이고 있는 location을 지정해준다. 그리고 Data format section에서는 현재 CSV 파일의 형태로 S3 Bucket에 데이터를 쌓고 있기 때문에 JSON을 선택해준다.<br>열 정보에서는 현재 쌓고 있는 JSON 타입의 데이터의 각 Key값을 column으로 타입과 함께 명시를 해준다.<br>(최종적으로 테이블을 생성한 다음은 테이블 미리보기를 통해서 테이블을 통해 값을 미리 확인해본다)</p></li><li><p>5분에 한 번씩 compaction해서 처리하는 Batch 작업을 하기 위해 <code>log-compactor Lambda 함수를 생성</code>한다.<br>log-compactor에서는 5분에 한 번씩 Athena에 쿼리를 날려서 압축 및 변환을 하는 query를 실행을 시키도록 하는 부분이다. (<code>실제 실행은 Athena에서 처리</code>)<br>실제 log-compactor Lambda 함수의 파이썬 스크립트를 살펴보면, 실제 Athena로 전송하는 CTAS 쿼리문의 조건 WHERE 절을 살펴보면, timestamp 칼럼의 start_time, end_time을 통해 특정 기간내의 데이터 파일만 모아서 하나의 파일로 압축하는 형태로 쿼리를 처리하고 있다.<br>log-compactor lambda 함수의 환경변수는 S3의 Bucket(1분에 한 개씩 센서 데이터를 누적하고 있는 버킷)과 Athena에서 생성한 테이블 정보와 테이블을 포함하고 있는 DB, 생성될 파일의 prefix(NEW_TABLE_NAME), 참조할 DB와 TABLE 정보를 입력해주면 된다. </p></li><li><p>새로 생성한 log-compactor Lambda function에는 이전 log-generator Lambda function과 동일하게 Athena와 S3에 접근해서 처리할 수 있도록 권한을 부여해야 한다. (<code>AmazonS3FullAccess, AmazonAthenaFullAccess</code>)</p></li><li><p>이제 생성한 log-compactor Lambda 함수와 mapping되는 EventBridge 이벤트를 생성해줘야 한다. 이 EventBridge는 log-generator와 동일하게 하되, 5분 간격으로 복수 개의 파일에 대해 compaction을 할 것이기 때문에 5분 단위로 lambda함수가 실행되도록 해야한다.</p></li><li><p>S3의 압축된 파일도 압축된 original file의 데이터 타입(JSON)을 통해 “Query with S3 Select”를 해서 제대로 데이터가 찍혀있는지 확인을 할 수 있다. </p></li></ul><h2 id="기타-주의사항"><a href="#기타-주의사항" class="headerlink" title="기타 주의사항"></a><ins><b>기타 주의사항</b></ins></h2><ul><li>Athena 테이블에서 ‘-‘ 특수기호를 넣어서 DB나 Table이름을 지정하게 되면, 쿼리에서 테이블을 호출 할때 에러가 발생한다.</li></ul><h2 id="중요-Apache-Kafka"><a href="#중요-Apache-Kafka" class="headerlink" title="(중요)Apache Kafka"></a><ins><b><code>(중요)</code>Apache Kafka</b></ins></h2><p>Apache Kafka는 데이터 엔진지어 분야 외에도 백엔드에서도 많이 사용된다.<br>MS에서 인수한 Linked에서 개발되었다가 2011년쯤에 오픈소스화된 프로젝트로, 스칼라 언어로 개발되었다. 주로 데이터 스트림을 위한 미들웨어, 메시지 큐, 메시지 브로커(이벤트 브로커)등의 말로 불리고 있다.<br>어플리케이션이나 소프트웨어 간의 메시지 중계자 역할을 해주고 있다.</p><ul><li><h3 id="Kafka의-특징"><a href="#Kafka의-특징" class="headerlink" title="[Kafka의 특징]"></a><strong>[Kafka의 특징]</strong></h3><ul><li><p>pub/sub 모델이다.</p></li><li><p>데이터의 영속성을 가진다.(<code>Data Persistency</code>)  일정기간동안 데이터를 보관하기 때문에 데이터의 재처리가 가능하다.</p></li><li><p>손쉽게 scale in/out이 가능하며, 고가용성이다. (<code>Highly scalable and available</code>)</p></li><li><p>At least once를 지원(<code>최근에는 exactly once를 지원하지만 성능이 낮아진다</code>)</p></li></ul></li></ul><ul><li><h3 id="비동기-처리를-위한-Pub-발행-Sub-구독-Model"><a href="#비동기-처리를-위한-Pub-발행-Sub-구독-Model" class="headerlink" title="비동기 처리를 위한 Pub(발행)/Sub(구독) Model"></a><strong>비동기 처리를 위한 Pub(발행)/Sub(구독) Model</strong></h3><p>마치 전화가 아닌 이메일과 같은 연락 방식과 유사하다고 생각할 수 있다.<br>송/수신 관계에 있는 양 극단의 Publisher와 Subscriber가 Direct로 연결하는 방식이 아닌, 중간에 중개자 역할을 해주는 Broker를 위치시킨다.<br>(<code>A----(Broker)----B</code>)</p><p>A는 B 서버로 보내기 위해 B의 서버 주소나 DNS에 관한 정보를 알 필요 없이 바로 Broker로 데이터를 보내서 적재하면 되며, 만약에 B 서버에 문제가 생겨서 다운되더라도 A에서 송신한 데이터는 Broker에 누적이 되고 있기 때문에 B 서버가 복구된 후에 Broker로부터 쌓인 데이터를 이어서 수신해오면 된다.</p><p>그리고 Subscriber 역할을 하는 B는 역량만큼만 Broker로부터 데이터를 취득해서 처리하며, 기존 API호출의 경우에는 DDos 공격과 같이 request를 무한정 보내서 문제가 될 수 있지만, 중간에 Broker역할을 해주는 Message Queue를 위치시켜주면, 구조상 Loose coupling이 가능해서 DDos와 같은 문제상황을 사전에 예방하고, 확장가능한 설계를 할 수 있다. (<code>Direct로 A와 B 서버를 연결하게 되면, 이러한 확장 가능한 구조가 되지 않는다</code>)</p><p>MSA(MicroService Architecture) - 확장 가능한 설계</p></li></ul><p><code>Kafka에서는 Producer(Publisher), Consumer(Subscriber)</code>라고 한다.</p><p>원래 RabbitMQ나 일부 메시지 큐의 경우에는 메시지 큐에서 받은 데이터를 가져가면 브로커 입장에서는 메시지가 사라지게 된다.(<code>일정기간 in-memory에 가지고 있다가 최종 전달자에게 전달하면, 사라지는 형태로 동작</code>) 하지만, Kafka의 경우에는 publisher가 데이터를 넣으면 스토리지에 저장한다. (<code>데이터의 영속성(Persistency</code>)<br>(Data retention 기간(<code>데이터를 스토리지에 저장하는 기간</code>)에 대한 설정을 할 수 있다. (해당 retention 기간 내에는 다른 consumer들이 요청을하면 같은 메시지를 처리할 수 있다 - <code>데이터 재처리 가능</code>))</p><br/><ul><li><h3 id="Kafka의-구조"><a href="#Kafka의-구조" class="headerlink" title="Kafka의 구조"></a><strong>Kafka의 구조</strong></h3><p>Kafka는 Broker라고 불리우는 서버가 여러대로 클러스터링 되어 구성되어있으며, Apache Zookeeper와 함께 쓰인다. Apache Zookeeper는 누가 리더인지 정해주는 코디네이터 역할을 해준다. Apache 3.0부터는 Zookeeper없이 Zookeeper의 기능을 Broker가 내장되서 기능이 개선되고 있다. 아직은 Production 단계이며, 올해 말쯤에는 도입될 새로운 기능이다.<br>이렇게 Kafka cluster + Apache Zookeeper에 producer와 consumer가 붙어서 데이터를 주고받는 형태를 갖는다. </p></li><li><h3 id="참고-KIP-500-Issue"><a href="#참고-KIP-500-Issue" class="headerlink" title="[참고] KIP-500 Issue"></a><strong>[참고] KIP-500 Issue</strong></h3><p>Kafka open source project 500번 이슈 -&gt; Kafka without Zookeeper(kafka 2.8)</p></li></ul><br/><ul><li><h3 id="Zookeeper의-기능"><a href="#Zookeeper의-기능" class="headerlink" title="Zookeeper의 기능"></a><strong>Zookeeper의 기능</strong></h3><p>Zookeeper는 분산된 카프카 브로커를 클러스터링해주는 중요한 컴포넌트이다. 클러스터링 되어있는 Broker들 사이에서도 리더가 있어서 어떤 친구가 데이터를 처리할 것인지, 쓸 것인지 구분해야 되기 때문에 Zookeeper가 이러한 리더의 선출을 담당한다. 그리고 파티션 수와 같은 Topic(<code>Topic은 메시지를 넘겨 줄때 사용되는 채널이며, 해당 토픽에 producer가 메시지를 push하고, consumer가 메시지를 pull해서 소비하는 구조의 형태로 되어있다</code>)의 메타데이터를 관리하며, 정보를 공유한다.<br>또한 새로운 브로커를 추가하거나 브로커의 장애를 감지하는 기능을 한다. </p></li><li><h3 id="Topic-Partition"><a href="#Topic-Partition" class="headerlink" title="Topic / Partition"></a><strong>Topic / Partition</strong></h3><div align="center">  <img src="/images/post_images/220615_partition_broker.jpg" alt="Partition과 Broker 사이의 관계"></div><div align="center">  <img src="/images/post_images/220615_producer_topic_broker_flow.jpg" alt="Producer와 Topic, Broker의 전체 흐름"></div><ul><li><p>Topic은 메시지를 분류해주는 채널이며, partition은 Topic을 여러 애들이 분산처리하기 위해서 파티션으로 나눈다.(<code>데이터 병렬 처리</code>) 그리고 나뉜 파티션들은 각 각 브로커들이 담당한다.</p></li><li><p>단일 파티션 생성이 가능하지만, 내결함성이 떨어진다.(<code>만약에 토픽에 브로커가 세 대있는데, 단일 파티션으로 하나의 브로커만 이용하도록 하면, 장애가 발생해서 데이터가 유실되었을 했을 때 대처하기 어렵기 때문에 세 개의 브로커를 사용하는 것이 좋다</code>)<br>단, 파티션은 늘릴 수는 있어도 줄일 수는 없다.</p></li><li><p>나뉜 각 각의 파티션은 데이터양이 많을 경우, 각 각의 파티션에 consumer를 물려서 확장성있게 대량의 데이터를 처리할 수 있다. (<code>확장성과 병렬처리 - 파티션</code>)</p></li><li><p>모든 메시지는 offset 값(bigint 기준)으로 어디까지 메시지가 읽혔는지 기록이 된다. (토픽이 새롭게 생성되지 않는 이상 계속 증가되는 형태이다)</p></li></ul><br/></li><li><h3 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a><strong>Replication</strong></h3><div align="center">  <img src="/images/post_images/220615_kafka_replication_conception.jpg" alt="Kafka의 Replication 개념"></div><p>Replication이란, Fault-tolerance(장애)에 대비하여 메시지를 설정한 <code>replication-factor</code>만큼 복제해서 각 각의 브로커들에 분산시키는 작업을 말한다.<br>데이터는 파티션 단위로 복제를 하며, 복제를 통해 일부 브로커가 불능 상태가 되어도 전체 클러스터는 정상 작동하도록 도와준다.<br>안전을 위해서 권장은 되지만, overhead가 존재하기 때문에 토픽의 특성과 리소스 사용량을 고려해서 replication-factor를 정해야한다.</p><ul><li><h4 id="ISR-In-Sync-Replicas-와-Producedr-acks"><a href="#ISR-In-Sync-Replicas-와-Producedr-acks" class="headerlink" title="ISR(In-Sync Replicas)와 Producedr acks"></a><strong>ISR(In-Sync Replicas)와 Producedr acks</strong></h4><ul><li>Replication을 위해서는 파티션별로 Zookeeper가 Leader와 Follower 역할을 할당한다.</li><li>Leader와 Follower는 ISR이라는 그룹으로 묶는다.</li><li>각 각의 파티션의 Leader들은 Follower들이 offset을 잘 맞춰서 따라오고 있는지 모니터링하고, 잘 못 따라오는 Follower가 있다면, ISR 그룹내에서 추방한다.</li><li>Follower들은 Leader가 가진 데이터와 자신의 데이터를 일치시키기 위해서 지속적으로 리더로부터 데이터를 땡겨온다.</li><li>Leader가 불능상태가 되면, ISR 그룹 내의 Follower 중 하나를 Leader로 선출한다.</li></ul></li></ul></li><li><h3 id="Consumer-amp-consmer-group-메시지를-가져오는-에이전트들의-집합"><a href="#Consumer-amp-consmer-group-메시지를-가져오는-에이전트들의-집합" class="headerlink" title="Consumer &amp; consmer group(메시지를 가져오는 에이전트들의 집합)"></a><strong>Consumer &amp; consmer group(메시지를 가져오는 에이전트들의 집합)</strong></h3><div align="center">  <img src="/images/post_images/220615_consumer_structure.jpg" alt="Kafka의 Consumer 구조 및 세부 개념"></div><ul><li><p>Consumer Group으로 나누는 이유는 그룹내의 Consumer들끼리 서로 중복된 데이터를 땡겨오지 않게 하기 위해서이다.</p></li><li><p>Consumer는 특정 Consumer Group에 반드시 속한다.</p></li><li><p>각 consumer들은 토픽의 파티션에 1:1로 매핑되어 메시지를 땡겨온다. </p></li><li><p>Consumer Group은 고유 group-id를 가지고 있고, 이를 통해 그룹을 구분하고, offset등의 정보를 관리한다.</p></li><li><p>Consumer를 띄울때 Consumer Group을 지정할 수 있다.</p></li><li><p>consumer group의 group id를 group1으로 띄우게 되면, 이 group1에 속한 Consumer들이 모든 데이터를 땡겨오고 나서 다음 offset부터 데이터를 땡겨올 수 있다. (<code>group-id: group1로 지정한 경우</code>)<br>하지만 <code>데이터의 처음 offset부터 데이터를 다시 땡겨오고 싶은 경우에는 어떻게 해야될까?</code> 방법은 group-id를 group1이 아닌 다른 이름의 consumer group(다른 이름)으로 해서 별도의 Consumer를 그룹화하면 된다. 그러면 새로 생성한 group-id: x는 offset을 처음부터해서 데이터를 읽어온다.<br>따라서 <code>지정한 group-id별로 offset이 관리가 된다는 것</code>을 이해할 수 있었다.</p></li><li><p>각 consumer group에 속한 consumer들은 메시지를 가져오고, 그 메시지에 해당하는 offset을 commit하게 되는데, 메시지를 어디까지 가져왔는지 Broker에 표시하기 위한 용도로 사용한다.</p></li><li><p>offset 값을 통해서 데이터를 생산해내는 쪽(producer)와 소비하는 쪽(consumer)간의 속도차이도 파악할 수 있다. 만약 producer에서 100의 데이터를 보내고 consumer에서 50만큼 소비했다면, 이 속도차이 50을 <code>Lag(데이터의 생산과 소비 속도차이)-offset 번호 차이</code>이라고 한다.<br>이를 통해 producer와 consumer의 갯수를 조정한다.</p></li></ul></li><li><h3 id="Consumer-장애-발생시-구동-방식"><a href="#Consumer-장애-발생시-구동-방식" class="headerlink" title="Consumer 장애 발생시 구동 방식"></a><strong>Consumer 장애 발생시 구동 방식</strong></h3><ul><li>Consumer group내에서 특정 Consumer에 장애가 발생하면, re-balancing이 일어나서 consumer 그룹 내의 다른 consumer가 장애 발생한 consumer의 몫까지 데이터를 소비한다. 이후에 장애가 발생한 consumer가 복구가 다 되면, 다시 Partition과 연결되어 데이터를 이어받는다.</li><li>데이터가 많아져서 파티션을 늘린 경우에는 다수의 Partition을 하나의 Consumer에 물릴 수 있다. 단, 하나의 파티션에 여러개의 Consumer가 붙을 수는 없다.</li></ul></li><li><h3 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a><strong>Producer</strong></h3><p>Kafka broker에 메시지를 send/publish/produce하는 인스턴스(에이전트)이다.</p><p>Producer관점에서는 Acks 옵션 [0, 1, -1(all)]이 매우 중요하다. 이전에 ISR 그룹 내에서의 작동원리에 대해서 배웠는데, Producer의 입장에서는 아래와 같이 acks의 옵션으로 다르게 할 수 있다.</p><p><code>acks=0</code>으로 설정을 하면, 안정성은 낮은 대신에 속도가 빠르다. 그 이유는 프로듀서가 리더 파티션에 메시지를 전송하고 나서 리더 파티션으로부터 별도의 acks(확인)을 하지 않는 방식이다. 재전송이나 offset을 신경쓰지 않기 때문에 별로 중요하지 않은 로그 데이터를 보낼때 사용되는 옵션이다.<br><code>acks=1</code>은 안정성과 속도가 중간으로, 프로듀서가 리더 파티션에 메시지를 전송하고, 리더로부터 ack를 기다린다. 하지만 팔로워들에게까지 잘 전송되었는지에 대해서는 신경쓰지 않는다. 리더들에게는 ack를 받았지만, 팔로워들에게 데이터를 복제하는 도중에 리더가 죽으면 fail이 발생한다.<br><code>acks=-1(all)</code>은 안정성이 높은 대신에 속도가 느리다는 특징을 가진다. 프로듀서가 전송한 메시지가 리더와 팔로워 모두에게 잘 저장이 되었는지 확인을 전부하기 때문에 속도가 느릴 수는 있지만 안정적이다. ISR에 포함된 모든 파티션에 전달이 되었는지 확인하는 것이 아닌 <code>min.insync.replicas</code>에 값을 별도로 지정해서 몇 번째 팔로워까지 복제본 저장이 되었는지 확인을 해야하는지 지정할 수 있다.<br>(<code>min.insync.replicas 값이 2이면 리더(1), 팔로워(1)까지 확인을 하고, 값이 3이면, 리더(1), 팔로워(2)까지 확인을 한다.</code>)</p></li></ul><h2 id="Amazon-MSK-Managed-Streaming-for-Kafka"><a href="#Amazon-MSK-Managed-Streaming-for-Kafka" class="headerlink" title="Amazon MSK(Managed Streaming for Kafka)"></a><ins><b>Amazon MSK(Managed Streaming for Kafka)</b></ins></h2><p>  이전에 완전 관리형 Kafka 서비스라고 들어본적이 있는데, 마침 오프라인 수업에서 다뤄주셔서 너무 좋았다.<br>  Amazon MSK는 Zookeeper의 설치 없이 손쉽게 Kafka cluster를 관리하고 Connector 연동하는 것이 가능하다.</p><p>  Kafka는 Connector가 중요한데, 앞/뒤로 데이터를 넣고 빼는 부분을 MKS에서는 플러그인을 설치해서 연결시켜줄 수 있다.</p><p>  아직 한국에서는 서비스를 하고 있지 않지만, Amazon MSK Serverless라고 하는 별도의 용량관리 없이 자동으로 프로비저닝해주고, 순수 Kafka의 기능 API만 사용하도록 지원하고 있다.<br>  (<code>Amazon Kinesis가 Amazon MSK Serverless와 같이 Kafka의 기능을 별도의 프로비저닝 없이 기능만 사용하고 사용한 만큼 요금만 납부하는 형태로 이용할 수 있도록 해주는 서비스이다</code>)  </p>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Data-Pipeline/">Data-Pipeline</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Data-Pipeline/">Data-Pipeline</category>
      
      <category domain="https://leehyungi0622.github.io/tags/AWS/">AWS</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/12/202206/220612_datapipeline_study/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>220611 Kubernetes 스터디 3일차</title>
      <link>https://leehyungi0622.github.io/2022/06/11/202206/220611_kubernetes_study/</link>
      <guid>https://leehyungi0622.github.io/2022/06/11/202206/220611_kubernetes_study/</guid>
      <pubDate>Sat, 11 Jun 2022 02:13:00 GMT</pubDate>
      
      <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;/images/post_images/220606_kubernetes.png&quot; alt=&quot;Kubernetes&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p&gt;이번 포스팅에는 쿠버네티스 클러스터에서 서비스와 관련된 이론내용을 정리하고, 실습한 내용에 대해서 정리하려고 한다.&lt;br&gt;저번시간에는 실습을 위한 쿠버네티스 클러스터 환경을 구축하고, Container, Label, NodeSchedule에 대한 부분을 학습하고 실습을 해보았다.&lt;br&gt;&lt;code&gt;컨테이너는&lt;/code&gt; 쿼버네티스 클러스터 내의 namespace 상에 존재하는 pod의 내부에 복수로 존재하며, 각 각의 컨테이너들은 복수 개의 IP 할당이 가능하지만, pod 내에서는 IP 충돌로 인해 동일한 IP의 할당이 불가하다는 것을 이론과 실습을 통해 이해했다.&lt;br&gt;&lt;code&gt;Label&lt;/code&gt;은 각 각의 용도에 따라 Pod에 명기된 label로 Pod들을 묶어서 하나의 서비스를 생성하거나, 또 다른 pod 집합으로 생성하기 위해 사용된다는 것을 이론과 실습을 통해 이해하였다.&lt;br&gt;마지막으로 &lt;code&gt;NodeSchedule&lt;/code&gt; 파트에서는 각 각의 Pod들은 Cluster에 연결된 노드들에 할당이 되어야 하는데, 할당하는 방법에 대해서는 직접 Pod를 Node에 할당하는 방법과 스케줄러에 의해 각 각의 노드가 가지고 있는 여유 메모리 공간에 대한 확인으로, 자동 할당되는 방법에 대해서도 이론과 실습을 통해 이해하였다. &lt;/p&gt;
&lt;p&gt;아직 개괄적으로 쿠버네티스에 있는 각 각의 요소들의 개념과 원리에 대해서 이해하는 과정이기 때문에 전체적인 그림 안에서 각 각의 요소가 어떤 기능을 하는지에 대해서 이해하면서 학습을 하고 있다.&lt;br&gt;엇그제 Label을 통해서 용도에 따라 pod(들)을 묶어서 하나의 Service로 만들어서 다른 운영자에게 IP와 Port를 제공한다고 배웠다. (&lt;code&gt;웹 개발자가 웹 화면만 확인하고자 할때 type이 web인 pod들만 service에 연결시켜서 웹 개발자들에게 제공 / 외부에서 원하는 service의 pod들에만 접그하고자 할 때 label이 production으로 지정된 pod들을 묶어서 하나의 service로 생성하고 제공&lt;/code&gt;)&lt;br&gt;이 Service에 대해서 이론적으로 학습을 하고 직접 실습을 해 볼 것이다. &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<div align="center">  <img src="/images/post_images/220606_kubernetes.png" alt="Kubernetes"></div><br/><br/><p>이번 포스팅에는 쿠버네티스 클러스터에서 서비스와 관련된 이론내용을 정리하고, 실습한 내용에 대해서 정리하려고 한다.<br>저번시간에는 실습을 위한 쿠버네티스 클러스터 환경을 구축하고, Container, Label, NodeSchedule에 대한 부분을 학습하고 실습을 해보았다.<br><code>컨테이너는</code> 쿼버네티스 클러스터 내의 namespace 상에 존재하는 pod의 내부에 복수로 존재하며, 각 각의 컨테이너들은 복수 개의 IP 할당이 가능하지만, pod 내에서는 IP 충돌로 인해 동일한 IP의 할당이 불가하다는 것을 이론과 실습을 통해 이해했다.<br><code>Label</code>은 각 각의 용도에 따라 Pod에 명기된 label로 Pod들을 묶어서 하나의 서비스를 생성하거나, 또 다른 pod 집합으로 생성하기 위해 사용된다는 것을 이론과 실습을 통해 이해하였다.<br>마지막으로 <code>NodeSchedule</code> 파트에서는 각 각의 Pod들은 Cluster에 연결된 노드들에 할당이 되어야 하는데, 할당하는 방법에 대해서는 직접 Pod를 Node에 할당하는 방법과 스케줄러에 의해 각 각의 노드가 가지고 있는 여유 메모리 공간에 대한 확인으로, 자동 할당되는 방법에 대해서도 이론과 실습을 통해 이해하였다. </p><p>아직 개괄적으로 쿠버네티스에 있는 각 각의 요소들의 개념과 원리에 대해서 이해하는 과정이기 때문에 전체적인 그림 안에서 각 각의 요소가 어떤 기능을 하는지에 대해서 이해하면서 학습을 하고 있다.<br>엇그제 Label을 통해서 용도에 따라 pod(들)을 묶어서 하나의 Service로 만들어서 다른 운영자에게 IP와 Port를 제공한다고 배웠다. (<code>웹 개발자가 웹 화면만 확인하고자 할때 type이 web인 pod들만 service에 연결시켜서 웹 개발자들에게 제공 / 외부에서 원하는 service의 pod들에만 접그하고자 할 때 label이 production으로 지정된 pod들을 묶어서 하나의 service로 생성하고 제공</code>)<br>이 Service에 대해서 이론적으로 학습을 하고 직접 실습을 해 볼 것이다. </p><a id="more"></a><h2 id="Service-in-k8s"><a href="#Service-in-k8s" class="headerlink" title="Service in k8s"></a><ins><b>Service in k8s</b></ins></h2><p>  쿠버네티스의 Service의 종류로는 크게 기본적으로 <code>ClusterIP, NodePort, Load Balancer</code>가 있으며, 순서대로 ClusterIP Service의 성격을 NodePort Service가 상속하고, NodePort Service의 성격을 Load Balancer가 상속을 한다. </p><ul><li><h3 id="ClusterIP-Service"><a href="#ClusterIP-Service" class="headerlink" title="[ClusterIP Service]"></a><strong>[ClusterIP Service]</strong></h3><p>ClusterIP는 가장 기본적인 Service의 형태로, 실제 서비스를 생성할 때 spec &gt; type의 값은 option인데, default가 ClusterIP이다. </p><p><code>service_sample.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">svc-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span> <span class="comment"># 이 부분이 Pod와 Service를 연결시키기 위한 부분이다.</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">pod</span> <span class="comment"># 연결하고자 하는 Pod의 이름</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9000</span> <span class="comment"># 9000번 포트로 들어오면, </span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8080</span> <span class="comment"># 8080번 포트로 연결</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span>   <span class="comment"># 이 부분은 optional한 부분이며, 기본값은 ClusterIP이다. </span></span><br></pre></td></tr></table></figure><p><code>pod_sample.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-1</span></span><br><span class="line">  <span class="attr">labels:</span> <span class="comment"># 이 부분이 Service와 연동시키기 위한 부분이다.</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">pod</span> <span class="comment"># 이 부분이 Service와 연동시키기 위한 부분이다.</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">tmkube/app</span></span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure><p>ClusterIP의 구성을 한 번 이미지 트레이닝 해보자.</p><p>우선 클러스터 내에 IP와 Port가 할당된 서비스 하나와 Pod이 두 개 있다고 가정한다.<br>Service에는 기본적으로 자신의 클러스터 IP를 가지고 있으며, 외부로부터 접근이 불가하다.<br>(<code>서비스와 Pod에 할당된 IP와 Port는 클러스터 내부에서 접근할 때만 사용</code>)<br>각 각의 Pod들에도 IP와 Port가 할당되어 있기 때문에 굳이 Service를 연결하지 않고, Pod의 IP와 Port로 접근하면 되지 않나? 라고 생각이 들겠지만, pod의 특징을 이해하면 왜 Pode들을 Service와 연동시켜서 Service를 통해서 Pod들에 접근을 하는지 이해할 수 있다.<br>그 이유는 <code>Pod</code>는 성능상에 문제가 생기면, Pod를 재성성하고 Pod에 할당되었던 IP를 재할당하게 된다. 따라서 Pod에 할당된 IP는 클러스터 내에서 접근이 가능한 수단이 될 수는 있지만, 신뢰성이 매우 낮다.<br>따라서 사용자가 지우지 않으면 사라지지 않는 Service라는 친구를 두고, 그 하위에 관리하고자 하는 Pod들을 연결시켜서 관리하게 된다. 서비스는 트래픽을 분산해서 pod에 전달하는 역할을 하며, <code>종류도 다양한데, Pod에 접근을 도와주는 방식이 다 다르다.</code></p><ul><li><h4 id="ClusterIP-Service의-사용-케이스"><a href="#ClusterIP-Service의-사용-케이스" class="headerlink" title="[ClusterIP Service의 사용 케이스]"></a><strong>[ClusterIP Service의 사용 케이스]</strong></h4><p>앞서 이미 살펴봤듯이 ClusterIP는 Service에 할당된 IP로, 외부에서 접근이 불가능하다. 따라서 ClusterIP Service를 사용하는 경우는 외부의 접근이 불가능하고, 오직 내부에서만 클러스터에 접근하는 경우에 사용 가능하다.<br>예를들면, 클러스터 내부에 접근이 가능한 운영자와 같은 인가된 사람이 클러스터의 dashboard를 관리하거나, 각 pod들의 서비스 상태를 직접 디버깅하는 업무를 하는 경우에 ClusterIP Servicer를 생성해서 사용할 수 있다.</p></li></ul></li><li><h3 id="NodePort-Service"><a href="#NodePort-Service" class="headerlink" title="[NodePort Service]"></a><strong>[NodePort Service]</strong></h3><p>NodePort Service는 용어에서 알 수 있듯이 Node의 Port와 관련있는 Service이다. 앞서 NodePort Service가 ClusterIP Service의 성격을 상속한다고 했는데, 클러스터 내의 구조를 살펴보면, Service에 복수 개의 Pod들이 물려있는 구조로 되어있다. 이전 시간에 NodeSchedule에 대해서 학습을 했을 때 각 각의 Pod는 Node에 할당이 되어야 한다. 따라서 각 각의 Pod들은 특정 Node에 종속이 되어있는데, 이름에서 알 수 있듯이 모든 Node들에 동일한 Port가 할당이 되어있다.</p><p>따라서 외부로부터 특정 노드의 IP와 동일하게 할당된 Port로 접근을 하게 되면, 어느 노드에서 접근을 했던 간에 각 각의 Pod들이 물려있는 Service로 연결이 가능하며, Service에 연결된 Pod에 트래픽이 전달된다.<br>단, 만약에 특정 노드의 IP와 Port를 통해 서비스에 연결했을 때, 연결할 때 접근했던 노드를 제외한 다른 노드에 포함된 Pod들에는 Service에서 트래픽을 보낼 수 없게 제한을 할 수 있는데, 아래와 같이 yml파일에서 <code>externalTrafficPolicy: Local</code>을 추가해주면 된다.</p><p><code>nodeport.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">svc-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">pod</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9000</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">nodePort:</span> <span class="number">30000</span> <span class="comment"># 30000~32767 범위내의 Port</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span> <span class="comment"># 옵션! 생략이 가능</span></span><br></pre></td></tr></table></figure><ul><li><h4 id="NodePort-Service의-사용-케이스"><a href="#NodePort-Service의-사용-케이스" class="headerlink" title="[NodePort Service의 사용 케이스]"></a><strong>[NodePort Service의 사용 케이스]</strong></h4><p>NodePort는 물리적인 호스트(Node)의 IP를 통해서 Pod에 접근이 가능하다.<br>대부분의 호스트 IP는 보안적으로 내부망에서만 접근이 가능하도록 네트워크가 구성이 되어있으며, 종종 내부 시스템을 개발하고 외부에서 간단하게 데모를 할 때 네트워크 중계기에 포트 포워딩을 해서 외부에서 내부 시스템으로 연동해서 일시적으로 외부 연동을 해서 사용할 때 사용된다.</p></li></ul></li><li><h3 id="Load-Balancer-Service"><a href="#Load-Balancer-Service" class="headerlink" title="[Load Balancer Service]"></a><strong>[Load Balancer Service]</strong></h3><p>Load Balancer Service는 NodePort Service의 성격을 상속받고, Node Balancer의 구조를 그대로 가진 상태에서 <code>Load Balancer</code>가 추가된 형태로 되어있다. Load Balancer는 각 각의 노드에 트래픽을 분산시켜주는 역할을 한다.<br>단, 주의해야 될 점은 Load Balancer에 할당된 IP(외부에서 접근하기 위한 IP)는 개별적으로 k8s를 설치했을 때 기본적으로 생성되지 않는다.<br>Load Balancer의 IP를 생성하기 위해서는 Load Balancer 외부 IP지원 플러그인을 설치해야 한다. 예를들어 AWS의 k8s 플랫폼 관련 서비스를 사용하게 되면, 해당 서비스 내에는 자체적으로 Load Balancer 외부 IP 지원 플러그인이 설치가 되어있기 때문에 Load Balancer 서비스 생성시에 자동으로 외부에서 서비스로 접근이 가능한 IP를 할당해준다.</p><p><code>load_balancer.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">svc-3</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">pod</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9000</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8080</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span> <span class="comment"># 이렇게 타입만 추가해주면, LoadBalancer service가 생성이 된다.</span></span><br></pre></td></tr></table></figure><ul><li><h4 id="LoadBalancer-Service의-사용-케이스"><a href="#LoadBalancer-Service의-사용-케이스" class="headerlink" title="[LoadBalancer Service의 사용 케이스]"></a><strong>[LoadBalancer Service의 사용 케이스]</strong></h4><p>Load Balancer 서비스는 실제로 외부에 서비스를 노출시킬 때 사용된다. Load Balancer를 사용하게 되면, 내부 IP가 노출되지 않고, 외부 IP를 사용해서 안정적으로 외부로 서비스를 노출 시킬 수 있다.</p></li></ul></li></ul>]]></content:encoded>
      
      
      <category domain="https://leehyungi0622.github.io/categories/Docker-K8s/">Docker&amp;K8s</category>
      
      
      <category domain="https://leehyungi0622.github.io/tags/Docker/">Docker</category>
      
      <category domain="https://leehyungi0622.github.io/tags/Kubernetes/">Kubernetes</category>
      
      
      <comments>https://leehyungi0622.github.io/2022/06/11/202206/220611_kubernetes_study/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
