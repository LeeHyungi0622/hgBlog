{"posts":[{"title":"210120 Python TIL - Type casting과 String관련 내장함수 그리고 String formatting","text":"Python오늘 배운내용 형변환(Type Casting) :변환하고자 하는 변수를 변환하고자하는 타입으로 감싸주면 형 변환이 된다. string list eval() :매개변수로 받은 문자열 값을 알맞은 형으로 변환 ex) eval(“2”) =&gt; 2 (integer type) 매개변수로 받은 수식을 문자열로 받아서 실행 ex) eval(“1+2”) =&gt; 3 외부입력(User input) input() :input의 인자로 문자열을 넣을 수 있다. 이는 사용자로부터 입력을 받을때 instruction역할을 한다. 입력받은 값은 기본적으로 str타입이다. 필요에 따라 입력과 동시에 형 변환을 할 수 있다. index를 활용하여 문자열 slicing 문자열의 index의 범위를 지정하여 slicing할 수 있다. ex) “Fastcampus”[4] =&gt; ‘c’ ex) “Fastcampus”[2:5+1] =&gt; ‘stca’ ex) “Fastcampus”[-2:-7:-2] =&gt; ‘umc’ ex)123target = 'Fastcampus'target[:1] #'F' 마지막 인덱스 -1 만큼의 범위까지 문자열을 자른다.target[2:] #두번째 인덱스부터 끝까지 문자열을 자른다. 내장함수를 활용 (string) 123sentence = 'Python is very easy, powerful and awesome LANGUAGE, I think.'word_list = ['F', 'a', 's', 't', 'c', 'a', 'm', 'p', 'u', 's']target = ' ....,,,,hello, python....,,fastcampus,, ' 위의 sentence 변수를 활용하여 아래 내장함수의 활용을 이해한다. count() : 12sentence.count('y') # 주어진 문자열 내에 문자 'y'가 몇 번 등장하는지 체크 (3번)sentence.count('very easy') # 주어진 문자열내에 문자열 'very easy'가 몇 번 등장하는지 체크 (1번) find() : 1sentence.find('is') # 주어진 문자열내에서 'is'가 몇 번째 index부터 시작되는지 체크 (7번 인덱스부터 시작) join() : 123# 배열의 요소를 전부 붙여서 하나의 문자열로 표기하기 위해서 아래와 같이 작성한다.''.join(word_list) #Fastcampus 출력','.join('python') #'p,y,t,h,o,n' split() : 12to_be_list = ' '.join('Fastcampus')to_be_list.split(' ') #space를 기준으로 문자를 나눠서 리스트화 한다. ['F', 'a', 's', 't', 'c', 'a', 'm', 'p', 'u', 's'] replace() : 1sentence = sentence.replace('Python', 'Golang') # 주어진 문장 내에 있는 'Python' 문자를 'Golang'으로 교체한다. strip() : 123target.strip().strip('.').strip(',') #target 문자열에서 공백과 '.', ','를 제거한다. 문자여려 앞과 뒤에서는 제거가 되지만, 문자와 문자 사이사이에 있는 지정문자는 제거되지 않는다.#위의 코드를 아래와 같이 간략하게 작성할 수 있다.target.strip(' .,') isalpha() : 1'LeeHyungi'.isalpha() #대상 문자열이 알파벳으로 구성되어있는지 체크 isalnum() : 1'LeeHyungi'.isalnum() #대상 문자열이 알파벳과 숫자 혼합으로 구성되어있는지 체크 isdigit() : 1'2021'.isdigit() #대상 문자열이 숫자로만 구성되어있는지 확인 islower() : 1'leehyungi'.islower() #대상 문자열이 소문자로만 구성되어 있는지 확인 isupper() 1'LEEHYUNGI'.isupper() #대상문자열이 대문자로만 구성되어 있는지 확인 print()의 sep, end 속성활용 1print(target[:1], 'e', target[2:], sep = '', end='python') #첫번째 문자(target[0]) 다음에 문자 'e'를 붙여주고, 그 다음에 target 문자열의 두번째 index부터 끝까지 슬라이싱한 문자를 붙여준다. sep는 빈 공백으로 해서 붙이고, end= 속성을 이용해서 문자열 맨 마지막에 'python'을 붙여준다. String Formatting 예전 방식 :Python의 옛 방식으로 문자열을 formatting하게 되면 아래와 같다.12#C언어와 비슷하게 출력하는 문자열 내에 %s, %d 와 같은 데이터 타입를 %와 같이 붙여서 지정해주고, %('value')를 넣어서 작성해준다.'I have a %s, I have a %s.'%('apple', 'pen') 새로운 방식 :Python의 String formatting을 새로운 방식으로 .format()를 사용해서 작성할 수 있다.123'I have a {}, I have a {}'.format('pen','apple') # 순차 formatting#위에서는 format에 넣어준 인자의 순서대로 값을 대입하는 방법으로 작성하였다. 다음은 인덱스를 지정해서 대입하는 방법이다.'I have a {1}, I have a {0}'.format('pen',10) 123456789# &gt;, &lt; , ^ (왼쪽에 space에 채워줄 문자를 작성해주고, 오른쪽에는 문자열의 최대길이를 설정해준다.)# &gt;, &lt; 각각 부등호 반대방향으로 문자열이 정렬되며, ^는 가운데 정렬을 한다. print('I have a {1:_&gt;10}, I have a {0}'.format('pen','apple'))print('I have a {1:*^10}, I have a {0}'.format('pen','apple'))print('I have a {1:-&lt;10}, I have a {0}'.format('pen','pineapple'))# 출력내용# I have a _____apple, I have a pen# I have a **apple***, I have a pen# I have a pineapple-, I have a pen 간단한 연습문제 오늘 배운 전반적인 내용을 활용하여 아래 문제를 해결한다. 사용자가 입력한 전화번호를 저장하려합니다. 전화번호의 구분자로 - 또는 를 혼합하여 사용할 때 이를 split과 join을 이용하여-로 통일하여 입력 받아 결과물을 출력하세요. 사용자가 다음과 같은 입력을 할때, 그 자료가 알파벳 단독인지, 숫자 단독인지, 알파벳 숫자인지, 소문자인지, 대문자인지를 formating 하여 출력하세요. 12345FASTcampus12FastCampus1107dynamite 문제풀이123456789101112131415161718&quot;&quot;&quot;campus12FastCampus1107dynamite&quot;&quot;&quot;user_input = input(&quot;Type some words: &quot;)&quot;{} is {},{},{},{},{}&quot;.format( user_input, user_input.isalpha(), user_input.isdigit(), user_input.isalnum(), user_input.islower(), user_input.isupper())","link":"/2021/01/20/202101/210120-python_til/"},{"title":"210122 Hexo 사용법","text":"Hexo 사용법 Hexo에 파일을 수정/업데이트 한 뒤에 사용하는 명령어에 대해서 정리 hexo generate는 로컬 환경에서 Hexo 페이지의 변경된 내용을 확인할 때 사용되며, hexo deploy는 업데이트 된 내용을 Git에 최종적으로 deploy함으로써 Github pages에 변경사항을 적용한다. 123$ hexo clean &amp;&amp; hexo generate$ hexo clean &amp;&amp; hexo deploy 2021.02.11 updateHexo의 내용을 수정하고 generate한 뒤에 deploy하는 명령어를 좀 더 간단하게 적는 방법 123$ hexo generate &amp;&amp; hexo deploy# 아래의 명령어로 간단하게 generate 및 deploy를 할 수 있다.$ hexo g -d","link":"/2021/01/22/202101/210122-Hexo-blog/"},{"title":"210119 Python TIL - Python의 기본 자료형과 내장함수","text":"Python오늘 배운내용 Python의 기본 자료형 string list number bool int float Python의 내장함수 print() type() round() : 반올림하고자 하는 값과 소숫점이하 N번째 자리까지 수를 표시하기 위해서 인자 N을 넣어준다. Python의 숫자 자료형과 연산 사칙연산(+,-,*,/) // (몫 구하는 연산자), % (나머지 구하는 연산자) ** (지수 연산자) Python 변수의 데이터 타입검사 변수의 데이터 타입검사는 type() 내장함수를 사용하여 체크 할 수 있다. for-loop를 활용한 반복연산 for - in - range() 구문을 활용하여 일정 구간에 있는 숫자를 순회하며 처리할 수 있다. 간단한 연습문제오늘 배운 전반적인 내용을 활용하여 아래 문제를 해결한다. 반지름(r=10)을 선언한 뒤, 이를 이용하여 원의 지름, 둘레, 넓이, 구의 겉넓이, 부피를 각각 출력하는 파이썬 파일을 만들어보세요.(pi=3.1415) sample output r(반지름)의 길이가 10일때의 출력 sample 123456r = 10 ==&gt; print(&quot;r =&quot;, r)d = 20c = 62.830a = 314.15gnb = 1256.0000v = 4188.666666666667 문제풀이1234567891011121314151617# 반지름과 pi값을 선언한다.r = 10pi = 3.1415# d: 지름, c: 둘레, a: 넓이# gnb: 구의 넓이, v: 부피d = 2 * rc = 2 * pi * ra = pi * r ** 2gnb = 4 * pi * r ** 2v = 4 / 3 * pi * r ** 3print(&quot;r =&quot;, r)print(&quot;c =&quot;, round(c, 4))print(&quot;a =&quot;, a)print(&quot;gnb =&quot;, round(gnb, 2))print(&quot;v =&quot;, round(v,4))","link":"/2021/01/19/202101/210119-python_til/"},{"title":"210121 Python TIL - List &#x2F; List관련 내장함수 + Tuple &#x2F; Destructuring assignment","text":"Python오늘 배운내용 List (리스트) 리스트 작성예시 12fastcampus = [2.71828, 3.1415]zoo = ['rabbit','dog','cat'] 리스트관련 내장함수 1zoo = ['rabbit', 'dog', 'cat'] append()12zoo.append('owl')# ['rabbit', 'dog', 'cat', 'owl'] insert()12zoo.insert(2, 'horse')# ['rabbit', 'dog', 'horse' 'cat', 'owl'] remove() 12zoo.remove('dog')# ['rabbit', 'horse' 'cat', 'owl'] pop() 1234# 아무 인자도 주지 않는다면, 리스트의 가장 끝 요소가 제거된다.zoo.pop(2) # 리스트의 index는 0부터 시작하기 때문에, 3번째 요소가 제거된다.# 만약에 index는 알지 못하지만 값을 알고 있는 경우, index()를 활용하여 index를 찾을 수 있다.zoo.pop(zoo.index('cat')) remove(): pop()의 경우에는 index값으로 리스트의 값을 제거했지만, remove()의 경우에는 값으로 접근해서 값을 제거한다. 1zoo.remove('cat') sort() 123zoo.sort() #zoo리스트에 있는 요소들을 오름차순으로 정렬한다.#내림차순으로 정렬하기 위해서는 sort()함수의 속성으로 reverse=를 사용하면 된다.zoo.sort(reverse=True) reverse() 1zoo.reverse() # zoo리스트의 요소를 거꾸로 정렬한다. +, extend() :서로다른 두 개의 리스트를 접붙이기 위해서 사용된다. 1234numbers1 = [1, 2]numbers2 = [-1, -2]number1 + number2 # [1, 2, -1, -2]number1.extend([3, 4]) # [1, 2, 3, 4] 문자열 (not) in 리스트 1234zoo = ['rabbit', 'owl', 'horse', 'aligator']'dog' in zoo # False'owl' in zoo # True'dog' not in zoo # True Tuple (튜플) :immutable 자료형으로 리스트와 달리 기존에 선언한 데이터를 수정할 수 없다. 튜플은 index로 값에 접근할 수 있다. 12grade = (90, 0, 16, 40)grade[1] # 0 튜플 형태로 Destructuring Assignment를 할 수 있다. 12345678def split_hello(word): result = word.split() return result[0], result[1](greet, word) = split_hello('hello world')greet # 'hello'word # 'world' 튜플을 활용하여 두 변수를 swapping할 수 있다. 1(greet, word) = (word, greet) 튜플을 리스트로, 리스트는 튜플로 type casting이 가능하다. 간단한 연습문제Practice(1) Practice(1)list와 tuple을 이용하여 다음의 문제를 해결하세요. 변수 animals에 다음의 자료를 입력하세요. rabbit, cat, dog, aligator, tiger, lion animals에 새로운 동물을 3마리 추가하세요.(insert, append, extend 꼭 쓸것) animals를 알파벳 내림차순으로 정렬하세요. 정렬된 animals의 짝수(index 기준) 동물을 even_animals라는 tuple에 할당하세요. 123456789101112131415161718192021animals = ['rabbit', 'cat', 'dog', 'aligator', 'tiger', 'lion']animals.insert(3, 'kangaroo') # ['rabbit', 'cat', 'dog', 'kangaroo', 'aligator', 'tiger', 'lion']animals.append('puma') # ['rabbit', 'cat', 'dog', 'kangaroo', 'aligator', 'tiger', 'lion', 'puma']animals.extend(['cheetah'])&quot;&quot;&quot;['rabbit', 'cat', 'dog', 'kangaroo', 'aligator', 'tiger', 'lion', 'puma', 'cheetah']&quot;&quot;&quot;animals[::2] # 리스트에서 짝수번째 있는 동물들을 출력한다.animals[1::2] # 홀수번째 있는 동물들을 출력한다.even_animals = tuple(animals[::2])","link":"/2021/01/21/202101/210121-python_til/"},{"title":"210122 Git + Vim editor usage TIL","text":"Shell, Vim command or git오늘 배운내용 Shell command 기본 Linux shell command 활용 cd : 쉘의 현재 위치 이동 mv : 디렉토리, 파일 이동 cp : 파일 복사 rm : 디렉토리, 파일 삭제 mkdir : 디렉토리 생성 pwd : 쉘의 현재 위치 확인 ls (-a, -l option) : 쉘의 현 위치에 있는 디렉토리, 파일을 출력 (옵션을 이용해서 세부사항 출력, 리스트로 출력가능) touch : 새로운 파일을 생성 cat : 파일의 내용을 terminal 상에서 확인 head : 파일의 앞 부분의 내용을 terminal 상에서 확인 tail : 파일의 뒷 부분의 내용을 terminal 상에서 확인(option으로 -[N] N 번째 라인까지 확인할지 설정할 수 있다.) chmod : 파일권한 설정 1$ chmod [옵션] (8진수) (파일명) 1234567drwxr-xr-xd or -: directory or file(user)(group)(other)r: readw: writex: execute-: no permission Application 계층과 Kernel 계층 그리고 하드웨어 계층으로 분류하여 이해할 수 있다. 커널은 하드웨어와 Application 사이에 위치하여 하드웨어와 응용프로그램을 이어주는 운영체제의 핵심 소프트웨어이다. Shell의 정의에 대해 이해할 수 있다. - 커널과 사용자가 소통할 수 있도록 그 사이를 이어주는 소프트웨어이다. - 다양한 쉘이 존재 (sh, csh, bash, zsh) Vim command Vim editor를 이용한 commit message 작성 및 수정 git commit -m “”을 통한 commit message를 작성하는 것은 되도록 하지 말고 git commit 을 통해 vi editor를 사용해서 commit message를 작성하는 것이 좋다.123456789101112131415Vim 기본 사용키h,j,k,l - move cursori - insert modev - visual moded - deletey - yankp - pasteu - undor - replace$ - move end of line^ - move start of line:q - quit:q! - quit w/o write(no warning):wq - write and quit:{number} - move to {number}th line Git:Git은 VSC(Version Control System)으로, SCM(Source Code Management)라고도 한다. Git과 Github가 다름을 이해한다. Git은 VCS(Version Control System)로 tool 자체이며, Github는 이 Git이라는 tool을 활용하여 웹 기반으로 만든 웹 서비스이다. Git을 활용하여 소스코드를 관리한다. 전체적인 git flow를 이해한다.workspace에서 add를 하게 되면, stage(=index)에 Blob의 형태로 파일 정보가 업로드 된다. stage에 올라간 파일을 로컬 repository 에 업데이트 하기 위해서 commit을 하고, 최종적으로 remote repository에 저장하기 위해 push라는 과정을 거친다. git의 환경설정을 할 수 있다. 기본적으로 local 환경에서 git을 사용하기 전에 user.name, user.email, core.editor 를 설정해준다. 12345$ git config --global user.name &quot;{github username}&quot;$ git config --global user.email &quot;{github email address}&quot;$ git config --global core.editor &quot;vim&quot;$ git config --global core.pager &quot;cat&quot;$ git config --list core.editor로 등록한 vim을 활용하여 commit message를 작성할 수 있다. Commit Convention commit제목은 50자 이내로 요약하여 작성한다. 제목과 내용사이에는 한 칸 띄어준다. prefix를 활용하여 commit의 용도를 한 눈에 알아볼 수 있도록 한다.123456789feat: featuresdocs: documentationsconf: configurationstest: testfix: bug-fixrefactor: refactoringci: Continuous Integrationbuild: Buildperf: Performance ex) Commit Convention - example1234feat: Create server.py to start flask projectdocs: Create README.mdconf: poetry inittest: User model CRUD test complete 기존에 remote repository를 local 환경에 clone해서 작업을 할 수 있다. 123$ git clone$ git clone [git repository url] ./[생성할 폴더 이름(옵션)]","link":"/2021/01/22/202101/210122-git_til/"},{"title":"210122 Python TIL - Dictionary와 Set의 활용","text":"오늘 배운 내용 Python 메인 강의 과제점검(1월 22일자 파이썬 과제 풀이/해설) Dictionary : Dictionary는 key와 value로 구성된 자료형으로, 간편하게 key값으로 value를 참조해서 가져올 수 있다. 1ganji = {} Dictionary 활용 Dictionary 데이터 초기화 하기 1234# 직접 데이터를 넣고 초기화 시키기ganji = {'ja':0, 'chuk':1} #{'ja': 0, 'chuk': 1}ganji = dict(ja=1, chuk=0)ganji = dict([('ja',1),('chuk',2)]) Key값으로 Value 데이터 업데이트하기 12# 기존의 dictionary에 데이터 추가하기ganji['in'] = 'tiger' # {'ja': 1, 'chuk': 0, 'in': 'tiger'} del 키워드로 Dictionary 내의 데이터 삭제하기 12345678# del 키워드는 값을 참조해서 삭제한다.del ganji['dragon']# del 키워드를 사용해서 선언되어있는 변수를 삭제할 수 있다.fast = 1del fast# ------ 이하에서는 fast 변수에 접근이 불가하다. (fast 변수는 메모릴상에서 제거된 상태)result = fast + 2print(result) .keys()로 key값 참조하기 12345ganji.keys() # dict_keys(['ja', 'chuk', 'in', 'myo'])# 위의 .keys()를 for-loop내에서 활용할 수 있다.for key in ganji.keys(): # 각 각의 value를 출력 print(ganji[key]) .values()로 value값 참조하기 1ganji.values() # dict_values(['rat', 'cow', 'tiger', 'rabbit']) .items()로 [(key, value)] 형태(리스트 내 튜플 자료형)로 값 참조하기 12ganji.items() # dict_items([('ja', 'rat'), ('chuk', 'cow'), ('in', 'tiger'), ('myo', 'rabbit')]) zip() 을 활용하여 튜플형태로 (key, value)로 묶어서 리스트에 담아 표현하기 12list(zip(ganji.keys(), ganji.values()))# output : [('ja', 'rat'), ('chuk', 'cow'), ('in', 'tiger'), ('myo', 'rabbit')] .pop([‘key’]) 을 활용하여 Dictionary 내 데이터 제거하기 1ganji.pop('ja') # 'rat' .popitem() 을 활용하여 Dictionary의 마지막 요소 제거하기 (list에서 .pop()과 같은 기능) 1ganji.popitem() # 리스트의 가장 마지막 요소를 제거한다. ('myo', 'rabbit') 12345678&quot;&quot;&quot;hong_midterm = {}hong_midterm['Korean'] = 70hong_midterm['Math'] = 60hong_midterm['English'] = 100hong_midterm['Social'] = [45,30,20]hong_midterm['Science'] = [45,45,40,10]&quot;&quot;&quot; .get(‘[‘key’]’, [default value] ) : get의 첫번째 인자로 Key값을 넣고, 존재하지 않을 경우 반환할 Default 값을 선언할 수 있다. 1hong_midterm.get('Ethics', False) .setDefault(‘Ethics’,[]) : Dictionary 변수에 Ethics 항목을 추가하고, 동시에 Default 값을 넣어 초기화 시켜 줄 수 있다. 1hong_midterm.setdefault('Ethics', []) 리스트만큼 잘 사용되지는 않지만, Dictionary도 len()을 사용해서 길이를 구할 수 있다. **를 활용하여 Dictionary 변수의 key값으르 Destructuring Assignment하기 12some_dict = {'name': 'Hong Gil-dong', 'locale': 'Seoul, KR'print(&quot;Hello, I'm {name}, from {locale}&quot;.format(**some_dict)) Set: set은 중복요소를 제거하고, 교집합(&, intersection), 합집합(|, union), 차집합(-, difference), 대칭차집합(symmetric difference)를 활용하여 두 집합변수의 관계형 데이터를 만들어낼 수 있다. 1ppap = ['pen', 'pineapple', 'apple', 'pen'] Set 활용하기 set과 list는 상호 type casting이 가능하다. 12345a_word = 'fastcampus'b_word = 'python'a_set = set(a_word) #{'u', 'm', 'p', 'a', 'c', 's', 'f', 't'}b_set = set(b_word) # {'y', 'p', 'h', 'n', 't', 'o'} 두 집합의 교집합 구하기 123a_set &amp; b_seta_set.intersection(b_set)# output : {'p', 't'} 두 집합의 합집합 구하기 123a_set | b_seta_set.union(b_set)# output : {'a', 'c', 'f', 'h', 'm', 'n', 'o', 'p', 's', 't', 'u', 'y'} 두 집합의 차집합 구하기 123a_set - b_seta_set.difference(b_set)# output : {'a', 'c', 'f', 'm', 's', 'u'} 두 집합의 대칭차 구하기 123a_set ^ b_seta_set.symmetric_difference(b_set# output : {'a', 'c', 'f', 'h', 'm', 'n', 'o', 's', 'u', 'y'} 간단한 연습문제 다음 인적사항을 입력한다. (list에 각 정보가 dict로 존재) Name Locale Age Username Johnny Silverhand Night City 120 Johnny John Doe CA, USA 40 Doh Jane Doe Seoul, Korea 24 jane-doe Foo Bar Busan, Korea 31 foo-bar 위에서 입력한 데이터를 활용하여 아래 문제를 해결하시오. 1-1. Jane Doe의 Locale을 London, UK 로 수정하세요.2-1. list의 모든 아이템에 대해 key와 value를 모두 출력하세요.(반복문 쓰지 말 것) 3. 다음 list의 중복 아이템을 제거한 뒤, 내림차순으로 정렬하여 출력하세요. (아래 cities 리스트 활용) 1234567cities = [ 'Daejeon', 'Ulsan', 'Seoul', 'Jeju', 'Busan', 'Ulsan', 'Daegu', 'Daejeon', 'Seoul', 'Seoul', 'Daejeon', 'Gwangju', 'Busan', 'Daegu', 'Gwangju', 'Daejeon', 'Ulsan', 'Jeju', 'Gwangju', 'Seoul'] 1번의 경우, user_info 리스트 변수를 선언하여, 각각의 열에 dict 타입의 데이터를 초기화해서 넣어준다. 12345678910111213141516171819202122232425user_info = []user_row = dict([('name','Foo Bar'), ('locale', 'Busan, Korea'), ('age', 31), ('username', 'foo-bar') ])user_info.append(user_row)user_info&quot;&quot;&quot;output :[{'name': 'Johnny Silverhand', 'locale': 'Night City', 'age': 120, 'username': 'Johnny'}, {'name': 'John Doe', 'locale': 'CA, USA', 'age': 40, 'username': 'Doh'}, {'name': 'Jane Doe', 'locale': 'Seoul, Korea', 'age': 24, 'username': 'jane-doe'}, {'name': 'Foo Bar', 'locale': 'Busan, Korea', 'age': 31, 'username': 'foo-bar'}]&quot;&quot;&quot; 1-1번의 경우, 각각의 행 요소에는 index로 접근을 하고, 접근한 후에는 dict 데이터에 key값으로 접근을 해서 해당 데이터를 수정해주면 된다. 1user_info[2]['locale'] = 'London, UK' 2-1번의 경우, 반복문을 사용하지 않고 각 행의 dict형 데이터들을 출력해야 되기 때문에 아래와 같이 각 행의 dict형 데이터에는 index로 접근을 하고, .items()로 key와 value를 Tuple 데이터형으로 묶어서 읽어온다. 12345678910111213print(list(user_info[0].items()))print(list(user_info[1].items()))print(list(user_info[2].items()))print(list(user_info[3].items()))&quot;&quot;&quot;output:[('name', 'Johnny Silverhand'), ('locale', 'Night City'), ('age', 120), ('username', 'Johnny')][('name', 'John Doe'), ('locale', 'CA, USA'), ('age', 40), ('username', 'Doh')][('name', 'Jane Doe'), ('locale', 'London, UK'), ('age', 24), ('username', 'jane-doe')][('name', 'Foo Bar'), ('locale', 'Busan, Korea'), ('age', 31), ('username', 'foo-bar')]&quot;&quot;&quot; 3번 문제의 경우, 중복제거는 set()을 사용해서 제거하고 내림차순으로 정렬하기 위해서 list()로 변환한 후에 sort() 메서드를 사용해서 정렬한다. 12unique_cities = list(set(cities))unique_cities.sort(reverse=True)","link":"/2021/01/22/202101/210122-python_til/"},{"title":"Pseudo code + Algorithm 연습노트 준비","text":"첫 주에 강사님께 배웠던 내용중에 가장 인상깊었던 부분이 Pseudo code를 작성하는 것이었다. 그리고 단순 코더가 아닌 컴퓨팅 사고를 할 줄 아는 그런 개발자가 되라는 말씀이 마음에 가장 와닿았다. [ 나의 첫 Pseudo code + Algorithm 연습노트 ] 그래서 첫주가 끝난는 21일 금요일날 집에 가는 길에 노트를 사서 Pseudo code와 알고리즘을 연습하기 위한 전용 노트를 만들었다. 그리고 인터넷에서 Pseudo code의 정의와 중요성 그리고 작성하는 방법에 대해서 간단하게 정리해보았다. 이렇게 글을 정리해놓는 이유는 이제부터 매일매일 적어도 하나의 알고리즘 문제를 풀면서 Pseudo code를 작성하는 연습도 같이 병행할 것이기 때문이다. 혹시나 훗날 힘들어질때 다시 지금 이때 작성한 글을 읽고 다시 마음을 다잡아야 겠다. [ Pseudo code의 중요성과 작성방법 ][ Pseudo code 작성 sample ]","link":"/2021/01/23/202101/210123-Algorithm_start/"},{"title":"Git Repository 이전하기","text":"오늘은 기존에 관리하던 Github 계정에서 필요한 Repository만 선별해서 새롭게 만든 계정으로 이전하는 작업을 하려고 한다. 이제 정말 필요한 Repository를 생성하고, 모든 Repository는 README 파일을 꼼꼼하게 적는 습관을 갖을 것이다. 새롭게 만든 Github 계정에서는 좀 더 꼼꼼하고 잘 관리된 개발자로서의 모습을 보여 줄 수 있도록 노력해야 겠다. 단순하게 기존의 코드만 이전을 하려면 이전하려는 Repository에 Push를 해주면 되지만, 나는 그동안 작성했던 커밋 로그까지 같이 옮기고 싶기 때문에 그 방법에 대해서 찾아보고 정리해보려고 한다. Repository 이삿짐 싸기1$ git clone --mirror {git Repository 주소} git clone 명령과 –mirror 옵션과 함께 복사하려는 repository 주소를 입력해준다. 위의 작업을 하게 되면 [Repository 이름].git 을 가진 디렉토리가 생성된 것을 확인 할 수 있다. 이 디렉토리의 이름을 .git 으로 변경을 해주도록 한다. 이사할 주소 지정하기이제 Repository의 정리가 되었으면 이제 이전할 새로운 Repository의 주소를 알려줘야 한다. 1$ git remote set-url origin {새로 이사갈 레파지토리 주소} Repository 이사하기1git push --mirror","link":"/2021/01/21/202101/210123-Git_repository/"},{"title":"Baekjoon Online Judge 2920번 음계 문제","text":"백준 저지 2920번 문제 Pseudo code + Python code Pseudo code와 본 코드의 작성은 모두 손코딩으로 하기로 했다. 손코딩을 하게 되면 좀 더 기억에 오래 남고, 현장 코딩테스트에서 화이트보드에 코드를 적으면서 코드를 설명할 기회가 있을 수도 있기 때문에 좋은 연습이 될 것 같다. 즉흥적으로 코드를 바로 작성하지 말고, 간단한 논리 연산 문제라도 우선 나의 생각을 간략하게 순차적으로 정리하고 논리적 오류가 있는지 점검한 다음에 코드를 작성하는 습관을 들이도록 하자.","link":"/2021/01/24/202101/210124-Algorithm_baekjoon_2920/"},{"title":"Ethiopian_multiplication","text":"Ethiopian Multiplication Pseudo code + Python code 12345678m, n = map(int, input(&quot;Enter two numbers separated by space&quot;).split(' '))result = 0while m &gt;= 1: if m % 2 != 0: result += n m //= 2 n *= 2print(result) Ethiopian multiplication 방식으로 입력받은 두 숫자의 곱으르 구하는 코드를 구현해보았다. 원리는 m과 n 두 숫자를 입력받고, 왼쪽의 숫자는 나눠지지 않을 때까지 계속 2로 나눠주며, 오른쪽 수는 왼쪽의 수가 나눠지지 않을때까지 2를 곱해준다. 만약에 m이 홀수인 경우, n의 값은 특정 변수에 누적해서 저장을 한다. 이 누적된 값이 최초 입력받은 m과 n의 곱의 값이 된다.","link":"/2021/01/25/202101/210125-Ethiopian_multiplication/"},{"title":"Baekjoon Online Judge 2798번 블랙잭 문제","text":"백준 저지 2798번 문제 Pseudo code + Python code 이 문제는 3중 for-loop을 활용하여 주어진 숫자 리스트 내의 숫자들을 3개 단위로 묶을 수 있는 모든 경우의 수를 고려하여 연산을 한다. 3개 단위로 묶인 합이 입력한 m의 값보다 크지 않으면서 가장 큰 수를 출력하도록 하는 문제이다.","link":"/2021/01/26/202101/210126-Algorithm_baekjoon_2798/"},{"title":"210125 Python TIL - 삼항연산자(Ternary operator)와 for-loop, while-loop","text":"Python오늘 배운 내용 Python 과제 점검 21년 1월 3주차 파이썬 과제 풀이 정리하기 Python 메인 강의 과제점검 삼항연산자 (Ternary operator) Iteration(Loop) for while 문자열 * True / 문자열 * False break, continue in conditional statement 21년 1월 3주차 파이썬 과제 풀이 정리하기기본적으로 문제풀이에서 요구하는 것은 isupper()와 islower()를 사용하여 주어진 문자열, 배열의 값을 소문자는 대문자로, 대문자는 소문자로 토글하는 문제였다. 아래는 내가 작성한 문제의 답으로, solution1에서는 isupper()와 islower()를 사용해서 입력된 값을 조건처리하여 작성하였다.solution2에서는 입력된 값의 조건처리를 알파벳의 ASCII 코드 값을 활용하여 접근하였다. solution1과 solution2에서 공통적으로 입력된 값이 문자열인지 리스트인지 구분하여, 문자열인 경우 문자열의 값을 리스트화하고, 출력을 할때에도 입력된 값이 문자열인 경우와 그렇지 않은 경우를 구분하여 출력하는 구문도 조건처리하였다. 12345678910111213141516171819202122232425262728293031323334353637383940# solution 1) isupper(), islower()를 활용# string이 list로 convert되었는지 구분하기 위한 flag변수flag = Falseiv = ['A', 'b', 'C']# iv = &quot;AbCdEf&quot;# 만약 입력한 값이 문자열인 경우, 리스트로 변환하고 flag변수를 True로 변환한다.if type(iv) == str: iv = [w for w in iv] flag = Truefor idx, w in enumerate(iv): if w.isupper(): iv[idx] = w.lower() else: iv[idx] = w.upper()if flag: print(''.join(iv))else: print(iv)# solution 2) ASCII CODE를 활용# string이 list로 convert되었는지 구분하기 위한 flag변수flag = Falseiv = ['A', 'b', 'C']# iv = &quot;AbCdEf&quot;# 만약 입력한 값이 문자열인 경우, 리스트로 변환하고 flag변수를 True로 변환한다.if type(iv) == str: iv = [w for w in iv] flag = Truefor idx, w in enumerate(iv): # w의 ASCII 코드가 65이상 90이하인 경우 (대문자의 경우) if ord(w) &gt;= 65 and ord(w) &lt;= 90: iv[idx] = chr(ord(w)+32) else: iv[idx] = chr(ord(w)-32)if flag: print(''.join(iv))else: print(iv) Python 메인 강의과제 점검(1) 기존의 Numguess 코드에 추가할 요소에 대해서 생각해보기 (2) Hacker Rank문제 풀이 (2문제) https://www.hackerrank.com/challenges/py-if-else/problem https://www.hackerrank.com/challenges/find-second-maximum-number-in-a-list/problem Hacker Rank 두번째 문제 풀이 참고 : 리스트(list)를 집합(set)으로 변환해서 중복을 제거하고 sort와 sort의 reverse= 속성을 활용해서 내림차순으로 정렬을 한다. 삼항연산자(Ternary operator) Conditional Expressions123# a if condition else bTrue if 3&gt;2 else FalseTrue if 3&lt;2 else False 123# a if condition else b if condition else c (다중 if문)# 2~3개의 조건은 삼항연산자로 작성해도 괜찮지만, 그 이상 작성을 하면, 가독성이 떨어진다.'a' if 3&lt;2 else 'b' if 4&gt;3 else 'c' 삼항연산자를 사용해서 직접적으로 변수의 값을 초기화 해 줄 수 없다.1result = 'pass' if 3&gt;2 else result = 'Non-pass' #Error Practice(1)내가 작성한 코드 (코드 점검하기)123456789101112131415161718day = input(&quot;요일을 입력하세요 : &quot;)# conditional statementday_list = ['월', '화', '수', '목', '금', '토', '일']if day in day_list[:5] or day.replace(&quot;요일&quot;, &quot;&quot;) in day_list[:5]: print(&quot;평일이네요ㅜㅜ&quot;)elif day in day_list[5:] or day.replace(&quot;요일&quot;, &quot;&quot;) in day_list[5:]: print(&quot;주말입니다^^&quot;)else: print(&quot;요일을 입력하세요&quot;)# conditional expressprint(&quot;평일이네요ㅜㅜ&quot;) if day in day_list[:5] or day.replace(&quot;요일&quot;, &quot;&quot;) in day_list[:5] else \\print(&quot;주말입니다 ^ ^&quot;) if day in day_list[5:] or day.replace(&quot;요일&quot;, &quot;&quot;) in day_list[5:] else \\print(&quot;요일을 입력하세요&quot;) 강사님이 작성하신 코드 솔루션12weekday = ('월','화','수','목','금')weekend = ('토','일') 튜플의 형태로 평일과 주말을 분류하고, 입력한 문자열이 각각의 튜플의 내부에 있는지 검사해서 출력한다. For123for 변수 in (리스트 or 문자열): 실행문1 ... 12for i in [&quot;python&quot;, &quot;java&quot;, &quot;golang&quot;]: print(i) 1234# 복수 변수는 변수이름을 명명할때 복수로 명사로 선언한다.animals = ['cat', 'dog', 'aligator', 'tiger', 'lion']for animal in animals: print('There is {} in cage.'.format(animal)) 1234567# 1부터 N까지의 합을 구하기# (수식 : n(n+1)/2) - O(1)# For-loop를 사용한 풀이 - O(N)# 가독성을 높이기 위해 101이 아닌 100+1의 형태로 숫자를 작성해준다.for i in range(1, 100+1): result += iprint(result) 시간복잡도 이해 만약에 위의 문제를 for-loop를 사용해서 해결하지 않고, n(n+1)/2라는 수식으로만 해결을 한다면, 빅오 표기법으로 O(1)의 시작 복잡도를 갖는다. 반면에 for-loop를 사용해서 해결을 한다면, N번만큼 반복해야되므로 빅오 표기법으로 O(N)만큼의 시간복잡도를 갖는다. While123while 조건: 실행문1 ... 12345while True: message = input('Type anything: ') if message == 'exit': break print(message) 123456789101112import randommagic_number = random.randint(1,20)print(&quot;Stop at &quot;, magic_number)for i in range(1, 100+1): if i == magic_number: break elif i%2==0: print('skip for now') # continue는 다음 index에 대해서 새로운 cycle을 시작하라는 의미이다. continue print(i) continue는 현재 실행한 cycle을 끝내고 다음 cycle로 시작하고, break는 구문(for-loop)을 끝낸다. Practice(2)내가 작성한 코드1번 문제) 12345num = int(input())result = 1while num &gt; 0: result *= num num -= 1 2번 문제) 123456num_list = [[1, 0, 7, 0], [0, 2, 0, 9], [0, 6, 3, 7], [4, 0, 5, 1],]sum = 0for i in range(len(num_list)): for j in range(len(num_list[i])): sum += num_list[i][j]print(sum) 강사님이 작성하신 코드 솔루션123456user_num = int(input('Enter just one number(2-100): '))print(user_num, type(user_num))result = 1for i in range(1, user_num+1): result *= iprint(result) 강사님이 올려주신 파일보고, 코드 다시 점검하기 12345678910111213141516171819202122lists_in_list = [ [1, 0, 7, 0], [0, 2, 0, 9], [0, 6, 3, 7], [4, 0, 5, 1],]final_result = 0for list_ in lists_in_list: result = 0 for item in list_: result += item final_result += resultprint(final_result)# 출력값# 8# 11# 16# 10# 45 123456789# 위의 코드를 Refactoring하기for list_ in lists_in_list: # sum함수를 사용함으로써 for-loop를 하나 생략할 수 있다. print(sum(list_))# [1, 0, 7, 0]# [0, 2, 0, 9]# [0, 6, 3, 7]# [4, 0, 5, 1] 123456result = 0for list_ in lists_in_list: result += sum(list_)print(result)# 45 Fizzbuzz 문제내가 작성한 코드 123456789for i in range(1, 100+1): if i % 15 == 0: print(&quot;FizzBuzz&quot;) elif i % 3 == 0: print(&quot;Fizz&quot;) elif i % 5 == 0: print(&quot;Buzz&quot;) else: print(i) 강사님이 작성하신 코드 솔루션Python에서는 True*”string” = string이고, False*”string” = ‘’이다. bool 값을 문자열에 곱해서 해당 문자열을 표시할 수도 표시를 안할 수도 있다. 연산속도를 생각하면, 연산을 통한 값 출력이 더 빠르다는 것을 알 수 있다. 12345for i in range(1, 30+1): if i%3==0 or i%5==0: print('fizz'*(i%3==0) + 'buzz'*(i%5==0)) else: print(i) Numguess123456789101112131415161718192021 import randomanswer = random.randint(1, 100)print('DEBUG: {}'.format(answer))chance = int(input('Choose level(10-easy 1-hell): '))while chance &gt; 0: guess = int(input('Guess the number(1-100): ')) print(guess, type(guess)) if guess-answer == 0: print('Correct') break elif guess &lt; answer: msg = 'Up!' elif guess &gt; answer: msg = 'Down!' chance -= 1 print('{} You have {} life(s).'.format(msg, chance)) if chance == 0: print('Wrong! The answer was {}'.format())","link":"/2021/01/25/202101/210125-python_til/"},{"title":"Monty Hall Problem","text":"Monty Hall Problem (Pseudo code + Python code) 앞서 작성한 Pseudo code와 손코딩 한 부분의 코드상에 논리적 문제를 발견했다. 만약 사용자가 선택한 문과 당첨 문이 일치하지 않는 경우, 진행자는 당첨문과 사용자가 선택한 문을 제외한 범위 내의 문을 열어서 보여줘야 하기 때문에 이 경우에는 당첨 문과 참가자가 선택한 문 둘 다 제거를 해줘야 한다. 만약 사용자가 선택한 문과 당첨 문이 같은 경우에는 사용자가 선택한 문을 제외한 두 개의 문 중에서 하나를 선택해서 문을 열어서 보여주면 된다. 12345678910111213141516171819202122232425262728293031323334353637import randomNUM_OF_SIMULATION = 10000num_of_win = 0num_of_condition = int(input( &quot;Enter the number of condition(1: Keep first choice, 2: Change first choice)&quot;))for _ in range(NUM_OF_SIMULATION): car_door = random.randint(1, 3) participant_select_door = random.randint(1, 3) door = [i for i in range(1, 3+1)] # 당첨되는 문을 door 리스트에서 제거한다. if participant_select_door != car_door: door.remove(car_door) door.remove(participant_select_door) else: door.remove(car_door) open_door = random.choice(door) if num_of_condition == 1: if participant_select_door == car_door: num_of_win += 1 elif num_of_condition == 2: # 참가자가 변심으로 처음 선택한 문을 바꾸는 경우, door 리스트를 다시 초기화한다. new_door = [1, 2, 3] new_door.remove(open_door) new_door.remove(participant_select_door) new_select_door = new_door[0] if new_select_door == car_door: num_of_win += 1print(f&quot;{(1.0 * num_of_win)/NUM_OF_SIMULATION * 100}%의 확률로 당첨될 수 있습니다.&quot;)&quot;&quot;&quot;위의 코드로 시뮬레이션을 해 본 결과 :게임 참가자가 처음 선택한 문을 바꾸지 않을 경우(Condition1)에는당첨될 확률은 32.48%(오차범위 ±2%)이고,처음 선택한 문을 바꾸는 경우(Condition2)에는 66.24%(오차범위±2%)의 확률로 당첨이 될 수 있습니다.따라서 처음 선택한 문을 바꾸는 것이 당첨될 확률이 더 높습니다.&quot;&quot;&quot; 이 Monty Hall 문제에 대해서는 처음 들어봤지만, 이전에 21이라는 영화를 보면서 교수가 학생에게 확률과 관련한 질문을 하였던 것을 기억하는데 이 문제가 바로 Monty Hall문제였다. 이 영화에서 질문을 받은 학생은 “한 번 선택을 하면 33%의 확률이지만, 바꿀 수 있는 한 번의 기회를 더 받았으므로 당첨될 확률이 두 배가 된다”고 대답했던 것 같다. 이 문제가 Monty Hall 문제라는 것은 처음 알고 재미있게 Pseudo code도 적고 손코딩으로 코드도 작성해보았다.","link":"/2021/01/26/202101/210126-monty_hall/"},{"title":"Baekjoon Online Judge 5397번 키로거 문제","text":"백준 저지 5397번 키로거 문제 Pseudo code + Python code 위에서 손코딩한 코드에서는 left_stack과 right_stack을 for문 외부에서 초기화 해주었지만, 이렇게 해주면 매 테스트 케이스마다 초기화된 스택을 사용할 수 없기 때문에 for-loop 내에서 left_stack과 right_stack을 초기화 시켜주어야 한다. 1234567891011121314151617181920test_case = int(input())for _ in range(test_case): left_stack = [] right_stack = [] p = input() for i in p: if i == '-': if left_stack: left_stack.pop() elif i == '&lt;': if left_stack: right_stack.append(left_stack.pop()) elif i == '&gt;': if right_stack: left_stack.append(right_stack.pop()) else: left_stack.append(i) left_stack.extend(reversed(right_stack)) print(''.join(left_stack)) 이 문제는 키로거로부터 읽은 키보드의 입력을 읽어, 사용자가 입력한 비밀번호가 무엇인지 알아내는 프로그램을 작성하는 문제이다. 커서를 기준으로 좌, 우 이동 그리고 삭제하는 처리가 있기 때문에 좌, 우 이동시 리스트 내의 문자의 이동을 처리하기 위해 두 개의 스택을 준비해서 처리한다. 주의해야 될 부분은 오른쪽 스택은 반대 방향으로 데이터가 쌓이기 때문에 최종적으로 오른쪽 스택은 좌우 반전을 하여 왼쪽 스택과 같이 이어줘야 알맞는 결과를 얻을 수 있다.","link":"/2021/01/27/202101/210127-Algorithm_baekjoon_5397/"},{"title":"210127 TypeScript TIL","text":"이 포스팅은 Udemy : Understanding TypeScript - 2021 Edition 에서 개별 학습한 내용을 기반으로 작성하였습니다. 오늘 공부한 내용 TypeScript에 대한 개념 이해 TypeScript는 JavaScript의 Superset으로, 기존의 자바스크립트 위에 building된 언어이다.기존에 JavaScript에는 없던 각종 이점들을 추가한 언어라고 생각하면 된다. 하지만 브라우저에서는 실행할 수 없기 때문에 작성한 TypeScript는 JavaScript로 컴파일 된 후에 실행된다. TypeScript의 이점 JavaScript에서의 변수는 단순히 상수(const)와 변수(let)으로만 구분해서 사용하고, 별도로 타입(Type)을 지정하지 않았다. 이로인해 함수의 인자로 넘겨주는 인자값이 어떤 것이든 입력이 되었고, 이로 인해 발생하는 에러는 컴파일(compile)단계에서 잡아서 수정할 수 있었다. 하지만 TypeScript를 사용해서 개발하게 되면, 개발단계에서 미리 에러를 잡아서 수정할 수 있다. 물론 JavaScript에서도 변수의 타입을 검사해서 유효성 검사 처리를 추가할 수 있다. 하지만 TypeScript를 사용해서 Type을 지정해서 처리하는 것 만큼 효율적이지 않다. 12345678function add(num1, num2) { if (typeof num1 === 'number' &amp;&amp; typeof num2 === 'number') { return num1 + num2; } else { return +num1 + +num2; }}console.log(add('2', '3')); Non-JavaScript Features인 Interfaces와 Generics을 활용할 수 있다. Meta-Programming 특징인 Decorators를 활용하여 개발할 수 있다. TypeScript 설치 및 사용 npm command를 사용하기 위해 node.js를 설치한다. global하게 TypeScript를 설치한다. 1$ npm install -g typescript tsc를 사용하여 TypeScript 파일을 컴파일할 수 있다. 1$ tsc test.ts 앞서 작성했던 JavaScript 코드에 TypeScript를 적용해서 작성해보도록 한다. using-ts.ts 1234567891011const button = document.querySelector('button');const input1 = document.getElementById('num1')! as HTMLInputElement;const input2 = document.getElementById('num2')! as HTMLInputElement;function add(num1: number, num2: number) { return num1 + num2;}button.addEventListener('click', function () { console.log(add(+input1.value, +input2.value));}); TypeScript 공부순서 TypeScript Basics Compiler &amp; Configuration Deep Dive Working with Next-gen JS Code Classes &amp; Interfaces Advanced Types &amp; TypeScript Features Generics Decorators Time to Practice - Full Project Working with Namespace &amp; Modules Webpack &amp; TypeScript Third-Party Libraries &amp; TypeScript React + TypeScript &amp; NodeJS + TypeScript 공부하는 방법(1) Watch the lecture(2) Code Along (Pause &amp; Rewind)(3) Practice (Advance on your Own(Partly))(4) Debug &amp; Search (Use attached Code, Google, Udemy Search)(5) Ask &amp; Answer (Ask in Q&amp;A section, but also help others!)","link":"/2021/01/27/202101/210127-Typescript/"},{"title":"210127 Python + Self Development TIL - Function, *args, **kwargs, Recursive function","text":"TO DO LIST 백준 5397번 키로거 문제 (Pseudo code/손코딩 + 본 코딩) https://leehyungi0622.github.io/2021/01/27/210127-Algorithm_baekjoon_5397/ 210122 분 Git TIL 블로그 글 업데이트 https://leehyungi0622.github.io/2021/01/22/210122-git_til/ 210128 분 Python + Self Development TIL 블로그 글 업데이트 Python오늘 수업을 통해 배운 내용 Python 메인 강의 과제점검 Ethiopian Multiplication 1234567891011result = 0print('{:7},{:7}, ,etc'.format('num1', 'num2'))while num &gt; 0: if num1 % 2 == 0: print('{:7}, {:7}, Pass'.format(num1, num2)) else: print('{:7}, {:7}, Keep'.format(num1, num2)) result += num2 num1 = num1//2 num2 = num2*2print(&quot;The result is {}&quot;.format(result)) &lt;!-- more --&gt; Monty Hall Simulation 123456789101112131415161718import randomstay = 0switch = 0trial = int(input(&quot;How many times do you want?&quot;))for _ in range(trial): # goats: 0, sportcar: 1 doors = [0, 0, 1] random.shuffle(doors) # print(doors) users_choice = doors.pop() # print(users_choice, doors) doors.remove(0) if users_choice == 1: stay += 1 else: switch += 1print(&quot;stay: {}, switch: {} for {}th trial&quot;.format(stay/trial, switch/trial, trial)) 함수(function)12345def function(parameter): 실행문1 실행문2 ... return output function with input 12345678# a, b는 parameter이다.def awe_sum(a, b): result = a + b return resulta = 2b = 3# a, b는 함수의 실행단계에서 사용하는 argument이다.print(awe_sum(a, b)) function without input 1234def print_hello(): return &quot;hello&quot;result_hello = print_hello()print(result_hello) function with multiple return 123def mul_return(a): b = a + 1 return a,b return skill 12345def id_check(id): if id == &quot;admin&quot;: print(&quot;invalid id: admin&quot;) return print(&quot;valid id: &quot;, id) parameter with initialize 123456def say_hello(name=&quot;Fool&quot;, nick=True)print(&quot;Hi, &quot;, name)if nick == True: print(&quot;But, you are Fool&quot;)else: print(&quot;Oh, you are not Fool&quot;) arguments 123456def mul_sum(*args): # 넘겨받은 args는 튜플 자료형이다. sum = 0 for i in args: sum += i return sum keyword arguments :키와 값 타입으로 구성되어 있는 arguments이다. 12345678910def show_kwargs(**kwargs): print(str(kwargs))show_kwargs(a=10, b=&quot;google&quot;)# {'a': 10, 'b': 'google'}def get_user_info(**kwargs): print(kwargs, type(kwargs)) for k,v in kwargs.items(): print('{}: {}'.format(k, v)) print('Your phone number is {}'.foramt(kwargs['phone'])) keyword arguments 1234567891011121314151617181920# 사용자가 도메인을 입력한다.('www.google.com', naver, daum)# 앞에 프로토콜 붙여주기# path admin.html에 들어오면 된다.# query string을 마음대로 넘겨받으면 된다.# ex) uri_complete('www.google.com', utm_source='social', utm_media='campaign')# =&gt; https://www.google.com/admin.html?utm_source='social' &amp; utm_media='campaign'# https://www.google.com/admin.html?utm_source=social&amp;utm_media=campaigndef uri_complete(domain, **query): uri = &quot;https://&quot;+domain+&quot;/admin.html&quot; for k in query.keys(): if k == list(query.keys())[0]: uri += &quot;?&quot; uri += k+&quot;=&quot;+query[k] if k != list(query.keys())[-1]: uri += &quot;&amp;&quot; return uriprint(uri_complete(&quot;www.google.com&quot;, utm_source='social', utm_media='campaign')) 1234567891011# keyword arguments는 맨 앞에 위치할 수 없다. 맨 앞에 위치하면 그 뒤에 위치한 parameter에 값이 없다는 컴파일 에러가 발생한다. parameter상에서 initial value를 초기화할 것은 뒤에 정의해줘야 한다.def complete_uri(hostname,protocol='https', **query): url = '{}://{}/admin.html'.format(protocol,hostname) if not query: return url else: query_string = '?' for k, v in query.items(): query_string += '{}={}&amp;'.format(k,v) query_string = query_string[:-1] return url + query_string 12345def print_(*args, **kwargs): for arg in args: print(arg) for k, v in kwargs.items(): print('{}={}'.format(k,v)) 1234567# **로 넘겨준 인자 중 지정되지 않은 나머지 인자에 대해 grouping 해서 표현할 수 있다. (key=value형태로 접근)def kwargs_url(server, port, **query): url = &quot;https://&quot;+server+&quot;:&quot;+port+&quot;?&quot; for key in query.keys(): url += key+&quot;=&quot;+query[key]+&quot;&amp;&quot; return urlkwargs_url(&quot;localhost&quot;, &quot;8080&quot;, utm_source=&quot;google&quot;, keyword=&quot;naver&quot;) 외부 변수와 지역변수 12345678910111213x = &quot;awesome&quot;def myfunc(): x = &quot;fantastic&quot; print(&quot;Python is &quot; + x)myfunc()print(&quot;Python is &quot; + x)# output:&quot;&quot;&quot;# 지역변수 내에서 재 정의한 변수는 외부의 같은 이름의 변수에 변화를 주지 않는다.Python is fantasticPython is awesome&quot;&quot;&quot; global variable (전역변수) use return 12345678a = &quot;hello&quot;def glob_test(a): a += &quot;world&quot; return aa = glob_test(a)print(a)# output : helloworld use global 12345678# 안좋은 케이스 : 'global'이라는 명령을 사용하여 전역변수로 사용하게 되면, 함수는 독립성을 잃게 되어 함수가 외부 변수에 의존적이게 된다.a = &quot;hello&quot;def glob_test(): global a a += &quot;world&quot; return aprint(glob_test())# helloworld 간단한 연습문제 Leap year(윤년) 구하는 문제 사용자로부터 연도 (0~9999 사이의 정수)를 입력받아 4로 나뉘어 떨어지면 윤년, 100으로 나뉘어 떨어지면 평년, 400으로 나뉘어 떨어질땐 윤년을 출력하는 함수를 작성하시오. 1234567891011121314def leap_year(year): if year % 400 == 0: return &quot;Leap year&quot; elif year % 100 == 0: return &quot;Plain year&quot; elif year % 4 == 0: return &quot;Leap year&quot; else: return &quot;Plain year&quot;y = int(input(&quot;연도(0~9999) 사이의 정수를 입력하시오.&quot;))print(leap_year(y)) 123456leap = Falsedef is_leap(y): if y % 4 == 0 and (y % 100 != 0 or y % 400 == 0): leap = Truey = int(input(&quot;Is leap?? &quot;))print(is_leap(y)) numguess with function이전 시간에 작성한 numguess 문제를 함수화 시켜서 작성한다. 1234567def guesser(guess): if guess == answer: print(&quot;Correct! The answer was &quot;, str(answer)) break else: print(&quot;That's not what I wanted! Try again!!&quot;) Recursive function(재귀함수)재귀함수는 반복 호출하는 구조이기 때문에 함수 내에는 함수를 종료할 수 있는 탈출조건을 가지고 있어야 한다. 123456789times_to_curse = int(input(&quot;How many times do you want to curse ? &quot;))print(times_to_curse, type(times_to_curse))def recurse_to_beast(num): print('Abracadabra') print('Curse has no effect. {} times left.'.format(num-1)) if num == 1: return &quot;Beast is now dead&quot; return recurse_to_beast(num-1) Fibonacci(for-loop활용 풀이) 123456n = int(input())num = [0,1]for i in range(2, n+1): sum_ = num[i-1] + num[i-2] num.append(sum_)print(num[-1]) Fibonacci(재귀함수를 활용 풀이) 123456def Fibonacci(n): if n &lt;= 1: return n else: return Fibonacci(n-1) + Fibonacci(n-2)print(Fibonacci(9)) Binet’s Fibonacci formula :재귀호출을 하지 않고, Fibonacci를 한 번에 구하는 공식 123456from math import sqrtdef binet_fibo(n): return ((1+sqrt(5))**n - (1-sqrt(5))**n)/(2**n*sqrt(5))print(int(binet_fibo(3))) 123456789101112131415from time import timef_start_time = time()fibo(30)f_end_time = time()fibo_time = f_end_time - f_start_timeprint(fibo_time) # 6.294sb_start_time = time()binet(30)b_end_time = time()binet_time = b_end_time - b_start_timeprint(binet_time) # 3.155s 피보나치 문제를 재귀함수로 풀이하면, 시간복잡도 O(N^2)만큼 걸리지만, 수학공식을 대입해서 구하는 경우, O(1) 상수만큼의 시간이 걸리기 때문에, 속도를 빠르게 하기 위해서는 공식을 활용한 처리가 유리하다.","link":"/2021/01/27/202101/210127-python_til/"},{"title":"Baekjoon Online Judge 10814번 나이순 정렬문제","text":"백준 저지 10814번 나이순 정렬문제 Pseudo code + Python code (오류1) 손코딩을 한 부분에 오류가 있다. age, name을 한 줄에서 입력받는데, 두 입력 데이터의 타입은 같지 않기 때문에 integer 형으로만 설정할 수 없다. 이 경우에는 string 형으로 우선 받고나서 데이터를 삽입할때 integer형으로 type casting을 해주면 된다. (오류2) 두번째 오류는 문제의 조건에서 튜플의 첫번째 요소(나이)로 우선 오름차순 정렬을 하고, 튜플의 두번째 요소(이름)으로 오름차순 정렬을 하는 문제가 아닌, 나이로 오름차순 정렬을 하고, 나이가 같으면 입력한 순서대로 정렬을 하는 문제였다. 따라서 정렬 조건을 key = lambda x:(x[0], x[1]) 가 아닌 key = lambda x: x[0]로만 해주면 된다. 123456789101112131415161718test_case = int(input())join_members = []for _ in range(test_case): age, name = map(str, input().split(' ')) join_members.append((int(age), name))join_members.sort(key=lambda x:x[0])for i in join_members: print(i[0], i[1])# input :321 Junkyu21 Dohyun20 Sunyoung# output :20 Sunyoung21 Junkyu21 Dohyun 이 문제는 리스트 내의 튜플을 이용해서 정렬하는 문제이다. 사용자로부터 나이와 이름에 대한 정보를 입력받고, 그 입력받은 정보를 튜플 자료형으로 묶은 다음에 리스트에 누적 저장한다. 최종 완성된 리스트는 sort() 메서드를 사용해서 조건에 맞게 정렬시켜서 출력시키면 된다.","link":"/2021/01/28/202101/210128-Algorithm_baekjoon_10814/"},{"title":"Baekjoon Online Judge 10930번 Hash String 변환문제","text":"백준 저지 10930번 Hash String 변환문제 Pseudo code + Python code 123456import hashlibinput_string = input()encoded_input_string = input_string.encode()hash_string = hashlib.sha256(encoded_input_string).hexdigest()print(hash_string) 이 문제는 입력받은 문자열을 hashlib를 활용하여 Hash string으로 변환하는 문제이다. 입력받은 문자열은 encode를 한 뒤에 Python의 hashlib에서 지원하는 sha256 함수를 사용하여 처리를 해준 다음에, hex decimal 값만을 포함하는 문자열을 생성하기 위해 hexdigest()로 처리를 해준다.","link":"/2021/01/28/202101/210128-Algorithm_baekjoon_10930/"},{"title":"Baekjoon Online Judge 5585번 거스름돈 문제","text":"백준 저지 5585번 거스름돈 문제 Pseudo code + Python code 1234567891011product_price = int(input())coin_unit = [1, 5, 10, 50, 100, 500]coin_unit.sort(reverse=True)changes = 1000 - product_pricecoin_count = 0for unit in coin_unit: if changes &gt;= unit: num_of_coin = changes // unit changes -= unit * num_of_coin coin_count += num_of_coinprint(coin_count) 이 문제는 대표적인 그리디 알고리즘 문제 유형으로, 최소한의 동전 갯수를 사용해서 거스름돈을 만드는 문제이다.사용될 동전의 종류를 저장하고 있는 리스트를 역정렬한 뒤에, 해당 리스트를 순회하면서 사용될 동전의 갯수를 카운트하면 되는 간단한 문제이다.","link":"/2021/01/28/202101/210128-Algorithm_baekjoon_5585/"},{"title":"Baekjoon Online Judge 10989번 수 정렬하기3 문제 (시간&#x2F;공간 복잡도 조건문제)","text":"백준 저지 10989번 수 정렬하기3 문제 Pseudo code + Python code [시간 제한] 시간복잡도 고려Java 8: 3 초Java 8 (OpenJDK): 3 초Java 11: 3 초Kotlin (JVM): 3 초 [메모리 제한] 공간복잡도 고려Java 8: 512 MBJava 8 (OpenJDK): 512 MBJava 11: 512 MBKotlin (JVM): 512 MB 12345678910n = int(input())counting_sort_list = [0] * 10001for _ in range(n): num = int(input()) counting_sort_list[num] += 1for i in range(1, 10001): for _ in range(counting_sort_list[i]): print(i) 이 문제는 계수정렬(Counting sort)을 사용하여 입력한 숫자를 오름차순으로 정렬하는 문제이다. 수의 갯수가 최대 10,000,000개이므로, 반복문을 통해서 값을 입력받게 되면, 해당 문제의 조건에서 초과된 시간으로 실행이 된다. 계수정렬은 시간복잡도가 O(N)이기 때문에 문제에서 조건으로 제시한 시간 내에 실행이 가능하다.","link":"/2021/01/28/202101/210128-Algorithm_baekjoon_10989/"},{"title":"210128 List와 Tuple 자료형을 이용한 정렬","text":"오늘 공부한 내용리스트와 튜플을 활용한 정렬에 대해서 정리해보려고 한다. 코딩테스트 문제에서 자주 활용할 수 있는 부분이기 때문에 사용에 대해서 숙지해두면 좋다. Tuple sorting 1v = [(3,4),(2,2),(3,3),(1,2),(1,-1)] 리스트 v를 튜플의 첫 번째 원소를 기준으로 오름차순 정렬하기 123v.sort(key = lambda x : x[0])print(v)# [(1, 2), (1, -1), (2, 2), (3, 4), (3, 3)] 리스트 v를 튜플의 첫 번째 원소로 내림차순 정렬하기 123456789# 방법 1v.sort(key = lambda x : -x[0])print(v)# [(3, 4), (3, 3), (2, 2), (1, 2), (1, -1)]# 방법 2v.sort(key = lambda x : x[0], reverse=True)print(v)# [(3, 4), (3, 3), (2, 2), (1, 2), (1, -1)] 리스트 v를 튜플의 두 번째 원소로 오름차순 정렬하기 123v.sort(key=lambda x : x[1])print(v)#[(1, -1), (2, 2), (1, 2), (3, 3), (3, 4)] 리스트 v를 튜플의 첫번째 원소로 오름차순 정렬하고, 첫 번째 원소가 같은 경우, 두 번째 원소로 오름차순 정렬하기 12345v.sort()# [(1, -1), (1, 2), (2, 2), (3, 3), (3, 4)]v.sort(key=lambda x : (x[0],x[1]))# [(1, -1), (1, 2), (2, 2), (3, 3), (3, 4)] 리스트 v를 튜플의 첫번째 원소로 내림차순 정렬하고, 첫 번째 원소가 같은 경우, 두 번째 원소로 오름차순 정렬하기 123v.sort(key=lambda x:(-x[0],x[1]))print(v)# [(3, 3), (3, 4), (2, 2), (1, -1), (1, 2)]","link":"/2021/01/28/202101/210128-Algorithm_til/"},{"title":"210128 계수정렬(Counting sort)","text":"오늘 공부한 내용Baekjoon Online Judge의 10989문제의 풀이에서 사용한 계수정렬(Counting sort)에 대해서 정리를 해본다. 계수정렬은 O(N)의 시간복잡도를 갖는다.계수 정렬을 사용하려면 정렬 대상 배열의 최소값과 최대값을 알고 있어야 한다. 최대값만큼 최대 길이를 설정해서 별도의 리스트를 생성해야 되기 때문이다. 정렬 이름대로 요소의 값에 해당하는 index의 위치에 요소의 갯수만큼 카운트해서 값을 증가시켜 주는 형태를 갖는다. 예를 들어 [1, 5, 1, 5, 6] 이라는 리스트가 있다고 가정하자. 총 5개의 숫자, 최소값은 1, 최대값은 6이라는 전제 조건이 성립하며, 계수정렬을 사용하기 위한 조건을 만족한다. 자 그럼 카운팅을 위해서 길이가 6인 0으로 초기화된 리스트를 별도로 생성해준다. 그리고나서 리스트를 순회하며 해당 값을 index로 갖는 새로 생성한 리스트의 위치값을 1씩 증가시켜준다.이제 index를 새로 카운트해서 넣어 준 값만큼 순회하며 값을 출력해주면 된다. 이미지 출처 : https://brilliant.org/wiki/counting-sort/ 아래는 Big-O 표기법으로 표기한 시간복잡도의 관계를 표현하고 있다.","link":"/2021/01/28/202101/210128-counting_sort/"},{"title":"210128 자료구조(Data structure) Array(배열)","text":"이번 포스팅에서 정리 할 내용은배열에 대한 개념이다. 파이썬에서는 리스트 타입이 배열의 기능을 제공하지만, c언어의 경우에는 배열을 독립적으로 사용한다.배열이란 데이터를 나열하고, 각 데이터를 인덱스에 대응하도록 구성한 데이터 구조를 말한다. 그럼 배열이 왜 필요해? 데이터에는 다양한 종류가 있고, 이러한 데이터들을 같은 종류로 분류해서 효율적으로 관리하기 위해서 배열이 사용된다.배열의 장점은 index 번호로 접근하기 때문에 특정위치에서 상대ㅐ적인 위치로의 데이터 접근이 빠르다는 장점을 가지고 있다. index 번호를 통해서 데이터에 접근을 하기 때문에, O(1)만큼의 시간복잡도를 갖는다.배열의 단점은 데이터의 추가/삭제의 어려움이 있다는 점이다. 미리 최대 길이를 지정해서, 한정된 자원내에서 데이터를 관리해야 되기 때문이다.","link":"/2021/01/28/202101/210128-array/"},{"title":"210128 자료구조(Data structure) &amp; 알고리즘(Algorithm) 개념","text":"오늘 정리할 내용은자료구조에 대한 개념이다. 기본적인 내용이지만 다른 사람에게 설명하듯이 정리해보려고 한다. 자료구조란 데이터 구조(Data Structure) 라는 용어와 혼재해서 사용된다.대량의 데이터를 효율적으로 관리할 수 있는 데이터 구조를 의미하는 것으로, 현실에 있는 정보를 프로그래밍으로 바꾸려면 정보에서 데이터로 변환이 필요한데 이때 필요한 것이 자료구조이다. 개발자에게 자료구조(데이터 구조)란?개발자는 현실에 있는 데이터를 활용해서 프로그래밍 언어로 개발을 하기 때문에 데이터를 가지고 개발을 할때 자료구조에 대한 지식은 필수이다. 그리고 데이터 구조에는 다양한 구조들이 있는데, 어떤 데이터 구조를 사용하느냐에 따라 코드의 효율이 즉, 프로그램의 퍼포먼스가 달라진다. 현실에서의 데이터 관리의 예우리 주변에서 이 데이터를 관리하는 예를 찾아보면, 우편번호와 학생 관리 번호가 있다. 우편번호는 앞의 3자리는 시, 군, 자치구를 의미하고, 뒤의 두자리는 일련번호로 구성이 된다. 학생 관리 번호 또한 학년, 반, 번호로 구성된 번호로 학생들을 관리한다.이처럼 우리의 일상에서도 데이터를 구조화시켜서 관리하는 경우가 많다. 그럼 대표적인 자료구조(데이터 구조)는?그럼 데이터 구조에 다양한 구조들이 있다고 했는데, 대표적으로 어떤 자료구조(데이터 구조)가 있는지 살펴보자.컴퓨터의 이론에 대해서 조금이라도 관심있는 사람이라면 다들 들어보았겠지만, 배열, 스택, 큐, 링크드 리스트, 해쉬 테이블, 힙 등..이 있다.우리 현실 세계에서 가장 대표적인 데이터 구조는 사전이 있다. 기존에 있는 자료구조(데이터 구조)는 왜 공부해야 하는가?앞서 이미 언급했듯이 개발자에게 있어 자료구조는 필수이다. 모나리자를 그릴 수 있는 실력은 모나리자를 모방함으로써 그 기술을 습득할 수 있다.이 말을 왜 하냐면 기존에 존재하는 대표적인 자료구조들을 모방해가면서 필요에 따라 새로운 자료구조를 만들 수 있는 역량을 키울 수 있다는 의미로 예시를 들었다.실제로 프로그램을 만들때나 기술면접시에도 많이 묻는 개념이기 때문에 알아두면 좋다. 다음으로 정리할 내용은 알고리즘이다.알고리즘(Algorithm)은 어떤 문제를 풀기 위한 절차/방법을 의미하며, 어떤 문제에 대해서 특정한 입력을 넣으면 원하는 출력을 얻을 수 있도록 만드는 프로그래밍 이다.우리 현실에서 가장 대표적인 알고리즘은 요리를 만드는 레시피를 예로 들 수 있다.요리를 하기 위해서는 재료준비 및 손질 단계와 숙성단계 조리단계 등 다양한 단계가 있는데, 얼마만큼의 시간과 저장공간이 필요한지 각 각의 절차적 특성을 고려해야 하기 때문에 여러 측면에서 보았을때 알고리즘과 비슷한 특징을 갖고 있다. 결론적으로 말하자면,어떤 자료구조를 사용해서 알고리즘을 작성하느냐에 따라 성능이 천지차이고, 요즘과 같이 데이터가 중요해진 시대에는 더욱이 알고리즘의 작성방법은 매우 중요해졌다.프로그래밍을 잘 할 수 있는 기술과 역량을 보여주기 위해서는 이러한 자료구조와 알고리즘을 잘 활용할 줄 알아야 한다.","link":"/2021/01/28/202101/210128-data_structure/"},{"title":"210128 자료구조(Data structure) Linked List(링크드 리스트)(작성중...)","text":"이번 포스팅에서 정리 할 내용은링크드 리스트(Linked list)에 대한 개념이다. 우선 링크드 리스트란 연결 리스트라고도 하며, 순차적으로 연결된 공간에 데이터를 나열하는 구조인 배열과 대조적으로 링크드 리스트는 떨어진 곳에 존재하는 데이터들을 화살표로 연결해서 관리하는 구조로 되어있다. C언어에서는 주요한 데이터 구조이지만, Python에서는 리스트 타입이 링크드 리스트의 기능을 모두 지원한다. 링크드 리스트의 기본 구조와 용어 노드(Node) : 데이터 저장 단위(데이터값, 포인터)로 구성이 되어있다. 포인터(Pointer) : 각 노드 안에서 다음이나 이전의 노드와의 연결 정보를 가지고 있다. 링크드 리스트의 장단점(전통적인 C언어에서의 배열과 링크드 리스트) 장점 배열은 미리 미리 데이터 공간을 할당해야되는 것에 반해 링크드 리스트는 미리 데이터 공간을 할당하지 않아도 된다. 단점 연결을 위한 별도의 데이터 공간이 필요하므로 저장공간 효율이 높지 않다. 연결정보를 찾는 시간이 필요하기 때문에 접근 속도가 느리다. 중간 데이터 삭제시, 앞 뒤 데이터의 연결을 재구성해야 하는 부가적인 작업이 필요하다. Node 구현12345678910class Node: def __init__(self, data, next=None): self.data = data self.next = next# 포인터를 활용하여 Node와 Node를 서로 연결하기node1 = Node(1)node2 = Node(2)node1.next = node2head = node1 새로운 Node를 추가하는 method 작성하기12345678def add(data): node = head # node의 다음에 Node가 존재하면, while node.next: # node를 node의 다음 Node로 업데이트 해준다. node = node.nex # 제일 마지막 node에 추가하고자 하고자 하는 새로운 Node를 param 값으로 받은 data로 초기화 시켜서 다음 노드로 삽입해준다. node.next = Node(data) 생성된 링크드 리스트 출력해보기12345node = headwhile node.next: print(node.data) node = node.nextprint(node.data)","link":"/2021/01/28/202101/210128-linked_list/"},{"title":"210128 자료구조(Data structure) Stack(스택)","text":"이번 포스팅에서 정리 할 내용은스택에 대한 개념이다. 스택을 비유로 들면, 바닥에 쌓여있는 책들을 예로 많이 들고 있는데, 그 이유는 스택은 데이터를 제한적으로 접근할 수 있는 구조로 되어있기 때문이다. 여기서 제한적인 접근이란?한 쪽 끝에서만 자료를 넣거나 뺄 수 있는 구조로 되어있다는 것을 말한다.따라서 LIFO(Last-In-First-Out)의 정책을 가지고 있는 구조이다.이는 다음에 정리할 큐(queue)의 FIFO(First-In-First-Out)정책과 상반되는 구조이다. 스택은 어디에서 사용되나?스택은 대표적으로 우리가 사용하는 컴퓨터 내부의 프로세스 구조의 함수 동작 방식에서 사용이 된다.스택 구조는 프로세스 실행 구조의 가장 기본으로, 함수 호출시 프로세스 실행 구조를 스택과 비교해서 이해할 필요가 있다. 123456789101112131415161718192021222324def recursive(data): if data &lt; 0: print(&quot;ended&quot;) else: print(data) recursive(data - 1) print(&quot;return&quot;, data)recursive(4)# output :&quot;&quot;&quot;43210endedreturned 0returned 1returned 2returned 3returned 4&quot;&quot;&quot; 위의 재귀함수 실행을 살펴보면, 4부터 0까지 데이터가 들어간 다음에 탈출조건에 만족하는 입력 데이터 들어가게 되면, “ended”가 출력이 되고, 그 다음에 마지막에 삽입된 데이터부터 역으로 함수 내의 재귀호출 실행부의 아래에 있는 출력문이 실행되고 있음을 알 수 있다. 스택의 주요 기능스택의 주요 기능은 데이터를 스택에 넣는 (push())와 데이터를 스택에서 꺼내는 (pop())이 있다. 스택의 장단점스택의 장점은 구조가 단순해서 구현이 용이하다는 점과 데이터 저장과 데이터 읽기 속도가 빠르다는 점이 있다.스택의 단점은 일반적인 스택을 구현할 경우, 데이터의 최대 갯수를 미리정해야 된다는 점이 있다. 이러한 한계는 링크드 리스트(Linked List)를 구현해서 극복할 수 있으며, 최대로 쌓을 수 있는 공간을 확보함으로써 생기는 메모리 낭비에 대한 문제점도 해결할 수 있다.","link":"/2021/01/28/202101/210128-stack/"},{"title":"210129 Git TIL","text":"branch에 대한 개념 및 관리 Branch란? 분기점을 생성하고 독립적으로 코드를 변경할 수 있도록 도와주는 모델이다. 기본적인 Branch 관리 관련 명령어 사용가능한 Local Branch를 출력 1$ git branch Remote Branch에 대한 정보를 출력 1$ git branch -r 사용가능한 모든 Branch 정보를 출력 1$ git branch -a 새로운 Branch 끊기 1$ git branch [new branch name] 새로운 Branch를 끊고, 동시에 checkout하기 1$ git checkout -b [new branch name] 새로 만든 Branch 로 switch 하기 1$ git switch [new branch name] (Additional step)merge하기 전에 새로 생성한 branch와 master branch와 비교해서 달라진 부분을 비교해야 할 필요가 있다면 아래의 명령으로 변경된 부분을 확인한다. 1$ git diff master [compared branch name] master branch로 다시 이동을 해서 새로 생성한 branch를 merge하도록 한다. 1$ git merge [새로 생성해서 작업한 branch 이름] (Additional step)만약에 새로 생성한 remote branch를 repository에 push해서 관리를 해야 한다면, 아래와 같이 명령어를 입력한다. 1$ git push origin [new branch name] 새로 생성된 branch에서의 모든 작업은 끝났고, merge작업까지 마무리 되었기 때문에 삭제해준다. 1$ git branch -D [new branch name] 실습내용: master branch에서 hello.py파일을 생성하고, 파일내용의 수정은 iteration branch에서 작업을 한다. 그 후에 master branch로 이동을 해서 파일이 변경되어 있지 않음을 확인한다. (파일 수정 작업은 iteration branch에서 작업을 하였기 때문) master branch에 새로 끊어서 작업한 iteration branch를 merge해주기 위해서 master branch로 switch하고, 새로 끊은 branch에 대해서 merge 작업을 해준다. case study) 별도의 branch에서 파일 변경작업을 하다가 commit을 하지 않고 main branch로 이동을 하고, 별도로 끊은 branch를 삭제한 경우, 기존에 수정된 내용이 불필요하다면(변경사항이 의도하지 않게 넘어온 경우에는), 아래의 명령을 이용해서 이동한 main branch에서 이전에 작성했던 파일을 원래 상태로 돌릴 수 있다. 1234$ git checkout -- [file name]예시) $ git checkout -- hello.pycf) git의 최신버전에서는 restore를 사용해서 위의 작업을 한다. Branching models의 개념과 Pros &amp; Consbranch를 관리하는 모델에는 다양한 종류가 있다. git flow (hotfix) - master - (release) - develop - feature pros : 가장 많이 적용되며, 각 단계가 명확히 구분된다. cons : 관리가 복잡하다. master에서 바로 branch를 끊어서 수정한 다음에 바로 push해주고자 할때 hotfix branch를 사용한다. github flow master - feature pros : branch model의 단순화ㅏ, master의 모든 commit은 deploy할 수 있다. cons : CI의 의존성이 높다. pull request를 통해 실수를 방지할 수 있다. gitlab flow production - pre-production - master - feature pros : deploy, issue에 대한 대응이 가능하도록 보완되었다. cons : git flow와 순서가 반대이다.(master-develop, production-master) Git flow strategygit flow는 git 명령어를 간소화해서 사용할 수 있는 툴이다. 이전에 git을 사용했을때에는 새로 끊은 branch에서 merge하고자하는 branch로 switch한 다음에 merge하고 merge된 branch를 삭제하는 이 일련의 작업을 일일이 해줘야 했는데, git flow를 사용하면 start finish 명령 한 번으로 처리를 해준다. 너무 편리한 것 같다.그래도 구 방식의 CLI가 얼마나 번거로운지 알기 때문에 현재 git flow가 얼마나 편한지 이해가 되는 것 같다. (git-flow reference site)http://danielkummer.github.io/git-flow-cheatsheet/ mac에서는 간단하게 brew를 사용해서 git-flow를 설치할 수 있다. 1$ brew install git-flow-avh 편하긴 하지만, 아직 명령어 사용에 익숙하지 않기 때문에, 위의 홈페이지를 참고해서 명령어에 익숙해지도록 하자. :) Rebase에 대한 이해 Rebase는 새로 끊은 branch의 base를 베이스가 되는 branch의 최신 point로 분기점을 이동하는 것을 말한다. 실제로 Rebase의 사용을 선호하는 기업(배달의 민족)도 있고, 그렇기 않은 기업도 있다. Rebase를 사용하게 되면, 새로 끊은 branch의 merge commit이 없는 상태로 branch가 붙기 때문에 merge commit과 새로 끊은 branch가 어느 시점에서 pruning되었고, 어떠한 작업으로 마무리가 되었는지 세부 과정을 확인하는 것이 필요하다면, 이러한 경우에는 rebase를 사용하지 않는 것이 좋다. Rebase하는 방법 (1) rebase하고자 하는 해당 branch에서 stage에 수정한 파일을 올리고 rebase명령을 해준다. 12$ git add [file]$ git rebase --continue (2) merge하고자 하는 base branch(master branch)로 switch한 뒤에, rebasing branch를 merge시켜준다. 1$ git merge rebasing Collaborate with your Co-worker 실습내용Practice-flow : PM(Project Manager)는 Github에 repository를 새로 파고, 팀원들은 해당 repository를 fork해서 작업을 해준다. 12345678910111213141516171819202122(PM)1. github에서 새로운 repo를 생성하고 내 컴에 clone 한다.- remote 에서 repository를 만들고 clone했기 때문에 git init을 통해 git을 초기화 해 줄 필요가 없다.2. git flow init 후 develop에서 numguess.py를 만들고 add, commit, push3. 팀원에게 알린다.(DEV)1. 알림 받은 후, 방문하여 fork 한다.2. 팀장 repo에 방문하여 issue를 생성한다.3. fork한 나의 repo를 내 컴에 clone한다.4. git flow init 후, dev -&gt; feature/{MYFEATURE} 브랜칭 하여 작업한다. (commit 시 발급한 issue 번호 매기기)5. feature finish 후 나의 develop으로 push 한다.6. create pull request 하고 팀장에게 알린다.(PM)4. pull request를 리뷰한다.5. 수정할 것을 지시한다.(DEV 7. 수정사항을 반영하여 다시 add,commit,push)6. merge한다.DEV8. PM repo 업데이트 발생시 PM의 develop을 pull한다. Repository에 collaborator를 추가해서 작업 (settings - Manage access - Invite collaborator) 이 방법은 추천을 하지 않는다. 그 이유는 collaborator에 추가된 사람은 해당 repository를 소유한 사람과 같이 해당 repository에 대해서 모든 권한을 가지기 때문이다. 이 방법은 큰 Open source project에서 pull request를 관리하는 권한으로만 Collaborator를 추가한다. 위의 collaborator추가를 통한 repository 공유는 추천하지 않기 때문에 대안으로 나온 방법이 Pork &amp; Merge이다.구체적인 순서는 위에서 작성한 순서를 따른다. commit message 작성 팀장이 팀원을 위해 Repository에 파일을 생성시,ex) : 123456feat: Create index.htmlTODO- semantic elements- web standard- seperaate css, js files into static/ 다음에 할 일을 위와 같이 TODO와 함께 작성을 해주면, 팀원이 참고해서 일을 하게 된다. push upstream set option에 대한 이해Clone한 경우에 만약에 main branch가 아닌, 별도의 branch에서 작업을 하는 내용이라면, push할때 -u 옵션을 사용해서 remote branch를 설정해줘야 한다. 12예) 현재 local에서 작업하는 내용을 remote의 develop branch로 push하는 경우,$ git push -u origin develop 만약에 main/master branch에서 작업을 하는 경우에는 clone시에 이미 지정되어있는 부분이기 때문에 지정해주지 않아도 된다. 1$ git branch --set-upstream &lt;remote-branch&gt; 위와 같이 현재 local branch에 대한 remote branch를 설정합니다.설정을 해주게 되면, 향후에 $git pull명령에 대해서 remote-branch에서 현재 로컬 분기로 commit된 정보를 가져옵니다. 위와 같이 명시적으로 입력을 하지 않고 처리하는 방법으로는 push할때 -u옵션을 사용하는 것이다.그러면 향후 Push / Pull 시도에 대한 upstream 연결이 자동으로 설정된다. 1$ git push -u origin &lt;local-branch&gt; fork한 PM의 repository의 Issue는 코드관련해서 구성원들끼리 communication하는 장이다. 만약에 할당받은 업무에 대한 catch-up message는 작성하는 본문 내용에 자세하게 작성을 해줘야 한다.(업무의 연관성에 대해서) 그럼 PM은 작성한 ISSUE 내용을 보고 Assignee에 적합한 사람이라면 업무할당으르 해준다. (+Label로 구체적인 업무 구분을 해준다.) Commit을 할때에는 파일단위로 나눠서 commit을 하지 말고, 기능 단위로 파일들을 묶어서 해줘야 한다. 나중에 commit을 확인한 하는 개발자가 해당 commit을 보고 이 파일들의 수정을 통해서 어떤 기능구현이 가능했었는지 알 수 있도록 하기 위해서 이다. Commit을 할때에는 Issue와 Pull request는 numbering이 되는데(Issue와 Pull request는 같이 진행이 된다), 이 번호를 commit log에 Issue번호를 같이 적어주게 되면 Commit - Issue - Pull request가 서로 연결이 된다. 따라서 이 commit message만 보더라도 해당 commit이 어떤 Issue를 해결했고, 어떤 Pull request와 연관이 되어있는지 이해할 수 있게 된다. 1234567feat: Seperate css, js into static/solved: #1 (issue number)path- static/css/style.css- static/js/index.js 하나의 작업을 하더라도 꼭 하나의 feature를 따서 작업을 하는 습관 들이기. 팀원은 master(main) branch에 접근할 필요가 없고, Release 작업은 PM의 권한을 가지고 있는 개발자가 담당한다. 개발 담당자는 develop branch까지 merge한 내용을 fork한 본인의 branch에 push를 해주면 된다. 1$ git push -u origin develop 프로젝트 협업시에 참고하면 좋은 꿀 Tip fork한 본인의 branch에 push를 한 뒤에 pull request 작성으르 하게 되면, 본문 작성시, ‘#’ 입력과 함께 Issue에 작성했던 Issue의 title 내용이 자동완성되는 것을 확인할 수 있다.(직접입력하는 것과 동일) Issue를 먼저 만들고 Pull request를 만들어주는 것이 더 모양이 좋다. (선 Issue, 후 Pull request) Pull request를 한 뒤에 PM(Project Manager) developer가 comment를 달아주면 팀원 developer는 해당 내용을 확인하고, 해당 Pull request는 merge되지 않고 open된 상태이기 때문에,수정하는 내용은 feature branch를 따서 작업을 해도 되고, develop branch에서 바로 작업을 해줘도 된다.변경사항에 대해서 파일을 수정한 다음에는 이전과 동일하게 add, commit을 해주면 된다. (comment의 title은 feat:로 해서 달아주면 된다.) commit을 해주고 fork한 본인의 repository의 developer branch에 push를 해주면 된다. 수정작업을 한 뒤에는 다시 Pull request를 열 필요 없이, 기존의 Pull request를 확인하면 업데이트 한 사항을 확인할 수 있다.(Pull request를 안닫아주면, develop branch에서 작업한 다른 모든 내용들이 전부 open되어있는 pull request에 쌓이게 된다. 따라서 PM은 빨리 Pull request를 확인해서 닫아줘야 한다) 다른 팀원들은 미리 PM의 remote를 별도로 등록해줘야 한다. 나중에 다른 팀원들의 내용이 PM의 repository에 merge된 후에 해당 내용을 당겨서 받아야 되기 때문에, 미리 등록을 해두는 것이 좋다. 123456789$ git remote add pmorigin [PM Repository address]//연결되어있는 remote branch 확인 (origin, pmorigin)$ git remote -v$ git pull pmorigin develop//팀장은 본인(origin)의 repository 정보를 pull해주면 된다.$ git pull origin develop (2021.02.22 업데이트) 로컬에서 작업중인 특정 파일을 수정 이전의 상태로 되돌리기(undo)1$ git checkout -- [target file] (2021.05.25 업데이트) local에서 commit한 내용 되돌리기Git에서 이력을 되돌리는 방법에는 두 가지가 있는데, 바로 Reset과 Revert이다. Reset은 시계를 다시 맞추는 것 처럼 돌아가려는 특정 commit 시점으로 재설정하고, 해당 commit 시점 이후의 이력은 사라지게 된다. 1$ git reset &lt;option&gt; &lt;return commit point&gt; option에는 hard, mixed, soft 세 가지가 있다. 우선 hard의 경우에는 돌아가려는 이력이후의 모든 내용을 지워버린다. 12# 예시 a3bbb1c 시점으로 commit 시점을 되돌리고, 이후의 이력들은 삭제한다.$ git reset --hard a3bbb1c 반면에 soft 옵션의 경우에는 돌아가려는 시점의 이력으로 되돌린다는 점에서는 hard 옵션과 같지만, 이후의 이력내용들은 지워지지 않고, 해당 내용의 index(or stage)는 그대로 존재하며, stage에 올라가 있는 상태이다.따라서 바로 다시 commit을 할 수 있는 상태로 남아있다. 12# 예시 a3bbb1c 시점으로 commit 시점을 되돌리고, 이후의 이력들은 남겨둔다.(stage에 올라가 있는 상태)$ git reset --soft a3bbb1c 마지막으로 설명할 옵션인 mixed은 특정 option을 지정하지 않은 경우 적용되는 option이다.이 옵션은 앞의 두 옵션과 같이 이력이 되돌려지지만, 이후의 이력들은 남겨둔다. soft 옵션과 다른 점은 남겨진 이력들이 stage에 올라가 있지 않고, commit을 하기 위해서는 변경된 내용을 stage 추가해야 되는 상태이다. 또한 되돌아가는 commit을 commit hash를 통해서 직접 지정을 할 수도 있고, 현재시점부터 몇 개의 커밋을 되돌릴 수 있다. 아래와 같이 입력을 하면 1개 이전의 이력으로 되돌아가며, 5개 이전의 이력으로 되돌리고 싶은 경우에는 HEAD~5와 같이 입력을 해주면 된다. 1$ git reset HEAD~1 다음으로 정리할 내용은 Revert에 대한 내용이다.Revert도 Reset과 마찬가지로 상태를 되돌린다는 특징을 가지고 있다. revert하고 현재 작성중인 코드를 보면 hard option을 제외한 옵션으로 reset을 한 것과 동일한 결과를 갖는다.하지만 세부 이력을 살펴보면, 같지 않음을 알 수 있다. reset으로 이력을 되돌린 경우에는 이전 이력들에 대한 내용이 stage에 올라가거나(soft) 올라가지 않은(mixed) 상태로 남아있지만, revert를 한 경우에는 이전에 commit한 이력들이 그대로 남아있는 상태에서 특정 commit 시점으로 되돌아간 상태가 된다. 되돌아간 commit 시점에 해당하는 내용만 삭제가 되고, 이후의 commit 내용들은 그대로 남아있게 된다. 12# revert 예시$ git revert a3bbb1c 만약에 되돌릴 commit이 여러개인 경우에는 범위를 줘서 revert를 할 수 있다. (reset의 기능과 같이 revert를 사용하고 싶다면 구간을 지정해서 revert를 해주면 된다) 12# 범위를 지정해서 revert$ git revert a3bbb3c..e10665e reset과 revert 사용만약에 이력들 중에 특정 commit만 취소하고 싶은 경우에는 reset을 사용해서 이후의 이력을 모두 제거하기 보다는 revert를 사용해서 해당 commit의 내용만 되돌리는 편이 좋다.또한 이미 remote repository에 push를 한 경우에는 reset을 사용하게 되면, reset하기 이전으로 되돌리기 전까지 push를 할 수 없게 된다. (force 옵션의 사용을 제외)따라서 이미 remote repository에 push를 한 상태라면, revert를 사용해서 이력을 되돌려야 한다. revert를 사용하게 되면 특정 이력으로 되돌아감과 동시에 새로운 rollback에 대한 이력도 남기게 됨으로 좀 더 안전한 방법으로 생각될 수 있다. 특정 commit 시점으로 revert를 하게 되면, 해당 시점 이후의 commit들이 삭제되는 것이 아닌, 되돌아간 commit의 시점에 해당하는 내용만 삭제가 된다. (삭제된 내용은 별도의 revert commit 메시지로 남긴다) 그리고 혼자 사용하는 브랜치가 아닌 경우나 reset이후에 삭제될 commit들 중에 다른 사람이 작성한 commit이 있는 경우에는 revert를 사용해서 이력을 되돌려야 한다.","link":"/2021/01/29/202101/210129-Git_til/"},{"title":"210130 Git Commit Message TIL","text":"Commit Convention commit제목은 50자 이내로 요약하여 작성한다. 제목과 내용사이에는 한 칸 띄어준다. prefix를 활용하여 commit의 용도를 한 눈에 알아볼 수 있도록 한다. 123456789feat: featuresdocs: documentationsconf: configurationstest: testfix: bug-fixrefactor: refactoringci: Continuous Integrationbuild: Buildperf: Performance ex) Commit Convention - example 1234feat: Create server.py to start flask projectdocs: Create README.mdconf: poetry inittest: User model CRUD test complete 구글링하다가 commit convention에 대해서 정리가 잘 되어있는 페이지가 있어서 정리해보려고 한다. Semantic Commit MessagesSee how a minor change to your commit message style can make you a better programmer. Format: &lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; &lt;scope&gt; is optional Example123456feat: add hat wobble^--^ ^------------^| || +-&gt; Summary in present tense.|+-------&gt; Type: chore, docs, feat, fix, refactor, style, or test. More Examples: feat: (new feature for the user, not a new feature for build script) fix: (bug fix for the user, not a fix to a build script) docs: (changes to the documentation) style: (formatting, missing semi colons, etc; no production code change) refactor: (refactoring production code, eg. renaming a variable) test: (adding missing tests, refactoring tests; no production code change) chore: (updating grunt tasks etc; no production code change) References: https://www.conventionalcommits.org/ https://seesparkbox.com/foundry/semantic_commit_messages http://karma-runner.github.io/1.0/dev/git-commit-msg.html 위의 참고자료들을 보고, 프로젝트시에 적극 활용해서 commit convention을 따라 standard한 commit message를 잘 적을 수 있는 개발자가 될 수 있도록 하자. 목표 추가 : 협업 잘하는 개발자가 되기","link":"/2021/01/30/202101/210130-Git_commit_message_rule/"},{"title":"210130 JavaScript의 탄생과 발전","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 JavaScript의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. JavaScript의 탄생과 발전 JavaScript는 1995년에 만들어졌고, Netscape 브라우저에서 당시에 다른 브라우저와 차별화를 두기 위한 기술로써 사용되었었다.현대에 들어서는 장바구니 담기, 좋아요 클릭, instagram - javascript , 가상현실, VR 게임/앱, 드론 컨트롤, 로봇 컨트롤 등 다양한 분야에서 Javascript가 사용이 되고 있다. JavaScript는 과거에 웹 브라우저에 종속되었던 특성에서 웹 브라우저와 독립적으로 사용이 가능해짐에 따라 React, NodeJS, ExpressJS 등 front-end와 back-end에서 모두 JavaScript 사용이 가능하게 되었다. JavaScript를 이용해서 할 수 있는 브라우저상의 기본적인 작업들 페이지에 있는 HTML 요소들을 변경할 수 있다. 페이지에 있는 HTML의 속성들을 변경할 수 있다. 페이지에 있는 CSS 스타일들을 변경할 수 있다. 기존의 HTML 요소나 속성을 제거할 수 있다. 새로운 HTML 요소나 속성들을 추가할 수 있다. 자바스크립트는 페이지에 있는 기존의 HTML 이벤트와 상호작용할 수 있다. 자바스크립트는 페이지에서 새로운 HTML 이벤트들을 생성할 수 있다. 위의 브라우저의 조작은 DOM(Document Object Model)을 활용해서 가능하다. HTML 요소에 접근하기123456789101112document.getElementsByTagName(&quot;h1&quot;);document.getElementByClassName(&quot;second&quot;);document.getElementById(&quot;first&quot;);document.querySelector(&quot;h1&quot;) : selector가 찾은 태그의 첫번째 요소를 찾는다.document.querySelectorAll(&quot;li&quot;) : selector가 찾은 태그의 모든 요소를 찾는다.getAttribute : 속성값 찾기ex) document.querySelector(&quot;li&quot;).getAttribute(&quot;random&quot;);setAttribute : 속성값 셋팅ex) document.querySelector(&quot;li&quot;).setAttribute(&quot;random&quot;, &quot;1000&quot;); 스타일 변경하기12//style.{property}document.querySelector('h1').style.background = 'yellow'; 일반적으로 HTML은 텍스트에서 분리되고, CSS는 스타일에 집중된다. 그리고 Javascript는 액션에 집중한다. JavaScript로 action 이벤트 변경하기12345678910111213141516document.querySelector('li').classList;// 클래스이름에 리스트를 추가할 수 있다.document.querySelector('li').classList.add('coolTitle');document.querySelector('li').classList.toggle('done');// innerHTMLdocument.querySelector('h1').innerHTML = '&lt;strong&gt;!!!!!&lt;/strong&gt;';document.querySelectorAll('li')[1].parentElement.parentElement.children;// It is important to CACHE selectors in variables/** ※ 이렇게 페이지를 새로고침하기 전까지 h1 변수에 h1태그에 대한 querySelector를 지정해준 것을 저장해서 사용하는 것을 CACHE selector라고 한다. */var h1 = document.querySelector('h1'); DOM EVENT12345var button = document.getElementsByTagName('Button')[0];button.addEventListener('mouseleave', function () { console.log('CLICK!!!!');}); Event referenceMozila Event Reference! 12&lt;input id=&quot;userinput&quot; type=&quot;text&quot; placeholder=&quot;enter items&quot; /&gt;&lt;button id=&quot;enter&quot;&gt;Enter&lt;/button&gt; 123456789101112131415161718192021222324252627282930313233var button = document.getElementById('enter');var input = document.getElementById('userinput');var ul = document.querySelector('ul');function inputLength() { return input.value.length;}function createListElement() { var li = document.createElement('li'); li.appendChild(document.createTextNode(input.value)); ul.appendChild(li); input.value = '';}function addListAfterClick() { console.log('click is working'); if (inputLength() &gt; 0) { createListElement(); }}function addListAfterKeypress(event) { console.log(event.which); //Enter key event(which):13 if (inputLength() &gt; 0 &amp;&amp; event.keyCode === 13) { createListElement(); }}button.addEventListener('click', addListAfterClick);input.addEventListener('keypress', addListAfterKeypress); Callback function앞서 코드를 refactoring할때 버튼 클릭 리스너와 input 태그에 대한 이벤트 리스너의 이벤트 메소드에서 argument가 있는 메소드 형태가 아닌 변수 형태로 작성을 해주었다 button.addEventListener(“click”, addListAfterClick); input.addEventListener(“keypress”, addListAfterKeypress); 이러한 작성을 콜백함수라고 한다. 자바스크립트에서는 클릭 이벤트가 호출될때, 함수를 실행하지 않고, 함수에 대한 참조를 전달한다. 리스트의 항목을 클릭했을때, class=”done”을 toggle로써 추가/제거될 수 있도록 작성, 각각의 리스트 옆에 삭제할 수 있는 delete 버튼을 삽입 solution1) 1234567891011121314151617181920212223// Toggle eventfunction toggleListItemClass() { li.classList.toggle('done');}//1. toggles the .done class on and offli.addEventListener('click', toggleListItemClass);//2. Add delete button beside of the list item.var li = document.querySelectorAll('li');for (var i = 0; i &lt; li.length; i++) { li[i].innerHTML += &quot;&lt;button id='delete_btn&quot; + i + &quot;' onclick='deleteListItem(&quot; + i + &quot;)'&gt;delete&lt;/button&gt;&quot;;}function deleteListItem(i) { ul.removeChild(ul.childNodes[i]);} solution2) 12345678910&lt;ul id=&quot;foodList&quot;&gt; &lt;li class=&quot;bold red&quot; random=&quot;23&quot;&gt; Notebook&lt;button class=&quot;delete&quot;&gt;Delete&lt;/button&gt; &lt;/li&gt; &lt;li&gt;Jello&lt;button class=&quot;delete&quot;&gt;Delete&lt;/button&gt;&lt;/li&gt; &lt;li&gt;Spinach&lt;button class=&quot;delete&quot;&gt;Delete&lt;/button&gt;&lt;/li&gt; &lt;li&gt;Rice&lt;button class=&quot;delete&quot;&gt;Delete&lt;/button&gt;&lt;/li&gt; &lt;li&gt;Birthday Cake&lt;button class=&quot;delete&quot;&gt;Delete&lt;/button&gt;&lt;/li&gt; &lt;li&gt;Candles&lt;button class=&quot;delete&quot;&gt;Delete&lt;/button&gt;&lt;/li&gt;&lt;/ul&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455var button = document.getElementById('enter');var input = document.getElementById('userinput');var ul = document.querySelector('ul');var li = document.querySelector('li');var deletebtn = document.getElementsByClassName('delete');// class=&quot;delete&quot;인 버튼에 click 이벤트 set upfor (var i = 0; i &lt; deletebtn.length; i++) { deletebtn[i].addEventListener('click', removeParent, false);}function inputLength() { return input.value.length;}function createListElement() { var li = document.createElement('li'); var btn = document.createElement('button'); btn.innerHTML = 'Delete'; btn.onclick = removeParent; li.appendChild(document.createTextNode(input.value)); li.appendChild(btn); ul.appendChild(li); input.value = '';}function addListAfterClick() { if (inputLength() &gt; 0) { createListElement(); }}function addListAfterKeypress(event) { if (inputLength() &gt; 0 &amp;&amp; event.keyCode === 13) { createListElement(); }}button.addEventListener('click', addListAfterClick);input.addEventListener('keypress', addListAfterKeypress);// Toggle eventfunction toggleListItemClass() { li.classList.toggle('done');}//toggles the .done class on and offli.addEventListener('click', toggleListItemClass);//delete parent functionfunction removeParent(evt) { evt.target.removeEventListener('click', removeParent, false); evt.target.parentNode.remove();} JQuery의 등장DOM을 이용해서 화면의 구성요소를 조작할때에는 각 브라우저에서 호환이 되는지 확인을 해야했으며, querySelector를 지원하는 브라우저도 몇 안되었다. 이러한 호환문제와 DOM 객체를 이용한 웹 페이지 요소 조작을 간편하게 할 수 있도록 등장한 것이 바로 JQuery이다. 하지만 ES6 이후부터는 JavaScript가 많이 발전되어 JQuery를 사용하지 않는 추세이다.(JQuery는 간단하게 JQuery CDN을 추가하고, $sign을 이용해서 간단하게 화면 구성요소들을 호출해서 사용할 수 있다.) 123456ex. $(document).ready(function(){ $(&quot;p&quot;).click(function(){ $(this).hide(); });});(= p.addEventListener(&quot;click&quot;, function(){ });) JQuery는 매우 Imperative한 언어이다. 따라서 프로그램에 정확히 무엇을 해야하는지 하나 하나씩 알려야 한다. 하지만 프로그램이 커지면 다른 조치는 또 다른 조치로 종속되고, 오류와 버그가 많아지며 해결하기 위한 추적이 매우 힘들어진다.Imperative : 반드시 해야하는, 긴요한, 명령적인 Javascript를 대중화시키고 편리하게 사용하는데에 JQuery는 많은 것을 기여를 했지만, 점점 시스템은 커져가고 있고, 단순히 모든 것들을 절차지향 언어와 같이 하나 하나 알려줘야하는 프로그램이라면 나중에 운용보수면에서 보았을때 힘들 것이라고 생각한다. 개발자의 입장에서 본 DOM 조작DOM 조작의 양을 최소화하는 것이 좋다. 이 것은 곧 웹 페이지의 퍼포먼스로 직결되는 문제이며, 페이지에서의 일부변경이 전체 페이지의 렌더링되어버리면 매우 효율이 떨어지는 어플리케이션이 될 것이다.웹 응용 프로그램을 구축할 때 매우 중요한 개념이며 똑똑하고 최상의 방법은 필요한 부분만 re-painting하는 것이다. JavaScript Engine각 브라우저는 JavaScript engine을 가지고 있으며, 아래와 같이 각기 다른 engine을 사용하고 있다. (1) Chrome - ‘V8’(2) Edge - ‘Chalker Core’(3) Safari - ‘Nitro’(4) Firefox - ‘SpiderMonkey’ JavaScript engine은 웹 브라우저상에 동작하는 자바스크립트 파일을 읽는 역할을 한다.","link":"/2021/01/30/202101/210130-javascript-start/"},{"title":"Baekjoon Online Judge 11650번 x, y 좌표 정렬문제","text":"백준 저지 11650번 x, y 좌표 정렬문제 Pseudo code + Python code 1234567891011n = int(input())axis_list = []for _ in range(n): x, y = map(int, input().split()) axis_list.append((x, y))axis_list.sort(key=lambda x:(x[0], x[1]))# Tuple은 정렬을 해주면, 첫 번째 요소에 대해서 오름차순 정렬을 해주고, 첫 번째 요소가 같은 경우, 두 번째 요소에 대해서 자동으로 오름차순 정렬을 해준다.# 아래의 정렬은 위와 같은 결과를 갖는다.# axis_list.sort()for i in axis_list: print(i[0], i[1]) 이 문제는 리스트 내의 튜플 자료형을 활용하여 정렬을 하는 문제이다. 입력된 데이터를 순차적으로 정렬을 하는데, 첫번째 요소에 대해서 오름차순, 두번째 요소에 대해서 내림차순 등의 복합 조건 정렬을 하는 문제라면, 리스트에 입력 데이터를 튜플 자료형으로 삽입을 하여 정렬하면 유용하다.","link":"/2021/01/31/202101/210131-Algorithm_baekjoon_11650/"},{"title":"Baekjoon Online Judge 1668번 트로피 진열문제","text":"백준 저지 1668번 트로피 진열문제 Pseudo code + Python code Solution1) 123456789101112131415161718192021222324n = int(input())height_list = []for _ in range(n): t_height = int(input()) height_list.append(t_height)left_max_height = 0left_view_count = 0for h in height_list: if h &gt; left_max_height: left_max_height = h left_view_count += 1 else: breakright_max_height = 0right_view_count = 0for h in reversed(height_list): if h &gt; right_max_height: right_max_height = h right_view_count += 1print(left_view_count, right_view_count, sep='\\n') Solution2) 함수로 공통화처리 하기 12345678910111213141516171819def ascending(array): now = array[0] result = 1 for i in range(1, len(array)): if now &lt; array[i]: result += 1 now = array[i] return resultn = int(input())array = []for _ in range(n): array.append(int(input()))print(ascending(array))array.reverse()print(ascending(array)) 이 문제는 일렬로 정렬되어있는 트로피들을 왼쪽, 오른쪽 두 측면에서 보았을때 보이는 트로피의 갯수를 구하는 문제이다. 각 측면에서 트로피의 높이를 저장하고 있는 리스트를 순회하면서 최대 높이를 비교하며 카운트해주면 되는 간단한 문제이다.","link":"/2021/01/31/202101/210131-Algorithm_baekjoon_1668/"},{"title":"210131 Git Practice (Feature&#x2F;Release&#x2F;Hotfix)","text":"오늘은 저번 수업시간(29(금))에 배운 Git branch 관리와 협업에 대한 내용을 최대한 활용해서 연습을 해 볼 것이다. 물론 강사님 말처럼 입사하게 되면 main(master) branch, release branch를 건드릴 필요 없이, develop, feature branch만 가지고 작업을 하겠지만 개인 프로젝트할 때 버전별로 업데이트 해서 release도 해 볼 것이기 때문에 이를 위해 제대로 연습을 해 보겠다. Branching model : git flow Release branch 실습 Hotfix branch 실습 git flow 활용 꿀팁git flow를 활용해서 start, finish를 하게 되면 branch가 merge가 된 뒤에 자동으로 생성했던 branch를 삭제하고 merge된 branch로 switch해주다. 편리하긴 하지만 만약에 graph 구조상에 생성했던 branch에 대한 내용도 keep하고 싶다면 아래와 같이 간단하게 -k(eep)옵션을 붙여주면 된다. 1$ git flow feature finish -k [feature branch name] 성공적으로 develop branch에 feature branch를 merge하고 난 뒤에 feature branch를 제거하지 않는 것은 merge를 되돌리는 것을 손쉽게 할 수 있도록 도와준다.간단하게 merge commit을 제거함으로써 merge를 되돌릴 수 있다. 12# On develop branch$ git reset --hard HEAD^","link":"/2021/01/31/202101/210131-Git-practice/"},{"title":"Baekjoon Online Judge 1543번 문서 검색 문제","text":"백준 저지 1543번 문서 검색 문제 Pseudo code + Python code 손코딩한 코드에서 논리적 오류를 발견했다. document에서 검색하고자 하는 문자열의 길이만큼 slicing할때, 시작 인덱스(start_index)부터 검색 문자열의 길이(search_word)까지가 아닌, 시작 인덱스(start_index)에서 검색 문자열의 길이(len(search_word))만큼 더한 곳 까지 slicing한 문자열을 비교해야 한다. 123456789101112131415document = input()search_word = input()search_count = 0start_index = 0while (len(document)-start_index) // len(search_word) != 0:# 아래의 조건도 위의 조건을 대체할 수 있다.# while len(document)-start_index &gt;= len(search_word): if document[start_index:start_index+len(search_word)] == search_word: start_index += len(search_word) search_count += 1 else: start_index += 1print(search_count) 이 문제는 Brute force Algorithm (완전탐색) Brute Force Algorithm은 이름에서 유추해볼 수 있듯이 맹목적으로 모든 경우의 수를 탐색하여 결과를 도출하는 알고리즘이다. 이 알고리즘은 모든 경우의 수를 요구하는 문제를 풀때 사용된다.1부터 100까지의 합을 구하는 문제도 1부터 100까지 완전탐색을 하여 그 합을 구하는 방식으로 Brute Force 기법의 문제라고 할 수 있다.Brute Force 기법은 굉장히 단순한 기법이며, 문제해결에 있어서 첫번째로 고려되어야 할 기법이다.","link":"/2021/02/01/202102/210201-Algorithm_baekjoon_1543/"},{"title":"Baekjoon Online Judge 2212번 센서문제","text":"백준 저지 2212번 센서문제 Pseudo code + Python code 1234567891011sensor_num = int(input())station_num = int(input())sensor_loc_list = list(map(int, input().split()))sensor_loc_list.sort()distance = []for i in range(1, sensor_num): distance.append(sensor_loc_list[i]-sensor_loc_list[i-1])distance.sort(reverse=True)for i in range(station_num-1): distance[i] = 0print(sum(distance)) 이 문제는 설치하고자 하는 수신 기지국의 전파 수신 범위를 최소로 하기 위해서는 어떻게 수신 기지국을 설치를 하며, 최소한의 수신 범위는 어떻게 되는지 구하는 문제이다.고속도로 위의 각 센서의 설치 위치에 대해서 오름차순으로 정렬을 하고, 각 센서간의 사이 거리에 대한 값을 별도의 리스트에 담아 내림차순으로 정렬하였다. 이는 최소 수신 범위를 만들기 위해서 가장 큰 수신 범위를 제외하기 위해서 이다. 내림차순으로 정렬하여 기지국 N개를 설치할때 기지국의 수신구역을 나누는 분기점의 갯수 N-1개를 센서 사이 거리에 대한 리스트에서 순차적으로 0으로 초기화 시켜주면 된다.","link":"/2021/02/01/202102/210201-Algorithm_baekjoon_2212/"},{"title":"210201 The purpose of using pull request on my own repository.","text":"side project를 하면서 궁금해진 부분이 생겼다. 그래서 구글링을 하다가 나와 비슷한 고민을 가진 외국인 개발자분이 개발자 커뮤니티에 질문을 올려서 답변을 받은 내용이 있는데 어느정도 나의 궁금증을 해결하는데 도움이 되어 포스팅으로 글을 남겨 놓으려고 한다. https://softwareengineering.stackexchange.com/questions/178402/ 내가 문득 궁금해진 부분은 개인 프로젝트(Side project)를 하는데 내가 유일무이한 개발자라면 해당 Repository에서 작업을 할때 과연 Pull Request가 필요할까? 라는 점이었다.여러 팀원끼리 일을 하고, PR과 DEV로 구성되어있다면, 수정사항을 main branch에 반영할때 pull request를 통해 PR에게 코드리뷰를 받고 적절한 코드수정을 통해 merge되어야 한다. 그런데 과연 나 혼자 작업하는 개인 프로젝트인데 Pull request가 필요할까? 우선 여기에 대한 대답은 YES이다. 아래에 답변중에 채택을 받은 답변을 첨부했다. 이 답변의 서두에는 각 각의 개발자들이 스스로의 작업을 할때에 pull request를 생성하는 것은 아마 가치가 없는 일이지만, 한가지 잠재적인 이유로 해야 한다고 말한다. Pull requests는 나의 프로젝트의 history를 더욱 쉽게 추척하는데 사용이 될 수 있다. 그리고 각 각의 Pull request는 issue ID가 있는데 commit message와 changing-log를 통해서 참조 될 수 있다. 아래의 링크에서는 GitHub로 개인 프로젝트를 시작할때 참고하면 좋은 내용을 담고있다. https://medium.com/@cwlsn/github-personal-project-7d9d40b62e39 Github에서의 개인 프로젝트 진행Step1) 개인 프로젝트를 위한 Repository 준비 Step2) 개인 프로젝트를 위한 계획 수립 단계이 단계가 가장 까다롭고 힘들다. 프로젝트의 큰 그림을 작은 tasks와 함께 목표로 나눠서 계획해야 하기 때문이다. 전체적인 것을 당장 해야하는 것은 아니지만 어떻게 시작해야하는지에 대한 아이디어와 구상은 초반 코드작성을 위해서 어느정도 필요하다. Step3) GitHub issue를 생성코드 작성을 위해 editor를 열기전에 Github에서 issue를 작성하도록 한다.이 issue작성은 개인 프로젝트에 있어 나의 tasks를 관리 및 추적을 할 수 있고, 경우에 따라서 option을 추가작성함으로써 구체적인 task관리가 가능하게 해준다.GitHub는 여러 default labels을 제공해주는데, 개인의 취향에 다라 색상이나 이모티콘을 추가해 줄 수 있다.나 자신을 스스로 assignee로 할당을 해줄 수 있고, 훗날 history를 통해서 나의 avatar를 확인할 수 있다. issue 작성법 (1)Description - 어떤 issue를 해결할 것인지 서술(2)Acceptance Criteria - 어떠한 결과로 인해 해당 issue가 해결되었다고 고려되는지 서술(3)User persona - 왜 해당 feature가 요구되는지에 대한 이유와 해당 feature를 통한 user의 사용효과. 예를 들어 “a user of my utility class will be able to save time and effort with this function” 과 같이 짧게 작성을 해줄 수 있다. Step3) Branch 생성 작성한 코드의 작업은 Branch위에서 완성이 되어야 한다.branch 이름 작성에 있어 naming convention이 있는데, issue number로 시작해서 branch의 목적을 묘사하는 각 의미단위의 단어는 -(dash)로 연결해서 naming하는 것이다. 1$ git checkout -b 1-hello-world 작업이 완료되어 commit이 되었다면, local changes를 GitHub에 push해야 한다. 첫번째 push를 할때에는 아래와 같이 upstream으로 해줘야 한다.그 다음부터는 git push만 해줘도 충분하다. 1$ git push -u origin 1-hello-world Step4) Pull Request 생성 일단 생성했던 issu에 대한 코드 작업이 다 되었다면, 이제 Pull request를 작성해 줄 차례이다.이 단계에서는 작업을 통한 변화와 코드상에서 어떤 부분이 수정되었는지 확인할 수 있다.팀 단위로 일할때에는 reviewers를 할당하거나 다른 멤버들을 통해 내가 작성한 code에 대한 comment도 받을 수 있다. 만약 merge하고자 하는 Pull request가 issue #1과 관련되어 있다면, 간단하게 Fixes #1라고 작성을 해주면 된다. 만약 branch에서 작업을 통한 변화가 생겼다면 GitHub는 Pull Request를 생성할 것을 권고할 것이다.A sole personal project에 있어서 대부분의 작업 branch에 대한 pull requests는 master branch와 비교가 되고 merge된다.base: master &lt;- compare: 1-hello-world # 210206 업데이트Step5) Merge Pull Request merge할 준비가 되었다면, GitHub의 자동 issue close 기능을 포함해서 message를 수정해줘야 한다. Step6) README에 Issue page의 링크 추가해서 작성 README파일에 issue page의 링크를 추가해서 viewer로부터 issue를 참조할 수 있도록 해야한다. 이러한 GitHub관리는 단순히 코드를 cloning하거나 tracking하는 것을 넘어서서 더 진보적인 GitHub flow를 따르고 있다는 것을 보여 줄 수 있다. 210206 업데이트 간단하게 손으로 Git의 전체 흐름을 작성해보았다.","link":"/2021/02/01/202102/210201-Git-pull-request/"},{"title":"210201 Pandas TIL","text":"Pandas 이전에 데이터 시각화와 관련해서 관심이 많았아서 우연히 Pandas에 대해서 접하게 되었는데, 오늘 Python 수업관련해서 강사님이 Pandas에 대해서 말씀해주셔서 간단하게 Pandas에 대해서 조사해보았다. Pandas? pandas는 Python에서 데이터 분석 기능을 제공하는 라이브러리이다. 특히 몇 테이블 및 시계열 데이터를 조작하기위한 데이터 구조와 연산을 제공한다. Pandas는 BSD License하에 제공되고있다. 여기서 BSD License란, BSD(Berkeley Software Distribution)로, 자유 소프트웨어 저작권의 한 가지로 분류된다. 조만간 시간내서 간단하게 데이터 시각화하는 웹 기반의 애플리케이션을 개발해봐야겠다.","link":"/2021/02/01/202102/210201-Pandas/"},{"title":"210201 Python TIL - List&#x2F;Dictionary Comprehension","text":"List / Dictionary Comprehension**Python을 더 Python스럽게 작성하기위해서 이 Comprehension을 이해하고 적절히 잘 사용해야한다. List Comprehension 리스트의 각 요소에 2를 곱해서 새로운 리스트 생성하기 (for-loop + list comprehension) 1234567numbers = [1, 2, 3, 4, 5]new_numbers = []for i in numbers: new_numbers.append(i*2)print(new_numbers)# list comprehension을 사용해서 아래와 같이 간단하게 작성을 해줄 수 있다.new_numbers = [i*2 for i in numbers] 리스트의 요소중에 짝수인 수를 선별해서 해당 수에 제곱을 해서 새로운 리스트 생성하기 (for-loop + list comprehension) 12345678numbers = [1, 2, 3, 4, 5]new_numbers = []# numbers 자료 중 짝수일때만 제곱하여 new_numbers에 추가한다.for i in numbers: if i % 2 == 0: new_numbers.append(i**2)# 조건문이 하나일때에는 for문 뒤에 위치하도록 한다.new_numbers = [i**2 for i in numbers if i % 2 == 0] 리스트의 요소중에 짝수는 제곱을, 홀수인 경우에는 세제곱을 해서 새로운 리스트를 생성하기 (list comprehension + Ternary operator) 123456numbers = [1, 2, 3, 4, 5]new_numbers = []# numbers 자료 중 짝수일때만 제곱하여 new_numbers에 추가한다.# 그렇지 않을 경우, 세제곱하여 new_numbers에 추가한다.# expression내에 조건을 달아줄때에는 삼항연산자로 조건처리를 해준다.new_numbers = [item**2 if item % 2 == 0 else item**3 for item in numbers] FizzBuzz (for-loop &amp; if-statement) 1234567891011121314# Fizzbuzz# 3의 배수 : Fizz, 5의 배수 : Buzz, 15의 배수 FizzBuzznumbers = [i for i in range(1, 100+1)]fizzbuzz_list = []for i in numbers: if i % 15 == 0: fizzbuzz_list.append('FizzBuzz') elif i % 3 == 0: fizzbuzz_list.append('Fizz') elif i % 5 == 0: fizzbuzz_list.append('Buzz') else: fizzbuzz_list.append(i)# list comprehension으로 convert FizzBuzz (list comprehension + Ternary operator) 123456# fizzbuzz with list comprehensionnumbers = [i for i in range(1, 100+1)]fizzbuzz_list = ['fizzbuzz' if i%15==0 else \\ 'fizz' if i%3==0 else \\ 'buzz' if i%5==0 else \\ i for i in numbers] Leap year(list comprehension + for-loop, if-statement) 123456789101112131415# Leap year# Constraint: 1999~2101year_list = [i for i in range(1999, 2101+1)]# 100으로 나눠지면 leap year가 아니다.# 4로 나눠지고, (100으로 나눠지지 않거나, 400으로 나눠지는 경우)is_leap_list = []# 2000 = leap_year# 2100 = plain_yearyear = 2000for year in year_list: if year%4==0 and (year%100!=0 or year%400==0): is_leap_list.append('leap_year') else: is_leap_list.append(year) Leap year(list comprehension with Ternary operator) 12345678910# Leap year with list comprehension# Leap year: 4의 배수이면서, 100의 배수가 아니거나 400의 배수인 연도# Constraint: 1999~2101# Output: 'leap_year' if i is leap_year else, iyears = [i for i in range(1999, 2101+1)]is_leap_list = ['leap_year' if year%4==0 and (year%100!=0 or year%400==0) else year for year in years]# 2000 = leap_year# 2100 = plain_yearprint(is_leap_list) Functionalize the process of filtering leap year 12345678def is_leap(year): if year%4==0 and (year%100!=0 or year%400==0): return True return Falseis_leap_list = ['leap_year' if isi_leap(year) else year for year in years]print(is_leap_list) Dictionary Comprehension1fruits = {'a':'apple', 'b':'banana', 'c':'coconut', 'd':'durian'} 12345over_six_fruits = {}# fruite value 중에 문자열 길이가 6보다 큰 경우에 새로운 dict에 담아서 새로운 dict 변수를 생성for k, v in fruits.items(): if len(v) &gt; 6: over_six_fruits[k] = v 위의 코드를 Dictionary comprehension으로 작성 12# Dictionary comprehensionover_six_fruits = {k:v for k,v in fruits.items() if len(v) &gt; 6} Dictionary + for-loop with if-statement 123456number_dict = {'pi':3.1415, 'e':2.71828, 'year':2021, 'month':2, 'day':1}new_number_dict = {}for k,v in number_dict.items(): if v &lt; 3: new_number_dict[k] = v ** 2print(new_number_dict) 위의 코드를 Dictionary comprehension으로 작성 1234number_dict = {'pi':3.1415, 'e':2.71828, 'year':2021, 'month':2, 'day':1}# 아래와 같이 삼항연산자를 이용해서 조건처리를 해서 작성을 해 줄 수 있다.# 앞에서 key를 이미 정의해주었기 때문에 else구문에서는 key에 대한 정의를 생략하고 value에 대한 선언만 넣어준다.new_number_dict = {k:v**2 if v &lt; 3 else v-1 for k,v in number_dict.items()} 리스트의 각 문자열 요소의 길이를 구해서 새로운 리스트 변수 생성 (list comprehension + for-loop with map) 123456789101112a_word_list = ['aback','abacus','abandon','abandoned','abandonment','abashed','abate','abbey',]실행되는 순간에 나오는 것a_word_len_list = [len(s) for s in a_word_list]# dict.get()# dict.setdefault()a_word_len_list = []for len_ in map(len, a_word_list): a_word_len_list.append(len_) dict.get([key], [default value])와 dict.setdefault([key], [default value]) 활용 123456789#len_dict = {}# 현재 len_dict에 len(apple)를 key값으로 가지는 값이 없으면 해당 key에 해당하는 변수를 0으로 초기화해서 넣어주고, 1을 누적해서 더해준다.# 현재 leln_dict에 len(apple) 5라는 키가 있다면 기존 값에 1을 더해서 누적해준다.len_dict[len('apple')] = len_dict.get(len('apple'), 0) + 1# key로 len('amp')의 값인 3으로 조회를 했을때 값이 없으면 default로 0을 선언해주고, 초기값에 1을 누적해서 더해준다.# 존재한다면 기존 값에 1을 더해서 누적을 해준다.len_dict[len('amp')] = len_dict.get(len('amp'), 0) + 1 12345# 숙제) 위의 코드를 dictionary comprehension 으로 바꿔보기.len_dict = {}for word in a_word_list: len_dict[len(word)] = len_dict.get(len(word), 0) + 1print(len_dict) 1234567891011121314151617181920&quot;&quot;&quot;sample output:{ 5: ['apple', 'among']}&quot;&quot;&quot;filtered_dict = {}filtered_dict.setdefault(len('among'), []).append('among')filtered_dict.setdefault(len('among'), []).append('among')filtered_dict.setdefault(len('among'), []).append('amp')# 단어의 길이를 기준으로 단어들을 분류for word in a_word_list: filtered_dict.setdefault(len(word), []).append(word)# 첫번째 알파벳 기준으로 단어들을 분류for word in a_word_list: filtered_dict.setdefault(word[0], []).append(word)print(filtered_dict) File I/O 파일을 열고(open), 쓰고(write), 읽기(readline) 실습 123456# numbers.txt 파일 열고 쓰기 설정 및 문자열 encoding 방식을 'utf-8'으로 설정f = open('./numbers.txt', 'w', encoding='utf-8')# 파일에 hello 쓰기f.write('hello')# file 객체의 life-cycle 종료f.close() 12345678# numbers.txt 파일 열고 읽기 설정 및 문자열 encoding 방식을 'utf-8'으로 설정f = open('./numbers.txt', 'r', encoding='utf-8')# 파일의 한 줄을 읽고(readline) text 변수에 담기text = f.readline()# 파일로부터 읽은 text 변수를 출력print(text)# file 객체의 life-cycle 종료f.close() 위에서는 open으로 파일을 연 후에 그 내용을 표시하고 close하였다. 아래 코드는 with을 활용한 코드 작성으로, 바로 옆에 수행 할 함수를 명시하고 alias로 f를 해주었다.with 구문을 사용하면 명시적 클로즈 처리가 불필요하게 되므로 close를 안하는 실수를 사전에 방지할 수 있다. 123with open('./numbers.txt', 'w', encoding='utf-8') as f: for _ in range(10): f.write('hello') 123456with open('./numbers.txt', 'r', encoding='utf-8') as f:# 복수의 줄을 읽을때에는 .readlines() text_list = f.readlines() # 한 줄을 읽을때에는 .readline() for line in text_list: print(line.replace('\\n','')) 12345678910111213# append mode의 사용# append mode를 사용하는 경우, 이전 데이터의 EOF(End Of File)부터 이어서 추가 데이터가 붙기 때문에 데이터를 입력할때 enter를 쳐서 개행(\\n) 처리를 하고나서 그 다음 append할 데이터에 대해서 append mode를 실행해줘야 한다.# 이전에 삽입된 데이터 끝에 \\n 가 없는 경우, 다음 append mode로 추가되는 데이터는 이전에 삽입된 데이터의 끝에 붙는다.numbers = [i for i in range(1,10+1)]with open('./numbers.txt', 'a', encoding='utf-8') as f: for i in numbers: # file 객체를 사용해서 데이터를 쓸때에는 반드시 str 타입으로 입력을 해주어야 한다. f.write('{}\\n'.foramt(i))with open('./numbers.txt', 'w', encoding='utf-8') as f: print(f.readlines()) CSV module : CSV File Reading and Writing Python documentation : CSV module reference. 123456789101112131415import csvwith open('customers.csv', newline='') as customer_csv: customers = csv.reader(customer_csv) for row in customers: print(row) &quot;&quot;&quot; ['CustomerID', 'CustomerName', 'ContactName', 'Address', 'City', 'PostalCode', 'Country'] ['1', 'Alfreds Futterkiste', 'Maria Anders', 'Obere Str. 57', 'Berlin', '12209', 'Germany'] ['2', 'Ana Trujillo Emparedados y helados', 'Ana Trujillo', 'Avda. de la Constitución 2222', 'México D.F.', '05021', 'Mexico'] ['3', 'Antonio Moreno Taquería', 'Antonio Moreno', 'Mataderos 2312', 'México D.F.', '05023', 'Mexico'] ['4', 'Around the Horn', 'Thomas Hardy', '120 Hanover Sq.', 'London', 'WA1 1DP', 'UK'] ['5', 'Berglunds snabbköp', 'Christina Berglund', 'Berguvsvägen 8', 'Luleå', 'S-958 22', 'Sweden'] ........ &quot;&quot;&quot; JSON (dump, load 주로 사용) Python documentation : json module reference. 123456789101112131415161718192021222324252627import jsondata = {'users': [ {'name': 'KD Hong', 'locale': 'Seoul, KR'}, {'name': 'John Doe', 'locale': 'New York, US'}, {'name': 'Jane Doe', 'locale': 'London, UK'}]}with open('users.json', 'w', encoding='utf-8') as f: json.dump(data, f)with open('users.json', 'r', encoding='utf-8') as f: # deserialize des_data = json.load(f)print(des_data)&quot;&quot;&quot;{'users': [{'name': 'KD Hong', 'locale': 'Seoul, KR'}, {'name': 'John Doe', 'locale': 'New York, US'}, {'name': 'Jane Doe', 'locale': 'London, UK'}]}&quot;&quot;&quot;for row in des_data['users']: print('{} is from {}'.format(row['name'], row['locale']))&quot;&quot;&quot;KD Hong is from Seoul, KRJohn Doe is from New York, USJane Doe is from London, UK&quot;&quot;&quot;","link":"/2021/02/01/202102/210201-Python_til/"},{"title":"Baekjoon Online Judge 1966번 프린터 큐 문제","text":"백준 저지 1966번 프린터 큐 문제 Pseudo code + Python code 1234567891011121314151617test_case = int(input())for _ in range(test_case): task_num, search_index = map(int, input().split()) test_priority_list = list(map(int, input().split())) print_task_list = [(p, i) for i, p in enumerate(test_priority_list)] print_task_count = 0 while True: if print_task_list[0][0] == max(print_task_list, key=lambda x: x[0])[0]: print_task_count += 1 if print_task_list[0][1] == search_index: print(print_task_count) break else: print_task_list.pop(0) else: print_task_list.append(print_task_list.pop(0)) 이 문제는 프린트의 task에 priority를 부여하여, 기존에 FIFO 정책으로 동작하는 프린트를 우선순위 우선 방식으로 프린트가 작동하도록 만드는 프로그램을 작성하는 문제였다. max 내에서 tuple의 첫 번째 요소(우선순위 값)를 기준으로 최대값을 찾고 [0] (priority) 값이 우선인 프린트를 우선적으로 프린트 될 수 있도록 작성하는 부분이 포인트이다.","link":"/2021/02/02/202102/210202-Algorithm_baekjoon_1966/"},{"title":"210202 ExpressJS Installation","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 NodeJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. ExpressJS 설치NPM(Node Package Manager) : 업데이트된 각종 NodeJS 관련 패키지의 중앙집중화된 개념으로 NodeJS 월드의 중심과 같은 곳이다. NPM은 NodeJS를 다운받게 되면 자동으로 설치가 되기 때문에 별도로 설치할 필요는 없다. Package manager로 npm을 사용하기 위해서는 우선 npm이 정한 방식으로 프로젝트를 시작해야한다. npm init : npm 초기화 (package name/description/author 설정)→ 결과적으로 package.json 파일이 생성이 된다. npm 설치 : ‘npm install express’ 명령실행한다.(package.json 파일이 있는 폴더위치에서 실행을 해야한다) 설치가 끝난뒤에 살펴보아야하는 것은 node modules 부분이다. 이 부분은 npm을 통해서 다운을 받은 것들인데 여기보면 dependency라는 것을 갖게 된다.어플리케이션 개발시에 필요한 것들을 자동으로 npm module에서 찾아서 사용해준다. 프로젝트 협업시에는 작성한 코드와 Package.json 파일만 넘겨주기 두 파일만 받아서 프로젝트 폴더 내부에서 npm install만 실행해주면 자동으로 필요한 파일들을 다시 전부 생성해준다.","link":"/2021/02/02/202102/210202-ExpressJS_Installation_and_Setting/"},{"title":"210202 NodeJS Express Framework","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 NodeJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. ExpressJS?ExpressJS는 간단히 말하면 훌륭한 개발자들이 만들어준 NodeJS 프레임워크이다. 이를 사용해서 우리는 원하는 걸 쉽고 빠르게 해낼 수 있다. 예를 들어 NodeJS로 서버를 만드는 것이 목표라면 수작업으로 초반작업을 해줄 필요가 있는데 Express를 이용해서 단 몇 줄의 코드로 서버를 만들 수 있다. Framework다른 프로그래밍 언어에도 프레임워크가 있다. 예를 들어 Django는 Python으로 된 프레임워크이고, Rails는 Ruby로 된 프레임워크이고, Laravel은 PHP로 된 프레임워크이다. 이러한 프레임워크들은 개발자들의 개발을 손쉽게 할 수 있도록 도와준다. 서버를 개발하는 것은 대부분의 경우 거의 동일한 패턴을 가지고 있다. Connection 열기, Connection Listening, 파일을 처리, html 전송, 데이터를 저장, form에서 데이터를 받아오기와 같은 일련의 과정의 반복이라고 볼 수 있다. 모든 어플리케이션이 대부분 동일한 일을 하기때문에 앞서 언급한 프레임워크는 다른 방식이지만 결국에는 같은 일들을 처리한다고 볼 수 있다. ExpressJS는 NodeJS의 프레임워크 중에 대중적인 프레임워크이고, 안정적이다. ExpressJS Reference : ExpressJS Reference ExpressJS를 사용하는 일부 기업들은 아래와 같다.","link":"/2021/02/02/202102/210202-NodeJS_Express_Framework/"},{"title":"210202 NodeJS Installation","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 NodeJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. NodeJS 설치나는 Mac 사용자이기 때문에 MacOS 기준으로 설치내용을 정리해보겠다. NodeJS는 간단하게 brew로 설치가 가능하다 brew install node NodeJS Installation Guide : NodeJS Installation Guide","link":"/2021/02/02/202102/210202-NodeJS_installation/"},{"title":"210202 NodeJS의 기본 개념과 사용","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 NodeJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. NodeJS JavaScript는 브라우저에 내장이 되어있고, 브라우저상에서 동작한다. NodeJS는 이 JavaScript를 browser 밖으로 끄집어내서 유저가 browser 밖에서도 JavaScript를 사용가능하도록 한 것이다.NodeJS = 브라우저 밖의 JavaScript를 의미 따라서 NodeJS는 컴퓨터상에서 동작하기 때문에 JavaScript를 이용해서 FileSystem에 접근을 할 수 있고, 서버를 만들 수도 있고, WebScrapping도 할 수 있다. JavaScript를 브라우저와 독립적으로 활용할 수 있도록 해주는 것이 NodeJS의 주된 개념이다. 기존에 간단한 JavaScript compile의 경우, chrome inspector에서 했었지만, NodeJS를 설치하면 위와 같이 PC의 로컬환경에서도 JavaScript코드를 compile해 볼 수 있다. Browser와의 종속성을 깼다는 점에 있어 JavaScript언어의 가능성이 넓어졌다.NodeJS의 선택과 사용 Back-end, Sever를 Build해야하는 경우, Django(Python), Laravel(PHP) 등 다양한 언어와 프레임워크가 있지만 JavaScript에 능숙하고 Front-end와 Back-end를 모두 JavaScript로 만들고 싶다면 NodeJS를 선택한다. 두번째 만약 제한된 시간이 한정되어있고, 프로젝트가 빨리 시작되어야 한다면, 그리고 거의 모든 것을 다 customize할 수 있는 능력을 가지고 있다면 NodeJS를 선택한다. 하지만 상대적으로 거의 대부분이 Configuration되어있는 것을 원한다면 Django나 Laravel을 선택하는 것이 좋다.왜냐하면 NodeJS는 아무것도 들어있지 않다. 작은 블럭들을 위로 쌓아서 큰 성을 짓는 것이 바로 NodeJS를 활용해서 개발을 하는 것을 의미한다.Django의 경우는 이미 완성되어 있는 큰 성과 같아서 전체적인 사용법을 이해하고 이용을 해야하는데 node.js는 그와 다르게 완전 아무 것도 없이 무에서 시작해서 모든 것을 하나씩 붙여가면서 개발을 한다. 세번째로 많은 데이터를 다뤄야 할때 NodeJS를 선택한다.database를 생성하고 삭제, 사용자에게 전송하고 저장하는 작업과 같이 데이터를 다루는 성능에 있어서는 NodeJS는 성능이 매우 좋다.(Data Science를 말하는 것이 아니라 유저들 간에 메시지를 전송하고 하는 것과 같은 실시간 처리 데이터를 의미한다)대신 Hardware쪽을 고려해서 개발을 해야한다면, JavaScript는 Hardware의 memory에 접근할 수 없기 때문에 NodeJS는 전혀 도움이 되지 않는다. NodeJS의 현 주소그럼 누가 어느 기업이 NodeJS를 사용하고 있을까? 거의 모든 사람이 아는, 그리고 사용하는 Paypal, Uber, Netflix, Linkedin, Walmart, Trello, New York Times, … 이들의 모든 서비스들이 모두 NodeJS를 사용하여 만들어졌다. 이들의 서비스들이 NodeJS로 만들어진 이유는 앞서 말한 조건들 중에 실시간 처리 데이터를 다루기 때문에 사용되었다. 웹 서비스를 만들때 꼭 한 가지 언어로만 Back-end를 구성할 필요는 없다. Back-end 개발시에 여러 언어를 복합해서 쓸 수 있다. 물론 메인 언어는 존재하지만 다양한 언어로 만들 수 있다.Netflix의 경우는 엄청 많은 언어로 만들어졌다. 그 이유는 영상을 처리해야하기 때문이다. 영상을 압축하고 스트리밍하고 다운로드하고 다양한 처리들을 가능하게 하려면 한 가지 언어로는 어렵기 때문이다.Paypal의 경우는 금융거래를 다루기때문에 은행과 연결할때는 Java를 사용하고 웹 사이트를 개발할때는 프론트에서는 JavaScript, 백에서는 다양한 언어들이 사용된다.수많은 회사에서 NodeJS로 Back-end를 돌리고 있고 그 만큼 많은 수요를 필요로 하고 있다. 이 의미는 수 없이 많은 웹 서비스 회사에서 검증이 되었고 사용이 되고 있다는 의미이다S.","link":"/2021/02/02/202102/210202-NodeJS_start/"},{"title":"210202 Plotly + Pandas Project","text":"Pandas 방대한 양의 데이터를 가져와서 데이터를 깔끔하게 만들고, 처리하고, 구조를 변경(데이터 가공)하는데 사용된다. 데이터 분석과 처리를 쉽게 할 수 있도록 도와준다. 1$ pip install pandas Plotly 모든 그래픽(그래프)를 만드는데에 사용된다. No JavaScript required 1$ pip install dash==1.14.0 Project settingvirtualenv &amp; virtualenvwrapper 설치 및 환경설정 virtualenvwrapper는 virtualenv 도구에 대한 확장 세트이기 때문에 우선적으로 virtualenv를 설치해줘야 한다. 1$ pip install virtualenv virtualenv의 위치에 대한 환경설정을 해줘야 한다. 1$ export VIRTUALENVWRAPPER_VIRTUALENV=/usr/local/bin/virtualenv virtualenvwrapper reference virtualenvwrapper : virtualenvwrapper는 lan Bicking의 virtualenv 도구에 대한 확장 세트이다. 확장된 기능에는 가상 환경을 생성 및 삭제하고 개발 work flow를 관리하기 위한 wrapper가 포함되어있어 종속성에 충돌을 일으키지 않고 한 번에 둘 이상의 프로젝트에서 더 쉽게 작업을 할 수 있다. 1234567891011121314151617$ pip install virtualenvwrapper...$ export WORKON_HOME=~/Envs$ mkdir -p $WORKON_HOME$ source /usr/local/bin/virtualenvwrapper.sh$ mkvirtualenv env1Installingsetuptools.................................................................................................................................................................................done.virtualenvwrapper.user_scripts Creating /Users/dhellmann/Envs/env1/bin/predeactivatevirtualenvwrapper.user_scripts Creating /Users/dhellmann/Envs/env1/bin/postdeactivatevirtualenvwrapper.user_scripts Creating /Users/dhellmann/Envs/env1/bin/preactivatevirtualenvwrapper.user_scripts Creating /Users/dhellmann/Envs/env1/bin/postactivate New python executable in env1/bin/python(env1)$ ls $WORKON_HOMEenv1 hook.log zsh환경설정 파일에 virtualenv 관련 환경설정 넣기 (~/.zshrc) 12345678# Setting PATH for Python 3 installed by brewexport PATH=/usr/local/share/python:$PATH# Configuration for virtualenvexport WORKON_HOME=$HOME/.virtualenvsexport VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3export VIRTUALENVWRAPPER_VIRTUALENV=/usr/local/bin/virtualenvsource /usr/local/bin/virtualenvwrapper.sh virtualenv setting env1 가상 환경에 설치된 패키지 리스트 확인하기 1(env1)$ lssitepackages 가상환경 간 전환하기 1$ workon env2 가상환경으로 설정을 변경하면 VSCode에서 Python관련 파일을 생성시, 자동으로 Interpreter설정을 바꾸는 알람이 뜨는데, 이곳에서 가상환경의 Python interpreter를 선택할 수 있다.~/.virtualenvs/env1/bin/python Jupyterlab 설치1$ pip install jupyterlab","link":"/2021/02/02/202102/210202-Plotly_Pandas/"},{"title":"210202 Python TIL - Error exception, RegEx, Lambda, Function, map, filter, reduce","text":"Section0) Assignment &amp; Review HackerRank 문제 (List comprehension) 123x, y, z, n = (int(input()) for _ in range(4))result_ = [[i, j, k] for i in range(x+1) for j in range(y+1) for k in range(z+1) if (i+j+k) is not n]print(result_) Section0-1)zip() 활용1234numbers = [i for i in range(1, 10+1)]chars = list(set('abcdefghij'))for i,c in zip(numbers, chars): print(i,c) Section0-2)File I/O 실습 readlines, csv.reader()로 csv파일 읽기readlines() : readlines()로 파일을 읽으면, 아래와 같이 row 데이터가 구분자(, comma)로 구분되어 출력이 된다. 1234567with open('customers.csv', 'r', encoding='utf-8') as f: rows = f.readlines() for row in rows: print(row) # output : # CustomerID,CustomerName,ContactName,Address,City,PostalCode,Country # 1,Alfreds Futterkiste,Maria Anders,Obere Str. 57,Berlin,12209,Germany csv.reader() : csv.reader()를 사용해서 csv 파일을 읽으면 아래와 같이 row가 list형으로 출력이 된다.import csv 12345678with open('customers.csv', 'r', encoding='utf-8') as f: customers = csv.reader(f) # reader object for row in customers: # print(row) # output : # ['CustomerID', 'CustomerName', 'ContactName', 'Address', 'City', 'PostalCode', 'Country'] # ['1', 'Alfreds Futterkiste', 'Maria Anders', 'Obere Str. 57', 'Berlin', '12209', 'Germany'] customers.csv 파일을 읽어서 customers.json파일로 추출하기 12345678910111213141516171819202122232425# customers.csv -&gt; customers.jsonimport csvimport json# csv 파일을 읽어서 아래와 같이 dictionary형으로 각 row의 데이터를 key-value 로 삽입을 해준다.# {'users':[# {},# {},# ]}users = {'users': []}with open('customers.csv', 'r', encoding='utf-8') as f: customers = csv.reader(f) customer_list = list(customers) # list -&gt; dictionary for row in customer_list[1:]: user_dict = {} for index, key in enumerate(customer_list[0]): user_dict[key] = row[index] users['users'].append(user_dict)with open('customers.json', 'w', encoding='utf-8') as f: json.dump(users, f) Exception(Error handling, try-except), RegEx, Lambda Function Section1) Error handling(Exception, try-exception) (예외처리)Section1-1) Exception 처리의 기본 format1234try: 실행문except: 실행문 1234try: passexcept: pass Section1-2) 예외처리하기1234try: result = 3/0except: print('You could divide by anything, not zero.') recursive하게 에러구문 처리하기 123456789def get_num_from_user(): try: guess = int(input('Guess the number')) return guess except: print(&quot;Plz, don't bother programmer.&quot;) return get_num_from_user()get_num_from_user() Section2) RegEx (정규표현식) 특정한 규칙을 가진 문자열의 집합을 표현하는데 사용하는 형식언어 Python은 re module로 정규표현식의 사용이 가능 기본적인 문법은 비슷하나 언어별로 사용법이나 문법이 조금씩 달라서 외우기 보다는 Reference를 참조하는 것이 좋다. Python RegEx Reference : Python RegEx Reference Section2-1) Regular Expressions - match()123456789import rea = 'penpineapple'b = 'applepen'# r: regex string 구분자# match([regex pattern], [check string])m = re.match(r'^pen', a)n = re.match(r'.+pen$', b)print(m,n) Section2-2) Regular Expressions - match object12345# match한 string을 뽑아낼때에는 group()을 사용한다.m.group() # 'pen'm.span() # (0, 3)# group()말고 m.start(), m.end()사용해서 substring해보기a[m.start():m.end()] # 'pen' Section2-3) Regular Expressions - Meta Characters .(Dot) : Any character (except newline character)줄바꿈 문자(\\n)을 제외한 모든 문자와 match됨을 의미한다. 12345678a.b # a + 모든문자 + b # a와 b라는 문자 사이에는 어떤 문자가 들어가도 모두 matching됨을 의미&quot;&quot;&quot;&quot;aab&quot;, &quot;a0b&quot;, &quot;abc&quot;가 정규식 a.b 검사를 했을때,- &quot;aab&quot;는 가운데 문자 &quot;a&quot;가 모든 문자를 의미하는 .과 일치하므로 정규식과 매치된다.- &quot;a0b&quot;는 가운데 문자 &quot;0&quot;가 모든 문자를 의미하는 .과 일치하므로 정규식과 매치된다.- &quot;abc&quot;는 &quot;a&quot;문자와 &quot;b&quot;문자 사이에 어떤 문자라도 하나는있어야 하는 이 정규식과 일치하지 않으므로 매치되지 않는다.&quot;&quot;&quot; 12345a[.]b # &quot;a + Dot(.)문자 + b&quot;# 따라서 정규식 a[.]b는 &quot;a.b&quot; 문자열과 매치되고, &quot;a0b&quot; 문자열과는 매치되지 않는다.&quot;&quot;&quot;※ 만약 앞에서 살펴본 문자 클래스([]) 내에 Dot(.) 메타 문자가 사용된다면 이것은 &quot;모든 문자&quot;라는 의미가 아닌 문자 . 그대로를 의미한다. 혼동하지 않도록 주의하자.&quot;&quot;&quot; *(Repetition) : Zero or more occurrences * 바로 앞에 있는 특정 문자가 0부터 무한대로 반복될 수 있다는 의미한다. ca*t에서 *앞에 a라는 문자가 있고, 이 문자가 0~무한대로 반복하는 조건을 만족한다면 match된다. 123456&quot;&quot;&quot;정규식 문자열 Match 여부 설명ca*t ct Yes &quot;a&quot;가 0번 반복되어 매치ca*t cat Yes &quot;a&quot;가 0번 이상 반복되어 매치 (1번 반복)ca*t caaat Yes &quot;a&quot;가 0번 이상 반복되어 매치 (3번 반복)&quot;&quot;&quot; +(Repetition) : One or more occurrences 위에서 살펴본 반복을 나타내는 *외에 또 다른 반복을 나타내는 메타 문자로 +가 있다. +는 최소 1번 이상 반복될 때 사용한다. 즉 *가 반복 횟수 0부터라면 +는 반복 횟수 1부터인 것이다. 123456789# ca+t 정규식이 있다고 가정했을때,# &quot;c + a(1번 이상 반복) + t&quot;을 의미한다.&quot;&quot;&quot;정규식 문자열 Match 여부 설명ca+t ct No &quot;a&quot;가 0번 반복되어 매치되지 않음ca+t cat Yes &quot;a&quot;가 1번 이상 반복되어 매치 (1번 반복)ca+t caaat Yes &quot;a&quot;가 1번 이상 반복되어 매치 (3번 반복)&quot;&quot;&quot; {}(Curly brackets) : Exactly the specified number of occurrences반복 횟수를 3회만 또는 1회부터 3회까지만으로 제한하고 싶을때 {}을 사용해서 범위를 지정해 줄 수 있다. { } 메타 문자를 사용하면 반복 횟수를 고정할 수 있다. {m, n} 정규식을 사용하면 반복 횟수가 m부터 n까지 매치할 수 있다. 또한 m 또는 n을 생략할 수도 있다. 만약 {3,}처럼 사용하면 반복 횟수가 3 이상인 경우이고 {,3}처럼 사용하면 반복 횟수가 3 이하를 의미한다. 생략된 m은 0과 동일하며, 생략된 n은 무한대(2억 개 미만)의 의미를 갖는다. ※ {1,}은 +와 동일하고, {0,}은 *와 동일하다. 1234567# ca{2}t 정규식이 있다고 가정했을때# &quot;c + a(반드시 2번 반복) + t&quot;를 의미한다.&quot;&quot;&quot;정규식 문자열 Match 여부 설명ca{2}t cat No &quot;a&quot;가 1번만 반복되어 매치되지 않음ca{2}t caat Yes &quot;a&quot;가 2번 반복되어 매치&quot;&quot;&quot; ?(Question mark) : 반복은 아니지만 이와 비슷한 개념이다. 메타 문자가 의미하는 것은 {0,1}이다. 0번부터 1번까지 ?앞의 문자가 사용이 되었다면 match되는 경우로 본다. 12345&quot;&quot;&quot;정규식 문자열 Match 여부 설명ab?c abc Yes &quot;b&quot;가 1번 사용되어 매치ab?c ac Yes &quot;b&quot;가 0번 사용되어 매치&quot;&quot;&quot; Section2-4) Regular Expressions 사용하기 123456789# 정규식 표현을 compile해두면 regular expression object를 재사용하는 것이 가능하다.apples = ['a'+'p'*i+'le' for i in range(10)]# 미리 regex를 compile해서 재사용 할 수 있다.apple_finder = re.compile(r'^ap{2}le$')for item in apples: m = apple_finder.match(item) print(itme, m) ^(Caret) : Starts with 1234a = 'penpineapple'b = 'applepen'c = 'pen'm = re.match(r'^.*(pen)$', b) # (0, 3) ()(Parentheses) : Capture and group 123fastcampusr'(cam)' : cam이 연속 순서로 나와야한다.r'(fastcampus)' |(Vertical bar) : Either or 1r'.*(fast|campus).*' : fast나 campus가 등장을 하는 경우 (Backslash): Signals a special sequence (can also be used to escape special characters) 1r'[A-Za-z0-9\\-\\_\\.\\+]@gmail\\.com' # alphabet, number, -, ., _, + [](Square brackets) : A set of characters 123456789r'[A-Za-z0-9\\-\\_\\.\\+]@gmail\\.com' # alphabet, number, -, ., _, +# 위의 정규표현식은 한글자의 username@gmail.com 만 작성할 수 있다.r'[A-Za-z0-9\\-\\_\\.\\+]*@gmail\\.com' # alphabet, number, -, ., _, +# 위와같이 *를 붙여서 주면, 1개이거나 없는 경우를 허용하므로, 이 경우는 사용하지 않는다.r'[A-Za-z0-9\\-\\_\\.\\+]+@gmail\\.com' # alphabet, number, -, ., _, +# 위와같이 +를 붙여서 앞의 사용자 이름(아이디)가 1개 이상 있다는 것을 표현할 수 있다.r'[A-Za-z0-9\\-\\_\\.\\+]{n,m}@gmail\\.com' # alphabet, number, -, ., _, +# 위와같이 사용자 이름(아이디)의 글자수 범위(n~m)를 지정할 수 있다.# {4, 32} 보통 2의 배수인 4부터 32까지의 범위로 username을 잡는다. $(Dollar sign) : Ends with Section2-5) re module functions파이썬은 정규 표현식을 지원하기 위해 re(regular expression)모듈을 제공한다. re module은 파이썬을 설치할 때 자동으로 설치되는 기본 라이브러리이다. 12import rep = re.compile('ab*') re.compile을 사용하여 정규 표현식을 컴파일한다. re.compile()의 결과로 돌려주는 객체 p(compile된 패턴 객체)를 사용하여 그 이후의 작업으르 수행할 수 있다. 12import rep = re.compile('[a-z]+') re.match(patterns, string[, flags]) : 문자열의 처음부터 정규식과 매치되는지 조사한다. 12345678m = p.match(&quot;python&quot;)# Python 문자열은 [a-z]+ 정규식에 부합되므로 match 객체를 돌려준다.print(m)&lt;_sre.SRE_Match object at 0x01F3F9F8&gt;m = p.match(&quot;3 python&quot;)print(m)# output : None match의 결과로 match 객체 혹은 None을 반환해주기 때문에 Python 정규식 프로그램은 보통 아래와 같은 흐름으로 작성한다. 1234567p = re.compile('[regular expression]')m = p.compile('[string goes here]')if m: # match의 결과값이 있을때에만 다음의 작업을 수행 print('Match found: ', m.group())else: print('No match') re.search(patterns, string[, flags]) : 문자열 전체를 검색하여 정규식과 매치되는지 조사한다. 123m = p.search(&quot;python&quot;)print(m)&lt;_sre.SRE_Match object at 0x01F3FA68&gt; 123m = p.search(&quot;3 python&quot;)print(m)&lt;_sre.SRE_Match object at 0x01F3FA30&gt; “3 python”문자열의 첫 번째 문자는 “3”이지만, search는 문자열의 처음부터 순차검색을 하는 것이 아니라 문자열 전체를 검색하기 때문에 “3” 이후의 “python” 문자열과 매치된다. 이러한 match와 search 매서드는 문자열의 처음부터 검색할지 말지의 여부에 따라서 다르게 사용해야 한다. match, search는 정규식과 매치될 때는 match된 객체를 돌려주고, match되지 않을 때는 None을 돌려준다. re.findall(patterns, string[, flags]) : 정규식과 매치되는 모든 문자열(substring)을 리스트로 돌려준다. 1234result = p.findall(&quot;life is too short&quot;)print(result)# ['life', 'is', 'too', 'short']# &quot;life is too short 문자열의 각 단어르르 각각 [a-z]+ 정규식과 match해서 리스트로 반환한다. re.finditer(patterns, string[, flags]) : 정규식과 매치되는 모든 문자열(substring)을 반복 가능한 객체로 돌려준다. 12345678910111213result = p.finditer(&quot;life is too short&quot;)print(result)&lt;callable_iterator object at 0x01F5E390&gt;for r in result: print(r)&lt;_sre.SRE_Match object at 0x01F3F9F8&gt;&lt;_sre.SRE_Match object at 0x01F3FAD8&gt;&lt;_sre.SRE_Match object at 0x01F3FAA0&gt;&lt;_sre.SRE_Match object at 0x01F3F9F8&gt;# finditer는 findall과 동일하지만 그 결과로 반복이 가능한 객체(iterator object)를 반환해준다. 반복 가능한 객체가 포함하는 각 각의 요소는 match의 객체이다.map(lambda x: x.group(), result) 로 작성을 해주면, for-loop을 활용해서 각각의 객체를 출력할 수 있다. re.compile(pattern[, flags]) : 주어진 정규식 pattern을 compile해서 재사용 가능한 객체로 만든다. re.split(patterns, string[,maxsplit=0]) re.sub(pattern, repl, string[, count]) : sub(stands for substitution) 주어진 문자열을 pattern으로 평가해서 match되는 요소에 두 번째 인자로 넣어준 function(or lambda)를 적용한다. 123456789import redef square(match): number = int(match.group(0)) return str(number**2)print(re.sub(r&quot;\\d+&quot;, square, &quot;1 2 3 4 5 6 7 8 9&quot;))#output : 1 4 9 16 25 36 49 64 81 Section3) Lambda Function (람다함수) 익명함수 (이름없는 함수) 간단한 수식을 함수로 지정해서 한 두번 쓸 용도로 사용 두 줄 이상 실행되는 함수는 별도로 함수로 선언해서 사용 코드의 가독성(Readibility)와 재사용성 고려하여 사용 Section3-1) Traditional function1234567# 최근 python에서는 parameter type과 return type을 지정해 줄 수 있다.def get_next_integer(i:int -&gt; int):def get_next_integer(i): return i + 1print(get_next_integer(10)) # 11 Section3-2) lambda - lambda function123# (lambda function)(argument)(lambda a: a+1)(10) # 11(lambda a,b: a+b+1)(10,11) # 22 Section3-3) map, filter, reduce의 사용 map의 사용 map의 기본 format12map(function, iter)# list의 각 element에 대해 특정한 함수를 적용 사용예시 12345678numbers = [i for i in range(1, 10+1)]def int_plus_one(a): return a + 1list(map(int_plus_one, numbers))# lambda functionlist(map(lambda a:a+1, numbers)) map은 generator를 생성해서 함수와 인자를 바인딩하고, 필요할 때 순차적으로 처리 1234def do_square(num): return num ** 2list(map(do_square, numbers)) 1list(map(lambda a:a**2, numbers)) 수업시간에 map의 사용까지 (2020/02/02) Section4) Variable &amp; Function in memory Lambda Function의 경우에는 힙(heap)에 저장이 되고 사용이 된다. 한 번 사용되고 제거된다. (일회성)Python은 Garbage collector를 가지고 있기 때문에 힙(Heap)의 메모리를 자동으로 관리해준다. 일반적으로 Function의 선언의 경우 스택(Stack)에 저장이 되고 사용이 된다.(FILO(First-In-Last-Out), LIFO(Last-In-First-Out))","link":"/2021/02/02/202102/210202-Python_til/"},{"title":"210203 Git Stash Practice","text":"Git Stash Practicebranch를 끊어서 작업을 하는데, 일부 작업한 내용을 잠시 stage에 올려서 다른 branch에서 작업을 하고자 할때 stash를 활용할 수 있다. 아직 파일의 수정이 끝나지 않아서 commit 하기가 곤란하고, revert로 인해 다른 branch로 checkout을 하기도 곤란한 상황을 처리할 때 git stash를 사용한다. git stash 명령은 git add를 통해서 트래킹 중인 파일에 대해서만 사용이 가능하다. 새로운 stash 생성1234# saved to `refs/stash`$ git stash or$ git stash save 생성된 stash 리스트 확인1$ git stash list 생성된 stash 제거1$ git stash drop stash@{n} git stash apply git stash를 사용해서 stage에 올렸던 수정중인 파일의 상태를 원 파일상태로 복구하는 방법 1$ git stash apply 가장 최신의 commit 상태로 working space의 상태를 변경commit 하지 않은 수정중인 파일 내용을 삭제한다. 1$ git reset -hard 123456789101112131415161718$ git reset --hard HEAD # 최신 commit으로 reset (변경중인 파일 내용 삭제)$ git stash liststash@{0}: WIP on exp: ba5adba 1 # 가장 최신의 stash 내용stash@{1}: WIP on exp: ba5adba 1stash@{2}: WIP on exp: ba5adba 1$ git stash apply # 가장 최신의 stash 내용으로 다시 복원On branch expChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: f1.txt$ git stash drop # 가장 최신 stash 삭제$ git stash pop # stash 복원 및 삭제 (상기명령 동일)","link":"/2021/02/03/202102/210203-Git-Stash-practice/"},{"title":"210203 Git PR template","text":"다양한 사람들과 협업을 할 때 중요한 것은 문서작성 능력과 커뮤니케이션 능력이다. Github 프로젝트를 다른 사람들과 같이 진행을 할때, pull request나 commit 메시지를 작성하는데, 적절한 format과 convention을 따라 작성을 하는 것은 매우 중요하다. 실제로 업무를 할때에도 이 convention을 따라 적절한 format으로 작성을 하게 되면, 해당 pull request나 commit 메시지를 확인하는 사람에게 업무처리에 대한 깔끔한 인상과 명확하게 처리한 업무에 대한 내용 전달을 할 수 있다. 따라서 이번 포스팅에서는 Github project에서 어떻게 문서를 관리하는지와 PR Template을 어떻게 자동으로 generate하는지에 대해서 살펴보고, 잘 formatting된 Template의 sample을 살펴보며 어떻게 하면 좀 더 전달력있는 Pull Requst message를 작성할 수 있는지에 대해서 배워보도록 하겠다. PR Template 생성하기Github 프로젝트를 위한 PR Template를 만들어보도록 하겠다. 프로젝트를 위한 PR template을 생성하는 방법은 간단하다. PULL_REQUEST_TEMPLATE 파일을 생성해서 아래의 세 가지 위치 중에 한 곳에 위치시켜주면 된다. GitHub가 PULL_REQUEST_TEMPLATE파일을 프로젝트 내에 있다는 것을 알고 새로운 PR을 생성할때 auto-populate 해 줄 것이다. (1) The root of your project(2) .github folder(3) docs folder 나는 project의 root에 .github folder를 생성해서 PULL_REQUEST_TEMPLATE 파일과 ISSUE_TEMPLATE 파일을 작성해서 관리할 것이다. PR를 작성할때 고려되어야 할 내용들 What is this change? What does it fix? Is this a bug fix or a feature? Does it break any existing - functionality or force me to update to a new version? How has it been tested?","link":"/2021/02/03/202102/210203-Git-pull-request-template/"},{"title":"210203 Plotly + Pandas Practice","text":"Pandas를 사용해서 csv파일을 읽어보는 간단한 실습을 해보았다.1234# -*- coding: utf-8 -*-import pandas as pdread_csv = pd.read_csv(&quot;data/test.csv&quot;) Excel sheet의 head(Column title)만 추출하기1print(read_csv.head()) 특정 Column name에 해당하는 데이터를 출력1print(read_csv[&quot;Column name&quot;]) Column name(1), Column name(2), Column name(3)에 해당하는 데이터를 출력1234read_csv = read_csv[[&quot;Column name(1)&quot;, &quot;Column name(2)&quot;, &quot;Column name(3)&quot;]]# Column이름과 함께 지정한 특정 Column들의 데이터들이 출력된다.print(read_csv) Confirmed, Deaths,Recovered 각 칼럼에 해당하는 값들의 합을 구해서 출력123456789read_csv = read_csv[[&quot;Column name(1)&quot;, &quot;Column name(2)&quot;, &quot;Column name(3)&quot;]].sum()# 각 Column 별로 값의 합을 출력한다.# 출력은 column name | sum 형태로 출력이 되며,# type은 pandas.core.series.series 로 출력이 된다.print(read_csv)# 만약에 excel sheet 형태로 위에 column name 아래에 값을 출력하고 싶다면, series type을 data frame type으로 type을 바꿔주면 된다.read_csv = read_csv[[&quot;Column name(1)&quot;, &quot;Column name(2)&quot;, &quot;Column name(3)&quot;]].sum().reset_index(&quot;[총합을 표기하는 column의 이름]&quot;) 특정 Column의 title을 바꾸고 싶은 경우1read_csv = read_csv.rename(columns={'[이름을 바꿀 대상 column]':'[새롭게 변경할 column 이름]'}) 특정 Column을 기준으로 Grouping한 뒤에 각 항목의 최종합(sum)을 표시하고 싶은 경우1234567# Column1, Column2, Column3에 해당하는 데이터를 출력read_csv = read_csv[[&quot;Column1&quot;, &quot;Column2&quot;, &quot;Column3&quot;]]# Column1을 기준으로 grouping (Grouping만 하면 DataFrameGroupBy object)# read_csv 객체를 Column1을 기준으로 해서 정렬(groupby)# Column1기준 Column2, Column3의 합계를 index와 함께 표시read_csv = read_csv.groupby(&quot;Column1&quot;).sum().reset_index()","link":"/2021/02/03/202102/210203-Plotly_Pandas_Practice/"},{"title":"210203 [SyntaxError] Non-ASCII Character","text":"Python과 Pandas를 활용하여 csv파일을 읽는 간단한 처리를 하던 중에 Non-ASCII Character 에러가 발생했다.1SyntaxError: Non-ASCII character '\\xec' in file /Users/hyungilee/Documents/dev/side-projects/corona-dashboard/pandas_practice.py on line 3, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details Python의 경우, 기본적인 설정 상태에서 코드내에 한글이 있는 경우 코드 내의 한글을 Python 코드를 읽어들이지 못해서 위의 에러가 발생한다. 해결방법은 아래와 같이 파이썬 코드 맨 위 첫번째 혹은 두번째 줄에 한글 인코딩을 하도록 명령해주면 된다. 12# -*- coding: utf-8 -*-# -*- coding: euc-kr -*-","link":"/2021/02/03/202102/210203-Plotly_Pandas_project_error/"},{"title":"210203 Python Assignment","text":"공공데이터포털의 csv파일을 json으로 converting하기경기도 성남시_인구및세대_현황 1234567891011121314151617181920212223242526272829import csvimport jsonpopulation = {'sungnam_population': []}# csv 파일을 csv module로 읽어오기with open('20210201_sungnam.csv', 'r', encoding='euc-kr') as f: sungnam_population = csv.reader(f) # list -&gt; dictionary sungnam_population_list = list(sungnam_population) for row in sungnam_population_list[1:]: population_dict_row = dict() # 0번째 열에 있는 key값과 index를 이용해서 dictionary 데이터 만들기 for index, key in enumerate(sungnam_population_list[0]): population_dict_row[key.replace( &quot; &quot;, &quot;&quot;)] = row[index].replace(&quot; &quot;, &quot;&quot;) population['sungnam_population'].append(population_dict_row)# json.dump()를 활용해서 dictionary type의 데이터를 json 파일로 추출하기with open('sungnam_population.json', 'w', encoding='euc-kr') as f: json.dump(population, f)# 추출된 json 파일을 읽고 출력하기with open('sungnam_population.json', 'r', encoding='euc-kr') as f: des_data = json.load(f)print(des_data)","link":"/2021/02/03/202102/210203-Python_assignment/"},{"title":"210204 Build a server using ExpressJS &amp; Babel","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 NodeJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. Express 프레임워크를 사용해서 NodeJS 서버 작성1234567891011// express라는 이름의 파일을 찾아보고, 없으면 node_modules내부에서 찾아본다.const express = require('express');const app = express();const PORT = 4000;const handleListening = () =&gt; { console.log(`Listening on: http://localhost:${PORT}`);};app.listen(PORT, handleListening); package.json에 entry command 만들어주기1234// package.json&quot;scripts&quot;:{ &quot;start&quot;: &quot;node index.js&quot;} 위와같이 entry command를 작성해주면 매번 node 명령으로 index.js(server entry) 파일을 실행시키지 않고, npm start 명령으로 server entry 파일을 실행시킬 수 있다. BabelBabel은 최신의 JavaScript코드를 이전 버전의 JavaScript 코드로 변환해주는 역할을 한다. Babel에는 다양한 Loader가 존재하는데 NodeJS에서 사용할 것이기 때문에 Babel node를 설치해준다. npm install @babel/node 설치 npm install @babel/preset-env 설치 npm install @babel/core 설치 .babelrc 파일을 추가해준다. babel에는 다양한 stage가 있는데 최신 stage인 -env를 사용해본다. .babelrc 123{ &quot;presets&quot;: [&quot;@babel/preset-env&quot;]} index.js 123import express from 'express';const app = express(); package.json에 entry command 수정하기12345// package.json// babel이 최신 JavaScript 코드를 변환해줌과 동시에 node로 작성한 index.js 파일을 실행시켜 줄 것이다.&quot;scripts&quot;:{ &quot;start&quot;: &quot;babel-node index.js&quot;} 여지까지 작성한 방법으로는 server 코드에서 변화가 생기면 수동으로 서버를 끄고 다시 켜야 한다. 이러한 과정은 개발에 있어 매우 비효율적이므로, nodemon이라는 package를 설치해서 해결해보도록 하겠다.여기서 설치하는 nodemon은 개발자가 좀 더 편하게 개발할 수 있도록 도와주는 dependency이므로, 프로젝트의 실행과 직접적인 관련이 있는 dependency와 구별해서 설치해주는 것이 좋다. 따라서 nodemon을 설치할때에는 option으로 -D를 붙여주도록 한다. 1$ npm install nodemon -D 그럼 아래와 같이 별도의 devDependencies 객체로 구분되서 dependency가 설치되는 것을 확인할 수 있따. 123&quot;devDependencies&quot;: { &quot;nodemon&quot;: &quot;^2.0.4&quot;} 이제 nodemon이 설치되었으니 package.json에서 entry command 수정해준다.기존의 command에 nodemon --exec와 babel이 JavaScript 파일의 변환하는 것을 위해 2초간 delay될 수 있도록 --delay 2를 붙여준다. 123&quot;scripts&quot;: { &quot;start&quot;: &quot;nodemon --exec babel-node index.js --delay 2&quot;}","link":"/2021/02/04/202102/210204-ExpressJS_First_Server_and_Babel/"},{"title":"210204 HTTP - Request&#x2F;Response","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 NodeJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. GET/POST 방식 비교표 GET 방식과 POST 방식은 위와같이 정보 전송방식과 보안적 측면, 전송할 수 있는 데이터의 길이, 전송데이터의 Caching 가능 유/무 등에서 차이점을 보인다. HTTP의 동작방식에서 URL을 통해서 browser에 접속하게 되면, browser가 GET method 방식으로 Browser의 Page를 읽어온다. 로그인과 같이 중요한 정보를 전송할때에는 POST method로 browser에서 server로 정보를 전달하게 된다. 간단하게 browser상에서 /(root) 경로로 이동했을때의 처리를 작성해보자. index.js 12345const handleHome = () =&gt; { console.log('Hi from home!');};app.get('/', handleHome); 위와같이 작성을 해주면, console에서는 “Hi from home!”이라는 메시지를 확인할 수 있지만, 서버로 보내는 특정 response가 없기 때문에 웹 페이지가 무한 loading상태임을 확인할 수 있다. 1234567891011const handleHome = (req, res) =&gt; { console.log(req); res.send('Hello from home');};const handleProfile = (req, res) =&gt; { res.send('You are on my profile');};app.get('/', handleHome);app.get('/profile', handleProfile); 위와같이 NodeJS는 서버와 Route를 생성하고 그것에 응답하는 방식으로 작동한다. 지금은 간단한 동작 확인을 위해서 위와같이 callback function과 router 처리를 같은 파일에서 작성을 해주었지만, 프로젝트 구성시에는 callback function은 controller로써 기능별로 분류를 해주고, router 기능을 하는 부분도 별도로 분류해서 작성을 해 줄 것이다. 실제로 웹 서버를 동작시킬때에는 위와같이 res.send(&quot;[text]&quot;)가 아닌 완전한 HTML파일을 rendering해준다.","link":"/2021/02/04/202102/210204-HTTP_Request_Response/"},{"title":"Baekjoon Online Judge 1236번 성 지키기 문제","text":"백준 저지 1236번 성 지키기 문제 Pseudo code + Python code 손 코딩에서 이중 for문 처리하는 부분에서 수정이 필요하다. 이중 for문에서 i,j를 이용해서 내부 반복처리를 해야하는데, 손 코딩할때 n과 m을 넣어 처리를 했다. 이 부분을 i와 j로 수정해서 넣어줘야 한다. 123456789101112131415161718192021n, m = map(int, input().split())security_in_castle_list = []for _ in range(n): security_in_castle_list.append(input())castle_row = [0]*ncastle_column = [0]*mfor i in range(n): for j in range(m): if security_in_castle_list[i][j] == 'X': castle_row[i] += 1 castle_column[j] += 1castle_zero_row = 0for row in castle_row: if row == 0: castle_zero_row += 1castle_zero_column = 0for col in castle_column: if col == 0: castle_zero_column += 1print(max(castle_zero_row, castle_zero_column)) 이 문제는 (n x m) 행렬로 구성된 성 내부에서 각 행과 열에 최소 한 명의 경비원을 배치하도록 하는 문제이다. 문제 해결을 위해서는 행과 열의 크기만큼 리스트를 각각 선언하여 경비원이 아예 배치되어있지 않은 행과 열의 갯수를 구해야 한다.구해진 행과 열의 갯수 중에 최대값을 출력하면 최소 배치해야 되는 경비원의 수를 구할 수 있다.","link":"/2021/02/05/202102/210204-Algorithm_baekjoon_1236/"},{"title":"210204 Python Regular Expression Assignment","text":"원래 HackerRank 문제의 경우, 별도로 풀이를 정리하지는 않지만, 이번에 정규표현식의 경우, 헷갈리는 부분과 새롭게 공부하게 된 내용이 있어서 별도로 정리를 해본다. 문제 1Problem1 Link(HackerRank) 이 문제는 앞 뒤로는 자음만 위치하고, 가운데에는 2개 이상의 모음으로만 구성되어 있는 문자를 정규표현식을 통해 출력하는 문제이다. Task You are given a string . It consists of alphanumeric characters, spaces and symbols(+, -). Your task is to find all the substrings of that contains or more vowels. Also, these substrings must lie in between consonants and should contain vowels only. 우선 문제를 풀이하기에 앞서 다음 긍정/부정예측(positive/negative lookahead assertion), 긍정 후 읽기에 대한 개념에 대해서 알아보자. 긍정적인 예측x?=y : 이 패턴은 직후에 y가 존재하는 문자열 x에 match한다. 12foo(?=bar)# 직후에 bar가 있는 foo에 일치한다. 부정적인 예측x?!y : 이 패턴은 직후에 y가 없는 문자열 x에 match한다. 12foo(?!bar)# 직후에 bar가 없는 foo에 일치한다. 긍정 후 읽기x?&lt;=y : 이 패턴은 직전에 문자열 x가 있는 문자열 y(x는 포함하지 않음)에 일치한다. 12(?&lt;=bar)foo# 직전에 bar가 있는 foo(bar는 포함하지 않음)에 일치한다. 부정 후 읽기x?&lt;!y : 이 패턴은 직전에 x가 없는 문자열 y에 일치한다. 12(?&lt;!bar)foo# 직전에 bar가 없는 foo(bar는 포함하지 않음)에 일치한다. 본 코드123456789import reresult = re.findall(r'(?&lt;=[QWRTYPSDFGHJKLZXCVBNMqwrtypsdfghjklzxcvbnm])([aeiouAEIOU]{2,})(?=[QWRTYPSDFGHJKLZXCVBNMqwrtypsdfghjklzxcvbnm])', input())if result: print(*result, end='\\n')else: print(-1) 123456find_object = re.finditer( r'(?&lt;=[QWRTYPSDFGHJKLZXCVBNMqwrtypsdfghjklzxcvbnm])([aeiouAEIOU]{2,})(?=[QWRTYPSDFGHJKLZXCVBNMqwrtypsdfghjklzxcvbnm])', input())for i in map(lambda x: x.group(), find_object): print(i) 문제 2Problem2 Link(HackerRank) 이 문제는 입력받은 multiline statements에서 각 문자열에 &amp;&amp;가 있으면 and로, ||가 있으면 or로 변환하는 문제였다.이 문제를 풀때 처음에는 [\\s]로 앞/뒤 공백을 구분하려고 했는데 예상 결과값으로 출력이 안되어, ((?&lt;=[white space]), (?=[white space]))로 다시 정규표현을 작성하여 풀이하였다. You are given a text of lines. The text contains &amp;&amp; and || symbols. Your task is to modify those symbols to the following:Both &amp;&amp; and || should have a space &quot; &quot; on both sides. &amp;&amp; → and|| → or 본 코드1234567891011121314151617import redef cvtLogicalOperator(match): if match.group(0) == &quot;||&quot;: return &quot;or&quot; elif match.group(0) == &quot;&amp;&amp;&quot;: return &quot;and&quot;N = int(input())statement = []for _ in range(N): statement.append(input())statement = '\\n'.join(statement)print(re.sub(r&quot;(?&lt;= )(&amp;&amp;|\\|\\|)(?= )&quot;, cvtLogicalOperator, statement)) 문제 3Problem3 Link(HackerRank) 이 문제는 입력받은 이메일 주소를 정규표현식(regular expression)을 사용해서 유효성 검사를 하는 문제이다.이 문제에서는 이전에 배운 meta character를 다양하게 사용해 볼 수 있다. The first line contains a single integer, , denoting the number of email address. Each line of the subsequent lines contains a name and an email address as two space-separated values following this format: name &lt;user@email.com&gt; 본 코드\u001312345678910111213import reimport email.utilsn = int(input())for _ in range(n): name, email = map(str, input().split()) # &lt; 특수문자로 ^(시작)하고, \\w(alphanumeric character), # .을 문자 자체로 입력하기 위해 \\(back slash)와 같이 입력, # 끝은 &gt;로 끝나기 때문에 끝 문자 뒤에 $ 메타문자를 입력해서 처리해준다. ptn = r&quot;^&lt;[a-zA-Z](\\w|-|\\.|_)+@[a-zA-Z]+\\.+[a-zA-Z]{1,3}&gt;$&quot; if re.match(ptn, email): print(name, email) 문제 4Problem4 Link(HackerRank) 이문제는 입력받은 전화번호를 정규표현식을 사용해서 유효성 검사를 하는 문제이다.문제에서 입력받는 전화번호의 조건은 아래와 같다. condition1) 전화번호는 총 10자리 수로 구성이 된다.condition2) 앞 자리는 7, 8, 9로 시작한다.condition3) 나머지 자리수는 숫자(\\d)로 구성된다. 본 코드12345test_case = int(input())for _ in range(test_case): phone_number = input() ptn = r&quot;[7-9][0-9]{9}&quot; print('YES' if len(phone_number) == 10 and re.match(ptn, phone_number) else 'NO')","link":"/2021/02/04/202102/210204-Python_HackerRank_assignment/"},{"title":"210204 Plotly + Pandas Practice","text":"12import pandas as pddf = pd.read_csv(&quot;data/time_confirmed.csv&quot;) Table에서 Column 특정 Column 제거하기 제거할 Column은 drop()을 사용해서 제거해준다.12345# 제거할 column명을 column 속성을 명시해서 제거하거나df = df.drop(column=[&quot;Column1&quot;,&quot;Column2&quot;,&quot;Column3&quot;,&quot;Column4&quot;]).sum().reset_index(name=&quot;total&quot;)# 명시하지 않고 axis=1를 추가해서 제거할 수 있다.df = df.drop([&quot;Column1&quot;,&quot;Column2&quot;,&quot;Column3&quot;,&quot;Column4&quot;], axis=1).sum().reset_index(name=&quot;total&quot;)# 제거된 column을 제외한 column을 기준으로 합계(sum)를 구하고 series를 data frame으로 바꿔준다. 바꾼 후에 새로 추가된 합계 column의 이름은 &quot;total&quot;로 지정해준다. (.reset_index(&quot;total&quot;)) 기존의 column명을 rename()을 사용해서 수정해준다.1df = df.rename(column={'[target_column_name]':'[new_column_name]'}) 공통으로 묶을 수 있는 처리는 함수화 시켜서 작성해준다.123456789101112condition = [&quot;new_column_name1&quot;,&quot;new_column_name2&quot;,&quot;new_column_name3&quot;]def make_df(condition): df = pd.read_csv(&quot;data/{}.csv&quot;.format(condition)) df = df.drop(column=[&quot;Column1&quot;,&quot;Column2&quot;,&quot;Column3&quot;,&quot;Column4&quot;]).sum().reset_index(name=condition df = df.rename(column={'[target_column_name]':'[new_column_name]'}) return dffor condition in conditions: condition_df = make_df(condition) print(condition_df)","link":"/2021/02/04/202102/210204-Plotly_Pandas_Practice/"},{"title":"210330 bodyParser와 express embedded middleware(express.json())","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 NodeJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. Front-endFront-end에서 post방식으로 body에 데이터를 담아 보내는 경우, 12345// front-end에서 body를 함께 Request를 보내는 경우,axios.post('/products', { name: 'Lee Hyungi', description: 'Hi!'}); Back-end아래와같이 req.body를 통해 front-end로부터의 데이터를 넘겨받는다. 하지만 아래와같이 req.body로 값을 읽게 되면, undefined라고 출력됨을 확인할 수 있다.이에 대해 Express v4.16.0이전에는 bodyParser module을 설치해서 app에 추가해서 해결을 했지만, Express v4.16.0 이후에는 express의 내장 middleware인 express.json()을 사용해서 대체할 수 있다. 1234567// back-end에서 요청을 받는다....app.use(express.json());...app.post('/products', (req, res) =&gt; { console.log('req.body : ', req.body);});","link":"/2021/03/30/202102/210204-express_embedded_middleware/"},{"title":"210205 Book recommendation","text":"개발 관련 책 추천오늘은 평소에 내가 관심이 많이 있었던 Clean code, TypeScript 등 개발과 관련해서 강사님이 수업도중에 여러 책들을 추천해주셨다. Clean code는 이전부터 관심이 있던 내용이고, TypeScript는 최근에 공부를 시작해서 관심이 있던 내용이라 추천해주셨을때 너무 좋았다. 요즘 무슨 책을 읽어볼까 고민하던 참이었는데, 온라인 북으로 구매해서 아이패드에 넣고 통학시간이나 주말, 머리식힐때 읽어봐야겠다. 읽은 내용에서 유익한 내용이 있으면, 블로그에 포스팅해봐야겠다. 추천해주신 책은 아래와 같다. 일단 내가 최근에 공부를 시작한 TypeScript와 Clean code 책을 구매해서 읽어 볼 생각이다. 생각에서 끝나지 않기 위해서 내일 오전에 공부 시작할때 당장 구매해서 읽어봐야 겠다. Code 코드 파이썬을 이용한 클린 코드를 위한 테스트 주도 개발 TypeScript Programming Two Scoops of Django Two Scoops of Django책에서 다루고 있는 장고의 버전이 낮기 때문에 google에서 Two Scoops of Django를 검색해서 two scoops-of-django-3.x github의 코드파일을 참고하는 것을 추천한다. https://github.com/feldroy/two-scoops-of-django-3.x","link":"/2021/02/05/202102/210205-Book_Recommendation/"},{"title":"210205 Python TIL - filter, reduce, isinstance, OOP, scope, class","text":"오늘 배운내용 Section1) filter, reduce Section2) OOP, Class filter, reducefilterfilter의 기본 사용 format : filter([function], [iterable object]) 1부터 10까지의 수 중에 짝수인 수로 구성된 리스트 만들기123456789def even_selector(x): if x % 2 == 0: return True else: return False# 선언한 function을 인자로 넣기list(filter(even_selector, range(1, 10+1)))# lambda function로 별도의 함수 선언없이 함수처리list(filter(lambda a:a%2==0, range(1, 10+1))) isinstance(‘[check value]’, [check type])123456789101112131415**isinstance의 사용예)** isinstance('1', int) # True**isinstance의 사용예)** isinstance('1', float) # False# isinstance는 리스트 내부에 다양한 타입의 데이터가 존재할때 유용하게 사용이 가능하다.some_list = [1, 3.14, 2.71828, 'tau', 'lambda', [1, 0, 0, 1], {'kay': 'bamboo'}]# float인 요소만 filter해보기def validation_check(x): if isinstance(x, float): return Trueprint(list(filter(validation_check, some_list)))# filter는 반드시 lambda 함수식의 반환부분에서 True 또는 False를 반환해야 한다.print(list(filter(lambda x: isinstance(x, float), some_list))) reducereduce의 기본 사용 format : reduce([function], [iterable object]) iterable object의 모든 element에 대하여 연산결과를 출력 reduce는 Python3의 기본 내장함수에서 제외되어, functools에서 별도로 import해서 사용한다.from functools import reduce Rossum은 map, filter, reduce에 대해 readibility가 떨어진다는 이유로 제외 기본 내장함수에서 제외하였다. reduce 사용하지 않고 단순 반복문으로 1부터 100까지의 합을 구하기 1234sum = 0for i in range(1, 100+1): sum += iprint(sum) reduce를 사용하여 1부터 100까지의 합을 구하기 1234567891011121314from functools import reduce# reduce(function, iter)def accumulator(x,y): return x+y# reduce의 function은 두 개의 인자(x,y)를 받고 하나의 값(x+y)을 반환하는 구조이다.print(reduce(accumulator, range(1, 100+1)))# lambdaprint(reduce(lambda x,y:x+y, range(1, 100+1))) # 5050# 세번째 argument값으로 initial value를 줄 수 있다. (default:0)print(reduce(lambda x,y:x+y, range(1, 100+1), 100)) # 5150 옆 사람이랑 같이 pair coding하기아래의 간단한 예제 문제를 옆 사람과 해결방향을 모색해가며 문제해결을 한다. 1234567891011121314151617181920212223242526272829303132recycle_bin = [1, 2, &quot;Fastcampus&quot;, ['dog', 'cat', 'pig'], 5, 4, 5.6, False,&quot;패스트캠퍼스&quot;, 100, 3.14, 2.71828, {'name':'Kim'}, True,]# 정수이거나 실수인 수들로만 이루어진 리스트 생성# True(1), Flase(0) 또한 integer로써 인식이 되기 때문에 별도로 bool type이 아니라는 조건을 추가해줘야 한다.def validation_check(x): if (isinstance(x, int) or isinstance(x, float)) and not isinstance(x, bool): return Trueprint(list(filter(validation_check, recycle_bin)))# lambda function 방식으로 위의 코드 refactoring한다.print(list(filter(lambda x: (isinstance(x, int) or isinstance(x, float)) and not isinstance(x, bool), recycle_bin)))# 정수로만 이루어진 리스트의 각 요소를 제곱해서 리스트 생성# True(1), Flase(0) 또한 integer로써 인식이 되기 때문에 별도로 bool type이 아니라는 조건을 추가해줘야 한다.def validation_check_second(x): if isinstance(x, int) and not isinstance(x, bool): return x**2result = list(filter(validation_check_second, recycle_bin))# lambda function 방식으로 위의 코드를 refactoring한다.result = list(filter(lambda x: isinstance(x, int) and not isinstance(x, bool) , recycle_bin))print(result)# sum() function을 이용해서 리스트의 모든 구성요소의 합을 구한다.result_sum = sum(result)print(result_sum) 12345# 위에서 작성한 방식과 다르게 filter해서 나온 결과 리스트를 다시 filter하는 방식으로 처리result = list(filter(lambda b: not isinstance(b, bool), filter(lambda a:isinstance(a, (int, float)), recycle_bin)))print(result) 12345# 위에서 작성한 방식과 다르게 filter해서 나온 결과 리스트를 다시 filter하는 방식으로 처리result = list(map(lambda x:x**2, filter(lambda b: not isinstance(b, bool), filter(lambda a:isinstance(a,int), recycle_bin))))print(result) OOP(Object-Oriented-Programming) : 객체지향 프로그래밍 특징 Encapsulation 구현한 것을 드러나지 않도록 함 Information hiding: public, _protected, __private Abstraction 인터페이스로 클래스의 공통 특성을 묶어 표현 Inheritance 자식 클래스가 부모 클래스의 특성과 기능을 물어본다. Polymorphism 변수, method가 다른 상태를 가지는 것 프로그래밍 언어의 변천 Imperative Programming(명령적 프로그래밍) Procedural programming(절차지향 프로그래밍) Object-Oriented Progrmming(객체지향 프로그래밍) Functional Programming(함수형 프로그래밍) Scope local : outer_scope(함수내의 변수)의 binding을 바꾸지 못한다. nonlocal : outer_scope(함수내의 변수)의 binding을 바꾼다. global : 전역 binding을 바꾼다. 아래의 간단한 예제를 통해 global, local, nonlocal의 키워드에 대해서 이해한다. 12345678910111213141516171819202122232425262728293031323334## Scopemsg = &quot;fastcampus&quot;def outer_scope(): def inner_local(): msg = &quot;Fast&quot; def inner_nonlocal(): nonlocal msg msg = &quot;Campus&quot; def inner_global(): global msg msg = &quot;FastCampus&quot; msg = &quot;Seong-Su&quot; inner_local() # method가 내부 변수(msg)에 영향을 주지 않는다. print('inner local:', msg) inner_nonlocal() # nonlocal로 설정해주면 내부 변수에 영향을 준다. # Seong-Su를 Campus로 업데이트 print('inner nonlocal:', msg) # 외부에 선언된 msg 변수에 대해 처리 inner_global() # inner_global에서의 msg는 &quot;fastcampus&quot;이다. print('inner global:', msg)outer_scope()&quot;&quot;&quot;inner local: Seong-Suinner nonlocal: Campusinner global: Campus&quot;&quot;&quot; Class SOLID Single Responsibility Principle 한 클래스는 하나의 책임만 가져야 한다.(goto 19) Open/Closed Principle 확장에는 열려있지만, 변경에는 닫혀 있어야 한다.(override) Liskov’s Substitution Principle 프로그램의 정확성을 깨트리지 않으면서 하위 타입의 인스턴스로 바꿀 수 있어야 한다.(is-A) Interface Segregation Principle 사용하지 않는 method는 분리해야 한다.(Abstract) Dependency Inversion Principle 추상화에 의존하고, 구체화에 의존하지 않는다. 구성요소 Class Object Method 1234# class의 기본형태class ClassName: #CamelCase pass # statement # hello_world: snake_case Class 작성 및 인스턴스 생성123456class Hero: passIronMan = Hero()CaptainAmerica = Hero()Thor = Hero() Class 작성 및 인스턴스 생성(+인스턴스 변수 초기화) 및 Class 내부 method 작성12345678910111213141516class Hero: hp = 100 # class variable def __init__(self, name, weapon): self.name = name # instance variable self.weapon = weapon # instance의 행동을 정의 def attack(self): print('attack with {}',format(self.weapon)) def get_damaged(self): passIronMan = Hero('IronMan', 'Suit')print(IronMan.attack())print(IronMan.name)print(IronMan.weapon) 인스턴스 객체 참조를 통한 class 내부 변수의 값 변경하기1234567891011121314class LetsSeeAttributes: &quot;&quot;&quot; This is docstring &quot;&quot;&quot; integer = 1024 def function(): return 'fastcampus'fast = LetsSeeAttributes()print(fast.integer) # 1024fast.integer = 2048print(fast.integer) # class 내부 변수의 값이 업데이트 된다.campus = LetsSeeAttributes()campus.integer # 1024","link":"/2021/02/05/202102/210205-Python_til/"},{"title":"210206 ReactJS의 개념과 기본사용","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. ReactJS NPM에서 React library를 살펴보면, 매주 400만건 이상의 유저가 React Library를 다운받고 있고, Facebook에 의해서 관리가 되고 있다. React를 가장 좋은 기술로 만들기 위해 Facebook에서 전적으로 자원과 커뮤니티, 서포트를 아끼지 않고 있다. Front-end 개발의 생태계를 보면, 71.7%의 사용자가 이미 React를 이용했고, 다른 비슷한 프레임워크와 비교를 했을때 압도적으로 높은 수치의 사용자가 사용하고 있다. 요즘 회사들의 대부분이 React를 사용하고 있고, Netflix, Slack과 같은 대형 회사들 또한 React를 사용하고 있다.React만을 위해 존재하는 것은 모두 JavaScript로 구성되어 있기 때문에 쉽게 배울 수 있다는 장점이 있다. 기본적으로 browser는 React의 코드를 이해할 수 없기 때문에 web pack, babel을 이용해서 React code를 compile한 뒤에 비로소 browser가 이해를 할 수 있다. ReactJS 프로젝트 생성 방법1) local환경에 create-react-app 설치후 ReactJS 프로젝트 생성 12345# create-react-app 설치$ npm i create-react-app# create-react-app를 사용하여 새로운 ReactJS 프로젝트 생성$ create-react-app [Project_folder_name] 방법2) 별도로 create-react-app를 local환경에 설치하지 않고, npx를 사용해서 가장 최신버전의 create-react-app로 ReactJS 프로젝트 생성 12# create-react-app를 사용하여 새로운 ReactJS 프로젝트 생성$ npx create-react-app [Project_folder_name] 설치가 끝난 뒤에 terminal에서 $ npm start 해주면 아래와 같이 default 3000 port로 ReactJS 어플리케이션이 활성화되는 것을 확인할 수 있다. Network로 할당되는 별도의 IP주소로 휴대폰을 이용해서도 접속이 가능하다. ReactJS 프로젝트의 동작원리 ReactJS의 프로젝트의 동작원리를 내부 파일들을 분석해서 알아보자.우선 public&gt;index.html 파일 내부의 &lt;body&gt;태그를 보면, 아래와 같이 id가 root인 &lt;div&gt;태그를 찾을 수 있다. public/index.html 1234567891011121314&lt;body&gt; &lt;noscript&gt;You need to enable JavaScript to run this app.&lt;/noscript&gt; &lt;div id=&quot;root&quot;&gt;&lt;/div&gt; &lt;!-- This HTML file is a template. If you open it directly in the browser, you will see an empty page. You can add webfonts, meta tags, or analytics to this file. The build step will place the bundled scripts into the &lt;body&gt; tag. To begin the development, run `npm start` or `yarn start`. To create a production bundle, use `npm run build` or `yarn build`. --&gt;&lt;/body&gt; 이제 src&gt;index.js파일 내부를 보면 아래와 같이 &quot;root&quot;라는 id를 가진 요소에 &lt;App/&gt; component를 rendering해주고 있음을 알 수 있다. src/index.js 1ReactDOM.render(&lt;App /&gt;, document.getElementById('root')); component의 내용은 src&gt;App.js 파일의 내부에 정의가 되어있다. src/App.js 1234567891011import React from 'react';function App() { return ( &lt;div&gt; &lt;h1&gt;Hello!!!!!&lt;/h1&gt; &lt;/div&gt; );}export default App; 위와같이 기존의 &lt;App/&gt; component의 반환부분에 &lt;h1&gt;태그로 Hello!!!!! 문자를 넣어주면 아래와 같이 웹 페이지에 해당 문자열이 출력됨을 알 수 있다. 이처럼 React는 component로 분리되서 어플리케이션이 구성되어 있으며, 처음부터 화면에 보여질 HTML 코드를 넣지 않고, Virtual DOM(Virtual Document Object Mode) 개념으로 실제로 소스코드 상에는 존재 하지 않지만, 화면이 rendering될때 component로 작성된 부분이 화면에 보여지게 된다. chrome browser에서 inspection을 사용해서 화면의 요소검사를 하면 실제 화면에 존재하는 요소로 나오지만, 실제 코드를 보면 실제 화면에 표시되는 요소들은 나오지 않는다. 자 그럼 앞의 설명에서 자주 언급된 Component에 대한 개념을 정리해보자. Component? component는 HTML을 반환하는 함수라고 이해하면 된다. 앞에서 살펴보았던 component의 내부에서 실제 화면에 보여질 html코드를 가지고 있었던 것처럼 Component는 HTML코드를 반환하는 형태를 하고 있다. 이처럼 React는 화면에 보여질 각 구성요소들을 Component단위로 쪼개서 재사용이 가능한 형태로 화면을 구성한다. 모든 Component 파일은 import React from &quot;react&quot;를 포함해야하며,React에서는 오직 한 번에 하나의 Component만을 rendering할 수 있다. 따라서 반환해주는 부분(return)에서 jsx코드를 하나의 객체로 묶어서 반환해줘야 한다.(&lt;&gt;&lt;/&gt; fragment로 묶어주기) JSX? React에서는 이처럼 JavaScript 코드 내부에서 HTML을 혼합해서 사용을 하는데 이러한 코드를 jsx코드로 정의한다. JSX의 특징 정의된 Component에는 아래와 같이 Props라는 형태로 데이터를 전달할 수 있다. 123456789101112131415161718192021function NewComponent(props) { console.log(props); return &lt;h1&gt;NewComponent Message&lt;/h1&gt;;}/**props : {fmessage: &quot;message1&quot;, smessage: true, tmessage: Array(3)}*/function App() { return ( &lt;div&gt; &lt;h1&gt;App h1 message&lt;/h1&gt; &lt;NewComponent fmessage=&quot;message1&quot; smessage={true} tmessage={['hello', 7, true]} /&gt; &lt;/div&gt; );} Component에서 선언한 속성을 아래와 같이 parameter부분에서 destructuring해서 필요한 props 요소만 가져다가 사용할 수도 있다. 123function NewComponent({ fmessage }) { return &lt;h1&gt;NewComponent Message : {fmessage}&lt;/h1&gt;;} Component를 활용해서 동적 데이터를 처리할때에도 JSX + Props의 특성을 활용하면 쉽게 처리할 수 있다.","link":"/2021/02/06/202102/210206-ReactJS_start/"},{"title":"210206 What is difference between Framework and Library?","text":"What is difference between Framework and Library?이전에 개발을 하면서 내가 사용하고 있는 것이 프레임워크인지 라이브러리인지 헷갈렸던 경험이 있었기 때문에, 나같은 사람이 없을 수도 있겠지만, 혹시 나와 같은 의문을 가진 사람들을 위해 프레임워크와 라이브러리의 차이에 대해서 정리해보려고 한다. 우선 메인으로 첨부한 그림을 보면 대략적으로 알 수 있듯이 Framework는 내가 작성한 Application을 감싸안고 포함고 있는 개념이라면, Library는 내가 작성한 Application과 아래의 우주선과 같이 도킹(docking)되어 있는 것이라고 정의할 수 있겠다.따지고 보면 국제 우주정거장 == Application이 되겠고, 크루드래곤 == Library 라고 생각하면 될 것이다. 위 사진은 작년에 2020년 5월 31일 스페이스 X의 크루드래곤이 국제 우주정거장에 도킹을 성공하는 실제 장면이다. 이제 프레임워크와 라이브러리의 기본적인 형태에 대해서 이해가 되었으니, 내부 동작에 있어서는 어떤 차이가 있는지 살펴보자. 위에 첨부한 그림에서 알 수 있듯이 우선 Libary는 Framework에 포함된 개념이다.Framework자체를 개발에 필요한 여러 Library의 집합으로 정의를 할 수도 있습니다.Libray를 사용하기 위해서는 코드에서 호출을 해야하지만 Framework는 애플리케이션의 골격으로, 해당 골격과 맞지 않으면 Framework에서 애플리케이션 코드로 호출을 해서 알려주게 됩니다. 이상, 간단하게 Library와 Framework의 컨셉과 차이점에 대해서 정리해보았습니다.","link":"/2021/02/06/202102/210206-Self-Framework_and_Library_Difference/"},{"title":"210207 API &amp; Networking with Axios instance","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. Axios vs Fetch Fetch() Fetch API는 Request나 Response와 같은 HTTP Pipeline에 접근 및 조작을 하기 위한 JavaScript interface를 제공한다.Network를 통해 비동기방식으로 resource를 fetch하기 위해서 사용된다고 이해하면 된다. 사용방법은 아래와 같이 fetch() method에 자원을 얻기 위한 path를 넣어주고 request, response에 접근하면 된다. 12345678// fetch의 기본적인 사용fetch('examples/example.json') .then((response) =&gt; { // Do stuff with the response }) .catch((error) =&gt; { console.log('Looks like there was a problem: \\n', error); }); Request object12345678910111213// Request object{ method: 'POST', // *GET, POST, PUT, DELETE, etc. mode: 'cors', // no-cors, *cors, same-origin cache: 'no-cache', // *default, no-cache, reload, force-cache, only-if-cached credentials: 'same-origin', // include, *same-origin, omit headers: { 'Content-Type': 'application/json' }, redirect: 'follow', // manual, *follow, error referrerPolicy: 'no-referrer', // no-referrer, *client body: JSON.stringify(data) // body data type must match &quot;Content-Type&quot; header} Response object아래와같이 Response 객체를 통해서 서버로부터 넘겨받은 데이터를 여러 형식으로 받아 올 수 있다.response.json(): Parse the response as JSONresponse.text(): Read the response and return as textresponse.formData(): Return the response as FormData objectresponse.blob(): Return the response as Blobresponse.arrayBuffer(): Return the response as ArrayBuffer Axios? Axios는 native JavaScript API가 아니기 때문에 사용하려면 우선 npm i axios를 통해 Axios를 설치해줘야 한다. Axios는 JavaScript library로, node.js 또는 XMLHttpRequests로부터 http requests 만들때 사용이 된다. 그럼 Fetch() method와 비교했을때 Axios library는 어떤 장점이 있을까? 첫번재, Fetch() method에서는 사용할때마다 매번 인자로 resource를 가져올 path를 넣어줘야 하는데, Axios는 baseURL과 기타 설정 부분을 공통처리해서 만든 instance 객체로 재사용해서 사용이 가능하다는 점이다. 12345678910111213import axios from 'axios';const api = axios.create({ baseURL: 'https://api.baseurl.com/', params: { api_key: '06e43891a2b919ee11ba3f3894d63374', language: 'euc-kr' }});api.get('test');export default api; 두번째, Axios는 download progress를 관리할 수 있는 build-in function이 있다. 세번째, Axios는 XSRF에 대항하여 client-side 보호가 가능하다. XSRF? 사이트 간 요청 위조(또는 크로스 사이트 요청 위조, Cross-site request forgery, CSRF, XSRF)라고 한다. 웹 사이트 취약점 공격들 중에 하나로, 사용자가 자신의 의지와는 무관하게 공격자가 의도한 행위(수정, 삭제, 등록)를 특정 웹 사이트에 요청하게 하는 공격을 말한다. Reference website : XSRF-wikipedia 네번째, 자동으로 request와 response를 변환해준다. 아래의 예시에서 볼 수 있듯이, data를 JSON으로 자동 convert해주고, 이것을 request body로써 전송해준다. 123456789// send a POST requestaxios({ method: 'post', url: '/login', data: { firstName: 'Hyungi', lastName: 'Lee' }}); 다섯번째 Axios는 각기 다른 HTTP requests를 위한 shorthand methods를 제공한다. axios.request(config) axios.get(url[, config]) axios.delete(url[, config]) axios.head(url[, config]) axios.options(url[, config]) axios.post(url[, data[, config]]) axios.put(url[, data[, config]]) axios.patch(url[, data[, config]]) 예를들어, post방식으로 /login 에 입력받은 firstName과 lastname을 request body에 담아 보내고자 한다면 아래와 같이 간단하게 작성해 줄 수 있다. 12345678910111213axios .post('/login', { firstName: 'Hyungi', lastName: 'Lee' }) .then( (response) =&gt; { console.log(response); }, (error) =&gt; { console.log(error); } ); axios.post는 아래의 response 정보를 반환해준다. 12345678910111213141516// `data` is the response that was provided by the server data: {}, // `status` is the HTTP status code from the server response status: 200, // `statusText` is the HTTP status message from the server response statusText: 'OK', // `headers` the headers that the server responded with // All header names are lower cased headers: {}, // `config` is the config that was provided to `axios` for the request config: {}, // `request` is the request that generated this response // It is the last ClientRequest instance in node.js (in redirects) // and an XMLHttpRequest instance the browser request: {}}","link":"/2021/02/07/202102/210207-ReactJS_axios/"},{"title":"210208 Jest의 다양한 matcher","text":"Jest Matcher이전에 사용했던 toBe() matcher는 숫자나 문자와 같은 기본 타입(Primitive Type)\b의 데이터를 비교할때 사용했다. 그럼 그 외에 다른 matcher에는 어떤 것들이 있는지 알아보고 실습을 해보도록 하겠다. Jest의 다양한 matcher toEqual() : primitive type의 변수나 객체를 비교할때 사용된다.객체는 참조변수이기 때문에 값은 같더라도 참조하는 주소가 다르다. toStrictEqual() : 객체를 좀 더 엄격하게 검사할때 사용된다. 123456789// makeUser()를 toEqual()로 검사를 하게 되면 passed// toStrictEqual()를 toEqual()로 검사를 하게 되면 failedconst fn = { add: (a, b) =&gt; a + b, makeUser: (name, age) =&gt; ({ name, age, gender: undefined }), throwErr: () =&gt; { throw new Error('xx'); }}; toBeNull() : Null이 되는 경우를 검사한다. 123test('null은 null입니다.', () =&gt; { expect(null).toBeNull();}); toBeUndefined() toBeDefined() toBeTruthy() 1234// 빈 문자열이 아닌, helloworld 문자열을 반환하기 때문에 true를 반환한다.test('helloworld 문자열은 True입니다.', () =&gt; { expect(fn.add('hello', 'world')).toBeTruthy();}); toBeFalsy() 1234567test('0은 false입니다.', () =&gt; { expect(fn.add(1, -1)).toBeFalsy();});// .not.toBeFalsy()test('helloworld 문자열은 False가 아닙니다.', () =&gt; { expect(fn.add('hello', 'world')).not.toBeFalsy();}); toBeGreaterThan() : 초과 toBeGreaterThanOrEqual() : 이상 toBeLessThan() : 미만 toBeLessThanOrEqual() : 이하 1234test('입력한 ID는 10자 이하여야 합니다.', () =&gt; { const id = 'USER_ID'; expect(id.length).toBeLessThanOrEqual(10);}); toBeCloseTo()JavaScript에서는 0.1 + 0.2가 0.3이 아니다.&lt;&gt;Expected: 0.3&lt;/&gt;Received: 0.30000000000000004컴퓨터는 이진법을 사용하기 때문에 몇 몇 연산은 무한소수로 표현되어 버린다.이러한 경우에 toBeClose()를 사용하면 된다. 123test('0.1 더하기 0.2은 0.3이다.', () =&gt; { expect(fn.add(0.1, 0.2)).toBeCloseTo(0.3);}); toMatch()정규표현식과 toMatch()를 사용해서 정규식 검사를 할 수 있다.정규표현식의 대소문자 구분을 안하기 위해서는 정규표현식 뒤에 i 옵션을 붙여주면 된다. 123test(&quot;Hello World 에 'a'라는 글자가 있는지 확인&quot;, () =&gt; { expect('Hello World').toMatch(/h/i);}); toContain()배열에서 특정 요소가 있는지 확인한다. 12345test('유저 리스트에 Mike 사용자가 있는지 확인', () =&gt; { const user = 'Mike'; const userList = ['Tom', 'Mike', 'Kai']; expect(userList).toContain(user);}); toThrow()함수실행시 예외가 발생하는 경우,toThrow()를 사용해서 에러가 발생하는지 확인할 수 있다.toThrow()의 인자로 발생한 에러를 비교할 수도 있다. 123test('이거 에러 나나요?', () =&gt; { expect(() =&gt; fn.throwErr()).toThrow('xx');});","link":"/2021/02/08/202102/210208-JS_Jest_matchers/"},{"title":"210208 Jest의 개념과 기본사용","text":"JestJest는 Facebook에 의해서 개발된 JavaScript 테스트 프레임워크이며, 대규모 Web application의 테스트를 좀 더 심플하게 할 수 있도록 도와준다. Jest framework의 사용간단한 JavaScript 코드를 Jest framework를 사용해서 테스트 해보도록 하자. 실습 내용실습 Repository : https://github.com/LeeHyungi0622/javascript-jest-test-practice-repo 프로젝트의 branch 기본 구성은 master branch로 구성한다. README.md 파일의 documentation작업을 제외한 모든 작업은 별도의 branch를 새로 끊어서 작업을 시작한다. Basic setting in project Jest 프레임워크를 활용한 테스트를 위해 간단한 JavaScript 파일들을 추가해준다. (Add simple javascript files to do unit test using Jest framework.) 프로젝트에 새로운 npm package를 초기화 시켜준다. (Generate project folder without asking any questions.) 1$ npm init -y Jest를 developer dependency로 구분해서 설치해준다. 1$ npm i -D jest package.json의 test script를 “jest”로 해준다. 123&quot;scripts&quot;: { &quot;test&quot;: &quot;jest&quot;}, 기본적으로 Jest에서는 test 폴더 아래에 있는 테스트 파일들을 검사하기 때문에 테스트를 하기 위해 작성한 파일들은 test 폴더의 아래에 위치시키도록 한다. 작성하는 테스트 파일의 이름은 테스트 하고자 하는 function이 위치한 파일명.test.js 와 같은 형태로 파일명을 지어주도록 한다. test code의 기본 작성법 - 작성한 test file에서는 테스트하고자 하는 function을 가진 파일을 import해서 해당 function를 테스트해야 한다. 12345678const { test, expect } = require('@jest/globals');const sum = require('../sum');// 작성한 테스트코드가 무엇을 하는지에 대해서 첫번째 parameter로 작성해준다.test('properly adds two numbers', () =&gt; { // expected result expect(sum(1, 2)).toBe(3);}); 테스트하고자 하는 function은 위와 같이 expect()에 인자와 함께 넣어주고, 예상되는 값을 toBe()의 인자로 넣어서 검사를 하게 된다. toBe() 외에도 다양한 검사 조건이 있다. Test 결과 확인 12345678910&gt; jestPASS __test__/sum.test.js✓ properly adds two numbers (2 ms)Test Suites: 1 passed, 1 totalTests: 1 passed, 1 totalSnapshots: 0 totalTime: 1.844 sRan all test suites. 구체적인 Test의 결과 확인package.json에서 test command script를 아래와 같이 수정을 해준다.다음의 –coverage 옵션을 넣어주면 각 test파일별로 구체적인 test의 정보를 확인할 수 있다. 123&quot;scripts&quot;: { &quot;test&quot;: &quot;jest --coverage&quot;} 1234567891011121314151617&gt; jest --coveragePASS __test__/cloneArray.test.jsPASS __test__/sum.test.jsPASS __test__/subtract.test.js---------------|---------|----------|---------|---------|-------------------File | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s---------------|---------|----------|---------|---------|-------------------All files | 100 | 100 | 100 | 100 |cloneArray.js | 100 | 100 | 100 | 100 |subtract.js | 100 | 100 | 100 | 100 |sum.js | 100 | 100 | 100 | 100 |---------------|---------|----------|---------|---------|-------------------Test Suites: 3 passed, 3 totalTests: 3 passed, 3 totalSnapshots: 0 totalTime: 3.778 s 자동생성된 coverage 폴더의 Icov-report 폴더 내부의 index.html 파일을 열어보면 아래와 같이 test 정보를 웹 페이지에서 확인할 수 있다.","link":"/2021/02/08/202102/210208-JS_Jest_start/"},{"title":"Baekjoon Online Judge 1302번 베스트 샐러 문제","text":"백준 저지 1302번 베스트 샐러 문제 Pseudo code + Python code 손 코딩을 한 코드에서 sorted로 정렬을 한 뒤에 list로 변환을 하였는데, dictionary type의 데이터를 items()로 하게 되면, 리스트 내에 튜플(key, value쌍)로 변환이 되기 때문에 그럴 필요가 없었다. 정렬된 튜플 데이터 리스트에서 첫번째 요소의 첫번째(book title-(key))를 추출하게 되면 가장 높은 판매 수를 기록한 책의 제목을 추출할 수 있다. 12345678n = int(input())sold_book = dict()for _ in range(n): book_title = input() sold_book[book_title] = sold_book.get(book_title, 0) + 1sorted_sold_book = sorted(sold_book.items(), key=lambda x: x[1], reverse=True)print(sorted_sold_book[0][0]) 이 문제는 Python의 dict() 자료형을 활용하여 풀면 간단하게 해결할 수 있는 문제이다. dict() 자료형을 .items()로 변환하여 sorted() function내에서 lambda 함수식을 사용하여 간결하게 정렬하는 부분은 다른 알고리즘 문제에서도 유용하게 사용될 수 있는 부분인 것 같다.","link":"/2021/02/10/202102/210210-Algorithm_baekjoon_1302/"},{"title":"210209 Python Assignment","text":"과제1) 성수역 지하철 열차 시스템 지하철 노선 클래스1234567891011121314151617181920212223242526272829# 2호선 내부순환선 (총 43개 역) 시청역 &lt;-&gt;시청역 (내부순환)# 성수지선 (총 5개 역) 성수역 &lt;-&gt; 신설동# 신정지선 (총 38개 역) 성수역 &lt;-&gt;까치산# class instance를 만들때 각 노선의 최대 정착역의 수를 구한다.class SubwayLine: def __init__(self, num_of_stn): # 지하철역의 수 만큼 [0]리스트를 만들어서 # 역별로 정차하고 있는 지하철의 수를 관리 self.num_of_stop_train_list = [0]*num_of_stn # 해당 index위치의 count값을 1증가 시켜준다. def increaseNumOfStopTrain(self, idx): if self.num_of_stop_train_list[idx] == 0: self.num_of_stop_train_list[idx] += 1 else: # 위험 알람 method를 호출해서 False값을 반환하도록 한다. self.setDangerAlert() def decreaseNumOfStopTrain(self, idx): if self.num_of_stop_train_list[idx] == 1: self.num_of_stop_train_list[idx] -= 1 # 해당 index위치에 정차되어있는 지하철의 수를 반환한다. def getNumOfStopTrain(self, idx): return self.num_of_stop_train_list[idx] # 해당 역 위치에 지하철이 정차되어있는 경우, False값을 반환해서 출발할 수 없도록 한다. def setDangerAlert(self): return False 지하철 열차 클래스1234567891011121314151617181920212223242526272829303132333435363738394041424344class Subway: # line : 노선 # train_num : 열차번호 # acc_dis : 누적 거리 # dep_stn : 이전 출발역 # arr_stn : 도착역 # speed : 속도 (기본속도 21km/h) # direct : 운행방향 (CW:시계 방향, CCW:시계 반대방향) # drive_flag : 운행유/무 # stop_flag : 정차유/무 def __init__(self, _line, _train_num, _acc_dis, _dep_stn, _arr_stn): self.line = _line self.train_num = _train_num self.acc_dis = acc_dis self.dep_stn = dep_stn self.arr_stn = arr_stn self.speed = 21 self.direct = 'CW' self.drive_flag = False self.stop_flag = True # 지하철 운행 def drive(self, distance): self.drive_flag = True self.stop_flag = False self.acc_dis += distance # 지하철 정차 def stop(self): self.drive_flag = False self.stop_flag = True # 방향 재설정 def setDirection(self): if self.direct == 'CW': self.direct = 'CCW' else: self.direct = 'CW' # 지하철의 속도 조정 def setSpeed(self, _speed): self.speed = _speed # 이전 출발역 업데이트 def setDepartStation(self, _dep_stn): self.dep_stn = _dep_stn # 도착역 업데이트 def setArrStation(self, _arr_stn): self.arr_stn = _arr_stn 과제2) Hacker rank class2 - Find the Torsional Angle 123456789101112131415import mathclass Points(object): def __init__(self, x, y, z): self.x = x self.y = y self.z = z def dot(self, no): return (self.x*no.x)+(self.y*no.y)+(self.z*no.z) def __sub__(self, no): return Points(float(self.x - no.x),(self.y - no.y),(self.z - no.z)) def cross(self, no): return Points((self.y*no.z - self.z*no.y),(self.z*no.x - self.x*no.z),(self.x*no.y - self.y*no.x)) def absolute(self): return pow((self.x ** 2 + self.y ** 2 + self.z ** 2), 0.5)","link":"/2021/02/09/202102/210209-Python_Assignment/"},{"title":"210210 Hexo Image Optimization","text":"Hexo 블로그를 운영하면서 업로드한 이미지들 때문에 웹 페이지의 로딩시간이 이전보다 많이 느려졌다.그래서 업로드한 이미지 파일들을 모두 compression해서 파일크기를 1/4 사이즈로 만든다음에 업로드하였다. 그런데도 웹 페이지 로딩시간이 생각보다 오래 걸려서 한 번 Googling을 해보았다. 그래서 발견한 것이 Hexo Image Optimization이다. Hexo 내에 존재하는 모든 image파일 확장자를 가진 파일들을 최적화 시켜주고, 별도의 설정으로 post의 thumbnail 이미지도 최적화 시켜준다고 한다. 그래서 친절하게 설명되어 있는 설명을 보며, 최적화 설정을 모두 마쳤다. 1$ hexo generate &amp;&amp; hexo img 혹시 나와같이 블로그를 운영하다가 같은 문제로 고민이 있는 사람을 위해 참고한 GitHub 주소를 첨부한다. 요즘에 블로그 포스팅을 하면서, 이전에 우연히 보게 된 영상에서 개발자 유튜버가 하신 말씀이 생각난다.&quot;개발자는 알고있는 지식을 소유하는 것이 아니라 공유하는 것이다.&quot; 아직은 아는 것이 많이 없지만, 이제부터 새롭게 알게 되거나 유익한 내용들을 블로그를 통해서 많이 공유해야겠다. [Hexo-img-optimization GitHub URL]https://github.com/vkuznecovas/hexo-img-optimization","link":"/2021/02/10/202102/210210-Image-Optimization/"},{"title":"210210 Python TIL 1&#x2F;2 - class object, class variable, instance variable, Constructor&#x2F;Deconstructed, Hide Information, Inheritance, Aggregation, Composition","text":"class object 얕은복사(shallow copy) vs 깊은복사(deep copy)12345678910fruits = ['apple']a = fruitsprint(a) #['apple']b = fruitsb.append('banana')print(b) #['apple', 'banana']print(fruits) #['apple', 'banana'] fruits 변수가 b에 의해서 업데이트된 값으로 초기화가 되었다.이러한 경우는 얕은 복사(shallow copy)로, 깊은 복사(deep copy)를 해주기 위해서는 아래와 같이 참조변수에 대해서 type casting을 하게 되면 참조한 변수에 대해서 새로운 객체로 참조된다. 12345678910fruits = ['apple']a = list(fruits)print(a) #['apple']b = list(fruits)b.append('banana')print(b) #['apple', 'banana']print(fruits) #['apple', 'banana'] dir([object])https://docs.python.org/3/library/functions.html Without arguments, return the list of names in the current local scope. With an argument, attempt to return a list of valid attributes for that object. argument를 넣어주지 않으면 현재 local scope 내의 이름을 리스트로 반환해준다. argument를 같이 넣어주면 argument로 넣어준 object의 속성에 대한 리스트를 반환해준다. 12345678910111213141516171819class SeeAttributes: &quot;&quot;&quot; You can see docstring. &quot;&quot;&quot; # good job integer = 1024 # attribute def function(self): # attribute return 'fastcampus'LetsSee = SeeAttributes()print(dir()) # show the names in the module namespace&quot;&quot;&quot;['LetsSee', 'SeeAttributes', '__annotations__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__']&quot;&quot;&quot;print(dir(LetsSee)) # show the names in the struct module&quot;&quot;&quot;['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'function', 'integer']&quot;&quot;&quot; ([object]).__doc__struct module 내에 정의된 documentation정보를 출력한다. 123456print(LetsSee.__doc__)# struct module 내에 정의된 documentation정보를 출력&quot;&quot;&quot;&quot; You can see docstring.&quot;&quot;&quot; mutable한 class variablemutable한 variable을 class variable로 선언을 해주게 되면, 해당 class로 class instance들 생성한 객체들은 모두 같은 class variable을 공유하며 사용을 하게 된다. 1234567891011# 예제 코드에서 inventory list variable은 class variable로 선언이 되었다.class Hero: # class health = 100 # class variable inventory = [] def __init__(self, name, weapon): self.name = name # instance variable self.weapon = weapon def attack(self): # instance method print(&quot;attack with {}&quot;.format(self.weapon)) def save_item(self, item): self.inventory.append(item) mutable한 instance variablemutable한 variable을 class variable로 사용했을때의 문제점을 해결하기 위해서 constructor에서 사용할 변수를 초기화시켜서 instance variable로써 사용될 수 있도록 해야 한다. 1234567891011# 아래 코드에서 inventory variable은 instance variable로써 class를 사용해서 instance를 생성했을때 초기화가 될 수 있도록 작성하였다.class Hero: # class health = 100 # class variable def __init__(self, name, weapon, inventory): self.name = name # instance variable self.weapon = weapon self.inventory = [] def attack(self): # instance method print(&quot;attack with {}&quot;.format(self.weapon)) def save_item(self, item): self.inventory.append(item) Deconstructor (소멸자)constructor(생성자)가 존재하면, deconstructor(소멸자)도 존재한다. 생성자가 해당 class instance를 생성할때 호출되는 함수라면, 소멸자는 해당 class instance를 사용 후에 소멸시킬때 사용된다. 123# Deconstructor의 형태def __del__(self): pass class 내의 method에서 class 내의 다른 method에 접근class 내에서 선언된 함수에서 class 내의 다른 method를 호출해서 사용할 수 있다. 123456789101112class Hero: # class health = 100 # class variable def __init__(self, name, weapon, inventory): self.name = name # instance variable self.weapon = weapon self.inventory = [] ...... def save_item(self, item): self.inventory.append(item) def save_item_multiple(self, item, num): for _ in range(num): self.save_item(item) Hide Information다른 언어와 달리 Python은 private, public, protected와 같은 접근제어자 키워드가 존재하지 않고, naming으로 접근제어를 합니다. naming의 기준은 variable, method name의 접두사(prefix)에 _(underscore)가 붙어있는지 유/무와 한 개 또는 두 개의 _(underscore)가 붙느냐에 따라서도 달라진다. () (public) : public은 접두사에 _(underscore)가 없는 경우 _() (protected) : protected는 접두사에 _(underscore)가 하나 붙어있는 경우 __() (private) : private는 접두사에 _(underscore)가 두 개 붙어있는 경우 protected와 private의 접미사는 _(underscore)를 한 개까지만 허용하고, private의 접미사가 _(underscore) 2개라면, public으로 간주한다. 12345678910111213141516171819202122232425class AccessControl: def __init__(self): self.public = &quot;PUBLIC&quot; self.__private = &quot;PRIVATE&quot; self._protected = &quot;PROTECTED&quot; def print_test(self): print(self.public) print(self.__private) print(self._protected)obj = AccessControl()obj.print_test()&quot;&quot;&quot;PUBLICPRIVATEPROTECTED&quot;&quot;&quot;# print(obj.public)print(obj._protected) # PROTECTED# AccessControl 객체 외부에서는 __private가 보이지 않기 때문에 접근할 수 없다.# AttributeError: 'AccessControl' object has no attribute '__private'print(obj.__private) 상속(Inheritance)부모 클래스의 속성을 물려받아서 새로운 instance를 생성하는 것을 말한다. 상속의 특징 상속은 두 클래스 간에 is-A 관계가 성립한다.ex) 사자(자식 클래스)는 동물(부모 클래스)이다. (상속관계 성림) 부모 클래스로부터 상속받은 method를 override할 수 있다. Python에는 오버로드(Overload)의 개념이 없다.오버로드 : method의 이름은 같지만 parameter의 인자가 다르다면 중복 적재가 가능하다는 개념 Python에서의 상속은 별도의 키워드 없이 class이름에 ()를 붙여서 내부에 상속하고자 하는 클래스의 이름을 넣어주면 된다. 12class DerivedClassName(BaseClassName): #Statements 치킨을 예로들어, 튀겨진 치킨을 부모 클래스로 하고, 각기 다른 Seasoning을 첨가한 치킨을 자식 클래스로 정의하여 클래스의 관계를 정의한다. 123456789101112131415161718class Fried: def __init__(self, mixture, ,chicken): self.mixture = mixture self.chicken = chicken def place_into_fryer(self): print('chicken is now frying for 15 min..')class Seasoned(Fried): def __init__(self, mixture, chicken, sauce='red chili'): # 자식 클래스의 생성자에서는 부모 생성자의 생성자를 아래와 같이 정의한다. (=super()) Fried.__init__(self, mixture, chicken) # 부모 생성자의 초기화에 사용되는 인자를 제외한 요소는 자식 클래스의 생성자에서 초기화시켜준다. self.sauce = sauce def place_into_fryer(self): print('chicken is now frying for 13 min..') def mix_with_sauce(self): print('mix with {}'.format(self.sauce)) 스마트폰을 예로들어 SmartPhone 부모 클래스를 상속받는 Iphone 클래스와 Galaxy 클래스를 정의해본다. 12345678910class SmartPhone: def __init__(self, ap, cam): self.ap = ap self.cam = cam def open_ai(self): print('Default Setting') def __str__(self): return 'I am {}'.format(self.__class__.__name__) Iphone 클래스 12345678910class Iphone(SmartPhone): def __init__(self, ap, cam, touch_id): SmartPhone.__init__(self, ap, cam) self.touch_id = touch_id def open_ai(self): print('Hey, Siri') def __str__(self): return super(Iphone, self).__str__() Galaxy 클래스 12345678910111213class Galaxy(SmartPhone): def __init__(self, ap, cam, sam_pay): # with super() 위에서는 상속하는 클래스 이름에 .__init__을 해줬지만, # super()를 사용하면 자식클래스의 이름과 self를 super의 인자로 넘겨준다. super(Galaxy, self).__init__(ap, cam) self.sam_pay = sam_pay def open_ai(self): print('Hi, Bixby!') def __str__(self): # 부모의 __str__() method를 호출한다. return super().__str__() 또 다른 의존관계로 has-A(Aggregation, Composition) 관계가 있다. 이 has-A 관계는 Attribute로 사용하는 경우를 예로 한다. has-A관계를 Aggregation과 Composition 이 두 측면으로 살펴볼 수 있다. 좀 더 쉽게 이해하기 쉽게 자동차의 튜닝과 바퀴를 예로들어 작성해보겠다. Aggregation : Composition보다 좀 더 느슨한 관계이다. (응집력이 약하다.)예를들어, 자동차 구매한 후에 개인적으로 자동차를 튜닝했다고 가정하자. 어느날 튜닝한 것이 마음에 안들어서 제거를 했을때, 제거를 한다고 해도 자동차의 객체에는 문제가 되지 않는다. A(container)와 B(contained class)의 관계에서 A(container)가 사라져도 B(contained class)는 사라지지 않는다. contained class는 container에 강한 dependency를 가지지 않는다. 전체와 부분이 독립적인 객체로써 존재할 수 있다. 전체를 여러개의 객체로 볼 수 있다. 12345678910111213141516171819202122232425262728293031323334# 종속관계에 있는 클래스를 생성자에서 초기화시켜주지 않고, instance객체를 생성한 다음에 argument로써 넘겨서 초기화를 시켜준다.class Hammer: def __init__(self, name): self.name = name def fly(self): print('Flying..')class Thor: def __init__(self): self.weapon = None def recall_hammer(self, weapon): self.weapon = weapon def throw_hammer(self): weapon = self.weapon self.weapon = None return weapon def fly(self): if self.weapon: self.weapon.fly() else: print(&quot;You can't do this&quot;)Thor = Thor()molnir = Hammer('molnir')Thor.recall_hammer(molnir)Thor.fly()Thor.throw_hammer()Thor.fly() Composition예를들어, 자동차의 일부 바퀴를 제거했다고 가정하자. 그럼 운전한다는 자동차의 본질적인 객체의 특성을 잃게 된다. A(container)와 B(contained class)의 관계에서 A(container)가 사라지면 B(contained class)도 사라진다. Aggregation 보다 좀 더 구체적인 관계를 정의한다. container class와 contained class가 강한 dependency를 가진다. container가 없어지면, 포함하고 있는 모든 contained clas도 없어진다. 전체는 단일 객체로써 본다. 전체를 통해서만 부분이 인식된다. 1234567891011# 종속관계에 있는 class 객체 자체를 class의 생성자에서 직접적으로 초기화시켜주는 경우를 Composition이라고 한다.class AP: passclass Cam: passclass SmartPhone: def __init__(self): self.ap = AP() self.cam = Cam()","link":"/2021/02/10/202102/210210-Python_til(1)/"},{"title":"210210 Python TIL 2&#x2F;2 - class method, instance method, static method,  Polymorphism, Abstract class","text":"오늘 배운 내용은 전반적으로 객체지향 프로그래밍 언어로써 Python을 잘 활용하기 위해 중요한 내용이기 때문에 포스팅을 두 개로 나눠서 작성해보았다. 오늘 배운내용Class, instance, static method Class method: @classmethod decorator를 사용하며, cls를 인자로 사용 cls 인자를 받는다. class variable과 연관된 일을 할때 사용이 된다. Instance method: self를 인자로 사용-객체의 고유 속성값을 사용한다. Static method: 아무 인자 없이 사용 일반 함수와 같은 역할을 한다. class와 관련있는 함수로써, class내부에서 정의하고자 할 때 사용된다. 1234567891011121314151617181920212223242526class Wallet: def __init__(self, account): self.account = account balance = 0 name = 'Your wallet' @classmethod def set_default(cls, amount): while amount &lt; 1: print(&quot;Err: You should set default value over 1. Try again!&quot;) amount = int(input(&quot;Set default value: &quot;)) cls.balance = amount print('Set default balance to {}'.format(amount)) #instance method def add_to_account(self, amount): self.account += amount print('Your total balance is {}'.format(self.account + Wallet.balance)) @staticmethod def see_static_name(): print(Wallet.name) def get_class_name(cls): print(cls.name)class MyWallet(Wallet): name = 'My wallet' 다형성(Polymorphism)앞의 포스팅에서 정리한 method의 override가 가능한 이유가 바로 이 다형성(Polymorphism)이라는 특성때문이다. 각 object가 상속받은 부모클래스의 method를 다르게 재정의 함으로써 본래 정의되어있던 method와 다른 행동과 기능 그리고 결과를 나타낸다. 추상 클래스(Abstract Base Class)https://docs.python.org/3/library/abc.html 공통 특성을 가진 부모 클래스인데, 가지고 있는 method들이 모두 method로서의 기능이 없고, 선언만 되어있는 클래스를 말한다. 추상 클래스는 한 개 이상의 추상 클래스를 포함하고 있어야 하며, 별도의 class instance를 생성할 수 없다. 추상 클래스(Abstract class의 정의와 사용)123456from abc import ABCclass MyABC(ABC): @abstractmethod def abstract_method(self): pass 123456from abc import ABCMetaclass MyABC(metaclass=ABCMeta): @abstractmethod def abstract_method(self): pass 123456from abc import *class MyABC(metaclass=ABCMeta): @abstractmethod def abstract_method(self): pass 추상클래스 예시123456789101112131415161718192021222324from abc import ABCMeta, abstractmethodclass AbstractClassExample(metaclass=ABCMeta): def __init__(self, value): self.value = value @abstractmethod def do_something(self): passclass DoAdd42(AbstractClassExample): def do_something(self): return self.value + 42class DoMul42(AbstractClassExample): def do_something(self): return self.value * 42x = DoAdd42(10)y = DoMul42(10)print(x.do_something()) # 52print(y.do_something()) # 420","link":"/2021/02/10/202102/210210-Python_til(2)/"},{"title":"210211 Blackjack game Class Design","text":"Blackjack Class Design < Blackjack class design Handwriting > < Blackjack class design UML > 이번 과제에서는 저번 시간의 수업시간의 개념을 최대한 반영해서 class를 설계한다. 우선, Player와 Dealer의 공통적 행위 및 요소를 PersonInCasino 추상 클래스(Abstract class)에 정의한다.각 각의 Player, Dealer class는 PersonInCasino abstract class를 상속받아 공통된 행위에 대한 다른 행동을 method 오버라이드(override)를 통해서 method를 재정의한다.method명은 Blackjack 게임의 룰에 따라 카드의 숫자의 합이 21을 넘지 않는 경우, 카드를 계속 뽑는 경우를 Hit, 멈추는 것을 Stay로 하기 때문에 doHit() doStay()로 정의하였다. 이외에 카드 숫자의 합이 21을 초과하는 경우 Burst라고 한다. 앞의 Handwriting의 Blackjack 게임의 룰에서도 정리를 하였지만, Dealer는 Dealer hit rule이라는 규칙을 가지고 있기 때문에, 카드의 숫자합이 16이하이면 Hit, 17이상이면 무조건 Stay하도록 한다. 이는 상대 Player의 카드 상태와 상관없이 무조건적으로 Dealer가 지켜야하는 Rule이다. 한 번 개인적으로 생각해서 재미있는 기능을 넣어보았는데, 이 기능은 사람 클래스에 각자의 인격을 정의할 수 있는 personality를 적용하는 것이다. PersonInCasino 추상 클래스에 personality라는 class variable이 있는데, 이는 사람 성격에 따라 21미만의 카드를 뽑은 상태에서 카드를 계속 뽑을지(Hit) 아니면 멈출지(Stay)를 각기 다른 확률을 적용한다. 예를들어 소심한 사람은 20~30%, 평범한 사람은 50%, 욕심이 많은 사람은 80~90%의 확률로 카드를 계속 뽑는 결정을 하게 된다. 구체적인 구현 방법은 Python의 randchoice method에서 주어진 리스트에 대해 각 각 weight를 부여해서 확률을 조정할 수 있는데, 이를 활용해서 각기다른 성격을 가진 사람들이 다른 확률로 Hit을 한다. Hand class의 ring finger, middle finger, back of hand를 정의한 이유는 게임을 할때 player의 gesture가 있기 때문이다.ring finger(검지)로 테이블을 톡 내려주는 gesture를 취하는 경우는 Hit, 검지와 중지(middle finger + ring finger)를 테이블에 향하여 친 후 손가락을 떼는 gesture는 Split을 의미한다. 그리고 손등(back of hand)이 위로 보이게 해서 손목스냅을 이용해 테이블을 한번 훑어주는 모션은 Stay이다. has-A 의존관계 중에 클래스간의 응집력이 높은 Composition의 형태로 Hand, Card, Game class로 Player와 Dealer의 대응되는 각 각의 instance variable을 초기화시켜줬다. Card class는 Player와 Dealer 모두 동일한 카드 뭉치로부터 카드를 받고 있기 때문에 Card class의 cardList는 class 변수로써 선언을 하였다. 가능한 한 설계 단계에서 생각할 수 있는 경우의 수를 토대로 클래스 설계를 해보았다. 좀 더 가시적으로 클래스 관계를 이해할 수 있도록 Class UML(Unified Modeling Language)를 만들어서 첨부하였다. 코드 구현실제 게임으로 각 Player 객체를 Dealer 객체가 관리하기 위해 설계단계에서는 고려되지 않았던 Player management class(PlayerMgmt)를 새로 추가하였다. 그리고 기존에 설계했던 class의 일부 method를 새로 추가하였다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276from abc import ABCMeta, abstractmethodfrom random import choices, shuffle# (PersonInCasino Abstract class)# Abstract method# doHit() : 카드를 계속 뽑는다. (단, 카드의 숫자가 21미만인 경우)# doStay() : 카드를 뽑지 않고 머무른다. (단, dealer의 경우, Dealer hit rule 적용)# checkStatusOfHand() : 손의 상태를 확인한다. (gesture : Hit, Stay, Split)# checkStatusOfGame() : 게임의 진행상태를 확인한다. (Hit, Stay, Burst, Blackjack, Split, Double down)# (단, Split과 Double down은 player를 위한 규칙이다)# checkCardsOnDeck() : Deck위에 카드를 확인한다. (완전탐색)# setPersonality() : 사람의 인격을 정의한다.# Player와 Dealer는 모두 Blackjack 카드 게임에 참여하므로, Blackjack게임을 위한# 공통 행동을 abstract class의 abstract method로 정의한다.class PersonInCasino(metaclass=ABCMeta): @abstractmethod def doHit(): raise NotImplementedError() @abstractmethod def doStay(): raise NotImplementedError() @abstractmethod def checkStatusOfHand(): raise NotImplementedError() @abstractmethod def checkStatusOfGame(): raise NotImplementedError() @abstractmethod def checkCardsOnDeck(): raise NotImplementedError() @abstractmethod def setPersonality(): raise NotImplementedError()# Dealer class# Personality는 &quot;timid&quot;(25%확률로 Hit), &quot;normal&quot;(50%확률로 Hit), &quot;greedy&quot;(75%의 확률로 Hit)로 구분하며,# timid =&gt; [0, 1] weight = [0.75, 0.25]# normal =&gt; [0, 1] weight = [0.5, 0.5]# greedy =&gt; [0, 1] weight = [0.25, 0.75]class Dealer(PersonInCasino): def __init__(self, _personality): self.personality = _personality self.statusOfGame = Game() self.statusOfHand = Hand() self.cardDeck = CardDeck() # Dealer가 일괄적으로 Player를 관리할 수 있도록 playerMgmt instance variable을 갖는다. self.playerMgmt = PlayerMgmt() # Blackjack 게임에서 사용될 카드들 self.card = Card() # Player의 카드숫자 합이 21을 초과한 경우에 Player의 Game Status를 Burst로 업데이트하고 # Player리스트에서 제거한 뒤에 BurstPlayer리스트에 업데이트한다. # Burst를 제외한 게임상태의 업데이트는 Player로부터 한다. def updateBurstPlayer(self, index): self.playerMgmt.updateBurstPlayer(index) def doHit(self): self.statusOfGame.updateGameStatus(&quot;Hit&quot;) def doStay(self): self.statusOfGame.updateGameStatus(&quot;Stay&quot;) def checkStatusOfHand(self): return self.statusOfHand def checkStatusOfGame(self): return self.statusOfGame def checkCardsOnDeck(self): return self.cardDeck def getDecisionOfHit(self): weight = {&quot;timid&quot;: [0.75, 0.25], &quot;normal&quot;: [ 0.5, 0.5], &quot;greedy&quot;: [0.25, 0.75]} return choices([0, 1], weights=weight[self.personality]) # 게임 참가자 수 만큼 카드를 한 장씩 나눠주고 1회 턴이 돌았을때 # 각 게임 참가자의 카드를 검사해서 카드 수의 총 합이 21 초과인 경우 burst player로 구분한다. def pickupAndGiveCardToPlayer(self): # card 뭉치에서 카드를 pop()으로 뽑느다. # 우선 카드정보 리스트를 정의할 필요가 있다. numOfPlayer = self.playerMgmt.playerList for i in range(len(numOfPlayer)): if self.card.cardList: card = self.card.pickupTheCard() self.playerMgmt.updateCard(i, card) # 카드를 전체적으로 한 장씩 나눠주고, 각 Player의 카드정보를 확인하여, # 카드 숫자의 합이 21을 초과하는 사람은 updateBurstPlayer() method를 # 사용하여 burst player로 구분한다. for j in range(len(numOfPlayer)): player = self.playerMgmt.checkPlayer(j) cardList = player['cardDeck'].checkCardStatusOfUser() _sum = 0 for n in cardList: if n == 'A': if sum(cardList) &gt; 10: sum += 1 else: sum += 11 else: sum += n if _sum &gt; 21: burstPlayer = self.playerMgmt.playerList.pop(j) self.playerMgmt.burstPlayerList.append(burstPlayer)# Player class# Personality는 &quot;timid&quot;(25%확률로 Hit), &quot;normal&quot;(50%확률로 Hit), &quot;greedy&quot;(75%의 확률로 Hit)로 구분하며,# timid =&gt; [0, 1] weight = [0.75, 0.25]# normal =&gt; [0, 1] weight = [0.5, 0.5]# greedy =&gt; [0, 1] weight = [0.25, 0.75]class Player(PersonInCasino): def __init__(self, _personality): self.personality = _personality self.statusOfGame = Game() self.cardDeck = CardDeck() self.playerMgmt = PlayerMgmt() # Player 객체 정의와 동시에 Player정보를 player management class의 # Player 정보 리스트에 담아준다. self.__updatePlayerInfo() def doHit(self): self.statusOfGame.updateGameStatus(&quot;Hit&quot;) def doStay(self): self.statusOfGame.updateGameStatus(&quot;Stay&quot;) def checkStatusOfHand(self): return self.statusOfHand def checkStatusOfGame(self): return self.statusOfGame def checkCardsOnDeck(self): return self.cardDeck def getDecisionOfHit(self): weight = {&quot;timid&quot;: [0.75, 0.25], &quot;normal&quot;: [ 0.5, 0.5], &quot;greedy&quot;: [0.25, 0.75]} return choices([0, 1], weights=weight[self.personality]) # playerMgmt(Player management) class의 Player 정보 리스트를 업데이트 def __updatePlayerInfo(self): playerInfo = {&quot;statusOfGame&quot;: self.statusOfGame, &quot;statusOfHand&quot;: self.statusOfHand, &quot;personality&quot;: self.personality, &quot;cardDeck&quot;: self.cardDeck} self.playerMgmt.updatePlayerInfo(playerInfo)# Hand class# 약지(ring finger)을 사용해서 gesture를 하는경우 =&gt; Hit# 손등(backOfHand)을 사용해서 gesture를 하는경우 =&gt; Stay# 약지와 중지(ring finger + middle finger)를 사용해서 gesture를 하는 경우 =&gt; Splitclass Hand: def __init__(self): self.ringFinger = False self.middleFinger = False self.backOfHand = False def useRingFinger(self): self.ringFinger = True def useMiddleFinger(self): self.middleFinger = True def useBackOfHand(self): self.backOfHand = True def resetStatusOfHand(self): self.ringFinger = False self.middleFinger = False self.backOfHand = False# Card class# 카드의 종류와 상관없이 2~9까지는 각 각 4장씩 넣고# 10은 10(4장) J,Q,K 각 각 4장씩 총 16장을 넣어준다.# Ace의 경우에는 1또는 11로 계산될 수 있기 때문에 'A'로 4장 넣어준다.class Card: cardList = [] def __init__(self): self.prepareCards() @classmethod def prepareCards(cls): # 2~9까지 숫자카드를 각 각 4장씩 넣는다. for i in range(2, 9+1): for _ in range(4): cls.cardList.append(i) # 숫자카드 10을 16장 넣어준다. for _ in range(16): cls.cardList.append(10) # Ace 카드를 4장 넣어준다. for _ in range(4): cls.cardList.append('A') # 카드를 섞어준다. cls.__shuffleCards() @classmethod def pickupTheCard(cls): return cls.cardList.pop() @classmethod def __shuffleCards(cls): shuffle(cls.cardList)# CardDeck classclass CardDeck: def __init__(self): self.bettingChipArea = 0 self.cardArea = [] def checkBettingChipArea(self): return self.bettingChipArea def checkCardStatusOfUser(self): return self.cardArea def updateBettingChipArea(self, chip): self.bettingChipArea += chip def updateCardArea(self, card): self.cardArea.append(card)# Game class# player가 split을 하는 경우, numOfLife를 증가시켜준다.class Game: def __init__(self): self.numOfLife = 1 self.status = &quot;&quot; def updateNumOfLife(self, life): self.numOfLife += life def updateGameStatus(self, _status): self.status = _status# Player객체를 일괄적으로 관리하기 위한 Player management classclass PlayerMgmt: playerList = [] burstPlayerList = [] # index번째 player의 객체 정보를 참조할 수 있도록 객체를 반환한다. @classmethod def checkPlayer(cls, index): if cls.playerList[index]: return cls.playerList[index] else: print(&quot;해당 Player는 존재하지 않습니다.&quot;) @classmethod # Blackjack참가 Player의 객체 정보를 playerList에 업데이트한다. def updatePlayerInfo(cls, player): cls.playerList.append(player) @classmethod def updateBurstPlayer(cls, index): # list의 index는 0번째부터 시작하기 때문에 -1을 해서 index-1번째 player를 pop한 뒤에 # burstPlayer list에 업데이트를 해준다. if cls.playerList[index-1]: burstPlayer = cls.playerList.pop(index-1) else: print(&quot;해당 Player는 존재하지 않습니다.&quot;) # index번째 Player의 carddeck의 카드를 업데이트한다. @classmethod def updateCard(cls, index, card): cls.playerList[index]['cardDeck'].updateCardArea(card)","link":"/2021/02/11/202102/210211-black_jack_game/"},{"title":"210212 React Router Unit Test","text":"React Router Test이번 포스팅에서는 React router testing에 대해서 정리해보려고 한다.React router test파일은 &lt;App/&gt; component에 대한 test file을 작성한다. 우선 기본적으로 작성할 테스트 케이스의 내용을 정리해보자. (Test case 1) 처음 &lt;App/&gt; component가 rendering될때 root path와 matching되는 Home component가 rendering되는가? (Test case 2) &lt;nav&gt; 태그로 만든 각 menu를 클릭했을때 각 각의 matching되는 component로 rendering이 되는가? (Test case 3) route에 해당하는 path가 없는 경우에는 404 Not Found component가 rendering되는가? (Test case 4) 페이지가 loading되는 동안 &lt;Loading /&gt; component가 페이지가 표시되는가? Test case 4의 Loading component가 DOM에 그려지는 것을 확인하기 위해서 component를 Lazy하게 가져와야 한다. Component를 Lazy하게 가져오기12345678910111213141516171819202122import React, { Suspense, lazy } from 'react';import { Route, Redirect, Switch } from 'react-router-dom';import Loading from './Loading';// page component를 가져올때 lazy하게 가져온다.const Home = lazy(() =&gt; import('../../pages/Home'));const Menu1 = lazy(() =&gt; import('../../pages/Menu1'));const Menu2 = lazy(() =&gt; import('../../pages/Menu2'));const NotFound = lazy(() =&gt; import('../../pages/NotFound'));// 기본적으로 component를 lazy하게 가져오기 위해서는 아래와 같이// Suspense의 fallback속성으로 Loading component를 넣어서// Switch를 감싸줘야 한다.&lt;Suspense fallback={&lt;Loading /&gt;}&gt; &lt;Switch&gt; &lt;Route exact path=&quot;/&quot; component={Home} /&gt; &lt;Route path=&quot;/menu1&quot; component={Menu1} /&gt; &lt;Route path=&quot;/menu2&quot; component={Menu2} /&gt; &lt;Route path=&quot;/notfound&quot; component={Notfound} /&gt; &lt;Redirect to=&quot;/notfound&quot; /&gt; &lt;/Switch&gt;&lt;/Suspense&gt;; history - createMemoryHistory createMemoryHistory() 객체를 만들어서 전이될 화면에 대한 정보를 push()를 이용해서 넣어 줄 수 있다. 준비된 history 객체는 Router의 history property로 넣어주면 자동으로 Route에서 해당 경로(path)에 맞는 component를 rendering을 해주게 되고, 결과적으로 화면에 표시된 정보를 확인할 수 있다. @testing-library/react - fireEventhttps://testing-library.com/docs/dom-testing-library/api-events/ fireEvent는 화면상에서 Click과 같은 Event를 발생시킬때 사용된다.이벤트를 발생시키고자 하는 Component에 data-testid 속성으로 id를 지정한 다음에 test하는 component를 render시킴으로써 getByTestId로부터 해당 id에 접근을 해서 fireEvent를 통해 event를 발생시킬 수 있다. wait()wait()함수는 Promise를 객체를 반환하고 기본적으로 50ms마다 wait()에 넘겨준 callback을 실행하고 timeout은 4500ms이다. wait()에 argument로 별도로 넘겨주는 callback 함수가 없다면, 빈 함수를 실행한다.서버와 api 통신하는 경우에는 timeout이 발생할 수 있으므로, 이 경우에는 별도로 callback함수를 넣어서 처리해준다.testing-library의 version이 10이상인 경우에는 waitFor()함수를 사용하고, 이전 버전을 사용하는 경우에는 wait()함수를 사용하도록 하자. Lazy &amp; Suspensehttps://ja.reactjs.org/docs/react-api.html#reactlazy React.lazy()를 사용하면 동적으로 된 구성요소를 정의할 수 있다. 이렇게 정의를 하게되면, bundle의 크기가 감소하여 최초 render시에 사용되지 않는 구성요소의 로드를 지연시킬 수 있다. Suspense를 사용하면 component가 rendering되기 전에 다른 작업이 먼저 이루어지도록 대기한다. 또한 React.lazy를 사용하여 Component를 동적으로 불러오는 것을 지원합니다. Sample Test code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import React from 'react';import { render, cleanup, wait, ,fireEvent } from '@testing-library/react';import { createMemoryHistory } from 'history';import { Route, Router, Switch } from 'react-router-dom';import App from './App';// page componentsimport Home from '../pages/Home';import Menu1 from '../pages/Menu1';import Menu2 from '../pages/Menu2';import NotFound from '../pages/NotFound';// 각 테스트가 끝난뒤에 초기화afterEach(cleanup);describe('&lt;App /&gt;', () =&gt; { // Test case 1 test('&lt;App /&gt; rendering시에 /(root)로 rendering이 되는가?', async() =&gt; { const { getByTestId, getByText } = render(&lt;App /&gt;); const navbar = getByTestId('navbar'); const link = getByTestId('HOME'); // 각 각의 page componet를 lazy하게 가져오고 있기 때문에, 비동기로 wait() 처리를 해준다. await wait(); expect(getByText(/Hello world/i)).toBeInTheDocument(); expect(navbar).toContainElement(link); }); // Test case 2 test('MENU1 menu를 클릭시, MENU1 component가 화면에 rendering되는가?', async() =&gt; { const { getByTestId } = render(&lt;App /&gt;); fireEvent.click(getByTestId('MENU1')); await wait(); // menu1-wrapper id를 가진 component가 화면에 rendering되어있는지 확인 expect(getByTestId('menu1-wrapper')).toBeInTheDocument(); }); // Test case 2 test('MENU2 menu를 클릭시, MENU2 component가 화면에 rendering되는가?', async() =&gt; { const { getByTestId } = render(&lt;App /&gt;); fireEvent.click(getByTestId('MENU2')); await wait(); // menu2-wrapper id를 가진 component가 화면에 rendering되어있는지 확인 expect(getByTestId('menu2-wrapper')).toBeInTheDocument(); }); // Test case 3 test('Route에 해당되는 path가 없는 경우, /notfound component로 rendering되는가?', async() =&gt; { const history = createMemoryHistory(); history.push('/not/found'); const { getByTestId } = render( &lt;Router history={history}&gt; &lt;Switch&gt; &lt;Route exact path=&quot;/&quot; component={Home} /&gt; &lt;Route path=&quot;/menu1&quot; component={Menu1} /&gt; &lt;Route path=&quot;/menu2&quot; component={Menu2} /&gt; &lt;Route component={NotFound} /&gt; &lt;/Switch&gt; &lt;/Router&gt; ); await wait(); expect(getByTestId('404-notfound')).toBeInTheDocument(); });}); (20210213 업데이트) &lt; React router unit test code practice &gt;진행중인 ReactJS Side Project에서 Router에 대한 unit test code를 작성해서 테스트해보았다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import React from 'react';import { render, cleanup, waitFor, fireEvent } from '@testing-library/react';import '@testing-library/jest-dom';import { createMemoryHistory } from 'history';import { Route, Router, Switch } from 'react-router-dom';import App from '../Components/App';// Screen componentsimport IT from '../Screens/IT';import Development from '../Screens/Development';import Search from '../Screens/Search';import NotFound from '../Screens/NotFound';// 각 테스트가 끝난뒤에 초기화afterEach(cleanup);describe('Routing test in &lt;App /&gt; component', () =&gt; { // Test case 1 test('&lt;App /&gt; component가 rendering될 시에 /로 rendering이 되는가?', async () =&gt; { const { getByText } = render(&lt;App /&gt;); await waitFor(() =&gt; { // toBeInTheDocument는 @testing-library/jest-dom을 import해야만 사용할 수 있다. // rendering된 page의 text확인 expect(getByText(/Computer\\/Internet/)).toBeInTheDocument(); }); }); // Test case 2 test('Navigation Header에 세 개의 menu(it, development, search) link가 존재하는가?', async () =&gt; { const { getByTestId } = render(&lt;App /&gt;); // 화면에 링크가 포함되어있는지 확인 const navbar = getByTestId('navbar'); const itLink = getByTestId('it'); const devLink = getByTestId('development'); const searchLink = getByTestId('search'); await waitFor(() =&gt; { expect(navbar).toContainElement(itLink); expect(navbar).toContainElement(devLink); expect(navbar).toContainElement(searchLink); }); }); // Test case 3 test('IT menu를 클릭시, IT component가 화면에 rendering되는가?', async () =&gt; { const { getByTestId, getByText } = render(&lt;App /&gt;); fireEvent.click(getByTestId('it')); await waitFor(() =&gt; { // 화면에 it component의 wrapper class가 있는지 확인 expect(getByTestId('it-wrapper')).toBeInTheDocument(); }); }); // Test case 3-1 test('Development menu 클릭시, Development component가 화면에 rendering되는가?', async () =&gt; { const { getByTestId } = render(&lt;App /&gt;); fireEvent.click(getByTestId('development')); await waitFor(() =&gt; { //화면에 development component의 wrapper class가 있는지 확인 expect(getByTestId('development-wrapper')).toBeInTheDocument(); }); }); // Test case 3-2 test('Search menu 클릭시, Search component가 화면에 rendering되는가?', async () =&gt; { const { getByTestId } = render(&lt;App /&gt;); fireEvent.click(getByTestId('search')); await waitFor(() =&gt; { //화면에 search component의 wrapper class가 있는지 확인 expect(getByTestId('search-wrapper')).toBeInTheDocument(); }); }); // Test case 4 test('Route에 해당되는 path가 없는 경우, NotFound component로 rendering이 되는가?', async () =&gt; { const history = createMemoryHistory(); history.push('/not/found'); const { getByTestId } = render( &lt;Router history={history}&gt; &lt;Switch&gt; &lt;Route exact path=&quot;/&quot; component={IT} /&gt; &lt;Route path=&quot;/development&quot; component={Development} /&gt; &lt;Route path=&quot;/search&quot; component={Search} /&gt; &lt;Route component={NotFound} /&gt; &lt;/Switch&gt; &lt;/Router&gt; ); await waitFor(() =&gt; { expect(getByTestId('404-notfound-wrapper')).toBeInTheDocument(); }); });});","link":"/2021/02/12/202102/210212-React_testing_library_router_test/"},{"title":"210212 React의 Router개념과 기본사용","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. ReactJS Router 웹페이지의 링크를 클릭하면, 다른 화면으로의 전이가 일어나는데, 이러한 화면 전이와 관련된 routing은 React의 react-router-dom이라는 Routing package를 사용해서 정의한다. React Router Installationreact-router-dom은 navigation을 만들어 주기 위한 package이다. 1$ yarn add react-router-dom Router.js 작성하기Router와 Route를 사용하여 path와 해당 path로 이동했을때 화면에 보여질 component를 정의한다. 123456789import React from 'react';import { HashRouter as Router, Route } from 'react-router-dom';import Home from 'Screens/Home';export default () =&gt; { &lt;Router&gt; &lt;Route exact path=&quot;/&quot; component={Home} /&gt; &lt;/Router&gt;;}; Switch는 child component의 Route에 match되는 첫번째 요소를 rendering해준다. Switch를 사용하면, 하나의 match되는 Route가 rendering되는 것을 보장한다. 1234567891011import React from 'react';import { HashRouter as Router, Route } from 'react-router-dom';import Home from 'Screens/Home';export default () =&gt; { &lt;Router&gt; &lt;Switch&gt; &lt;Route path=&quot;/&quot; exact component={Home} /&gt; &lt;/Switch&gt; &lt;/Router&gt;;}; Switch와 exact 사용의 차이그럼 Switch의 사용과 Route의 속성으로 exact으로 넣어주는 것은 어떤 차이가 있을까? 1234567891011// 아래 두 경우는 모두 같은 결과를 보여준다.// Switch와 exact 사용&lt;Switch&gt; &lt;Route exact path=&quot;/&quot; component={Home} /&gt; &lt;Route path=&quot;/a&quot; component={A} /&gt; &lt;Route path=&quot;/b&quot; component={B} /&gt;&lt;/Switch&gt;// exact 속성 사용&lt;Route exact path=&quot;/&quot; component={Home} /&gt;&lt;Route path=&quot;/a&quot; component={A} /&gt;&lt;Route path=&quot;/b&quot; component={B} /&gt; 만약 nested route를 사용하지 않는 경우에는 상관이 없지만, 아래와 같이 Home component의 내부에 중첩된 형태로 Route가 사용된다면, 그리고 사용자가 /dashboard 경로로 /dashboard 페이지에 접근하려고 한다면, top level의 Route에는 /dashboard와 일치하는 Route가 없기 때문에 page를 rendering할 수 없다. 12345678910// top level Router - Route&lt;Route exact path=&quot;/&quot; component={Home} /&gt;;// Home component - nested routesconst Home = (props) =&gt; ( &lt;&gt; &lt;Route path=&quot;/dashboard&quot; component={Dashboard} /&gt; &lt;Route path=&quot;/layout&quot; component={Layout} /&gt; &lt;/&gt;); 위와같이 중첩된 Route를 포함하여 모두 확인한 뒤에 일치하는 경로의 component를 rendering해주기 위해서는 아래와 같이 Switch 구문을 사용해서 top-level Route를 감싸줘야 한다. 12345&lt;Switch&gt; &lt;Route path=&quot;/a&quot; component={A} /&gt; &lt;Route path=&quot;/b&quot; component={B} /&gt; &lt;Route path=&quot;/&quot; component={Home} /&gt;&lt;/Switch&gt; '/'(root)에 해당되는 Route를 제일 마지막에 위치시킴으로써 별도로 '/'(root) Route에 exact를 넣어주지 않았다. 작성시 주의사항SPA(Single Page Application)의 특성을 가진 React는 별도의 화면 전환 없이 페이지에 표시되는 component를 다르게 함으로써 화면 전환이 되는 것 처럼 보여지게 한다.이러한 특징으로인해 Router내에 여러 Route를 정의할때 만약 같은 path주소를 기반으로 시작하는 Route가 복수 개 정의되어있다면, 화면에 중첩되어 표시가 된다. 이러한 문제를 해결하기 위해서 Route의 props에 exact={true} 혹은 exact속성을 추가해주거나 Switch를 사용해서 Route를 감싸서 처리한다.그러면 path주소의 기반 주소가 같더라도 path 전체와 완전히 일치하는 경우에만 해당 component를 rendering해준다. HashRouter와 BrowserRouter의 차이 BrowserRouter HTML5의 history API를 활용해서 UI를 업데이트 한다. 동적인 페이지에 적합하다. (서버에 있는 데이터들을 스크립트에 의해 가공처리한 후 생성되는 웹 페이지) 새로고침을 하면 경로를 찾지 못해서 에러가 발생한다.→ 해결을 위해서 서버에 추가적인 세팅이 필요하다. 페이지의 유무를 서버에 알려줘야하며, 서버 setting시에 search engine에 신경을 써야 한다. GitHub Page에 배포시에 설정이 복잡하다. HashRouter URL의 hash를 활용한 Router이다. 주소에 #이 붙는다. 정적인 페이지에 적합하다. (미리 저장된 페이지가 그대로 보여지는 웹 페이지) 검색 엔진으로 읽지 못한다. #값으로 인해 서버가 읽지 못하고 서버가 페이지의 유무를 알지 못한다. 새로고침을 해도 에러가 발생하지 않는다. GitHub Pages에서 배포시에 설정이 간편하다. withRouter의 사용 Router component의 하위에 있지 않지만, router의 기능을 사용해야될때 사용된다. react-router-dom은 withRouter를 제공해주는데 component를 감싸서 Router로부터 어떠한 정보를 전달받는 역할을 한다. HOC(Higher-Order-Component)로 with()명명 규칙을 따르며, Component를 인자로 받아서 새로운 Component를 반환하는 함수이다. 예를들어, Header component를 withRouter로 감싸주게 되면, parameter를 통해 Router의 location정보를 읽어올 수 있다. 사용예시) component export구문에서 component를 withRouter()로 감싸주기 1234567891011121314const Header = (props) =&gt; ( &lt;Header&gt; &lt;List&gt; &lt;Item current={props.pathname === '/'}&gt; &lt;SLink to=&quot;/&quot;&gt;Home&lt;/SLink&gt; &lt;/Item&gt; &lt;Item current={props.pathname === '/about'}&gt; &lt;SLink to=&quot;/about&quot;&gt;About&lt;/SLink&gt; &lt;/Item&gt; &lt;/List&gt; &lt;/Header&gt;);export default withRouter(Header); 사용예시) 별도의 component정의 없이 바로 export 구문에서 withRouter()로 감싸주기 123456789101112export default withRouter(() =&gt; ( &lt;Header&gt; &lt;List&gt; &lt;Item current={props.pathname === '/'}&gt; &lt;SLink to=&quot;/&quot;&gt;Home&lt;/SLink&gt; &lt;/Item&gt; &lt;Item current={props.pathname === '/about'}&gt; &lt;SLink to=&quot;/about&quot;&gt;About&lt;/SLink&gt; &lt;/Item&gt; &lt;/List&gt; &lt;/Header&gt;)); &lt;a&gt; tag 말고 react-router-dom의 &lt;Link&gt; 사용하기일반적인 웹 어플리케이션에서 특정 링크로 화면을 전이하기 위해서는 a 태그가 사용된다. 하지만 React에서는 react-router-dom module에서 Link를 제공한다.a tag를 이용한 페이지 이동과 Link를 이용한 페이지 이동의 차이는 a tag를 이용한 페이지 이동은 매번 페이지가 이동이 될때마다 기존의 React가 새 페이지와 함께 새로고침되는 현상이 일어난다.기존의 React의 상태를 보존시키면서 페이지를 전이시키기 위해서 Link를 이용해서 페이지를 이동시키도록 한다.Link의 사용조건은 반드시 Router 내부에서 사용이 되어야 한다는 것이다.그래서 Link가 사용이 된 Navigation component가 Router가 사용된 Router component내부에 종속되어 사용이 되었다. 12345678910111213import React from 'react';import { Link } from 'react-router-dom';function Navigation() { return ( &lt;&gt; &lt;Link to=&quot;/&quot;&gt;Home&lt;/Link&gt; &lt;Link to=&quot;/about&quot;&gt;About&lt;/Link&gt; &lt;/&gt; );}export default Navigation; Routes 사이에서 Props 공유하기페이지 전환을 할 functional component 내에서 props를 확인해보면, 아래 첨부한 캡처와 같이 history, location, match, staticContext등의 항목이 전달되고 있음을 확인할 수 있다. (history의 속성을 보면, go, goBack, goForward, push 등의 속성을 확인할 수 있는데, 이를 이용해서 페이지를 전이할 수 있다. 예) history.push(&quot;/&quot;)) 앞서 Link를 활용하여 페이지 이동을 할 pathname을 넣어주었는데, 단순 pathname이 아닌 객체의 형태로 여러 정보들을 일괄적으로 함께 보내줄 수도 있다.예를들어, 상세보기 페이지로 이동을 한다고 했을때, 상세보기 페이지로 넘겨 줄 정보를 Link props의 state에 객체형태로 전달해 줄 수도 있다.전달한 정보는 props의 location에 있는 state에 객체의 형태로 전달받을 수 있다. 12345678&lt;Link to={{ pathname: '/', search: '?sort=name', hash: '#the-hash', state: { fromDashboard: true } }}/&gt; Class component에서 전달받은 Props 확인하기render()가 끝난 다음에 실행되는 ComponentDidMount() method를 통해서 location.state의 데이터를 확인해 볼 수 있다. 123456789101112import React from 'react';class DetailPage extends React.Component { componentDidMount() { const { location } = this.props; console.log(location.state); } render() { return &lt;span&gt;Detail page&lt;/span&gt;; }}","link":"/2021/02/12/202102/210212-React_Router/"},{"title":"210213 Bundle size와 Lazy loading","text":"Bundle size와 Lazy loadingReact routing test를 하면서 각 각의 Component를 lazy loading하여 테스트를 진행하였다. 이 lazy loading(지연로딩)에 대해서 좀 더 자세하게 알아두고자 포스팅으로 정리를 해둔다. React app의 크기가 커짐에 따라 개발자는 bundle size에 대한 걱정을 시작해야 할 수 있다. 이 bundle size란 사용자가 React app을 load하기 위해 download해야만 하는 JavaScript의 양을 의미한다.이 Bundle size가 클수록 사용자가 app으르 보는데 시간이 오래걸린다. 따라서 이 Lazy를 사용해서 component가 rendering 될때에 사용되지 않는 구성요소에 대한 로드를 지연시킬 수 있다. As your React app gets larger, you may have to start worrying about its bundle size. An app’s bundle size is the amount of JavaScript a user will have to download to load your app. The bigger the bundle size is, the longer it will take before a user can view your app. Googling하면서 React app의 bundle 사이즈를 분석하고 코드 분할(Code splitting)을 통해 bundle 사이즈를 줄이는 것과 관련된 괜찮은 내용의 블로그를 찾아서 아래에 첨부해두었다. React.lazy와 Suspense를 사용한 Code-splitting도 bundle size를 줄이는 방법 중에 하나이다. https://www.emgoto.com/react-bundles-and-code-splitting/","link":"/2021/02/13/202102/210213-React_testing_library_lazy_suspense/"},{"title":"210214 Python HackerRank Assignment","text":"설날 과제로 강사님이 내주신 HackerRank의 과제에 대해서 정리한다. 문제 1Problem1 Link(HackerRank) 이 문제는 주어진 클래스의 상속관계를 완성하고, 주어진 조건으로 내부에 method를 완성시키면 되는 간단한 문제이다. 123456789101112131415161718192021222324252627282930313233class Student(Person): # Class Constructor # # Parameters: # firstName - A string denoting the Person's first name. # lastName - A string denoting the Person's last name. # id - An integer denoting the Person's ID number. # scores - An array of integers denoting the Person's test scores. # # Write your constructor here def __init__(self, _firstName, _lastName, _id, _scores): Person.__init__(self, _firstName, _lastName, _id) self.scores = _scores # Function Name: calculate # Return: A character denoting the grade. # # Write your function here def calculate(self): average = sum(self.scores) / len(self.scores) if 90 &lt;= average &lt;= 100: grade = &quot;O&quot; elif 80 &lt;= average &lt; 90: grade = &quot;E&quot; elif 70 &lt;= average &lt; 80: grade = &quot;A&quot; elif 55 &lt;= average &lt; 70: grade = &quot;P&quot; elif 40 &lt;= average &lt; 55: grade = &quot;D&quot; else: grade = &quot;T&quot; return grade 문제 2Problem2 Link(HackerRank) 이 문제는 추상클래스 관련 문제로, Book 추상 클래스를 MyBook에서 구현하는 문제이다. 12345678910111213141516171819from abc import ABCMeta, abstractmethodclass Book: __metaclass__ = ABCMeta def __init__(self,title,author): self.title=title self.author=author @abstractmethod def display(): pass#Write MyBook classclass MyBook(Book): def __init__(self, _title, _author, _price): Book.__init__(self, _title, _author) self.price = _price def display(self): print(&quot;Title: {}&quot;.format(self.title)) print(&quot;Author: {}&quot;.format(self.author)) print(&quot;Price: {}&quot;.format(self.price)) 문제 3Problem3 Link(HackerRank) 이 문제는 복소수의 덧셈, 뺄셈, 곱셈, 나눗셈, 나머지 연산에 대한 각 각의 class method를 작성하는 문제이다. 1234567891011121314151617181920212223242526272829303132333435import mathclass Complex(object): def __init__(self, real, imaginary): self.real = real self.imaginary = imaginary # 실수부와 허수부를 각 각 나눠서 더해준다. def __add__(self, no): return Complex(self.real + no.real, self.imaginary + no.imaginary) # 실수부와 허수부를 각 각 나눠서 빼준다. def __sub__(self, no): return Complex(self.real - no.real, self.imaginary - no.imaginary) # (a + bi)(c + di) = (ac - bd) + (ad + bc)i # 실수부끼리의 곱과 허수부끼리의 곱을 빼고, 허수부와 실수의 곱과 실수와 허수의 곱을 더해준다. def __mul__(self, no): return Complex(self.real*no.real - self.imaginary*no.imaginary, self.imaginary*no.real + self.real*no.imaginary) def __div__(self, no): # 실수부의 제곱과 허수부의 제곱을 더한 값 x = float(no.real ** 2 + no.imaginary ** 2) # imaginary의 부호를 반대로 하여 켤레수를 곱해준다. y = self * Complex(no.real, -no.imaginary) # 켤레수를 곱한 값의 실수값을 x로 나눠서 실수를 구한다. real = y.real / x # 켤레수를 곱한 값의 허수값을 x로 나눠서 허수를 구한다. imaginary = y.imaginary / x return Complex(real, imaginary) # 실수의 제곱과 허수의 제곱을 더해서 루트연산을 해준다. def mod(self): real = math.sqrt(self.real ** 2 + self.imaginary ** 2) return Complex(real, 0)","link":"/2021/02/14/202102/210214-Python_HackerRank_assignment/"},{"title":"210215 Python TIL 1&#x2F;2 - Module &amp; Package, Library &amp; Framework, Poetry, Virtualenv","text":"Module, Package Module? Python 코드로 이루어진 파일로, 본체에 대한 하위 단위로 정의한다. Package? 관련 여러 모듈들을 하나의 폴더로 묶은 것을 말한다. (Module, Package관련 실습내용)(1) fibo.py와 main.py 파일을 만들어서 실습을 진행한다.fibo에 정의되어있는 text변수의 값을 main.py에서 출력한다. 123from fibo import textprint(text) (2) fibo directory 내에 binet.py와 rec.py 파일을 생성해서 각 각에 맞는 함수를 구현한다. main.py에서 moulde로써 import해서 사용한다. (VSCode에서는 module로써 사용할 디렉토리내에는 __init__.py파일을 넣어준다.) 123456#from fibos import fibo#from fibos import binet as bn#from fibos import fibo, binet#from fibos import *#import print_something#from . import print_something Library &amp; Framework라이브러리와 프레임워크의 차이에 대해서 알아보았다.이전에 개인적으로 블로그에 포스팅했던 내용이 있는데, 참고해도 좋을 것 같다.https://leehyungi0622.github.io/2021/02/06/202102/210206-Self-Framework_and_Library_Difference/ Library 대표적인 예로 jQuery가 있다. 함수와 기능의 모음을 말한다. 파이썬의 print(내장함수)와 math(모듈)등과 같은 파이썬 표준 라이브러리도 라이브러리에 속한다. You are in control, you call the library Framework 대표적인 예로 django, flask, expressJS가 있다. 개념들의 추상화를 제공하는 클래스와 컴포넌트로 구성되어 있다. Framework is in control, it calls you 1234567from flask import Flaskapp = Flask(__name__)@app.route('/')def get_index(): return 'this is home' Poetry? Node.js에서 package.json과 같은 역할을 하는 것이 Django의 requirements.txt인데, Poetry를 사용해서 requirements.txt의 관리를 좀 더 편하게 할 수 있다. poetry 설치설치를 완료하고 poetry init 까지 완료되었다면, pyproject.toml config파일을 확인할 수 있다. 이 파일을 통해서 설치된 dependency를 관리할 수 있다. pyproject.toml 12345[too.poetry.dependencies]python = &quot;^3.9&quot;# 프로젝트 실행을 위한 dependencies[tool.poetry.dev-dependencies]# 개발을 위한 dependencies ( 참고 : poetry installation reference ) 12345# Poetry 설치하기$ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -# shell 설정파일에 poetry bin폴더의 경로를 설정$ echo '$HOME/.poetry/bin:$PATH' &gt;&gt; ~/.zshrc zsh configuration 파일에 poetry bin폴더의 경로를 설정 넣어주기 123456# pyenv의 PATHexport PYENV_PATH=$HOME/.pyenvif which pyenv &gt; /dev/null; then eval &quot;$(pyenv init -)&quot;; fiif which pyenv-virtualenv-init &gt; /dev/null; then eval &quot;$(pyenv virtualenv-init -)&quot;; fi# poetry실행파일의 PATH가 pyenv의 PATH보다 우선되도록 설정export PATH=$HOME/.poetry/bin:$PATH .commonrc 파일을 생성해서 관리를 하면 공통적인 개발환경을 관리하기에 좋다.→ 개발환경은 global하게 사용할 가상환경과 local하게 프로젝트별로 사용할 가상환경을 구분해서 설정해놓는 것이 좋다. pyproject.toml 파일이 있는 위치에서 poetry add django==1.11로 django package를 설치해주면 아래와 같이 설치된 package를 확인할 수 있다. 12345678910111213141516[tool.poetry]name = &quot;practice_project&quot;version = &quot;0.1.0&quot;description = &quot;&quot;authors = [&quot;Lee Hyungi &lt;hyungi.lee.622@gmail.com&gt;&quot;][tool.poetry.dependencies]# ^표시는 2.7이상의 python에서 실행해야 된다는 의미이다.python = &quot;^2.7&quot;Django = &quot;1.11&quot;[tool.poetry.dev-dependencies][build-system]requires = [&quot;poetry-core&gt;=1.0.0&quot;]build-backend = &quot;poetry.core.masonry.api&quot; Poetry 사용관련 Commands 12345678910# initialize$ poetry init# add package$ poetry add &lt;package name&gt;# remove package$ poetry remove &lt;package name&gt;# show installed package$ poetry show --no-dev --tree# export package list to requirements.txt$ poetry export -f requirements.txt &gt; requirements.txt Poetry 실습 순서 1234567891011(1) 가상환경 생성하기(2) 프로젝트 디렉토리 만들기(3) poetry init(4) poetry add &lt;package name&gt;requestsflaskpillowbeautifulsoup4locust( poetry add --dev locust ) 가상환경을 생성할때에는 pyenv version번호를 virtualenv 이름에 넣어 생성해준다. (이름으로 어떤 버전이 설치되었는지 쉽게 유추할 수 있게 하기 위해) 12# awesome-390이름의 가상환경을 pyenv로 설치한 3.9.0로 생성해준다.$ virtualenv 3.9.0 awesome-390","link":"/2021/02/15/202102/210215-Python_til(1)/"},{"title":"210214 Blackjack CLI Based Game","text":"Blackjack CLI Based Game이번 과제는 Python을 활용해서 간단한 CLI Based Game을 만드는 것이다. 게임 실행결과 화면 Blackjack으로 게임을 종료하는 경우 뽑은 카드들의 수의 합이 21을 초과하여 상대편이 이기는 경우 코드 (Python)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243from random import shuffleimport reimport sys# player와 bank의 게임진행 상태player_play = Truebank_play = True# 뽑은 카드의 숫자 합이 21을 초과하는 경우의 palyer와 bankplayer_lose = Falsebank_lose = Falseturn = 0# card_deck에 카드를 준비def prepareCards(): for i in range(2, 11+1): for _ in range(4): card_deck.append('A' if i == 11 else i)# card deck을 3회 shuffledef shuffleCardsTreeTimes(): return [shuffle(card_deck) for _ in range(3)]# card deck에 남아있는 카드가 10장 미만일 경우에 card deck을 reset하고 다시 shuffle한다.def resetCards(): # 카드를 다시 준비한다. prepareCards() # 카드를 다시 3회 섞는다. shuffleCards()# 사용자로부터 Y(y) or N(n)를 올바르게 입력받았는지 정규표현검사def regexCheck(value): answer_check = re.compile(r'^(?:Y|N)$', re.I) return answer_check.match(value)# 사용자로부터 한 번 더 shuffle할 것인지 물어보는 처리def askMoreShuffle(): # 사용자로부터 한 번 더 shuffle을 할 것인지 묻기 while True: shuffle_more = input( '✨✨✨✨✨✨✨Shuffle을 한 번 더 하길 원하십니까? (Y(y)/N(n))✨✨✨✨✨✨✨') # 사용자로부터 입력받을 Y/N를 정규표현식으로 검사 answer = regexCheck(shuffle_more) if answer: if answer.group() == 'Y' or answer.group() == 'y': print(&quot;✨✨✨✨✨✨✨한 번 더 card deck의 shuffle을 진행합니다.✨✨✨✨✨✨✨&quot;) shuffle(card_deck) break else: print(&quot;✨✨✨✨✨✨✨추가적인 card deck의 shuffle을 진행하지 않습니다.✨✨✨✨✨✨✨&quot;) break else: print(&quot;🥲🥲🥲🥲🥲🥲🥲 입력을 확인후 다시 입력해주세요.🥲🥲🥲🥲🥲🥲🥲&quot;)# 뽑은 카드들의 숫자 합을 반환한다.def getSumOfCards(cards): sum_of_cards = 0 for idx, card in enumerate(cards): if card == 'A': # 우선 Ace 카드를 11로 치환해서 A 카드를 제외한 카드들의 숫자 합에 더해준다. sum_of_cards += 11 # 카드의 합이 21을 초과하는 경우, if sum_of_cards &gt; 21: # Ace카드를 1로 치환해서 더해준다. sum_of_cards -= 10 else: sum_of_cards += card return sum_of_cards# 뽑은 카드들의 숫자 합을 기준으로 결과를 판별한다.def checkSumOfCards(player, sum_of_cards): if sum_of_cards &gt; 21: print( &quot;😵‍💫😵‍💫😵‍💫😵‍💫😵‍💫{0}가 받은 카드들의 숫자 합이 21을 초과했습니다. {0}의 턴을 종료합니다.😵‍💫😵‍💫😵‍💫😵‍💫😵‍💫&quot;.format(player)) if player == '뱅크': global player_lose bank_lose = True if player_lose: print(&quot;😮‍💨😮‍💨😮‍💨😮‍💨😮‍💨 사용자와 뱅크는 서로 비겼습니다. 😮‍💨😮‍💨😮‍💨😮‍💨😮‍💨&quot;) sys.exit(0) else: print(&quot;🎉🎉🎉🎉🎉🎉🎉 사용자가 이겼습니다. 🎉🎉🎉🎉🎉🎉🎉&quot;) sys.exit(0) else: player_lose = True elif sum_of_cards == 21: print( &quot;👍👍👍👍👍 {0}가 받은 카드들의 숫자 합이 21입니다. Blackjack으로 {0}가 승리하였습니다. 👍👍👍👍👍&quot;.format(player)) sys.exit(0) else: print(&quot;{0}가 받은 카드들의 숫자 합은 {1}입니다.&quot;.format(player, sum_of_cards))# 딜러로부터 카드를 받는 처리def receiveCard(player, cards): print(&quot;\\n&quot;) print(&quot;🌱🌱🌱🌱🌱🌱🌱 &lt;&lt; {0}의 {1}번째 카드 분배입니다. &gt;&gt;🌱🌱🌱🌱🌱🌱🌱&quot;.format( player, turn+2)) print(&quot;그 다음으로 {} 측 게임을 진행합니다. &quot;.format(player)) # 카드를 점검합니다. 카드 덱에 남아있는 카드가 10장 미만인 경우, # 카드덱의 상태를 재정비합니다. if len(card_deck) &lt; 10: resetCards() card = card_deck.pop() print('😃😃😃😃😃😃😃{0:&gt;3}는 {1:&gt;2} 카드를 받았습니다.😃😃😃😃😃😃😃'.format(player, card)) cards.append(card) print(&quot;💡💡💡💡💡💡{}의 Card deck을 확인 =&gt; {}💡💡💡💡💡💡&quot;.format(player, cards))# 카드를 뽑고, 카드의 합을 구하고, 카드의 합이 21을 초과하는지 검사하는 공통화 처리def commonCardGameProcess(player, cards, sum_of_cards): # card deck으로부터 카드를 뽑습니다. receiveCard(player, cards) # 여지까지 뽑은 카드의 합을 구합니다. sum_of_cards = getSumOfCards(cards) # 여기서 cards의 숫자의 합이 21을 초과하는지 검사한다. checkSumOfCards(player, sum_of_cards)# player와 bank의 게임def cardPlay(player, sum_of_cards, cards): global turn, player_play, bank_play print(&quot;\\n&quot;) print(&quot;🌱🌱🌱🌱🌱🌱🌱&lt; {0}의 {1}번째 카드 분배입니다. &gt;&gt;🌱🌱🌱🌱🌱🌱🌱&quot;.format( player, turn+2)) print(&quot;🔥🔥🔥🔥🔥🔥🔥 다음은 {}의 순서입니다. 🔥🔥🔥🔥🔥🔥🔥&quot;.format(player)) if player == &quot;뱅크&quot; and sum_of_cards &lt; 16: print(&quot;Dealer hit rule에 의해 카드의 합이 16을 넘지 않으므로, 카드를 뽑습니다.&quot;) # 카드를 뽑고, 뽑은 카드의 합을 구하고, 그 합이 21을 초과하지 않는지 검사 commonCardGameProcess(player, cards, sum_of_cards) # 사용자, 뱅크 모두 순회를 끝냈다면 turn 값을 1증가시켜준다. turn += 1 else: more_card = input( &quot;✊✊✊✊✊✊ 카드를 더 뽑으시겠습니까? (Y(y)/N(n)) ✊✊✊✊✊✊&quot;) result = regexCheck(more_card) if result.group() == 'Y' or result.group() == 'y': # 카드를 뽑고, 뽑은 카드의 합을 구하고, 그 합이 21을 초과하지 않는지 검사 commonCardGameProcess(player, cards, sum_of_cards) # 사용자, 뱅크 모두 순회를 끝냈다면 turn 값을 1증가시켜준다. if player == &quot;뱅크&quot;: turn += 1 else: if player == &quot;사용자&quot;: player_play = False else: bank_play = False result_dict[player] = sum_of_cards if player == &quot;사용자&quot;: player_play = False if player == &quot;뱅크&quot;: bank_play = False# 게임의 순번에 따라 게임을 진행# arg: player -&gt; 사용자, 뱅크# arg: cards -&gt; 사용자(cards_for_bank), 뱅크(cards_for_bank)def rotateTurnOfGames(player, cards): global turn global player_play global bank_play # print('{0}의 카드 수의 총 합은 {1}입니다.'.format(player, sum_of_cards)) sum_of_cards = getSumOfCards(cards) if turn &gt; 0: if (player == &quot;사용자&quot; and (player_lose != True and player_play != False)): cardPlay(player, sum_of_cards, cards) elif (player == &quot;뱅크&quot; and (bank_lose != True and bank_play != False)): cardPlay(player, sum_of_cards, cards) else: # 카드를 뽑고, 뽑은 카드의 합을 구하고, 그 합이 21을 초과하지 않는지 검사 commonCardGameProcess(player, cards, sum_of_cards) # 사용자, 뱅크 모두 순회를 끝냈다면 turn 값을 1증가시켜준다. if player == &quot;뱅크&quot;: turn += 1# 게임 실행card_deck = []print(&quot;👨👨👨👨👨👨👨👨👨 카드를 준비합니다. 👨👨👨👨👨👨👨👨👨&quot;)prepareCards()# card_deck을 3회 섞는다.print(&quot;✨✨✨✨✨✨✨✨ 준비된 카드를 3회 섞습니다. ✨✨✨✨✨✨✨✨&quot;)shuffleCardsTreeTimes()# 카드를 한 번 더 섞을 것인지 사용자로부터 확인한다.askMoreShuffle()print(&quot;✨✨✨✨✨✨✨✨ 카드섞기를 완료하였습니다. ✨✨✨✨✨✨✨✨&quot;)# card shuffle이 완성된 후에는 tuple로 카드의 순서를 고정한다.card_deck = tuple(card_deck)print(&quot;\\n&quot;)print(&quot;💪💪💪💪💪💪💪💪 &lt; Blackjack Game Start &gt;&gt; 💪💪💪💪💪💪💪💪&quot;)print(&quot;게임이 시작됩니다. 사용자와 뱅크는 각 각 한 장의 카드를 받습니다.&quot;)# 뱅크와 user의 card를 보관할 리스트를 정의한다.cards_for_bank = list()cards_for_user = list()# 게임이 시작되면 bank와 user는 카드를 한 장씩 받게 된다.# 카드는 한쪽방향으로 꺼내기때문에 pop()으로 꺼내도록 한다.card_deck = list(card_deck)card_for_user = card_deck.pop()card_for_bank = card_deck.pop()cards_for_user.append(card_for_user)cards_for_bank.append(card_for_bank)# 받은 카드는 출력을 하도록 한다.print('{0:&gt;3}는 {1:&gt;2} 카드를 받았습니다.'.format(&quot;사용자&quot;, card_for_user))print('{0:&gt;4}는 {1:&gt;2} 카드를 받았습니다.'.format(&quot;뱅크&quot;, card_for_bank))sum_of_user_cards = 0sum_of_bank_cards = 0# cards_for_bank, cards_for_userplayer = {'사용자': cards_for_user, '뱅크': cards_for_bank}turn = 0result_dict = {}while player_play or bank_play: for k, v in player.items(): result = rotateTurnOfGames(k, v) # print(result)print(&quot;\\n&quot;)print(&quot;✨✨✨✨✨✨✨✨✨✨✨✨✨✨&lt; Blackjack Game End &gt;&gt;✨✨✨✨✨✨✨✨✨✨✨✨✨✨&quot;)print(&quot;✨✨✨✨✨✨ 사용자가 뽑은 카드 수의 합은 {사용자}, 뱅크가 뽑은 카드 수의 합은 {뱅크}입니다. ✨✨✨✨✨✨&quot;.format( **result_dict))game_result = [(k, v) for k, v in result_dict.items()]if result_dict['사용자'] == result_dict['뱅크']: print(&quot;🎉🎉🎉🎉🎉🎉🎉 사용자와 뱅크는 비겼습니다. 🎉🎉🎉🎉🎉🎉🎉&quot;)else: sorted_result = sorted(game_result, key=lambda x: x[1], reverse=True) print(&quot;\\n&quot;) print(&quot;🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉게임의 승리자는 {}입니다.🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉&quot;.format( sorted_result[0][0]))","link":"/2021/02/14/202102/210214-black_jack_cli_game/"},{"title":"210215 Python TIL 2&#x2F;2 - Function, Closure, Decorator","text":"Function = First-class function 파이썬에서는 함수를 일급 시민(first class citizen)으로 취급한다.아래의 예시코드를 보면, 함수에서 또 다른 함수를 반환할 수 있으며, 함수에서 반환하는 lambda 함수의 경우 본래 heap에 저장이 되지만, 함수에서 반환된 lambda함수를 변수에 넣음으로써 data 영역에 저장된다. 123456789def make_difference(operator): if operator == '+': return lambda x,y:x+y if operator == '-': return lambda x,y:x-yplus = make_difference('+')# lambda는 기본적으로 heap에 저장이 되지만, 변수로 다시 넣기 때문에 데이터의 영역으로 저장이 된다.print(plus(1,2)) #3 Closure 함수 안에서 함수를 만들고 반환하는 형태 함수와 함께 함수 자신의 주변 환경을 저장한 레코드를 의미한다. 123456789101112131415def outer(): text_a = 'John' def inner(): b = 'Doe' print(&quot;My name is {} {}.&quot;.format(text_a, b)) return inner #함수 객체를 전달해줘야 한다.func = outer()print(func) #print(func.__dir__())print(func.__closure__)print(func.__closure__[0].cell_contents) # John이 출력된다.# inner function에서는 outer function의 text_a를 가져와서 사용하고 있다.# text_a는 closure라는 개념때문에 가능하다.# closure의 element에 접근 Decorator decorator란 어떤 함수에 미리 만든 규격화된 처리를 적용할때 사용하는 것으로 정의할 수 있다. 프레임워크를 사용하다보면 @controller나 @app.route(“/“)와 같은 데코레이터를 method위에 작성하게 되는데, 이처럼 이미 프레임워크에서 제공하는 규격화된 처리를 적용할때 decorator가 사용된다. 함수로 decorator를 만들면, 함수 decorator라고 하고 클래스로 decorator를 만들면 클래스 decorator라고 정의한다. 기존의 함수를 수정하지 않은 상태에서 추가 기능을 구현할때 사용된다. decorator의 기본 Syntax123456789# decorator1이라는 decorator 만들기def decorator1(func): def wrapper(): # statements return wrapper()@decorator1def actual_work(): # statements @ decorator를 사용하지 않고 구현하기decorator 함수는 아래와 같이 실행할 함수를 outer function의 인자로 넣어주고 내부에서는 추가해 줄 기능과 인자로 받은 function을 작성해주며, 최종적으로 outer function에서는 inner function을 반환하는 형태로 적어주면 된다. 123456789101112131415161718192021def trace(func): # 호출할 함수를 매개변수로 받는다. def wrapper(): # 호출할 함수를 감싸는 함수 print(func.__name__, 'fuction start') func() # 인자로 받은 함수 print(func.__name__, 'function end') return wrapperdef hello(): print('hello')def world(): print('world')# decorator에 호출할 함수를 넣어준다.trace_hello = trace(hello)# 반환된 함수를 호출한다.trace_hello()# decorator에 호출할 함수를 넣어준다.trace_world = trace(world)# 반환된 함수를 호출한다.hello_world() @ decorator를 사용해서 구현하기위의 코드를 @decorator를 사용해서 간단하게 작성을 해보자. 1234567891011121314151617def trace(func): # 호출할 함수를 매개변수로 받는다. def wrapper(): # 호출할 함수를 감싸는 함수 print(func.__name__, 'fuction start') func() # 인자로 받은 함수 print(func.__name__, 'function end') return wrapper@trace # @decoratordef hello(): print('hello')@trace # @decoratordef world(): print('world')hello() # 함수를 그대로 호출한다.world() # 함수를 그대로 호출한다. 매개변수와 반환값을 처리하는 decorator12345678910111213141516def trace(func): # 호출할 함수를 매개변수로 받는다. def wrapper(a, b): # 호출할 함수의 매개변수와 똑같이 지정을 해준다. r = func(a, b) # func에 매개변수 a, b를 넣어서 호출하고 반환값을 변수에 저장한다. print('{0}(a={1}, b={2}) -&gt; {3}'.format(func.__name__, a, b, r)) # 매개변수와 반환값을 출력해준다. return r # func의 반환값을 반환해준다. # func함수의 반환값을 반환해주지 않으면 add() 함수를 호출해도 아무값이 출력되지 않는다. # 만약에 func의 반환값이 필요가 없다면, return func(a, b)해주면 된다. return wrapper # wrapper 함수를 반환해준다.@tracedef add(a, b): return a + bprint(add(10, 20))# add(a=10, b=20) -&gt; 30# 30 가변 인수 함수 decorator 함수의 매개변수(인수)가 고정되지 않은 경우에는 wrapper 함수를 가변 인수 함수로 만들면 된다. 12345678910111213141516171819202122def trace(func): # 호출할 함수를 매개변수로 받는다. def wrapper(*args, **kwargs): # 가변인수 함수로 만든다. r = func(*args, **kwargs) # func에 args, kwargs를 unpacking해서 넘겨준다. print('{0}(args={1}, kwargs={2}) -&gt; {3}'.format(func.__name__, args, kwargs, r)) return r # func의 반환값을 반환 return wrapper # wrapper 함수를 반환def get_max(*args): # 위치 인수를 사용하는 가변 인수 함수 사용 return max(args)def get_min(**kwargs): # 키워드 인수를 사용하는 가변 인수 함수 사용 return min(kwargs.values())print(get_max(10, 20))print(get_min(x=10, y=20, z=30))&quot;&quot;&quot;get_max(args=(10, 20), kwargs={}) -&gt; 2020get_min(args=(), kwargs={'x': 10, 'y': 20, 'z': 30}) -&gt; 1010&quot;&quot;&quot; Decorator Practice 매개변수가 없는 decorator 작성해보기123456789101112131415161718192021def rapper(func): def wrapper(): print('너와 나의 연결고리..') func() return wrapper@rapperdef dok2(): print('이건 우리 안의 소리')dok2()# 너와나의 연결고리..# 이건 우리 안의 소리# func()의 위치를 바꿔줌으로써 출력 순서를 바꿀 수도 있다.rapper(dok2)()# 너와나의 연결고리..# 너와나의 연결고리..# 이건 우리 안의 소리# decorator로 지정한 rapper() function이 기본적으로 한 번 실행해주고# @rapper dok2() function을 실행시켜준다. decorator로 memoization 구현하기 (피보나치) 재귀호출로 피보나치를 구현하면 아래와 같다. 12345def fibo_rec(num): if num &lt; 2: return num else: return fibo_rec(num-2) + fibo_rec(num-1) 위의 재귀호출로 구현한 피보나치를 보면, 이미 연산된 수들의 조합으로 계속 누적해서 더해짐을 알 수 있다. 이미 연산된 수들의 조합을 momoization 개념을 사용해서 decoration 함수로 구현을 해보자. 12345678910111213141516def memoize(func): memo = {} def wrapper(seq): if seq not in memo: # 매개변수 seq가 memo에 없으면, func에 seq인자를 넣어서 momo 변수에 담아준다. #존재한다면, memo[seq]의 값을 반환해준다. # memo dictionary 객체에 해당 seq를 키값으로 하는 값이 없다면, func(seq)의 값을 넣어준다. memo[seq] = func(seq) return memo[seq] return wrapper@memoizedef fibo_memo(num): if num &lt; 2: return num else: return fibo_memo(num-2) + fibo_memo(num-1) decorator를 활용해서 1호차입니다. ~ 5호차 입니다. 출력하기123456789101112# 1호차입니다. ~ 5호차 입니다.def call_train(func): def wrapper(num): for i in range(1, num+1): func(i) return wrapper@call_traindef print_train(num): print(&quot;{}호차 입니다.&quot;.format(num))print_train(5) 반복문을 사용하지 않고 decorator를 활용해서 1호차입니다. ~ 5호차 입니다. 출력하기1234567891011121314def call_train(func): def wrapper(num): func(num) if num == 1: return else: call_train(wrapper(num-1)) return wrapper@call_traindef print_train(num): print(&quot;{}호차 입니다.&quot;.format(num))print_train(5) decorator로 피보나치 time checker 구현하기12345678910111213141516171819202122232425262728293031323334from time import timedef time_checker(func): def wrapper(): start_at = time() result = func() end_at = time() print(&quot;execution time: {}sec&quot;.format(end_at-start_at)) return result return wrapper @time_checker def fibo_rec(num): if num &lt; 2: return num else: fibo_rec(num-2) + fibo_rec(num-1) fibo_rec(10) #fibo_rec는 time_checker에서 func parameter이다. # 그런데 time_checker function의 내부에서 사용된 func()내부에는 # parameter가 없기 때문에 wrapper에 num을 인자로 넘겨준다. # outer function은 decoratorated function(실행될 함수객체)을 넘겨받기 위한 역할을 하고 # inner function은 실제로 넘겨받은 decorated function을 실행하기 위한 역할을 한다. def time_checker(func): def wrapper(num): start_at = time(num) result = func() end_at = time() print(&quot;execution time: {}sec&quot;.format(end_at-start_at)) return result return wrapper recursion 방식에서는 보통 decorator를 사용하지 않는다.반복 출력에 대한 문제를 해결하기 위해서 아래와 같이 작성을 해준다. 12# decorator로 구현된 함수를 직접 인자로 넣어서 처리를 해준다.time_checker(fibo_rec)(10) Do it yourself! (숙제) “Hi, {name}. You might be loved with {lang}” 이라는 문자열이 존재할 때,이 문자열의 앞 뒤로 &lt;h1&gt;, &lt;em&gt; 태그가 붙도록 하는 데코레이터를 생성하세요 ex output) &lt;h1&gt;&lt;em&gt; {text} &lt;/em&gt;&lt;/h1&gt; Advanced problem: Decorator 하나로 html 태그 이름을 지정할 수 있도록 수정 12345678910def appendHTMLTag(func): def wrapper(name, lang): return &quot;&lt;h1&gt;&lt;em&gt;{}&lt;/em&gt;&lt;/h1&gt;&quot;.format(func(name, lang)) return wrapper@appendHTMLTagdef getString(name, lang): return f&quot;Hi, {name}. You might be loved with {lang}&quot;print(getString(&quot;Lee Hyungi&quot;, &quot;Korean&quot;)) 가변인수를 활용하여 인수를 하나로 받기1234567891011def appendHTMLTag(func): def wrapper(*args): return &quot;&lt;h1&gt;&lt;em&gt;{}&lt;/em&gt;&lt;/h1&gt;&quot;.format(func(*args)) return wrapper@appendHTMLTagdef getString(*args): return &quot;Hi, {}. You might be loved with {}&quot;.format(*args)print(getString(&quot;Lee Hyungi&quot;, &quot;Korean&quot;))","link":"/2021/02/15/202102/210215-Python_til(2)/"},{"title":"210215 TypeScript Book TIL","text":"이 포스팅은 O’Reilly TypeScript책을 통해 공부한 내용과 실습한 내용을 기반으로 작성하였습니다. 오늘 공부한 내용 컴파일과 실행에 대한 이해이번에 읽은 내용에서 가장 유익하다고 느꼈던 내용이다. 실제 코드를 작성하고 실행을 했을때 내부에서 구체적으로 어떤 과정에 의해서 처리가 되는지 잘 몰랐었는데 이번 기회에 제대로 알 수 있었던 것 같다.특히 타입스크립트는 자바스크리트나 자바와 같은 주요 언어와는 다른 방식으로 동작하기 때문에 이 부분에 대해서 정리를 해보려고 한다. 일반적으로 프로그래머가 작성한 텍스트(코드)는 컴파일러(compiler)라는 프로그램이 파싱(Parsing)하여, AST(Abstract Syntax Tree, AST)라는 자료구조로 변환된다. 이 AST는 공백, 주석, 탭, 공백 등의 결과를 무시한다.컴파일러는 이 AST 자료구조를 바이트 코드(bytecode)라는 하위 수준의 표현으로 변환을 한다.이렇게 만들어진 바이트 코드는 런타임(runtime)이라는 다른 프로그램에 입력해서 평가하고 결과를 얻을 수 있는 것이다. 텍스트 코드 =&gt; 컴파일러(Compiler) =&gt; AST(Abstract Syntax Tree) AST(Abstract Syntax Tree) =&gt; 컴파일러(Compiler) =&gt; 바이트 코드(bytecode) 런타임(Runtime) =&gt; 바이트 코드(bytecode) 즉 일반적으로 프로그램을 실행한다는 것은 컴파일러가 소스코드를 파싱해서 AST로 만들고 이 AST를 바이트코드로 변환한 것을 런타임이 평가하도록 지시하는 일련의 과정을 의미한다. 일반적인 프로그램의 실행은 위의 과정을 거친다. 타입스크립트가 다른 언어의 일반적인 실행과 다른 점은 AST가 바이트 코드로 변환되는 것이 아닌, 자바스크립트 코드로 변환된다. (TS) 1. 타입스크립트 소스 → 타입스크립트 AST (compiler) (소스코드의 타입사용 O) (TS) 2. 타입 검사기(typechecker)가 AST를 검사 (compiler) (소스코드의 타입사용 O)(TS) 3. 타입 스크립트 AST → 자바스크립트 소스 (소스코드의 타입사용 X)=============================================(JS) 4. 자바스크립트 소스 → 자바스크립트 AST (JS) 5. AST → 바이트코드 (JS) 6. 런타임이 바이트코드를 평가 보통 자바스크립트 컴파일러와 런타임은 엔진이라는 하나의 프로그램으로 합쳐진다.프로그래머는 주로 이 엔진과 상호작용을 한다. 타입시스템에 대한 이해타입 시스템에는 명시적 타입 시스템과 추론적 타입 시스템이 있다.타입 스크립트는 두 시스템의 영향을 받은 언어이다. 그말은 즉슨 개발자는 타입을 명시하거나 별도의 명시없이 타입스크립트가 추론하도록 할 수 있다. 명시적 타입지정은 value:type의 형태로 작성한다. 추론적 타입지정은 별도로 type을 지정하지 않아도 값에 의해서 TypeScript가 추론을 하는 것을 의미한다. 자바스크립트와 타입스크립트의 차이자바스크립트와 타입스크립트의 차이점은 자바스크립트는 동적 타입 결정방식인 반면에 타입스크립트는 정적 타입 결정방식을 가진다. 그리고 자바스크립트의 타입체크와 에러검출은 런타임 단계에서 이루어지는 반면에 타입스크립트는 컴파일 단계에서 이루어진다. 이러한 차이로 인해, 타입스크립트는 코드를 실행하지 않고도 코드에 에러가 있음을 알 수 있다. 오늘 실습한 내용 실습 Repository : https://github.com/LeeHyungi0622/typescript-oreilly-practice TypeScript TypeScript 프로젝트 생성 NPM 프로젝트 초기화 1$ npm init typescript tslint @types/node 설치 1$ npm i -D typescript tsline @types/node tsconfig.json 파일생성직접 파일을 생성하여 실습하였지만, ./node_modules/.bin/tsc --init 내장명령을 통해서 자동으로 설정을 해줄 수 있다. 12345678910111213{ &quot;compilerOptions&quot;: { &quot;lib&quot;: [&quot;es2015&quot;], &quot;module&quot;: &quot;commonjs&quot;, &quot;outDir&quot;: &quot;dist&quot;, &quot;sourceMap&quot;: true, &quot;strict&quot;: true, &quot;target&quot;: &quot;es2015&quot; }, &quot;include&quot;: [ &quot;src&quot; ]} tslint.json탭을 사용할지 공백을 사용할지 등에 대한 코딩스타일을 정하는 파일로, ./node_modules/.bin/tslint --init 내장명령을 통해서 파일을 생성할 수 있다. 12345678910{ &quot;defaultSeverity&quot;: &quot;error&quot;, &quot;extends&quot;: [ &quot;tslint:recommended&quot; ], &quot;rules&quot;: { &quot;semicolon&quot;: false, &quot;trailing-comma&quot;: false }} src 폴더에 index.ts파일을 생성하여 간단한 코드를 작성한다. TSC로 타입스크립트 파일을 컴파일한다.ts-node를 설치하면 명령 한 번으로 타입스크립트를 컴파일하고 실행할 수 있다. 1$ ./node_modules/.bin/tsc NodeJS로 컴파일된 자바스크립트 코드를 실행한다. 1$ node ./dist/index.js typescript-node-starter와 같은 뼈대(Scafolding) 도구를 이용해서 프로젝트 디렉터리 구조를 빠르게 생성할 수 있다.","link":"/2021/02/15/202102/210215-Typescript/"},{"title":"Baekjoon Online Judge 2747번 피보나치 수 문제 (다양한 방법으로 풀기)","text":"백준 저지 2747번 피보나치 수 문제 Pseudo code + Python code 이 문제는 피보나치 수를 구하는 문제로, for-loop와 memoization, recursive 개념을 활용해서 풀이를 해보았다.피보나치 수열의 값은 이미 계산된 값들의 누적 합으로 구성이 된다. 따라서 이미 계산된 부분은 별도의 변수에 값을 저장했다가 필요할때 참조하게 되면 빠르게 원하는 값을 도출해낼 수 있다.(fibonacci-memoization) 손코딩으로 작성한 코드 중 for-loop로 풀이한 부분의 초기리스트로 [1,1]이 아닌 [0,1]로 초기화를 시켜줘야 한다.(수정) 12345678910111213141516171819202122# for-loopn = int(input())num_list = [0, 1]for i in range(2, n+1): num_list.append(num_list[i-1]+num_list[i-2])print(num_list[-1])# memoization__fibo_memo = {}def fibo(n): if n in __fibo_memo: return __fibo_memo[n] else: __fibo_memo[n] = n if n &lt; 2 else fibo(n-1) + fibo(n-2) return __fibo_memo[n]# recursive functiondef fibo(n): return n if n &lt; 2 else fibo(n-1) + fibo(n-2)print(fibo(5))","link":"/2021/02/16/202102/210216-Algorithm_baekjoon_2747/"},{"title":"210216 TypeScript Book TIL","text":"이 포스팅은 O’Reilly TypeScript책을 통해 공부한 내용과 실습한 내용을 기반으로 작성하였습니다. 오늘 공부한 내용 타입(type)에 대한 이해이번에 독서를 통해서 타입(type)대해서 다시 정의해 볼 수 있었다. 기존에는 type에 대한 정의를 단순 data의 형식(string, number, list)에 국한된 개념으로만 알고 있었다.하지만 이 책에서는 타입을 값과 이 값으로 할 수 있는 일의 집합으로 정의하고 있다.예를들어, number 타입을 정의하자면 number라는 type으로써의 값과 숫자에 적용할 수 있는 모든 연산(+,-,*,/,%,||,&amp;&amp;,?등)과 숫자에 호출할 수 있는 모든 메서드(.concat, .toUpperCase 등)를 포괄적으로 포함하는 개념으로 정의할 수 있다.이렇게 정의된 타입(type)의 정의를 통해 타입검사기(typechecker)는 주어진 값으로 할 수 있는 유효한 동작과 유효하지 않은 동작을 구분해서 유효하지 않은 동작이 실행되는 일을 예방할 수 있다. 타입의 종류 any타입들의 대부라고 할 수 있는 any. 이 any type은 꼭 필요한 상황이 아니라면 사용하지 않는 것이 좋다. 타입스크립트에서는 컴파일 타임에 모두가 타입이 있어야 하기 때문에 프로그래머와 타입스크립트 둘 다 타입을 알 수 없는 상황에서 any타입으로 가정한다. any를 사용해서 변수의 타입을 지정하게 되면, 자바스크립트처럼 동작하게 된다. (TSC Flag: noImplicitAny) : 타입스크립트는 기본적으로 any로 추론되는 값을 발견하더라도 예외를 발생시키지 않는다. 만약에 any type에 대한 예외를 발생시키고 싶다면, tsconfig.json에서 strict을 true로 정의하거나 noImplicitAny flag를 활성화시켜주면 된다. (noImplicitAny는 strict의 family이기 때문이다) unknown타입을 미리 알 수 없는 어떤 값에 대해 타입을 정의할때 any보다는 unknown을 사용하는 것이 권장된다.그 이유는 unknown을 사용해서 타입을 미리 알 수 없는 값에 대한 타입을 정의하게 되면, unknown type을 검사해 정제하기 전까지는 타입스크립트가 unknown 타입의 값을 사용할 수 없게 강제한다. unknown 타입은 비교연산(==,===,||,&amp;&amp;,?)과 반전(!)을 지원하고 다른 타입들과 마찬가지로 typeof, instanceof 연산자로 정제할 수 있다. 123456let a: unknown = 30;let c = a + b; // unknown type의 값은 + 연산자를 지원하지 않는다.(a 변수의 타입을 정제한 뒤에 기본 사칙연산 사용가능)// typeof를 사용해서 unknown type의 변수를 정제할 수 있다.if (typeof a === 'number') { let d = a + 10; // number} boolean기본적으로 개발자들은 대개 타입스크립트가 타입을 추론하도록 작성한다.타입스크립트에서는 값이 특정 boolean임을 명시적으로 타입스크립트에게 알릴 수 있다. 타입 리터럴(type literal)아래의 예시 코드에서 변수 e는 true라는 특정한 하나의 값으로 타입을 한정시켰다. 이처럼 오직 하나의 값을 나타내는 타입을 타입 리터럴(type literal)이라고 한다.이 타입 리터럴은 모든 곳에서 일어날 수 있는 실수를 방지해서 안전성을 추가로 확보해주는 강력한 언어의 기능이다.123const c = true; // 어떤 값이 특정 boolean인지 타입스크립트가 추론하게 만든다.let e: true = true;let f: true = false; // error TS2322 'false'type을 'true' type에 할당할 수 없다. numbernumber type 또한 boolean과 같이 타입스크립트가 타입을 추론하도록 작성을 한다.아래와 같이 number도 타입 리터럴(type literal)을 사용해서 특정 값을 타입으로 한정시켜서 정의할 수 있다. 12let f: 2.218 = 2.218;let g: 2.218 = 10; // error TS2322: '10'타입을 '26.218'타입에 할당할 수 없다. bigintbigint 또한 타입 리터럴(type literal)을 사용해서 특정 값을 타입으로 한정시켜서 정의할 수 있다.bigint는 자바스크립트와 타입스크립트에 새롭게 추가된 타입으로, 253까지의 정수를 표현할 수 있는 number 대비 bigint를 사용하면 더 큰 수도 표현할 수 있다.bigint는 기본 사칙연산을 포함한 연산을 지원한다.일부 자바스크립트 엔진에서는 지원을 안할 수도 있기 때문에 사용하기 전에 대상 플랫폼이 지원을 하는지 확인이 필요하다. 12345let a = 1234n; //bigintconst b = 5678n; //bigintvar c = a + b; //bigintlet g: 100n = 100n;let h: bigint = 100; // 100 type은 bigint 타입에 할당할 수 없다. stringstring 또한 타입 리터럴(type literal)을 사용해서 특정 값을 타입으로 한정시켜서 정의할 수 있다.string은 모든 문자열의 집합으로 연결(+)과 슬라이스(.slice)등의 연산을 수행할 수 있다. 12let f: 'john' = 'john'; // 'john'let g: 'john' = 'zoe'; // 'zoe'type을 'john'type에 할당할 수 없다. symbolsymbol은 실무에서는 자주 사용하지 않는다.주로 객체와 맵에서 문자열 키 를 대신하는 용도로 사용된다.Symbol(‘a’)는 주어진 이름으로 새로운 symbol(unique)을 만든다는 의미이다. Symbol타입의 값을 타입스크립트가 추론하도록 코드를 작성하면,추론되는 타입이 symbol이 아닌 typeof (variable)로 정의된다.Symbol type의 값을 const 변수에 넣어주면, unique symbol로 정의할 수 있다. 12const e = Symbol('e') // typeof econst f: unique symbol = Symbol('f') // typeof f 객체타입스크립트의 객체(Object) 타입은 객체의 형태(Shape)를 정의한다.객체의 타입만으로 간단한 객체({}로 생성)와 복잡한 객체{new로 생성}를 구분할 수 없다.이는 자바스크립트가 구조 기반 타입(structural type을 갖도록 설계가 되었기 때문이다) 구조 기반 타입화에서는 객체의 이름에 상관없이 객체가 어떤 프로퍼티를 갖고 있는지를 따진다. 이를 일부 언어에서는 덕 타이핑(duck typing)이라고 정의한다. 객체를 서술하는 방법(1) object 선언을 통한 객체 서술object는 any보다 조금 더 좁은 타입으로, object로 선언된 객체에 서술한 값에 관한 정보를 참조할 수 없다. 단지 해당 값 자체가 null이 아닌 자바스크립트의 객체라고 명시해준다. (2) 타입스크립트가 추론하도록 작성객체 리터럴 문법을 사용해서 객체 타입을 만들 수 있다. 타입스크립트가 객체의 형태를 추론하게 하거나 중괄호 안에 명시적으로 지정해 줄 수 있다.객체 리터럴 문법은 “이런 형태의 물건이 있어” 라고 타입스크립트에게 알려주는 기능을 한다. 12345678910111213141516171819202122232425262728293031let a = { // {b: string} b: &quot;x&quot;};let b = { // {c: {d: string}} c: { d: &quot;f&quot; }};let c: { firstName: string, lastName: string } = { firstName: &quot;john&quot;, lastName: &quot;barrowman&quot;};class Person { constructor( public firstName: string, public lastName: string ){}}c = new Person('matt', 'smith')// 타입스크립트에서는 형태를 지정해주면 지정해준 형태에 해당하는 값은 반드시 내부에 선언을 해줘야 한다.// 만약에 선택적으로 선언이 되는 속성이라면 아래와 같이 ?로 nullable한 속성이라고 정의를 해주면 된다.let d : { firstName: string, lastName?: string} = { firstName: &quot;merry&quot;} 인덱스 시그니처(index signature)란 [key: T]: U와 같은 문법을 말한다. 어떤 객체가 여러 키를 가질 수 있음을 알려주며, 해당 객체에서 모든 T 타입의 키는 U 타입의 값을 갖는다고 정의한다. 인덱스 시그니처를 이용하면 명시적으로 정의한 키 외에 다양한 키를 객체에 안전하게 추가할 수 있다.자바스크립트에서는 문자열을 키의 타입으로 사용하고 있기 때문에 인덱스 시그니처의 키(T)는 반드시 number나 string 타입에 할당할 수 있는 타입이어야 한다. 1234567891011121314151617let a: { b: number c?:string [key: number]: boolean}a = {b: 1} // 인덱스 시그니처로 작성한 인자의 경우, optional 하게 넣어줄 수 있다.a = {b: 1, c: undefined}a = {b: 1, c: 'd'}a = {b: 1, 10: true}let airplaneSeatingAssignments: { [seatNumber: string]: string} = { &quot;34D&quot;: &quot;Boris Cherny&quot;, &quot;34E&quot;: &quot;Bill Gates&quot;}; 객체의 타입을 정의할때에 선택형(?)만 사용할 수 있는 것이 아닌 필요하면 readonly 한정자를 이용해서 특정 필드를 읽기 전용으로 정의할 수 있다.즉, readonly로 정의한 특정 필드값은 초깃값을 할당한 다음에 그 값을 바꿀 수 없다. 12345let user: { readonly firstName: string} = { firstName: 'abby'} 2020/02/19 업데이트 내용또 다른 객체 리터럴 표기법에는 빈 객체 타입({})과 객체: Object이라는 특별한 경우도 있다.빈 객체 타입에는 null과 undefined를 제외한 모든 타입이 할당 될 수 있다. 따라서 가능한 한 빈 객체는 피하는 것이 좋다.두 객체 리터럴 표기법은 서로 비슷하며 사용하는 것을 권장하지 않는다. 1234let danger: {};danger = {};danger = { x: 1 };danger = []; object타입과 Object타입을 사용한 객체 정의object로 객체를 정의하는 것은 객체의 구체적인 필드 형태는 중요하지 않고 단지 객체 자체가 필요할때 사용된다.","link":"/2021/02/16/202102/210216-Typescript/"},{"title":"210217 Jest의 test, it, describe","text":"이번 포스팅에서는 Jest에서 테스트 코드를 작성할때 사용하는 test와 it 키워드의 사용과 여러 개의 테스트 케이스를 묶을때 사용하는 describe 키워드에 대해서 정리를 해보겠다. test와 it 키워드의 사용이전 포스팅에서 test라는 키워드를 사용해서 테스트 케이스를 작성해보았다. 이 키워드를 대체해서 it이라는 키워드도 사용될 수 있는데 작동 방식은 같다. 1234567891011121314// test keyword를 사용한 테스트 코드 작성const { test, expect } = require('@jest/globals');const sum = require('../sum');// 작성한 테스트코드가 무엇을 하는지에 대해서 첫번째 parameter로 작성해준다.test('properly adds two numbers', () =&gt; { // expected result expect(sum(1, 2)).toBe(3);});// it keyword를 사용한 테스트 코드 작성it('properly adds two numbers', () =&gt; { expect(sum(1, 2)).toBe(3);}); describe 키워드의 사용describe keyword를 사용해서 작은 단위의 테스트 코드를 그룹화할 수 있다.아래의 테스트 코드는 실제 side project를 하면서 작성했던 component의 rendering 및 routing test를 위해 작성했던 코드이다.이처럼 작성하는 테스트 코드가 특정 component에 속하는 테스트 코드라면 아래와같이 describe 키워드를 사용해서 해당 테스트 코드들을 그룹화하는 것이 좋다. 이렇게 그룹화를 해주게 되면, 나중에 테스트 결과를 확인시 좀 더 가시적으로 보기 편하게 테스트 케이스들을 확인할 수 있다. 1234567891011121314151617181920212223242526describe('&lt;App /&gt;', () =&gt; { // Test case 1 test('&lt;App /&gt; rendering시에 /(root)로 rendering이 되는가?', async() =&gt; { const { getByTestId, getByText } = render(&lt;App /&gt;); const navbar = getByTestId('navbar'); const link = getByTestId('HOME'); // 각 각의 page componet를 lazy하게 가져오고 있기 때문에, 비동기로 wait() 처리를 해준다. await wait(); expect(getByText(/Hello world/i)).toBeInTheDocument(); expect(navbar).toContainElement(link); }); // Test case 2 test('MENU1 menu를 클릭시, MENU1 component가 화면에 rendering되는가?', async() =&gt; { const { getByTestId } = render(&lt;App /&gt;); fireEvent.click(getByTestId('MENU1')); await wait(); // menu1-wrapper id를 가진 component가 화면에 rendering되어있는지 확인 expect(getByTestId('menu1-wrapper')).toBeInTheDocument(); }); ......(생략)...... 코드 리팩토링(Code Refactoring)만약에 기존의 소스코드를 리팩토링했다고 가정하자. 코드는 바뀌었지만, 결과적으로 output은 같기 때문에 리팩토링을 한 뒤에 기존의 테스트 코드를 실행시켜서 결과값이 같은지 확인을 하면 쉽게 코드가 제대로 작동을 하는지 검증을 할 수 있다. 이처럼 테스트 코드의 작성은 코드 리팩토링시에도 빠른 코드 검증을 가능하게 해준다.","link":"/2021/02/17/202102/210217-JS_Jest_test_it/"},{"title":"210217 Python TIL (작성중...)","text":"네트워크Web scraping 실습하기 환경설정 requests, beautifulsoup4, lxml, jupyter 설치하기 1234$ poetry add requests$ poetry add beautifulsoup4$ poetry add lxml$ poetry add —dev jupiter jupyter notebook 환경에서 requests를 사용해서 Web Scrapping하기 1234567891011121314151617import requestsurl = 'https://api.kurly.com/v2/categories?ver=1'response = requests.get(url)response# response data를 json 형태로 변환하기response.json()&quot;&quot;&quot;{'data': {'categories': [{'no': '907', 'name': '채소', 'show_all_flag': True, 'pc_icon_url': 'https://img-cf.kurly.com/shop/data/category/icon_veggies_inactive_pc@2x.1586324570.png', 'icon_url': 'https://img-cf.kurly.com/shop/data/category/icon_veggies_inactive@3x.1586324413.png', 'pc_icon_active_url': 'https://img-cf.kurly.com/shop/data/ ...(생략)...&quot;&quot;&quot; PostgreSQL 데이터베이스는 JSON데이터를 그냥 밀어넣을 수 있기 때문에 Django와 궁합이 좋게 사용된다.response로부터 취득한 text를 파싱하는 도구에는 ‘html.parser’과 ‘lxml’이 있는데, lxml은 c++로 만들어진 html text를 파싱해주는 라이브러리로 파싱하는 속도가 빠르다. 1234567from bs4 import BeautifulSoupimport lxmlurl = 'https://en.wikipedia.org/wiki/Coronavirus'resonse = requests.get(url)html_text = response.textsoup = BeautifulSoup(html_text, 'lxml') Flask를 사용한 routing 실습 1234567891011121314from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return &quot;flask works&quot;@app.route('/items')def items(): return &quot;items will be shown shortly&quot;if __name__=='__main__': app.run(host='localhost', port=8080, debug=True) templates/index.html, items.html 파일생성해서 렌더링하기 12345678910111213141516from flask import Flask, render_templateapp = Flask(__name__)@app.route('/')def index(): # string을 렌더링한 html파일로 넘겨줄 수 있다. msg='hello' return render_template('index.html', msg=msg)@app.route('/items')def items(): return render_template('items.html')if __name__=='__main__': app.run(host='localhost', port=8080, debug=True)","link":"/2021/02/17/202102/210217-Python_til/"},{"title":"210217 Development environment setting(pyenv, virtualenv, autoenv, pip, poetry 사용법)","text":"이번 포스팅에서는 파이썬 가상개발환경을 구성하는데 사용되는 pyenv, virtualenv, autoenv, pip, poetry의 사용법에 대해서 정리를 해보겠다. 우선 가상개발환경을 구축해서 개발환경을 분리하는 이유에 대해서 알아보자. 왜 개발환경을 가상개발환경으로써 분리해서 관리해야 하는가?파이썬에는 다양한 버전이 존재하고, 각 각의 파이썬 어플리케이션에서 사용되는 파이썬의 버전 또한 다양하기 때문에 가상개발환경을 구축해서 베이스가 되는 파이썬의 버전을 다르게 설정해서 관리해야 한다.그리고 파이썬은 많은 패키지들이 배포되고 있기 때문에 이러한 패키지들을 한 대의 컴퓨터에서 돌릴경우 Python runtime의 version과 Python libray가 서로 충돌을 하는 문제가 발생한다.이러한 문제는 기본적으로 한 대의 컴퓨터에 런타임 및 라이브러리가 전역적으로 설치가 되어 사용이 되기때문에 발생한다. 그럼 해결책은? 앞에서 언급한 문제점을 해결하기 위해서는 Python application별로 독립적인 가상개발환경을 구축함으로써 해결할 수 있다. 가상개발환경을 구축하기 위해 필요한 것이 이번 포스팅의 주제인 pyenv, virtualenv, autoenv, pip와 같은 툴이 필요하다. 자 그럼 각 각의 툴들에 대해서 정리를 해보자. (1)pyenv 파이썬 버전을 관리하는 툴로 하나의 컴퓨터에 다양한 파이썬 버전을 설치하고 관리할 수 있게 해준다. xcode command line tool과 zlib 설치Mac환경에서 pyenv를 설치하기 위해서는 xcode command line tool과 zlib의 설치가 필요하다. 12$ sudo xcode-select --install$ brew install homebrew/dupes/zlib homebrew를 이용한 pyenv 설치 및 업그레이드 12$ brew install pyenv # installation$ brew upgrade pyenv # update 환경변수 등록 및 업데이트(zsh기준) 1234$ echo 'export PYENV_ROOT=&quot;$HOME/.pyenv&quot;' &gt;&gt; ~/.zshrc$ echo 'export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;' &gt;&gt; ~/.zshrc$ echo 'eval &quot;$(pyenv init -)&quot;' &gt;&gt; ~/.zshrc$ source ~/.zshrc 설치된 pyenv 점검 1234# 설치된 python version list확인$ pyenv install --list# 설치된 pyenv의 버전확인$ pyenv --version pyenv 사용방법 1pyenv [서브 명령] [parameters] 서브 명령 옵션 설명 local 현재 directory의 python version 확인 및 지정 global {version} 전역으로 설정된 python의 버전 확인 및 변경(+version option) install {version} 파이썬 버전을 설치 uninstall 지정한 파이썬 버전을 삭제 version 현재 활성화된 파이썬 버전 출력 versions pyenv로 설치되어 이용가능한 버전을 출력 pyenv 전역설정 1234$ pyenv versions # pyenv 활성 버전 확인$ python --version # 현재 파이썬 버전확인$ pyenv global 3.5.3 # 글로벌 파이썬 설정 변경$ python --version # 현재 파이썬 버전확인 pyenv 로컬설정특정 디렉토리에 활성화되는 파이썬의 버전을 지정한다. 기본적으로 파이썬 버전이 지정되지 않은 디렉토리에는 global 설정이 적용된다. 1234# 특정 디렉토리를 만들고, 디렉토리 안에서 pyenv local [version]으로 local 설정한다.# 그럼 폴더 내부에 .python-version 파일이 생성되어있음을 확인할 수 있다.$ pyenv local 2.7.13 (2)virtualenv &amp; pyenv-virtualenv virtualenv는 파이썬 환경을 격리하는 툴이고, pyenv-virtualenv는 virtualenv의 pyenv 확장 플러그인이다.앞에서 살펴본 pyenv를 이용하게 되면 컴퓨터에 파이썬 버전별로 한 개의 파이썬 runtime으르 설치하고 관리할 수 있다. 그리고 virtualenv를 사용하면 파이썬 버전을 세분화하여 여러 개발환경으로 구분하여 관리하는 것이 가능해진다.여기서 말하는 버전의 세분화는 한 개의 파이썬 버전에 여러개의 런타임을 구성하고 그 각 각의 런타임을 각기 다른 어플리케이션에 할당한다는 의미이다.따라서 pyenv와 virtualenv를 같이 사용하면 효과적이기 때문에 pyenv-virtualenv 플러그인을 사용하는 것이 일반적이다. pyenv-virtualenv 설치 1$ brew install pyenv-virtualenv 기본적으로 pyenv로 파이썬을 설치하면 .pyenv/versions 디렉토리에 각 각의 설치된 버전들이 관리가 되고, 파이썬 런타임이 관리가 된다. pyenv의 관리 최소단위는 파이썬 버전이고, virtualenv를 사용하면 동일한 파이썬 버전을 여러 개의 개별적인 환경으로 구분하여 관리한다. pyenv-virtualenv 환경변수 설정(zsh기준) 1$ echo 'eval &quot;$(pyenv virtualenv-init -)&quot;' &gt;&gt; ~/.zshrc (수업시간에 배운 내용)virtualenv를 사용해서도 pyenv로 설치한 python 버전을 지정해서 가상환경을 생성할 수 있다.pyenv로 설치한 python버전을 지정해서 새로운 가상환경 설치 123# virtualenv name은 사용하는 python 버전이 유추 가능하게 작성하는 것이 좋다.$ virtualenv [python version] [virtualenv name]$ virtualenv 3.9.0 awesome-390 pyenv-virtualenv 사용virtualenv 플러그인을 사용하면 버전(옵션)과 추가적인 이름을 넣어서 파이썬의 런타임을 관리할 수 있다.pyenv-virtualenv의 특징은 python 프로젝트마다 각 각의 가상환경을 제공해준다는 점이다. 123456789# 버전은 샹략 가능하며, 버전을 생략할 경우 현재 시스템 버전으로 가상환경이 설정된다. 이렇게 생성된 가상환경의 경우, activate deactivate 명령을 사용해서 활성화/비활성화를 할 수 있다.$ pyenv virtualenv &lt;vertualenv-name&gt;$ pyenv virtualenv 2.7.10 my-virtual-env-2.7.10# 가상개발환경 활성화$ pyenv activate &lt;vertualenv-name&gt;# 가상개발환경 비활성화$ pyenv deactivate# 가상개발환경 삭제$ pyenv uninstall &lt;vertualenv-name&gt; 가상환경생성 실습 12345678910111213$ pyenv version # 현재 버전확인$ pyenv versions # 설치 버전확인$ pyenv virtualenv 2.7.13 awesome-2713 # awesome-2713 가상환경 생성$ pyenv activate awesome-2713 # 생성한 가상환경 activate$ python -V # awesome-2713 가상환경에서의 파이썬 버전확인$ pyenv deactivate # awesome-2713 가상환경 비 활성화$ python -V # 현재 전역으로 설정되어 있는 파이썬 버전확인# 설치 버전확인$ pyenv versions#2.7.13/envs/awesome-2713#*system# 생성한 가상환경 제거하기$ pyenv uninstall 2.7.13/envs/awesome-2713 (3)autoenv 앞서 살펴본 pyenv-virtualenv를 사용하면 새로 생성한 가상환경을 활성화 하기 위해 pyenv activate를 해줘야하는 번거로움이 있다. 이러한 번거로움을 autoenv를 사용해서 Python 프로젝트 진입시점에 자동으로 virtualenv환경을 로딩하는 기능을 제공한다. autoenv 설치 및 환경변수 설정(zsh 기준) 123$ brew install autoenv$ echo &quot;source $(brew --prefix autoenv)/activate.sh&quot; &gt;&gt; ~/.zshrc$ source ~/.zshrc autoenv 실습하기새롭게 진행할 프로젝트를 위한 디렉토리를 생성하고, .env 파일을 추가한다.이렇게 추가를 해주게 되면, 해당 프로젝트 디렉토리로 이동시 자동으로 해당 가상개발환경으로 activate된다. .env 123456echo “***********************************”echo “Python Virtual Env &gt; [가상환경이름]”echo “***********************************”pyenv shell [가상환경이름]pyenv activate (4)pip pip는 파이썬 패키지(라이브러리)를 관리하는 프로그램으로, 라이브러리를 설치할때 의존성을 갖는 모든 라이브러리를 자동으로 설치를 해주며, 개별 버전관리의 기능도 제공한다. (파이썬 2.7.9 버전 이후와 파이썬 3.4 버전 이상에는 기본적으로 설치) pip버전 업데이트 1$ sudo pip install -U pip pip package list 관리하기pip로 현재 사용중인 파이썬 환경의 모든 라이브러리를 조회하여 requirement 파일을 만드는 기능을 제공한다.bash # 현재 파이썬 환경에 설치된 모든 라이브러리를 requirements.txt로 export하기 $ pip freeze &gt; requirements.txt (5)poetry앞서 requirements.txt를 활용하여 파이썬의 패키지를 관리할 수도 있지만, poetry라는 툴을 사용하면 좀 더 간편하게 파이썬의 패키지를 관리할 수 있다.poetry는 파이썬 의존성 관리툴로, 단순 의존성 관리를 넘어서 poetry.lock을 사용해서 프로젝트의 의존성을 다른 환경에서도 동일하게 유지해줄 수도 있다.그리고 build와 publish도 지원을 해주고 있다.또한 사용법이 npm과 비슷한 부분이 많아서 npm을 사용해봤다면 빨리 익숙해질 수 있을 것 같다. [참고]Poetry command와 관련 실습내용은 일전에 수업시간에 배운 내용들과 함께 정리해뒀다.→ https://leehyungi0622.github.io/2021/02/15/202102/210215-Python_til(1)/","link":"/2021/02/17/202102/210217-dev_environment_setting/"},{"title":"210217 TDD(Test-Driven Development)","text":"TDD?이번 포스팅에서는 TDD에 대해서 정리를 해보려고 한다. 우선 TDD란 Test Driven Development의 약어로 테스트 주도 개발을 의미한다.즉, 테스트가 개발을 이끌어 나가는 형태의 개발론이다.이 TDD방식의 개발은 선 Test code 작성, 후 구현을 의미하며, 총 3가지 (실패, 성공, 리팩토링)의 절차로 구성된다. 구체적인 내용에 대해서는 테스트 자동화(Automated Testing)와 테스트의 종류에 관련된 내용을 우선 정리하고 정리해보도록 하겠다.우선 개발에서 테스트의 정의는 내가 작성한 코드가 잘 작동한다는 것을 검증하는 단계이다. 가장 기본적으로 만들어진 어플리케이션을 테스트하는 방법은 직접 마우스와 키보드를 사용해서 의도대로 잘 동작하는지 확인할 수 있다.하지만 이렇게 모든 기능을 사람 손으로 하나하나 확인하는 것은 여간 번거로운 일이 아닐 수 없다. 그래서 테스트 코드라는 것을 작성해서 테스트를 자동화시켜준다. 테스트 자동화란 사람이 직접 어플리케이션의 동작을 확인하는 것이 아닌, 작성한 테스트 코드를 통해서 테스트 시스템이 자동으로 확인을 해주는 것을 말한다. 테스트 자동화(Automated Testing) 테스트 자동화의 장점 여러 사람들과 협업하여 작업을할때 각자가 작성한 코드에 문제가 있는지 빠르게 검사를 할 수 있다. 새로운 기능을 추가하는 작업의 경우, 기존의 기능들을 망가뜨리는 것을 사전에 방지할 수 있다. 테스트 코드를 적는다는 것은 개발할때 실제 발생할 수 있는 상황에 대하여 미리 정리해놓는 일련의 작업이기 때문에 미리 정리해놓고 그에 상응하는 코드를 작성하게 되면 필요한 사항들을 까먹지 않고 넣어줄 수 있다. Code Refactoring시에도 좋다. 일부 기능에 대한 코드를 리팩토링할때 기존에 이 기능에 대한 테스트 코드가 존재한다면 리팩토링한 후에도 동일한 동작을 하는지 검사하면 되기 때문에 검증하기가 쉽다. 테스트 종류 Unit Test유닛 테스트는 어플리케이션의 각 기능들을 아주 작은 단위로 쪼개서 작성해서 테스트하는 것을 말한다. 큰 분류의 기능을 작은 단위로 쪼개서 테스트를 하게 되면 세부기능별로 잘 작동을 하는지 확인할 수 있다.테스트시에 보통 한 개의 파일만 불러와서 진행한다. ex) React에서의 Unit Test Component가 잘 rendering된다. Component의 특정 함수를 실행시키면 해당 상태가 원하는 형태로 바뀐다. Redux의 Action 생성 함수가 Action의 객체를 잘 만들어낸다. Redux의 Reducer에 상태와 Action 객체를 넣어서 호출하면 새로운 상태를 생성해준다. Integrated Test유닛 테스트가 각 기능의 세부 기능을 테스트했다면, 이제 통합 테스트를 통해서 전체적으로 테스트하는 어플리케이션이 잘 작동하는지 확인을 해야하는데 이때 하는 테스트가 통합 테스트(Integrated Test)이다.테스트시에 여러 파일들을 불러와서 진행된다. ex) React에서의 Integrated Test 여러 Component들을 rendering하고 서로 상호작용을 하고 있다. DOM event를 발생시켰을때 UI에 원하는 변화가 잘 발생한다. Redux와 연동된 Container Component의 DOM에 특정 event를 발생시켰을때 기대하는 Action이 제대로 dispatch된다. TDD의 3단계 절차 TDD방식으로 개발을 하기 위해서 우선 첫 번째 단계는 실패하는 테스트 케이스를 먼저 만드는 것이다. 실패하는 테스트 케이스의 작성은 프로젝트 전체의 기능에 대한 테스트 코드가 아닌 당장 구현할 기능 하나씩 테스트 케이스를 작성하는 것을 말한다. 두 번째 단계는 성공하기 위한 코드작성을 하는 단계이다. 첫 번째 단계에서 작성했던 실패하는 테스트 케이스들을 PASS시키기 위해서 실제 기능에 해당하는 코드를 작성하는 단계이다. 세 번째 단계는 작성한 코드를 리팩토링하는 단계이다. 두 번째 단계에서 구현한 코드에 중복되는 코드가 있거나, 혹은 코드를 좀 더 개선시킬 방법이 있다면 리팩토링을 진행한다. 리팩토링 후에 기존의 테스트 코드를 실행해서 문제없이 PASS가 되는지 확인을 한다. 이 단계까지 완료되었다면 다시 첫 번재 단계로 돌아가서 새롭게 추가해야되는 기능에 대한 실패하는 테스트 케이스를 작성하도록 한다. TDD가 좋은 점TDD로 개발을 진행하게 되면 테스트 케이스를 작성할때 기능을 작은 단위로 나눠서 작성을 하기 때문에 코드를 작성할때에도 코드가 너무 방대해지지 않고, 코드의 모듈화가 자연스럽게 잘 이뤄지는 개발로 진행이 된다.앞서 3 STEP에서 살펴보았듯이 실제 기능구현 이전에 테스트 코드를 작성하게 되면, 자연스럽게 Test Coverage가 높아진다. 이렇게 되면 향후에 코드를 리팩토링할때 그리고 유지보수를 할때에도 쉬워진다.이처럼 TDD방식은 프로젝트의 환경을 좋게 구성하는데 일조를 한다. 요구사항 =&gt; 테스트 코드 =&gt; 구현 =&gt; 테스트 코드 실행요구사항에 맞춰서 기능을 세분화해서 테스트 코드를 작성하기 때문에 실수로 요구사항을 빼먹거나 하는 실수를 사전에 방지할 수 있다.","link":"/2021/02/17/202102/210217-tdd_development/"},{"title":"210219 [PyenvError] Pyenv BUILD FAILED ERROR","text":"Pyenv로 새로운 Python version 설치시에 BUILD FAILED 에러 발생이번에 프로젝트를 진행할때에는 Pyenv를 사용해서 프로젝트별로 파이썬 버전을 다르게 적용하고, Virtualenv로 별도의 가상환경을 구축하여 진행하였다. 문제가 발생한 시점은 Pyenv로 사용할 파이썬 버전을 설치하려고 하는데 아래와 같은 BUILD FAILED에러가 발생하였다. 문제(Issue) 해결책(Solution)우선 에러를 보고 어떤 문제인지 이해해보았다. python-build: use zlib from xcode sdk 부분 이후에 build failed error가 발생한 것을 보아 뭔가 zlib의 설치에 문제가 있거나 시스템에 설정되어 있는 xcode의 sdk 환경변수 설정이 잘못되어있는 것이 아닌가하는 의심을 하게 되었다. 그 다음으로 개발자 커뮤니티에서 나와 같은 에러를 경험하고 해결한 사람이 있을꺼라는 생각에 구글링을 해보았다.완전 똑같은 에러 메시지는 아니었지만, 유사에러에 대한 여러 솔루션들을 stackoverflow와 같은 개발관련 커뮤니티에서 찾을 수 있었다. https://github.com/pyenv/pyenv/issues/1746 대부분의 사람들이 말하기를, XCode를 설치하게 되면 기본적으로 zlib은 설치가 되기때문에 근본적인 문제는 zlib에 있지 않고, 새롭게 업데이트 된 Mac OS인 Big Sur로의 업데이트에 따른 XCode의 새로운 업데이트가 이 문제를 야기했다고 말한다. 나는 다양한 솔루션들 중에서 하나인 아래의 명령을 복사해서 pyenv install –patch 뒤에 있는 버전만 내가 설치하고자 하는 파이썬의 버전으로 변경해서 설치를 하였다. 결과적으로 문제없이 설치가 된 것을 확인할 수 있었다. 1CFLAGS=&quot;-I$(brew --prefix openssl)/include -I$(brew --prefix bzip2)/include -I$(brew --prefix readline)/include -I$(xcrun --show-sdk-path)/usr/include&quot; LDFLAGS=&quot;-L$(brew --prefix openssl)/lib -L$(brew --prefix readline)/lib -L$(brew --prefix zlib)/lib -L$(brew --prefix bzip2)/lib&quot; pyenv install --patch 3.6.12 &lt; &lt;(curl -sSL https://github.com/python/cpython/commit/8ea6353.patch\\?full_index\\=1)","link":"/2021/02/19/202102/210219-pyenv_installation_error/"},{"title":"210219 TypeScript Book TIL","text":"이 포스팅은 O’Reilly TypeScript책을 통해 공부한 내용과 실습한 내용을 기반으로 작성하였습니다. 오늘 공부한 내용타입별칭일반적으로 변수를 선언하듯이 type 키워드로 type을 선언해서 사용할 수 있다.12345type Age = numbertype persono = { name: string age: Age} 12345678910type Person = { name: string age: number}type Age = numberlet age: Age = 55let driver: Person = { name: 'James May', age: age} 타입의 union과 intersection type 실전에서는 intersection보다는 union을 자주 사용한다. 1234567891011// 타입의 union과 intersection typetype Cat = { name: string, purrs: boolean };type Dog = { name: string, barks: boolean, wags: boolean };type CatOrDogOrBoth = Cat | Dog;// Union : Cat과 Dog의 모든 속성을 가질 수 있는 변수를 설정할 수 있다.let catOrDog: CatOrDogOrBoth = { name: 'name', purrs: false, barks: true, wags: false}; 아래 함수는 조건이 참이면 반환 타입을 string으로, 그렇지 않으면 number로 한다는 의미의 함수이다. 즉, string | number를 반환한다. 123function(a: string, b: number){ return a || b} 배열이형배열(heterogeneous) - union타입스크립트에서는 배열을 동형(homogeneous)로, 배열의 모든 항목이 같은 타입을 갖도록 설계한다. 12345// 이형배열에서의 union사용let a = [1, 2, 3]; // number[]var b = ['a', 'b']; // string[]let d = [1, 'a']; //(string | number)[]const e = [2, 'b']; // (string | number)[] 타입스크립트에서는 아래와 같이 우선 배열에서 삽입한 데이터의 타입을 기준으로 배열의 타입을 추론한다. 123let f = ['red']; // string[] array로 정의f.push('blue');f.push(true); // error 아무 데이터도 넣지 않고 배열을 초기화시키면 any[]type으로 정의된다. 123let g = []; // any[]g.push(1); // number[]g.push('red'); // (string | number)[] 튜플배열의 서브타입으로, 튜플은 길이가 고정되었고, 각 index의 type이 이미 알려진 배열의 일종이다. 튜플을 선언할때에는 타입을 명시해야 한다.자바스크립트에서의 배열과 튜플은 대괄호를 사용하고, 타입스크립트에서는 대괄호를 배열 타입으로 추론한다.튜플을 잘 활용하면 순수 배열에 비해 안전성을 높일 수 있다. 튜플의 사용은 권장된다. 1let b: [string, string, number] = ['malcolm', 'gladwell', 1963]; 튜플은 타입선언부에 선택적 요소를 지정할 수 있다. 123456789let trainFares: [number, number?][] = [ [3.75], [8.25, 7.70], [10.50]]// 아래의 표현과 위의 표현은 같다.let moreTrainFares: ([number] | [number, number])[] = [ //.......] 나머지 요소(…)를 사용해서 최소의 길이를 갖도록 설정 123let friends: [string, ...string[]] = ['Sara', 'Tali', 'Chloe', 'Claire']// 이형배열let list: [number, boolean, ...string[]] = [1, false, 'a', 'b', 'c'] 읽기전용 배열과 튜플만들기읽기전용타입의 배열에서는 읽기만 가능하다. 123let as: readonly number[] = [1, 2, 3] // readonly number[]let bs: readonly number[] = as.concat(4) // readonly number[]let three = bs[2] // number 다양한 읽기 전용 배열과 튜플의 선언방식1234567type A = readonly string[] // readonly string[]type B = ReadonlyArray&lt;string&gt; // readonly string[]type C = Readonly&lt;string[]&gt; // readonly string[]type D = readonly [number, string] // readonly [number, string]type E = Readonly&lt;[number, string]&gt; // readonly [number, string] null, undefined, void, never자바스크립트에서의 null과 undefined는 값의 부재를 의미하지만 타입스크립트에서는 undefined 타입의 값은 undefined, null 타입의 값은 오직 null뿐임을 의미한다. undefined: 아직 값을 정의하지 않았음을 의미 null : 값의 부재를 의미 void와 never는 값의 부재를 좀 더 세밀하게 분류하는 특수하고 특별한 용도의 타입니다. void : 명시적으로 아무것도 반환하지 않는 함수 반환 타입 never : 절대 반환하지 않는 함수 타입을 가르킨다. 예를들어 함수에서 exception을 던지거나 while과 같은 반복문으로 영원히 실행이 되며 반환하지 않는 경우가 이에 해당한다. 1234567891011// never를 반환하는 함수function d() { throw TypeError('I always error');}// never를 반환하는 또 다른 함수function e() { while (true) { doSomething(); }} 타입에 있어 unknown이 모든 타입의 상위타입이라면 never는 모든 타입의 서브타입니다. 이 말은 즉슨, 모든 타입에 never를 할당할 수도 있다는 의미이다. 열거형(enum)열거형이란 해당 타입으로 사용할 수 있는 값을 열거하는 기법이다. 키가 컴파일될때 고정된 객체로서 정의되며 이 키를 값에 할당하는 순서가 없는 자료구조이다.열거형의 예로 문자열-문자열의 형태나 문자열-숫자로 mapping하는 열거형이 있다.작성법은 열거형의 이름은 단수형 명사를 쓰고, 첫 문자는 대문자로 작성하며, 키 또한 첫 문자는 대문자로 작성하는 것이 관례이다. 12345enum Language { English, Spanish, Russian} 열거형의 각 키값에 별도로 값을 할당하지 않으면 타입스크립트가 자동으로 적절한 숫자를 추론해서 할당한다.하지만 필요에 따라 아래와 같이 명시적으로 설정을 해줄수도 있다.명시적으로 설정해 줄 수 있는 값으로는 수식, 문자열, 문자열과 숫자의 혼합을 사용할 수 있다. 12345enum Language { English = 0, Spanish = 1, Russian = 2} 열거형 값에 접근을 할때에는 일반적으로 객체에서 값을 가져올때와 같이 점 또는 괄호 표기를 사용해서 값에 접근할 수 있다. 12let myFirstLanguage = Language.Russian; // Languagelet mySecondLanguage = Language['English']; // Language 좀 더 안전하게 열거형(enum)을 사용하기 위해서는 const enum을 사용하도록 한다.값에 대한 접근은 .(dot)이나 [문자열 literal]를 통해 접근할 수 있다. ([index]로는 접근할 수 없다)ex. Language[‘English’] (0), Language.English (0), Language[0] (X) 12345const enum Language { English, Spanish, Russian} 열거형을 사용할때에는 아래와 같이 문자열 값을 갖는 열거형을 사용하도록 한다. 1234567891011121314const enum Flippable { Burger = 'Burger', Chair = 'Chair', Cup = 'Cup', Skateboard = 'Skateboard', Table = 'Table'}function flip(f: Flippable) { return 'flipped it'}flip(Flippable.Chair)flip(Flippable.Cup) 연습문제1234567891011121314151617// 연습문제 11. 값의 추론값 예상하기let a = 1042 // numberlet b = 'apples and oranges' // stringconst c = 'pineapple' // 'pineapple'let d = [true, true, false] // boolean[]let e = {type: 'ficus'} // { type: string}let f = [1, false] // (number | boolean)[]const g = [3] // number[]let h = null // any// 연습문제 2let l: unknown = 4if (typeof l === 'number'){ let m = l * 2 console.log(m)}","link":"/2021/02/19/202102/210219-Typescript/"},{"title":"210221 Memoirs 블로그 운영 30일차 회고록","text":"30일 112개 포스팅2021년 2월 21일 07시 58분 오늘은 블로그를 운영한 30일 동안의 나를 되돌아보며 회고록을 작성하는 것으로 하루를 시작하려고 한다.오늘까지 108개의 포스팅 글과 4개의 미완성 포스팅이 있다. 우선 내가 왜 이 블로그를 어떤 마음가짐으로 시작을 하게 되었는가에서부터 시작을 해보려고 한다. 왜 블로그를 시작하게 되었는가?첫 번째, 내가 이 블로그를 시작하게 된 이유는 내가 알고 있는 것과 모르고 있는 것 그리고 내가 알고 있다고 착각하고 있는 것 이 세 가지를 제대로 파악함으로써 메타인지(metacognition)를 높이고자 이 블로그를 시작하였다.이전에도 블로그를 다른 플랫폼으로 한 2번정도 운영해본적이 있었고, 그때는 뭔가 리소스도 부족하고 시간이 지남에 따라 블로그 내용의 일관성 및 관리의 부족으로 접었었다. 아마 그때는 뭔가 내가 블로그를 운영하는 이유가 나의 내면의 발전에 있지 않고, 블로그 포스팅에 목적을 두었었기 때문에 힘들었었던 것 같다. 그래서 이번 블로그는 단순 블로그 글 포스팅에 목적을 두지 않고 나의 메타인지를 높이기 위한 하나의 도구로써 블로그 운영을 시작하게 되었다. 왜 이렇게 짧은 시간동안 많은 글들을 올렸는가?아마 이 이유가 나의 블로그 운영하는 이유의 두 번째 이유일 것 같다. 그 두 번째 이유는 머리 속에 있는 지식들을 체계화시켜서 정리하기 위해서 였다. 사실 이 블로그를 운영하기 직전에 공부할때에는 별도로 블로그에 기록을 하지 않고 노트 어플에 작성을 하며 공부를 했었다. 그리고 시간이 흘러 나름 이것 저것 작성해놓은 노트 하나가 생겼고, 나의 머리 속에는 여러 지식들이 쌓여있었지만 체계적으로 정리가 되지 않았다는 느낌이 들었다.시간이 지나면서 좀 더 앞선 과거에 공부했던 내용들이 최근에 배운 지식들에 의해 잊혀져가면서 다시 복습에 대한 필요성이 생겨, 정리한 노트를 보았는데 생각보다 글을 정제시켜서 작성하지 않아서인지 내가 작성한 글인데도 한 눈에 읽히지 않았다.그래서 과거에 내가 작성해두었던 글들을 여러번 정제해서 지금의 블로그에 복습할겸 다시 포스팅을 하고 있다. 기존에 Private한 공간에 공부한 내용을 정리를 했을때에는 나만 보는 글이었기 때문에 별 부담없이 글을 따로 정제하지 않고 편하게 적어두었는데, 지금은 Public한 공간에 포스팅을 하고 있기 때문에 혹시 나와 비슷한 내용을 공부하고 있는 이에게 도움이 될 수도 있다는 생각에 다른 사람이 봐도 읽기 편하게 여러번 글을 정제하고 다듬어서 글을 포스팅하고 있다.우연히 어떤 개발자분의 포스팅 글을 보다가 개발자는 지식을 소유하는 것이 아닌 다른 사람들과 공유해야 한다.라는 글귀를 보았는데, 나도 이 개발자 분처럼 단순히 지식을 소유하는 개발자가 아닌 내가 알고 있는 것과 느꼈던 것을 다른 사람들과 공유할 줄 아는 그런 개발자가 되고 싶다고 느꼈다. 그래서 지금 이 블로그를 운영함에 있어 좀 더 잘 운영해야겠다는 생각이 들었다. 앞으로의 계획앞으로 아래의 세 가지를 목표로 블로그를 운영해보려고 한다.첫 번째, 아직 정리되지 않은 나의 이전 공부내용들을 꾸준히 복습을 하며 글을 정제시켜 포스팅을 할 것이다. 아직 이전에 공부했던 내용의 일부의 일부밖에 정리를 못했기 때문에 매일 매일 꾸준히 정리를 할 것이다.두 번째, 블로그를 GitHub와 유기적으로 연결해서 관리를 할 것이다. 현재 운영하고 있는 블로그는 GitHub page로 운영하고 있지만 여기서 유기적으로 연결을 시킨다는 것은 블로그에서 공부한 내용과 실습한 내용들을 GitHub의 Repository에 기록을 하고 정리를 해서 블로그의 글을 읽을때 연관된 repository를 참조할 수 있도록 관리를 한다는 것이다. 그러면 실질적으로 GitHub의 Repository관리 능력도 덩달아 향상될 것이기 때문이다.세 번째, 다른 사람들에게 좀 더 유익한 정보를 제공할 수 있도록 유익한 정보를 재미있게 전달할 수 있는 블로그를 만들 것이다. 블로그의 시작의 이유가 개인적인 측면이 강했지만 이제는 다른 사람들에게도 유익할 수 있는 그런 블로그를 만들어보려고 한다. 몇 일전에 같이 수업을 듣는 한 분이 블로그 너무 잘 보고 있고, 정리가 다른 블로그들보다 잘 정리되어 있어서 도움이 많이 된다는 이야기를 들었는데, 뭔가 뿌듯하고 다른 좋은 내용들을 많은 사람들과 공유하고 싶다는 생각이 들었다. 회고록을 마무리하며지금 이 회고록을 작성하는 시점이 블로그를 운영을 한지 30일째 되는 날이다. 이 블로그를 운영하는 동안 여러 시행착오도 있었고, 나의 개발자로서의 커리어의 방향을 설계하는데 많은 도움이 되었다. 중간 중간에 블로그에 대해 피드백과 여러 유익한 조언들을 주신 파이썬 최우영 강사님께도 너무나도 감사하다. 이제 초심을 다잡았으니, 다음 회고록을 작성하는 날까지 열심히 공부한 내용들을 포스팅하며 좋은 개발자로서 성장하기 위해 노력해야겠다.","link":"/2021/02/21/202102/210221-memoirs/"},{"title":"210222 HTML&#x2F;CSS TIL - HTML, CSS, float","text":"HTML/CSS 첫 수업오늘 HTML/CSS 첫 수업을 들으면서 너무 좋았다. 내가 정말 원했던 수업내용과 방식이었고, HTML과 CSS를 정말 제대로 배울 수 있겠다는 느낌을 받았다.프론트엔드 쪽으로 커리어를 쌓아 나갈 것이기 때문에 가장 기본이 되는 HTML/CSS를 제대로 알고 사용하는 개발자가 되기 위해서 정말 노력을 많이 해야겠다. 시작은 수업진행을 위한 전반적인 개발환경 구축 및 수업자료를 local환경에 준비하는 것으로 시작을 하였다. 본 수업 커리큘럼은 기본 HTML5/CSS3 ~ Sass(css 전처리기)사용까지 진행할 계획이라고 하셨다. Sass에 대해 부가적인 설명은 Sass 중에서 Ruby Sass, Node Sass, Dart Sass가 있는데 Ruby Sass와 Node Sass는 이미 deprecated 되었기 때문에 Sass를 사용하기 위해서는 Dart Sass를 사용해야 한다고 설명해주셨다. 포스팅 내용 정리는 수업 전체 내용을 정리하는 것이 아닌, 내가 기존에 제대로 알고 있다고 생각되는 부분은 생략하고 제대로 알고 있지 않았던 부분과 새롭게 알게 된 내용 그리고 중요하다고 생각되는 내용을 위주로 정리를 해 나갈 계획이다. HTML5 이전과 이후HTML5의 이전에는 HTML Tag의 요소를 block/inline 요소로만 구분을 했었다. 하지만 HTML5 이후에는 콘텐츠 모델이라는 개념이 등장을 하게 되었고, 기존에 block/inline 측면의 HTML 구조상에서 유효하지 않았던 태그 구조들이 콘텐츠 모델을 기준으로 가능하게 되었다.콘텐츠 모델이라는 기준으로 작성한 HTML태그가 유효한 코드 구조인지 알기 위해서는 validation 체크를 하면서 작성을 해야한다. https://validator.w3.org/#validate_by_uri+with_options W3C Markup Validation Service내가 작성한 HTML 코드의 태그가 제대로 배치되어서 사용되고 있는지 체크해주는 Vlidation check service이다.수업시간에 anchor tag와 button tag는 둘 다 interactive한 태그요소이기 때문에 중첩시켜서 작성하면 안된다고 설명해주셨다. 이러한 예처럼 각 태그의 특성을 제대로 파악하고 HTML의 구조를 만들어야 한다. HTML 태그에 class이름을 명명하는 방법 BEM이란? HTML 태그의 class이름을 Block 요소를 기준으로 작성을 하는 방법이다.HTML 태그의 class이름은 그냥 작성을 하는 것이 아니라 BEM과 같은 규칙으로 작성을 해야한다. HTML 태그로 화면구성 구상하기화면구성을 HTML태그로 구성하기 위해서는 화면 각 각의 Segment를 아래와 같은 sequence로 구성해나간다. 첫 번째, 3단으로 구성할지 4단으로 구성할지 생각한다. 일반적으로 Semantic HTML을 살펴보면, 기본적으로 정해놓은 프레임이 있다. 하지만 페이지 구성에는 정해진 구조와 정답이 있는 것이 아니다.예를들어, Header 태그와 nav 태그를 별도의 segment로 분리시킴으로써 4단으로 구성할 수도, 아니면 Header내부에 nav 태그를 위치시켜서 3단으로 구성할 수도 있다. 위에서 아래로 논리적인 흐름으로 화면 구성을 생각하도록 하자. 두 번째, 어떤 구조와 태그로 화면을 구성했는가에는 이유가 있어야 한다. 내가 구성한 페이지 구성에서 사용된 모든 것들에는 직접 설명할 수 있어야 한다. 세 번째, 기계가 알아들을 수 있는 각 각의 의미있는 태그로 페이지를 구성해야 한다. 이전에는 단순 div 태그로 페이지를 구성하였다. 하지만 HTML5 이후부터는 기본 화면 구성에 있어 다양한 태그들이 등장했다. 단순히 의미없이 화면의 Partition을 div 태그로 나누지 말고 의미있는 태그를 사용해서 나누는 연습을 하자. 네 번째, 화면 전체 컨텐츠를 하나의 container로 묶어주는 작업을 한다. 나중에 화면전체를 한 단위로 컨트롤하기 편하기 때문에 전체를 container class로 묶어주도록 하자. 다섯 번째, 페이지의 구성을 직접 손으로 그려가보면서 연습을 하도록 하자. 여섯 번째, HTML 태그는 id, class 중에 class를 선언해서 각 element의 스타일을 조작한다. SEO (Search Engine Optimization)검색 엔진 최적화. 이 부분은 HTML 태그의 title 부분에 대해서 설명을 해주시면서 나온 개념이다. 이 title에 대한 이해없이 그냥 아무거나 작성하는 경우가 있는데 이 부분은 웹에서 검색을 했을때 해당 웹 페이지를 노출시킬 수 있는데 크게 기여하는 부분이다.title 부분 이외에도 검색 엔진 최적화를 고려해서 개발을 할때 중요한 부분도 있으니 개별적으로 찾아보면서 공부를 할 필요가 있다.프로젝트를 진행할때 SEO를 최대한 녹여내서 진행해보도록 한다. 웹 브라우저의 작동과 화면 rendering에 대한 기본 개념 이해기술적인 부분에 치중해서 개발을 공부하는 경우가 많은데, 전체적인 골격을 우선 공부하는 것이 중요하다.전체적인 골격이 되는 부분을 우선적으로 공부하고, 세세한 부분에 대해서는 나중에 추가적으로 공부를 하는 것이 중요하다. Emmet을 제대로 사용해서 생산성을 높이자.Emmet 공식 홈페이지에 들어가면 사용하는 방법에 대한 documentation이 잘 되어있다.아래의 코드는 Emmet을 활용하여 빠르게 HTML 태그들을 작성하는 예시이다. 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html lang=&quot;ko-KR&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;웹카페-HTML5, CSS3, 웹 표준, 접근성&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- div.container&gt;header.header+div.visual --&gt; &lt;!-- &gt; : child, + : sibling --&gt; &lt;div class=&quot;container&quot;&gt; &lt;header class=&quot;header&quot;&gt; &lt;nav class=&quot;navigation&quot;&gt;내비게이션&lt;/nav&gt; &lt;/header&gt; &lt;div class=&quot;visual&quot;&gt;비주얼&lt;/div&gt; &lt;main class=&quot;main&quot;&gt; &lt;!-- div.group.group$*3 --&gt; &lt;!-- &lt;div class=&quot;group group1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;group group2&quot;&gt;&lt;/div&gt; &lt;div class=&quot;group group3&quot;&gt;&lt;/div&gt; --&gt; &lt;!-- div.group.group${그룹$}*3 --&gt; &lt;div class=&quot;group group1&quot;&gt;그룹1&lt;/div&gt; &lt;div class=&quot;group group2&quot;&gt;그룹2&lt;/div&gt; &lt;div class=&quot;group group3&quot;&gt;그룹3&lt;/div&gt; &lt;/main&gt; &lt;article class=&quot;slogan&quot;&gt;슬로건&lt;/article&gt; &lt;footer class=&quot;footer&quot;&gt;푸터&lt;/footer&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; Classic &amp; Modern 레이아웃 배치과거에는 페이지의 Layout 요소들을 배치할때 float, position을 사용하여 배치하였다. 하지만 요즘에는 flex나 grid를 사용해서 좀 더 편리하게 화면의 element들을 배치할 수 있다. 이 부분에 대해서는 별도로 포스팅하여 내용정리를 해 볼 것이다. 그외 도움되는 이야기 : Clone coding은 어떻게 제대로 진행을 해야하는가?클론코딩은 단순히 똑같이 만드는 것에 의미를 두는 것은 의미가 없다. 기존의 서비스를 클론코딩 해보았으면, 추가적으로 필요한 요소가 있는지 분석을 하고 추가를 하거나 개선되었으면 좋겠다는 부분도 추가적으로 생각해서 반영을 하는 것이 중요하다. 많은 사람들이 사용을 하고 있는 네이버의 로그인 화면의 배치에도 개선할 점(로그인 상태 유지의 위치)이 있다. 이처럼 이미 서비스하고 있는 유명한 웹이라고 하더라도 반드시 개선점은 있으니, 이 부분을 염두해두고 분석을 해보도록 하자.","link":"/2021/02/22/202102/210222-html_css_til/"},{"title":"210223 HTML&#x2F;CSS - float TIL","text":"float이번 포스팅에서는 float의 개념과 기본적인 사용에 대해서 정리해보려고 한다. 이미 과거에 짜여진 화면 레이아웃을 파악하거나 예전 버전의 브라우저와 호환되는 웹 레이아웃을 만들기 위해서는 기본적으로 알고 있어야 되는 내용이다. Thumbnail 이미지를 보면 알겠지만, float란 단어 자체의 의미처럼 떠있는 상태를 말한다. 물리적인 상태는 이해가 되었으니, 이제 이 float라는 속성이 HTML에서 어떠한 특성을 갖는지 살펴보도록 하겠다. 개별 요소에서의 float 적용HTML에서 이 floating된 요소는 기존의 속성과 관계없이 block 요소가 된다. 예를들어 흔히 링크를 삽입할때 많이 사용되는 &lt;a&gt; anchor tag의 경우 원래 inline-element이지만, 속성으로 float을 주게 되면, &lt;div&gt; 태그와 같이 하나의 block element처럼 변한다. 그리고 자기자신만의 영역을 가지고 있는 inline-block처럼 화면에 rendering된다. 위의 설명을 간단하게 정리하면, floating element는 inline-block과 같은 효과를 낸다라고 말할 수 있다. 이제 기본적으로 HTML 내부에 존재하는 개별요소에 float를 적용하게 되면 어떠한 변화가 있는지 살펴보았으니 이제 전체 document의 관점에서 float를 적용한 element의 변화에 대해서 살펴보도록 하겠다. 전체 document 관점에서의 float 요소Document내에 존재하는 float 속성이 적용된 자식요소는 페이지에 표시된 document의 흐름에서 벗어난 상태로, 레이아웃의 배치를 무너뜨리게 하는 요인이 된다.이러한 레이아웃 배치를 무너뜨리게 되는 현상을 해결하기 위해서는 float가 적용된 요소의 주변에 있는 요소들은 영향을 받지 않도록 해야한다. 주변 요소들로 영향이 없도록 하기 위해서는 주변 요소의 속성에서 float를 해제해야 하는데 그 방법에는 clear 속성을 적용하는 방법과 적용하지 않는 방법, 두 가지로 크게 분류해서 살펴볼 수 있다. float 해제하는 방법이제부터 주변 요소로의 float 영향을 제거하기 위한 float 해제의 방법에 대해서 정리해보겠다. 방법은 크게 clear 속성을 사용하지 않은 방법들과 clear 속성을 사용한 방법으로 분류하여 정리를 해보겠다. 우선 clear 속성을 사용하지 않고, float 해제방법에 대해서 정리해보도록 하겠다.첫 번째, 우선 수업시간에 배운 overflow: hidden 속성 지정을 통한 float 해제에 대해서 알아보도록 하겠다.자식요소가 부모요소의 크기보다 클 경우, 부모요소는 자식요소의 넘치는 컨텐츠를 숨기고 보이지 않게 한다. overflow의 속성 값 중에 auto도 있는데, 이 auto 속성값을 주게 되면, 자식요소의 너비가 부모요소보다 큰 경우에 스크롤이 생기게 되기 때문에 추천하지 않는 방법이다. 부모요소가 float 속성을 가진 자식요소의 높이를 인지하지 못한 상황에서 overflow: hidden 속성을 주게되면 부모요소는 자식요소의 overflow된 contents영역을 숨기기 위해 자식요소의 높이값을 자동으로 계산하게 되는데 이를 통해 화면상에서는 부모요소가 자식요소를 감싸안는 형태로 보이게 된다. float해제에서 overflow: hidden 사용의 단점이 방법은 동적으로 데이터를 보여주는 경우에는 적합하지 않다. 동적으로 보여지는 데이터가 부모 컨텐츠보다 큰 경우에는 넘치는 데이터 표시부분을 모두 숨겨버리기 때문이다. 두 번째,가장 널리 사용되는 방법으로 가상 클래스(Pseudo class)를 사용한 float 해제에 대해서 알아보자. 이 방법은 불 필요한 요소의 추가없이 CSS를 통해 가상 요소를 생성하여 float를 해제한다.display가 block일때만 clear를 적용시킬 수 있기 때문에 아래와 같이 display를 block으로 선언해준다. 12345.parent::after { content: ''; display: block; clear: both;} 인위적으로 float가 적용된 자식요소의 마지막에 의미없는 형제요소를 만들어서 clear 속성을 넣어 float를 해제시켜 줄 수도 있는데, 불필요한 요소를 추가하게 되므로, 추천되지 않는 방법이다. 이 방법의 단점을 보완한 방법이 가상 클래스를 사용한 방법이다. 이 가상 클래스를 사용하는 방법은 IE7에서는 지원되지 않기 때문에 가상 클래스 요소 대신에 부모요소에 zoom:1;를 별도로 속성으로 지정해줘야 한다. 세 번째,부모요소에 inline-block의 특징을 갖게 하는 방법이 있다. 이 방법에는 크게 두 가지 방법이 있는데, 부모요소에도 자식요소와 똑같이 float 속성을 지정해는 방법과 display 속성에 inline-block 속성값을 지정해주는 방법이 있다.부모요소에 float 속성을 주게 되면, 자식요소와 동일하게 float한 상태가 되기 때문에 자식요소의 높이를 인지하게 되지만 부모 요소도 inline-block의 특징을 갖게되어, 부모 요소에 지정한 너비만큼만 너비를 갖게 되기 때문에 레이아웃의 전체적인 구성을 고려해서 지정해줘야 한다. 이제 clear 속성을 사용한 float 해제방법에 대해서 정리해보도록 하겠다.네 번째,float를 지정한 자식요소의 부모요소에 clear 속성을 지정함으로써 float 해제하는 방법이 있다. 이 방법은 clearfix라는 방법으로 불리는데, float 속성이 적용된 자식요소를 감싸는 부모요소에 clear 속성을 지정함으로써 부모요소가 자식요소를 감싸는 형태가 될 수 있도록 한다.clear 속성에는 left, right, both, none등 다양한 속성값이 있으며, 내부 자식요소에 float: left를 주었다면, clear:left를, float: right를 주었다면 clear: right를 주면 된다. 그리고 left, right 자식요소 모두 flot해제를 해 줄때에는 both값을 넣어주면 된다. 다섯 번째,micro clearfix라는 방법이 있다. 이 방법은 아래와 같이 CSS로 float 속성을 가진 자식요소를 감싸는 부모요소에 지정을 해서 float를 해제하는 방법이다. 123456789101112.parent { /* For IE 6/7 only */ *zoom: 1;}.parent::before,.parent::after { content: ''; display: table;}.parent::after { clear: both;} 실습코드 (micro clearfix) index.html 123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;./style.css&quot; /&gt; &lt;title&gt;Document&lt;/title&gt; &lt;div class=&quot;above_container&quot;&gt;0&lt;/div&gt; &lt;div class=&quot;parent_container&quot;&gt; &lt;div class=&quot;child_container&quot;&gt;1&lt;/div&gt; &lt;div class=&quot;child_container&quot;&gt;2&lt;/div&gt; &lt;div class=&quot;child_container&quot;&gt;3&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;below_container&quot;&gt;4&lt;/div&gt; &lt;/head&gt; &lt;body&gt;&lt;/body&gt;&lt;/html&gt; style.css 123456789101112131415161718192021222324.parent_container::before,.parent_container::after { content: ''; display: table; border: 1px solid red;}.parent_container::after { clear: both;}.child_container { float: left; width: 50px; height: 50px; background-color: cadetblue; border: 1px solid black;}.above_container,.below_container { border: 1px solid green; width: 200px;}","link":"/2021/02/23/202102/210223-float_til/"},{"title":"210223 HTML&#x2F;CSS TIL - float, flex, positioning, 접근성을 고려한 개발","text":"HTML/CSS 두번째 수업오늘은 HTML/CSS 두번째 수업날이었다. 역시 오늘은 어제보다 수업내용이 더 좋고 유익했다. 자 그럼 오늘 수업내용을 정리해보자. float이전 시간에 float를 활용해서 화면의 layout을 배치하는 방법에 대해서 배워보았다. float를 지정함으로써 발생하는 레이아웃의 무너짐 현상을 예방하는 float 해제방법에 대해서 여러 방법을 알아보았다.관련내용은 아래의 포스팅에 정리를 해두었으니, 참고하면 좋을 것 같다. https://leehyungi0622.github.io/2021/02/23/202102/210223-float_til/ flexflex에 대한 내용은 별도의 포스팅에 정리를 할 예정이다. flex는 float를 사용해서 힘들게 했던 화면 레이아웃의 배치를 좀 더 간편하게 할 수 있도록 도와주는 CSS display 속성중에 하나이다. 웹 페이지 분석을 통한 Header의 구성구체적인 Header 구성도는 아래에 첨부한 노트의 내용을 확인하고, header태그의 내부구조는 &lt;h1&gt;태그의 내부에 a(anchor) 태그를 넣고 그 내부에 img태그를 넣는 구조로 내부구성을 하였다. 이미지 태그의 로고 이미지 구성에 관한 내용은 아래에 작성한 퍼포먼스를 고려한 웹 개발 부분을 참고하자. 시맨틱 작성과 UX시맨틱(semantic) 요소를 화면에 구성할때에는 사용자 경험(UX)을 고려해서 배치해야 한다. 예를들어, 네이버 로그인 페이지의 로그인 상태 유지하기 버튼은 아이디와 패스워드를 입력하고 Tab키를 눌렀을때에 자동으로 해당 section으로 이동을 하고, 선택된 후의 그 다음 이동은 로그인 버튼이 될 수 있도록 해야한다. Reset style CSS기본적으로 HTML 태그들은 default로 적용된 스타일이 있다. 이러한 기본 스타일 속성은 개발시에 번거롭기 때문에 이미 정의되어있는 CSS Reset 코드를 가져와서 HTML태그의 default 스타일들을 초기화한 상태로 개발을 시작한다. Normalize CSS : https://necolas.github.io/normalize.css/ CSS 단위 em (equal m) : 지정 사이즈를 상위 parent 태그의 사이즈를 기준으로 한다. 예를들어 2em은 상위 parent 태그 사이즈의 2배이다. rem (root equal m) : 지정 사이즈를 최상위 루트(body) 태그에 정의된 사이즈를 기준으로 한다. 예를들어 body 태그에서 10px을 지정했고, 하위의 다른 태그에서 2em을 지정한다면, 20px을 지정한 것과 동일한 효과를 낸다. 웹 접근성을 고려한 웹 개발오늘 들었던 수업내용들 중에서 제일 인상깊었던 수업내용이 웹 접근성을 고려한 웹 개발이었다. 이전에 공부했을때 img 태그의 alt 속성이 스크린 리더를 사용하는 시각 장애인들을 위한 부분이라는 것은 익히 들어 알고 있었지만, 강사님을 통해 좀 더 심도있게 웹 접근성을 고려한 웹 개발에 대해서 배울 수 있었던 것 같다. 이 웹 접근성이라는 부분이 대단한 것이 아닌 표준을 지키면 접근성 높은 웹 개발은 자연스럽게 따라 올 수 있다는 것을 오늘 배울 수 있었다. 예를들어, 웹 페이지의 최상단에 있는 Logo img 태그의 alt(대체 텍스트)를 통해서 웹 접근성을 살펴보자.HeadingsMap을 통해서 살펴보면 트리구조의 최상단 이름에서 heading이름을 확인할 수 있는데, 보통 사이트의 최상단에 위치되어있는 Logo의 img태그의 alt(대체 텍스트)를 통해 정의되어있음을 알 수 있다.만약, Logo의 img태그의 alt(대체 텍스트)를 정의하지 않으면, 작성한 HTML 코드의 유효성 검사시에 warning이 발생한다. 이는 표준에 어긋나는 태그 사용으로인해 발생한 경고 메시지이다. HTML 파일의 유효성 검사 사이트 : https://validator.w3.org/nu/#file 앞에서 살펴본 내용은 alt(대체 텍스트)가 설정되어 있지 않음을 HTML 태그의 표준화라는 시각으로 바라보았지만, 사실 이 부분은 스크린 리더를 사용하는 시각 장애인들에게 매우 중요한 부분이다. 스크린 리더를 사용해서 웹 페이지를 읽어보면 화면 구성에 있는 이미지는 이 alt(대체 텍스트)를 통해서 읽히고 설명된다.하지만 대부분의 개발자들은 img태그에서 alt(대체 텍스트) 속성에 대한 정의하지 않고 있고, 이러한 문제로 인한 정보접근 차별에 대해 4년전에 소송되었다가 최근에 판결이 나왔다고 강사님으로부터 이야기를 들을 수 있었다.(기사 캡쳐첨부) 그러던 중에 문득 이전에 시간이 날때마다 봉사활동을 다녔었던 기억이 났다. 지금은 시간이 없다고 봉사활동을 따로 하지 않고 있다. 하지만 개발 일을 본격적으로 시작하게 된다면 항상 이러한 정보 접근성의 차별이라는 문제점을 인지하고, 웹 접근성을 고려한 웹 개발이 가능한 그런 개발자가 된다면 따로 시간을 내서 봉사를 하지 않아도 내가 개발한 웹 어플리케이션을 통해서 또 다른 의미에서 봉사활동을 할 수 있다고 생각한다. 접근성을 고려한 태그페이지를 구성할때 h 태그를 사용해서 해당 부분에 대한 타이틀 텍스트를 넣어주게 되는데, 화면의 CSS를 제거하게 되면 화면에 보이지만, 웹 페이지상에서는 숨겨져서 보이지 않게 된다.이는 accessibility에 따른 작성으로, 이렇게 작성을 해주는 부분의 class이름은 .a11y-hidden과 같이 지정을 해서 컨트롤한다. 클래스 이름은 각 회사/기관에 따라 다르게 지칭한다. 퍼포먼스를 고려한 웹 개발퍼포먼스를 고려한 웹 개발의 예로 header에 위치한 Logo의 이미지 배치에 대해서 살펴보자.이 Logo이미지를 단순 이미지로 처리할지, 배경 이미지로 처리할지 두 가지 측면에서 살펴볼 수 있는데, 각 각의 방법에 있어 다른 퍼포먼스를 보인다.우선 화면크기에 따라 Logo를 다른 해상도 이미지로 화면에 출력하려면 다양한 해상도의 이미지 파일들을 준비해야 한다. 이렇게 되면, 문제점이 화면의 크기가 달라질때마다 매번 다른 해상도의 사진 파일을 서버에 요청을 하게 된다. 이렇게 서버로의 요청이 많아지면 퍼포먼스에 치명적인 문제를 야기한다.Logo 이미지를 배경으로 처리하는 방법으로는 다양한 해상도의 이미지를 화면에 미리 표시하고 CSS로 visible/invisible을 컨트롤 하는 것이다. 이렇게 처리를 해주게 되면, 앞서 살펴본 방법보다 더 나은 퍼포먼스를 가진 웹 페이지를 개발할 수 있게 된다. Positioning static : position의 default 속성값은 static이다. absolute : absolute로 지정한 태그 요소가 움직이는 기준을 position이 static이 아닌 parent 태그가 기준이 된다. relative : normal flow의 특성을 유지하는 특성을 지닌다. fixed : view port를 기준으로 움직인다. sticky : relative와 fixed의 장점을 합친 position의 속성값이다. remind를 위해 수업시간에 필기했던 노트를 첨부한다.","link":"/2021/02/23/202102/210223-html_css_til/"},{"title":"210224 Sectioning practice","text":"sample 웹 페이지의 main section을 sectioning우선 main content 부분을 크게 세 부분으로 나눠보겠다.단순 div 태그를 사용해서 각 section을 나누지 않고, 기계가 알아들을 수 있는 의미있는 적절한 semantic tag를 이용해서 나눠보도록 하겠다. 우선,sectioning element의 특징에 대해서 알아보자. sectioning element에는 &lt;body&gt;,&lt;nav&gt;,&lt;aside&gt;,&lt;article&gt;,&lt;section&gt;가 있으며, 기본적으로 아래와 같이 내부에 &lt;header&gt;와&lt;footer&gt;를 가질 수 있다. 하지만, &lt;header&gt;와 &lt;footer&gt; 내부에는 또 다시 nested한 형태로 &lt;header&gt;와 &lt;footer&gt;를 가질 수 없다는 것에 주의하자. 출처 : https://css-tricks.com/how-to-section-your-html/ 나는 main contents 부분을 세 개의 파트로 나눌때에 section 태그를 사용해서 나눠보려고 한다.자, 그럼 &lt;section&gt;태그는 어느때에 사용을 해야될까? 위의 css-tricks 페이지에서는 if you can't think of a meaningful heading to apply to a &lt;section&gt;, then it probably shouldn't be a &lt;section&gt;이라고 정의하고 있다.따라서 &lt;section&gt;를 사용할때에는 해당하는 부분에 의미있는 heading을 부여할 수 있는지 없는지 생각해서 &lt;section&gt;태그를 사용하면 된다. 위의 예제의 main contents 부분을 논리적 순서로 왼쪽에서 오르쪽으로 위에서 아래 방향으로 살펴보자. 가장 왼쪽에는 로그인, W3C Validation관련 링크, 웹 관련 용어로 각 각 의미있는 heading을 부여할 수 있다. 가운데에는 검색하는 부분, 공지사항 및 자료실 탭, 새소식 부분으로 각 각 의미있는 heading을 부여해서 section을 구분할 수 있다. 마지막으로 오른쪽은 신규 이벤트, 관련사이트, 인기 사이트 부분으로 section을 구분할 수 있다. 여기서 신규 이벤트와 관련 사이트 부분을 같은 section으로 묶어서 관리되고 있다. 각 컨텐츠의 구성을 분석하여, &lt;section&gt;태그를 사용해서 웹 페이지의 레이아웃을 구분할 수 있는지 살펴보았다. 결과적으로 main contents 부분은 아래와같이 의미있는 태그로 구분해보았다. &lt;aside&gt; 태그 사용에 대해서 정리를 해보면, &lt;aside&gt;태그는 페이지의 다른 컨텐츠들과 약간의 연관성은 가지고 있지만, 해당 컨텐츠들로부터 독립적으로 사용될 수 있는가를 기준으로 사용한다. &lt;main&gt; 태그 양옆으로 &lt;aside&gt; 태그를 사용해서 구분을 하였는데, 로그인, Validation관련 링크, 웹 관련 링크, 신규 이벤트, 관련 사이트, 인기 사이트는 페이지에 있는 컨텐츠들과 웹이라는 공통 분모를 가지고 있지만, 각 각 독립적으로 사용될 수 있기 때문에 &lt;aside&gt; 태그로 그룹화해서 분류하였다. 그럼 더 나아가 &lt;article&gt;태그를 사용해서 태그 내부를 좀 더 구체적으로 sectioning해보자. 우선 &lt;article&gt;태그의 사용에 대해서 이해하자. &lt;article&gt;태그란 문맥에서 벗어나도 그 자체로 의미가 있을때 (독립적인 컨텐츠) &lt;article&gt;태그를 사용해서 구분할 수 있다.대표적인 예로 블로그의 글(댓글포함)과 위젯이 있다. 그럼 현재 실습중인 main contents에서 어떤 부분을 &lt;article&gt;태그로 감싸줄 수 있는지 살펴보자.맨 왼쪽의 제일 하단에 위치한 웹 관련용어 웹 관련용어의 내부 컨텐츠(이미지, 글)와 맨 우측의 신규 이벤트의 내부 컨텐츠(이미지, 글)도 &lt;article&gt;태그로 감싸서 표현해 줄 수 있을 것 같다. Q1. 그런데 궁금한 점이 &lt;section&gt;태그는 의미있는 heading을 줄 수 있는 부분에서 사용을 하고, &lt;article&gt;태그는 독립적인 컨텐츠로써 사용이 될 수 있는 부분에서 사용이 된다고 했는데, 위에서 내가 생각한 것 처럼 &lt;section&gt; 태그의 내부에 &lt;article&gt; 태그를 포함시켜서 작성을 해도 되는지 궁금하다. → A1. 위의 질문에 대해서 강사님께 여쭤보았다. 답변은 &lt;section&gt; 태그가 &lt;article&gt;태그의 내부에 위치할 수도 있고, &lt;article&gt;태그가 &lt;section&gt;태그의 내부에 위치할 수도 있다고 한다. 다만 &lt;section&gt; 태그와 &lt;article&gt; 태그를 개념적으로 접근하지 말고, 그 차이를 heading을 주느냐 마느냐를 기준으로 해서 생각하는 것이 좋다고 한다. heading을 주는 부분은 &lt;section&gt;으로, 그렇지 않은 부분을 &lt;article&gt;로 생각해서 작성해주면 된다. Q2. 만약에 된다면, 스타일을 적용시킬때 외부에서 감싸고 있는 &lt;section&gt;태그의 class를 사용해서 스타일링을 해주는지 아니면 내부에서 &lt;article&gt;태그에 정의한 class를 사용해서 스타일링을 해주는지 궁금하다. → A2. 위의 질문에 대해서 강사님께 여쭤보았다. 답변은 실무에서는 되도록 작은 범주로 나눠서 스타일을 적용시킨다고 설명해주셨다.","link":"/2021/02/24/202102/210224-Sample-html_css_sectioning/"},{"title":"210224 HTML&#x2F;CSS TIL - ARIA(role), reset-css, 숨김 컨텐츠 관리, 스타일링과 CSS, Post CSS, Animation 효과","text":"HTML/CSS 세번째 수업오늘은 HTML/CSS 세번째 수업날이었다. 오늘도 어김없이 너무 즐겁고 유익한 수업시간이었다.HTML/CSS 이외에도 강사님께서 여러가지 개발자로서 성장하는데 좋은 팁들을 주셔서 너무나 감사하다고 느꼈던 하루였다. ARIA(Accessible Rich Internet Applications)에서 제공하는 태그의 role 속성ARIA이란 Accessible Rich Internet Applications의 약자로 다양한 role들을 제공한다.ARIA에서 제공하는 role들은 Landmark roles, Document structure roles, Window Role 등 다양한 role들을 제공한다.자세한 내용은 아래 링크를 참조하자. https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/ARIA_TechniquesHTML5의 시맨틱 태그를 살펴보면 &lt;nav&gt;와 같은 native하게 제공되는 태그가 있는데 이러한 native tag를 사용하는 경우에는 문제없지만, 기존에 &lt;div&gt;태그를 이용해서 구성을 했다면 Landmark Roles 중에 하나인 navigation 속성값을 &lt;div&gt; 태그의 role의 속성값으로 넣어주면 된다. (&lt;div role=&quot;navigation&quot; /&gt; - HTML태그의 구성은 아래의 노트를 참고하도록 하자.) reset-css / normalized css의 사용HTML태그들의 기본 스타일을 초기화시켜주는 reset-css를 사용하는 방법에는 두 가지가 있다. 첫번째로 HTML의 &lt;link&gt;태그로 파일을 import해서 사용하는 방법과 CSS 파일 내부에서 @import url([reset css file])의 형태로 선언해서 import 해주는 방법이 있다. 스타일을 초기화시켜주는 기본 코드가 길기 때문에 파일로 분리해서 import 시키지만, 이렇게 처리를 하게 되면 브라우저 상에서 페이지를 띄울때 import된 파일을 우선적으로 불러오고나서 페이지를 렌더링하기 때문에 퍼포먼스에 문제가 생기게 된다.이러한 퍼포먼스의 문제를 해결하기 위해 최근 개발 트랜드는 개발(development)와 배포(production)을 분리해서 프로젝트 파일들을 관리한다. 개발을 할때에는 src 폴더 아래에 있는 파일들을 사용해서 개발을 하지만, 배포시에는 dist라는 폴더 내부에 작성한 코드 파일들을 minify된 형태의 코드 파일형태로 merge시켜서 배포한다. 숨김 컨텐츠 관리nav태그의 경그럼 어떻게 숨김 컨텐츠를 처리해줘야 하나? 부분인지 작성을 해줘야 한다. 이러한 숨김 컨텐츠의 경우에는 Screen reader가 읽어주는 부분이 될 수도 있으며, Headings map을 통해서 웹 페이지의 구조를 파악할 수 있게 해준다. 그럼 어떻게 숨김 컨텐츠를 처리해줘야 하나?보통 개발자들은 이 숨김 컨텐츠를 display: none으로 처리를 하게 되는데, 이렇게 해주면 screen reader가 읽어서 처리해줄 수 없다. 그 이유는 screen reader는 페이지 위에 사이즈로써 존재하는 태그를 읽어주기 때문이다. 그렇다면 어떤 식으로 처리를 해줘야 할까?우선, 숨김 컨텐츠를 아래와 같이 position: absolute, top: -9999px;로 처리해서 작성해주도록 하자. 1234.a11y-hidden { position: absolute; top: -9999px;} 위와같이 처리를 해주게 되면 문제점은 screen reader가 현재 위치해 있는 페이지를 다 읽어주고 나서 페이지의 맨 위 위치로 튀어 올라가는 상황이 생긴다. 스크린 리더는 보통 시각 장애인이 많이 사용한다고 생각을 하지만 실제로 저시력자들도 많이 사용하기 때문에 이런식으로 페이지가 튀어 올라가는 현상은 좋지 않다. 그럼 이러한 페이지 튀어오름 현상을 보완한 방법에는 어떤 방법이 있을까? 아래 코드를 살펴보자. 12345678910.a11y-hidden { background-color: red; position: absolute; width: 1px; height: 1px; margin: -1px; overflow: hidden; clip: rect(0 0 0 0); clip-path: polygon(0 0, 0 0, 0 0);} margin은 padding과 달리 음수값을 지정해 줄 수 있다. 위와 같이 너비와 높이를 각 각 1px씩 지정을 해주고, margin을 -1px로 만들어주면 화면에 표시되는 텍스트는 없읕 것이다. 그렇지만 1px 너비와 높이를 설정해주었기 때문에, 배경색을 처리해보면 미세하게 페이지상에 표시되어 있음을 확인할 수 있다. 이러한 미세한 화면표시는 clip-path: polygon(x, y, z)이라는 속성을 활용하여 제거를 해 줄 수 있다. 이렇게 처리를 할거라면 왜 굳이 너비와 높이를 1px씩 줬는가 의문이 들 수도 있는데, 이는 스크린 리더가 페이지에서 특정 요소를 읽게 하기 위해서는 해당 요소는 화면에 없는 요소가 아닌 최소한의 사이즈로 존재해야 되기 때문이다. clip-path의 polygon 설정은 x,y 좌표 세 점을 만들어서 해당 영역에 있는 컨텐츠를 표시해주는 역할을 한다. 자세한 예제는 아래 링크를 확인해보도록 하자.만약에 IE 구형버전의 호환성을 고려해야 한다면, clip: rect(0 0 0 0)으로 처리를 해주면 된다. 참고 : https://css-tricks.com/almanac/properties/c/clip-path/ 가고싶은 기업의 홈페이지를 분석만약에 가고 싶은 기업이 있다면, 해당 기업이 서비스하고 있는 웹 페이지를 분석해서 버그 리포트를 만들어보는 것도 좋은 방법이다. 이 버그 리포트를 만들어서 나중에 회사 지원시에 첨부를 한다면, 해당 기업에 지원하는 그 누구보다 경쟁력에서 우위를 갖게 될 것이다. 버그리포트를 작성 자세한 방법은 아래의 링크를 확인해보자.https://www.softwaretestinghelp.com/how-to-write-good-bug-report/ 스타일을 적용시킬때에는 유지보수를 생각해서 class를 이용한 스타일링을 해야한다. navigation menu를 리스트의 항목으로 작성을 한 경우에는 리스트의 기본 스타일을 제거해줘야 한다. 아래는 직접적으로 태그를 지정해서 스타일을 적용시키는 방법이다. 123456ul,ol { list-style: none; margin: 0; padding-left: 0;} 만약에 개발중인 웹 어플리케이션이 규모가 작다면 상관없지만, 나중에 유지보수를 고려해야되는 어느정도 규모가 있는 개발이라면 직접 태그를 걸어서 스타일링을 주는 것보다는 태그의 class 속성을 활용해서 스타일링하는 방법이 좋다. 123456789.reset-list { list-style: none; margin: 0; padding-left: 0;}[class] { box-sizing: border-box;} post css - auto prefixcss를 적용할때 몇 몇 태그들은 호환성의 문제로 브라우저에 따라 각기 다른 prefix가 붙게 된다.이러한 prefix를 자동으로 넣어주는 autoprefixer와 같은 post css도 있으니 나중에 프로젝트할때에 참고를 하도록 하자.https://autoprefixer.github.io 오늘까지 배운 header 메뉴 구성을 처음부터 끝까지 실습해보자.수업시간 예제 웹 페이지는 강사님이 다양한 예제들을 연습할 수 있도록 구성하셨다고 한다. 이 예제를 모던한 flex로도 레이아웃을 구성해보고, float로도 구성을 해보면서 연습을 해야겠다. 이 예제 웹 페이지만 연습해도 충분한 연습이 될 것 같다. icon fontfontello.com 를 사용하면 다양한 아이콘 폰트를 웹 페이지에서 사용할 수 있다. 나중에 개인 프로젝트나 팀 프로젝트를 할때에 참고해서 사용하도록 하자.https://fontello.com white-space: nowrap = 줄바꿈 금지position을 absolute로 설정을 하게되면 position의 속성이 relative로 설정된 부모요소의 너비를 기준으로 자식 컨텐츠의 너비가 결정되어, 내부에 출력되는 컨텐츠에 자동 줄 바꿈이 일어날 수 있다. 이러한 경우에는 자식요소에 white-space: nowrap을 넣어줘서 자동 줄 바꿈을 해제시킬 수 있다. Animation 효과주기애니메이션 효과를 주기 전에 아래와 같이 애니메이션 효과에 대한 시나리오를 작성해야한다. 12345678910111213141516171819202122232425/* # 애니메이션 효과 시나리오1. 이름 : textAni2. 액션 : 이동은 (왼쪽 상단으로 00px -&gt; 오른쪽 하단으로 00px)구체적으로 좌표 위치도 명시해줘야 한다. 글자의 크기는 점진적으로 커지는 형태로 구현한다. 투명도는 점진적으로 진해지는 형태로 구현을 한다.3. 이동 : 컨텐츠의 이동은 아래의 4가지 속성을 이용해서 구현을 할 수 있다. 다만 퍼포먼스를 고려한다면(reflow/repaint로 인한 퍼포먼스 저하) 위치 속성을 바꿔주는 속성 대신에 transform과 같은 애니메이션 처리를 활용하는 것이 좋다. 애니메이션을 선언해서 사용을 할때에는 아래와 같이 @keyframe으로 애니메이션 효과를 정의해주고, 적용시킬 대상 태그의 속성에 작성한 애니메이션 효과를 적용시켜줘야 한다. *//* 예시) */keyframes textAni { 0% { font-size: 12px; } 100% { font-size: 24px; }}.visual-text { /* 적용할 애니메이션 */ animation-name: textAni; /* 애니메이션 시간 */ animation-duration: 2000ms; /* 최종 변화된 애니메이션 효과 상태에서 고정 */ animation-fill-mode: forwards;} 애니메이션 효과를 어플리케이션의 퍼포먼스 측면에서 분석하여 네이버의 한 주니어 개발자 분이 발표를 하신 자료라고 강사님이 소개해주셨다. 어플리케이션을 구현하는 것에만 그치지 않고 다양한 방법으로 같은 기능이 구현되었을때 어떤한 퍼포먼스의 차이가 있는지 분석한 PPT자료는 정말 인상깊었다. 나도 나중에 이런 발표자료를 만들어서 많은 사람들 앞에서 발표를 해보고 싶다는 생각에 나중을 기약하며 아래에 링크를 첨부해놓는다. https://www.slideshare.net/wsconf/css-animation-wsconfseoul2017-vol2?qid=2fcf41db-7f5e-4ae4-b47d-4c96f3266901&amp;v=&amp;b=&amp;from_search=15 remind를 위해 수업시간에 필기했던 노트를 첨부한다.","link":"/2021/02/24/202102/210224-html_css_til/"},{"title":"210225 Header Practice float&#x2F;flex 활용해서 구현하기 (Review) (작성중...)","text":"sample 웹 페이지의 header 구성 복습하기(float &amp; flex)우선 header 부분의 구성을 분석해보았다. 손으로 직접 그려보니, 확실히 그냥 타이핑을 쳤을때와는 다르게 전체적인 구성이 그려지는 것 같았다. 이제 분석한 header를 직접 코드로 구현해보도록 하겠다. 실습한 코드는 아래의 repository에서 연습해보도록 한다.","link":"/2021/02/25/202102/210225-Sample-html_css_header_practice/"},{"title":"210226 HTML&#x2F;CSS Layout practice (w&#x2F;float)","text":"HTML/CSS float 및 구조 선택자를 활용한 레이아웃 배치 연습하기 화면 구성 설계해보기 우선 화면의 배치상 이미지는 왼쪽, 타이틀은 오르쪽 상단, 용어 설명은 오른쪽 하단에 위치한다. 그럼 왼쪽에 높이가 100% , 너비가 50% 비율로 위치하면 되고, 오른쪽에는 높이가 50%, 너비가 50% 비율로 두 개의 박스를 수직방향으로 배치시키면 된다는 결론이 나온다. 이제 float 속성을 이용해서 왼쪽에 배치해야되는 이미지(dd 첫번째 요소)의 경우 left 속성값을, 오른쪽에 배치해야되는 타이틀(dt)과 용어설명(dd 두번째 요소)은 right 속성값을 위에서 생각한 너비, 높이 비율값과 함께 선언해주면 될 것 같다.그래서 아래와 같은 구조로 우선 생각을 해보았다. 그리고 용어설명 전체 컨텐츠를 담고 있는 부모요소가 자식요소의 높이 정보를 잃어버렸기 때문에 float를 해제시켜줘야 될 것 같다. 이제 코드로 구현해가면서 생각했던 부분과 달라진 부분을 수정해가면서 레이아웃을 잡아가보자.원래 레이아웃 설계 단계에서 너비 비율을 50%로 생각을 했었는데, 구현을 해보니, 너비를 이미지 부분은 30%로, 타이틀과 설명 부분은 70%로 해줘야 sample 원본과 비슷하게 나오는 것 같다. 그리고 추가적으로 기타 용어 설명부분을 원본 페이지와 비슷하게 구성하기 위해서 자간 높이를 line-height 속성을 이용해서 조절해주고, 글자의 weight값과 타이틀 색상을 바꿔줘야겠다고 생각했다. 코드와 결과화면은 아래와 같다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849.term { margin-top: 20px; padding: 16px; border-radius: 5px; background: linear-gradient(180deg, #ccc, #eee); border: 1px solid grey;}.term h2 { margin: 0; margin-bottom: 19px;}.term dd,dt,dl { margin: 0;}.term::after { content: ''; display: block; clear: both;}.term-list div dd:nth-of-type(1) { float: left; width: 30%; height: 100%;}.term-list dt { float: right; width: 70%; font-size: 14px; color: rgb(12, 110, 172); font-weight: 400; margin: 0 0px 10px 0; padding-left: 5px;}.term-list div dd:nth-of-type(2) { float: right; width: 70%; font-size: 14px; font-weight: 400; line-height: 19px; padding-left: 5px;} 그리고 추가적으로, 두 개 이상의 컨텐츠를 좌우 대칭 형태로 교차해서 화면에 표시하는 방법을 구조 선택자를 사용해서 실습해보았다. 각 컨텐츠가 div태그로 분리가 되어있고, 이 div태그가 홀수 번째인지, 짝수 번째인지 구분해서 내부 자식요소의 float 속성 값을 대칭으로 해주면 될 것 같고 생각을 했다. 결과 화면과 코드는 아래와 같다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/* 웹 관련 용어 */.term { margin-top: 20px; padding: 16px; border-radius: 5px; background: linear-gradient(180deg, #ccc, #eee); border: 1px solid grey;}.term h2 { margin: 0; margin-bottom: 19px;}.term dd,dt,dl { margin: 0;}.term::after { content: ''; display: block; clear: both;}/* div 태그가 홀수번째이면 */.term-list div:nth-child(odd) dt { float: right; width: 70%; font-size: 14px; color: rgb(12, 110, 172); font-weight: 400; margin: 0 0px 10px 0; padding-left: 5px;}.term-list div:nth-child(odd) dd:nth-of-type(1) { float: left; width: 30%; height: 100%;}.term-list div:nth-child(odd) dd:nth-of-type(2) { float: right; width: 70%; font-size: 14px; font-weight: 400; line-height: 19px; padding-left: 5px;}/* div 태그가 짝수번째이면 */.term-list div:nth-child(even) dt { float: left; width: 70%; font-size: 14px; color: rgb(12, 110, 172); font-weight: 400; margin: 0 0px 10px 0; padding: 20px 0 0 5px;}.term-list div:nth-child(even) dd:nth-of-type(1) { float: right; width: 30%; height: 100%; padding: 20px 0 0 0;}.term-list div:nth-child(even) dd:nth-of-type(2) { float: left; width: 70%; font-size: 14px; font-weight: 400; line-height: 19px; padding-right: 5px;}","link":"/2021/02/26/202102/210226-html_css_layout_practice/"},{"title":"210225 HTML&#x2F;CSS TIL - CSS 구체성, 구조 선택자, CSS 유효성 검사, 레이아웃 설계, 접근성 지침서(WCAG, KWCAG), 로그인 폼 설계 및 구현, explicit outline과 implicit outline, DTD(S,T,F), 웹 관련 용어 부분 레이아웃 설계 및 구현","text":"HTML/CSS 네번째 수업오늘은 HTML/CSS 네번째 수업날이었다. 오늘은 혼자서 레이아웃을 설계도 해보고, 직접 생각해서 레이아웃을 구성해보는 연습도 하였다.연습하면서 느낀점은 역시 많은 연습이 필요하다는 것이다. 계속해서 끊임없이 노력하자. CSS 구체성오늘은 수업의 첫 시작은 CSS의 구체성에 대해서 시작하였다. 이 CSS의 구체성이란 특정 태그에 스타일을 적용시킬때 태그의 어느 요소(id, class(가상클래스 포함), element(태그))를 가지고 얼마나 구체적으로 명시해서 스타일을 적용했느냐를 말한다. 표면적으로는 같은 태그의 스타일을 적용했지만, 선택자를 좀 더 구체적으로 명시한 쪽의 스타일이 적용되는 것은 선택자로 사용한 요소(id, class, element)에 각 각 우선순위 점수가 있기 때문에 점수가 높은 (선택자를 좀 더 구체적으로 지정한) 스타일이 최종적으로 적용되는 스타일이 되는 것이다. id(100) class(10) element(1) 0 2 1 0 1 1 12345678/* 점수표에서 첫 번째 행(21) */.member li::before { content: '|';}/* 점수표에서 두 번째 행(11) */.first::before { content: ' ';} 아래와 같이 p태그의 스타일을 적용한다고 했을때, 구체성 점수에 의해 p.test의 스타일이 적용된다. 123456789101112p { color: red;} /* 1점 */p.test { color: blue;} /* 11점 */.note { color: green;} /* 10점 */.test { color: puple;} /* 10점 */ 결과적으로 구체성 점수가 높은 첫 번째 스타일이 적용된다.개당 배점이 높은 순은 id &gt; class &gt; element이다.id의 우선순위는 점수상 id:1 == class: 10의 관계를 갖기 때문에 CSS 스타일링을 줄때 id를 사용하지 않고 class를 사용한다. 하지만 !important 를 사용하면 구체성 점수와 상관없이 !important가 붙은 스타일이 최우선으로 적용된다. 123p { color: red !important;} 구조 선택자좀 더 세밀하게 태그 내의 요소를 지정해주기 위해서는 구조 선택자를 사용하면 된다. 구체성에 대해 재미있게 정리해놓은 사이트가 있으니 아래 사이트를 참고하도록 하자.http://nthmaster.comnth-child나 nth-of-type을 사용하면 좀 더 세밀하게 태그를 선택할 수 있다. nth-child(3n+1): nth-child(even)는 지정한 태그를 3개씩 묶어서 각 묶음에 있는 짝수번째 자식요소를 선택한다는 의미이다.span: nth-of-type(3)는 span 태그 중에서 세 번째 span 요소를 선택한다는 의미이다.이러한 구조 선택자를 사용해서 좀 더 세밀하게 태그를 지정해서 스타일링을 해 줄 수 있다. CSS 유효성검사CSS도 유효성 검사를 해 줄 수 있기 때문에 필요에 따라서는 W3C CSS validator를 사용해서 작성한 CSS에는 문제가 없는지 유효성 검사를 하도록 하자.https://jigsaw.w3.org/css-validator/ 레이아웃을 만들때는 설계부터화면 구성을 컴포넌트 단위로 나눠서 화면을 설계해야 나중에 유지보수나 코드의 재사용성에 좋다. 따라서 화면을 컴포넌트 단위로 잘 나눠서 설계를 하기 위해서는 레이아웃을 작성할때 무작정 코드를 작성하지 말고, 어떤 태그를 사용하고 어떤 스타일을 적용할 것인지 구체적으로 화면을 설계하는 것이 좋다. 접근성 지침서 확인하기웹 접근성 지침 관련해서 미국에 대한 표준과 한국에 대한 표준이 있다. 사소해 보일 수 있지만, “변경약관 여기 클릭하세요”라는 문구에서 보통 여기라는 부분에 링크를 걸지만 원래 접근성 측면에서는 약관에 링크를 걸어서 처리하는 것이 맞다. WCAG(Web Content Accessibility Guidelines) KWCAG(Korean Web Content Accessibility Guidelines)WCAG 표준에 따르기 어렵다고 한다. 하지만 우리나라의 대한항공 영문 페이지를 확인하면, 이 WCAG 표준에 잘 따르고 있다고 한다. 아래의 사이트를 참고하도록 하자.대한항공 : https://www.koreanair.com/us/enKWCAG(한국 웹 컨텐츠 접근성 가이드라인)에 대한 내용은 아래 웹 페이지에서 확인할 수 있다.https://www.wah.or.kr:444/board/boardView.asp?page=2&amp;brd_sn=4&amp;brd_idx=975 로그인 폼 설계 및 구현하기우선 로그인 폼의 설계부터 진행해보자. 우선 로그인 폼에서 필요한 항목을 정리해보자. 필요한 항목으로는 id, password, 로그인 버튼, 회원가입/아이디・비밀번호 찾기의 항목이 필요하다. 레이아웃에 대한 wireframe은 아래 첨부한 두번째 노트를 확인하자. 레이아웃 설계중에 가장 까다로운 부분이 이 form 태그 부분이다. 그렇기 때문에 정확한 수치를 지정해서 레이아웃을 설계하도록 하자. 하나의 input 태그에는 하나의 label을 사용하도록 권고 &lt;label&gt;태그내에 for속성을 사용해서 해당 label이 어떤 태그를 위한 label인지 알려줘야 한다. (명시적 레이블링(explicit labeling)) 1&lt;div&gt;&lt;label for=&quot;userEmail&quot; /&gt;&lt;input type=&quot;text&quot; id=&quot;userEmail&quot; /&gt;&lt;/div&gt; 아래와 같이 label 태그로 관련된 태그를 감싸서 표현할 수도 있다. (암묵적 레이블링(implicit labeling))이러한 label처리도 19.레이블 제공이라는 항목으로 검사를 할 수 있다. (사다리 모양-OpenWAX) 두 번째 노트의 우측 그림을 참고하도록 하자. 이 그림은 한 개의 label 태그에 두 개 이상의 입력 속성이 존재하는 경우를 설명한다.예를들어, 주민번호 입력의 경우 “주민번호”라는 label에 주민번호 앞 6자리, 뒤 7자리라는 두 개의 입력 속성이 존재한다. 이러한 경우에는 input태그에 title 속성(title=&quot;주민 앞 6자리&quot;, title=&quot;주민 뒤 7자리&quot;)을 넣어서 구분해주면 된다. 필수 입력 항목과 필수 입력이 아닌 항목을 &lt;fieldset&gt;태그를 사용해서 분류 옵션이지만 &lt;legend&gt;태그를 사용해서 분류한 &lt;fieldset&gt;에 묶여있는 서식들의 성격을 알려주도록 하자. 예를들어 회원 로그인을 위한 서식들을 묶었다면 회원 로그인 틀이라고 태그내에 정의해주자. input 태그의 name 속성은 DB의 field 이름이다. React는 XHTML 문법을 차용한다.img 태그를 Self-closing tag로 처리한다. 유명한 서비스라고 완벽한 것은 아니다.네이버 로그인 페이지를 보면, 아이디와 비밀번호 입력란에 placeholder로 아이디와 비밀번호를 넣어주었고, 각 input 태그에 matching되는 label태그를 숨김컨텐츠로써 처리해서 추가해주었다. 이렇게 처리를 하게되면, 스크린리더가 화면을 읽어줄때, 숨김컨텐츠로써 처리해준 label과 placeholder를 연달아 두 번 읽어주게 된다. 이렇게 화면의 레이아웃을 처리하는 것이 회사의 규정으로 굳어진 경우도 있지만, 소리에 의존하는 시각장애인에게는 불편한 요소가 될 수 있다.그렇기 때문에 이러한 경우에는 placeholder대신에 input 태그위에 label 태그를 overlap해서 표시했다가 마우스 focus가 입력상자에 갔을때 해당 label의 텍스트 크기를 원래의 텍스트 크기보다 작게 줄여서 input 태그의 위에 표시될 수 있도록 애니메이션 효과를 처리할 수도 있다.이 부분은 꼭 실습해서 정리해두자. explicit outline과 implicit outline세 번째 노트의 우측에 그려놓은 section 태그를 보면 내부에 heading태그와 ul태그를 묶어주고 있다. 이 section 태그는 생략이 가능하며, 별도로 section 태그로 묶어주지 않아도 암묵적으로 heading태그와 ul태그가 묶인 형태로 인식이 된다.직접 section태그로 묶어서 레이아웃을 작성하는 것을 명시적(explicit)outline, section태그를 생략해서 레이아웃을 작성하는 것을 암시적(implicit)outline이라고 정의한다. 다양한 form tag의 submit 버튼 처리 input 태그로 type을 button으로 처리role을 “button”으로 넣어주고, 값을 value 속성으로 접근한다. button 태그로 처리값을 노드로 접근한다. DTD (S, T, F) DTD(Document Type Definition)?웹 페이지의 html 코드의 최 상단에 &lt;!DOCTYPE html&gt;이라고 선언되어 있는 것을 확인할 수 있다. 이 부분을 DTD라고 한다. 문서 형식 정의로, SGML 계열을 비롯해서 HTML, XHTML, XML등의 마크업 언어에서 문서 형식을 정의할때 사용되고 있다.여기서 말하는 마크업 언어란 태그를 이용하여 문서나 데이터의 구조를 명기하는 언어를 말한다.DTD는 해당 문서가 어떤 문서 형식을 따르고 있다고 정의하는 역할을 한다. 이 DTD 선언에 따라 브라우저의 렌더링 모드가 바뀌게 되고, 사용될 수 있는 태그와 속성이 바뀌게 된다.이 DTD를 선언하지 않을 경우, 브라우저가 표준모드가 아닌, 비표준모드(Quirks mode)로 렌더링되어 크로스 브라우징 문제를 겪게 된다. 문서의 타입으로는 S(trict), T(ransitional), F(rameset)이 있다. Strict Type은 엄격한 규격으로 CSS 사용을 장려하기 위해 점차 단계적으로 사라질 표현에 대한 태그와 속성을 배제한 문서 타입이다. Transitional Type은 과도기적인 규격으로, 표준이 정립되지 않은 때에 기존에 만들어진 문서들과의 호환성을 위해서 사용된다. Frameset Type은 현재는 거의 사용되지 않는 프레임셋(html 안에서 html을 분리하는 것)을 구현하는데 사용된다. HTML5의 경우에는 SGML(Standard Generalized Language)에 기반을 두지 않아서 DTD 참조가 필요없고, 최손한의 코드 작성이 기본 방향이기 때문에 간단하게 선언할 수 있다. 그래서 DTD 선언은 선택적이지만 하위 호환성을 위해서 &lt;!DOCTYPE html&gt;을 선언하는 것이 권장된다. DTD 선언 예시 1234567891011121314151617181920212223HTML 4.01 Strict&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&gt;HTML 4.01 Transitional&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;HTML 4.01 Frameset&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Frameset//EN&quot; http://www.w3.org/TR/html4/frameset.dtd&quot;&gt;XHTML 1.0 Strict&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Strict//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd&quot;&gt;XHTML 1.0 Transitional&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt;XHTML 1.0 Frameset&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Frameset//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd&quot;&gt;XHTML 1.1&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.1//EN&quot; &quot;http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd&quot;&gt;HTML5&lt;!DOCTYPE html&gt; 의미있게 HTML 태그를 사용하자. 웹 관련 용어 부분 레이아웃 설계 및 구현하기설계에 대한 부분은 네 번째 노트필기를 참고하자. 구체적인 구현에 관한 내용은 별도의 블로그 글로 포스팅하였다.https://leehyungi0622.github.io/2021/02/26/202102/210226-html_css_layout_practice/ 개발에 유용한 블로그 및 깃허브 추천김태곤 프론트엔드 개발자님 블로그→ https://taegon.kim/archives/5770 Yamoo9님의 깃허브→ https://github.com/yamoo9 remind를 위해 수업시간에 필기했던 노트를 첨부한다.","link":"/2021/02/25/202102/210225-html_css_til/"},{"title":"210226 HTML&#x2F;CSS TIL - 개발자로서의 또 다른 목표, 레이아웃 설계 방법 및 순서, 자손 선택자와 자식 선택자, inline element의 bottom gap, line-height에 대한 이해, 설득의 심리","text":"HTML/CSS 다섯번째 수업오늘은 HTML/CSS 다섯번째 수업날이었다. 오늘은 강사님의 열정적인 수업으로 많은 양의 유익한 내용들을 배울 수 있었다. 노트필기를 보니 아직도 강사님의 설명과 재미있는 에피소드가 생각난다. 접근성을 고려해서 레이아웃을 작성하지 않으면 불편함을 느끼는 개발자가 되자.이번 강사님의 수업을 통해 기존에 바라보았던 화면 레이아웃의 시각이 많이 바뀌었다. 이전에는 어떻게 빠르고 효율적으로 화면을 디자인하면 좋을지 시각적인 부분만 생각하고 레이아웃을 바라봤다면, 이제는 접근성과 콘텐츠의 흐름 그리고 사용성을 생각해서 레이아웃을 작성해야겠다는 생각을 했다. 이번에 자료실 및 공지사항 레이아웃을 작성하면서 느꼈던 점은 생각만큼 쉽지 않고, 생각보다 생각해야될 요소들이 많다는 것이다. 아마 기존에는 이러한 생각들을 해보지 않아서 그런 것 같다. 그렇지만 이러한 연습을 꾸준히 하고 레이아웃을 작성한다면 이것이 습관으로 잡혀서, 접근성을 고려해서 레이아웃을 작성하지 않으면 불편함을 느끼는 개발자가 될 수 있을 거라고 생각한다. 그래서 이제부터 레이아웃을 작성할때에는 각 각의 태그의 의미와 접근성을 고려해서 작성해보도록 하겠다.5일간 김데레사 강사님의 수업을 들으면서 또 다른 목표가 더 생겼다. 그것은 웹 표준과 접근성을 고려해서 개발할 줄 아는 그런 개발자가 되는 것이다. 용어설명 레이아웃 부분 설계 및 구현 (과제) 점검내가 작성했던 코드와 강사님이 작성하신 코드를 비교/분석하며 수업을 들었다.아래의 내용은 레이아웃을 작성하면서 강사님께서 강조하신 내용들이다. https://leehyungi0622.github.io/2021/02/26/202102/210226-html_css_layout_practice/ 내가 작성한 코드와 비교/분석하기 그라디언트가 적용이 안될 경우를 고려해서 default color를 앞에 지정해주자 chrome 개발자 툴을 이용해서 agent style을 확인하고 제거하자 실무에서 유지보수를 생각해서 CSS 스타일링에서 단축표기법을 주로 사용하자 레이아웃에 있는 각 요소들의 영역을 확인하기 위해서 배경색상을 넣고 확인하자 태그보다는 class 명으로 접근을 하자위의 내용은 다시 한 번 remind하면서 HTML/CSS를 사용해서 레이아웃을 작성하도록 하자 자손 선택자(descendant selector)와 자식 선택자(child selector)의 사용자손 선택자(descendant selector)와 자식 선택자(child selector)의 사용에 대해서 정리한다. 우선 자손 선택자는 A B{속성:속성값}문법의 형태로 사용한다. 자손은 자식, 손자, 그리고 그 이후의 후손을 모두 포함한다.A와 B 사이를 공백으로 분리하고, A요소 내에 있는 모든 B요소를 선택해서 스타일을 적용한다. 다음으로 자식 선택자는 A&gt;B{속성:속성값}문법의 형태로 사용한다. 자식 선택자는 특정 요소의 직계 자식만 선택하는 선택자이다. 자식 이후의 손자, 후손들은 포함하지 않는다.사용의 예로 div&gt;p{color:blue;}를 해주게 되면, div 태그 내에 있는 직계 자식 p만 파란색으로 색상 스타일을 바꿔준다. inline element인 &lt;img&gt;의 하단에 작은 gap이 발생하는 경우inline element인 &lt;img&gt; 태그를 사용해서 이미지를 삽입하는 경우, 하단에 약간의 gap이 생기는 현상이 발생한다. 왼쪽 이미지는 dd 태그 내부에 img 태그가 삽입된 형태이다. 이미지의 하단을 보면 약간의 갭이 발생한 것을 확인할 수 있는데, 내부의 img 태그는 inline element이기 때문에 block element의 내부에서 약간이 갭이 발생하게 된 것이다. 그럼 해결방법은?해결방법은 내부의 img 태그에 vertical-align: top을 주거나 display: block로 inline element의 성격을 가진 img 태그를 block의 성격을 갖도록 만들어 주는 것이다. img태그가 block 성격을 갖도록 만들어주는 해결법은 단일 이미지의 경우에는 괜찮지만 연속적으로 이미지를 나열해서 출력하는 경우에는 사용되어서는 안되기때문에 레이아웃에 맞게 해결법을 적용하면 된다. line-height 이해하기line-height는 줄 높이를 정하는 속성으로, 속성값으로는 normal, length, number, percentage, initial, inherit이 있다.글자 크기가 40px일 때 line-height의 값을 1.5로 주게 되면, 줄 높이는 글자 크기인 40px의 1.5배인 60px로 잡히게 됩니다.줄 높이가 60px인데 글자의 크기는 40px이기 때문에 글자의 위 아래로 각 각 10px의 여백이 생기게 된다.가독성 측면에서 글자크기의 1.5배의 line-height가 가장 좋다. 두 번째 노트필기의 우측 상단의 그림과 같이 글자의 배치는 top, middle, base, bottom을 기준으로 배치가 된다. 문단의 좌우 정렬 맞추기장문의 설명글을 포함한 컨텐츠를 보면 좌우가 정렬되지 않은 형태로 출력되는 경우가 있다. 그런 경우에는 text-align: justify로 CSS 스타일을 주면 된다. 상대를 설득하는 능력을 키우자. (feat. 설득의 심리학) 개발자로 일을 하게 되면, 디자이너, 기획자, 고객사등 다양한 직업군의 사람들과 함께 협업을 하면서 일을 하게 된다. 각자 자신의 직업에 대한 확고한 직업관이 있고, 고집이 있다. 그렇기 때문에 같이 협업을 하게 되면, 개발에 대한 배경지식이 없는 다른 직업군의 사람과 대화를 하면서 이해관계가 충돌하는 경우가 생긴다. 때로는 요구사항에 최대한 맞춰서 조율을 할 수도 있겠지만, 그것이 힘든 경우에는 상대를 설득하는 능력이 필요하다. 이러한 부분에 대해서 강사님께서 수업시간에 말씀해주셨다. 그래서 시간이 되면 추천해주신 설득의 심리학이라는 책을 읽어보려고 한다. clear를 사용해서 float를 해제시킬때에는 직계 부모 요소에서 해주는 것이 좋다.중첩된 형태의 레이아웃 구성의 내부에서 float를 사용해서 레이아웃 배치를 하는 경우, 어디에서 float를 해제시켜주는 것이 좋은 것인지 고민이었는데, 강사님이 이번 과제를 풀이해주시면서 float로 레이아웃을 배치한 요소들의 바로 위 직계 부모 요소에서 clear를 사용해서 float를 해제시켜주는 것이 좋다고 하셨다. text-align &amp; vertical-align보통 text-align과 vertical-align을 서로 반대의 개념으로 알고 있지만 그렇지 않다. 두 번째 노트필기의 우측 하단부를 보면 그림을 그려놓았는데, text-align은 좌측에서 우측방향으로 inline 요소들을 배치시키는 반면에, vertical-align의 경우에는 개별 inline요소를 배치할때 사용된다. 검색 폼 레이아웃검색 폼 레이아웃 관련 wireframe은 아래 첨부한 3번째 노트를 확인하도록 하자. 레이아웃을 잡을때 첫 번째 우선적으로 해야되는 것은 레이아웃에 표시되는 정보의 논리적 흐름을 파악하는 것이다. HTML5에 다양한 기능들의 태그가 추가되었다.HTML5에서 다양한 기능들의 태그가 추가되었다. 예를들어 날짜정보를 입력할때 사용되는 &lt;time&gt;태그가 있다. 날짜 정보를 넣을때 단순히 텍스트로써 처리하기보다는 해당 정보가 날짜정보를 가르키는 정보라는 것을 기계에서 알려줄 수 있도록 작성을 하는 것이 좋다. HTML을 디자인적 관점이 아닌 기계의 관점에서 컨텐츠의 흐름을 중점으로 파악해야 한다.이번 수업에서 가장 핵심이 되는 내용이 아니었나 싶다. 이번 수업에서 공지사항 및 자료실 부분의 레이아웃을 작성하고 강사님께서 돌아다니시면서 개개인마다 간단한 피드백을 주셨었다. 그 피드백 중에서 가장 기억에 남는 내용이 레이아웃을 디자인적인 관점에서 접근하지 말아라라는 말씀이었다. 그렇다. 나는 나름 접근성을 고려해서 디자인해봐야겠다고 생각은 했지만 막상 레이아웃을 디자인하다보니 어느새 화면에 보여지는 실제 레이아웃의 모양에 치중해서 레이아웃을 작성하고 있었다.이제는 기계의 관점에서 컨텐츠의 논리적 흐름에 중점을 맞춰 레이아웃을 바라보는 습관을 들여야 겠다고 느꼈다.하지만 기계적인 관점에서 레이아웃을 짜게 되면, 사용성에 문제가 생긴다. 구체적인 이해를 위해서 아래 첨부한 노트중에 6번째 노트필기를 참고하자. 필기한 내용에는 공지사항 및 자료실 레이아웃에 대한 1안, 2안, 3안이 있다.1안은 공지사항과 자료실을 &lt;section&gt;태그로 각 각 분류하였다. 그리고 2안은 리스트 태그로 공지사항과 자료실에 표시되는 내용들을 각 각의 하위 메뉴로써 분류하여 작성을 하였다. 3안은 탭에 대한 메뉴는 리스트 태그로 처리를 하고, 각 컨텐츠 내용(공지사항 및 자료실)은 별도의 section으로 분류하여 작성하였다. 이러한 웹의 접근성에 대한 연구를 하는 모임에서 운영하는 GitHub가 있는데, 아래 주소를 첨부한다. ARIA 레포지토리에 다양한 화면의 요소들을 어떤식으로 접근성 좋게 처리할 수 있는지 다양한 예제코드들을 확인할 수 있다. 과거에 작성된 코드이기 때문에 js코드가 jQuery 코드로 작성이 되어있지만, 코드의 내용을 보고 vanillaJS로 작성해서 사용해보면 좋을 것 같다. https://github.com/niawa/ARIA 수업시간에 실습으로 제공되었던 페이지에 Tab 관련 내용이 있었기 때문에 01.tab-ui의 예제코드를 살펴보자.→ 01.tab-ui/index.html 파일의 13번째 라인에서 시작하는 column-01 div태그(WAI-ARIA 미 적용 부분)와 column-02 div태그(WAI-ARIA 적용 부분)의 코드를 살펴보자. li 태그로 탭 메뉴를 구성한 경우에는 해당 리스트의 역할이 “tab”이라는 것을 명시(ARIA의 role 속성 사용)해주고, 각 탭이 어떤 부분과 연결되어 있는지 aria-controls라는 속성값을 이용해서 section태그의 id로 mapping시켜주면 된다. 12345678910111213141516171819202122&lt;!-- Tab --&gt;&lt;ul class=&quot;tab-list&quot; role=&quot;tablist&quot;&gt; &lt;li id=&quot;tab1&quot; role=&quot;tab&quot; aria-controls=&quot;section1&quot; aria-selected=&quot;true&quot; tabindex=&quot;0&quot; &gt; HTML &lt;/li&gt; ........ &lt;!-- Tab contents --&gt; &lt;div class=&quot;tab-contents&quot;&gt; &lt;section id=&quot;section1&quot; role=&quot;tabpanel&quot; aria-labelledby=&quot;tab1&quot;&gt; &lt;p&gt; HTML은 하이퍼텍스트 마크업 언어(HyperText Markup Language) 라는 의미의 웹 페이지를 위한 마크업 언어이다. &lt;/p&gt; &lt;/section&gt; &lt;/div&gt;&lt;/ul&gt; 위의 코드를 살펴보면서, 대한항공과 아시아나 항공의 홈페이지에서 탭으로 메뉴의 이동을 살펴보고, 어떠한 차이점이 있는지 관찰해보도록 하자. 강사님이 제공해주신 Figma 디자인 시안으로 접근성을 고려한 레이아웃을 디자인해보자.이번에 3.1절로 인해 주말 포함해서 3일 연속으로 쉬게 되었다. 그래서 강사님께서 반응형 레이아웃을 연습할 수 있는 Figma 디자인 시안을 주셨다. 디자인 시안은 네이버 로그인 화면으로, 개인 GitHub에 repository를 만들어서 과제를 하면, 강사님께서 개인적으로 피드백을 주신다고 하셨다.간단해보이는 UI이지만, 다각도로 생각하고 구상해서 과제를 진행해봐야겠다. Figma의 사용법에 대한 구체적인 사용법은 강사님이 추천해주신 유튜브 채널의 영상을 보고 개별적으로 공부해보도록 하자.(개발자의 입장에서 Figma 사용법을 익히는 것을 목표로 한다) https://www.youtube.com/channel/UCTZRL_OwTjiXYoTjwWg4bww vector 이미지(svg)를 사용해서 화면에 이미지를 배치하면 화면의 크기가 달라져도 이미지의 해상도를 유지해준다. 레이아웃을 작성할때에는 컨텐츠의 논리적 흐름을 나타내는 구조와 레이아웃 배치구조도를 나눠서 그려보도록 하자.아래에 첨부한 노트의 마지막 페이지를 살펴보도록 하자. 왼쪽 하단에는 화면에 배치할 각 태그를 컨텐츠의 논리적 순서에 맞게 작성을 하였다. 이 논리적 순서는 기계의 관점에서 화면의 컨텐츠를 탐색할때 이렇게 탐색할 것이라고 가정한 순서이다. 그래서 공지사항 및 자료실 컨텐츠에서 공지사항 관련 컨텐츠를 스캔할때에는 공지사항 탭 메뉴→항목 리스트→더보기 버튼 순으로 스캔되기 때문에 이런식으로 각 탭 항목을 배치하였다.단순히 화면에 보이는 레이아웃만 본다면 이러한 배치는 생각할 수 없지만, 기계적 관점에서 컨텐츠를 바라본다면 이러한 화면 배치가 나온다. 이렇게 작성을 해준다면, 개발측면에서 어려워진다. 하지만 이러한 컨텐츠적 측면에서 논리적 순서를 고려해서 개발을 하게 된다면, 접근성 측면에서 매우 좋은 레이아웃이 된다.그리고 오른쪽 상단에는 각 각의 화면 구성요소를 어떤 position 방식으로 어떻게 배치를 할지 작성을 해주었다.탭 메뉴의 position을 absolute로 주고 top, left값만으로 위치를 잡게 되면, 컨텐츠의 논리적 흐름에서 벗어나는 부분이 되기 때문에 왼쪽 하단의 그림에서 기존에 ul태그의 위에 배치했던 탭 메뉴관련 요소를 X로 표기하였다.그리고 + 더보기 부분도 마찬가지로 position을 absolute로 하고 top, right 값만으로 화면에 배치하였기 때문에 이 부분도 논리적 흐름에서 벗어나기 때문에 탭 메뉴 관련 요소와 같이 X로 표기하였다.이렇게 작성을 하고, 컨텐츠의 논리적 순서도를 보면 딱 공지사항 관련 컨텐츠 항목 리스트와 자료실 관련 컨텐츠 항목 리스트만 논리적 순서도에 남게 된다. 이렇게 된 상태에서 선택된 탭 메뉴에 따라 각 각의 항목을 display: block;, display: none;로 처리를 해주게 되면, screen reader는 화면에 보여지는 컨텐츠의 리스트 항목을 읽어주게 된다.공지사항과 자료실 컨텐츠 항목 리스트와 더보기 버튼의 경우, 서로 겹쳐져 있기 때문에 보여지는 컨텐츠의 내용에 따라 display 속성을 사용해서 컨트롤하면 된다.이렇게 작성을 해주고 앞서 살펴본 01.tab-ui의 예제코드에서와 같이 탭 메뉴 리스트와 각 각의 컨텐츠 리스트를 ARIA 속성을 이용해서 mapping시켜주면 화면을 탐색할때 컨텐츠의 논리적 순서에 맞게 focus가 이동할 수 있게 된다. remind를 위해 수업시간에 필기했던 노트를 첨부한다.","link":"/2021/02/26/202102/210226-html_css_til/"},{"title":"210227 A Real Developer","text":"개발자 취업준비생이 말하는 좋은 개발자기존에 좋은 개발자라는 의미가 사용되는 기술 잘 쓰고, 꾸준한 자기개발을 통해 기술적으로 뒤쳐지지 않는 그런 개발자가 좋은 개발자라고 생각했다.그래서 좋은 개발자가 되기 위해 회사를 그만두고 3~4개월간 개인적으로 기술적인 부분에 집중해서 자기개발을 했었다. 그런데 매번 드는 생각이 과연 기술적인 부분이 완벽하다고 좋은 개발자인가? 라는 의문이 생겼었다. 그런데 한 달 전부터 사설기관에서 파이썬과 HTML/CSS 수업을 들으면서 좋은 개발자라는 의미에 대해서 좀 더 확실해지고 있다는 느낌을 받았다. 파이썬 수업을 들었을때에는 그냥 코드를 잘 치는 사람이 아닌, 논리적 생각을 잘하고 그 생각을 얼마나 코드에 잘 녹여낼 줄 아느냐가 좋은 개발자의 조건 중에 하나라는 것을 배웠고, HTML/CSS 수업을 통해서는 웹 표준과 접근성을 고려해서 비장애인이든 아니든 사람의 상황에 관계없이 모두 평등하게 이용할 수 있는 어플리케이션을 개발할 줄 아는 사람이 좋은 개발자의 조건 중에 하나라는 것을 배웠다. HTML/CSS 수업을 들으면서 가장 충격적이었던 것은 내가 기존에 알던 HTML이 아니었다는 것이다. 물론 HTML을 이렇게 제대로 배운 것은 처음이었지만 단순 마크업 언어로써의 HTML이 아닌, 컨텐츠의 논리적 흐름과 접근성 및 사용성을 고려해서 사용해야 되는 레이아웃 설계에 필요한 마크업 언어였던 것이다.그 전에는 요구사항에 맞는 레이아웃이 화면에 나오면 괜찮다고 생각했는데, 좋은 레이아웃은 다각도에서 확인하고 검증해야 되는 부분이 많았다. 표준을 따르면 접근성이 좋은 양질의 어플리케이션이 개발된다. 이러한 양질의 어플리케이션을 개발하면서 개발자 자신으로서도 개발실력이 크게 성장된다.그래서 다시금 단순하게 기술적인 부분에 치중해서 생각했던 과거의 나는 과거에 두고, 이제는 기술적인 것은 기본으로 더 나아가 표준과 접근성이라는 다각도의 측면을 고려해서 개발을 하는 그런 참된 개발자가 되어야겠다고 느꼈다.이러한 깨달음을 주신 최우영 강사님과 김데레사 강사님께 감사하다고 말씀드리고 싶다.","link":"/2021/02/27/202102/210227-web_standards_and_accessibility_tdd/"},{"title":"반응형 네이버 로그인 페이지 구현 및 웹 표준 &amp; 접근성에 기반한 취약점 분석 및 개선안 적용","text":"프로젝트 Repository : https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility Project Status 프로젝트 진행기간 : 🗓️ 2021.02.28(Sun) ~ 2021.03.01(Mon) (2일간) 🗓️ 2021.02.28(Sun) (1) Issue1) README 파일에 프로젝트 초기 Task 작성하기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/1 (2) Issue2) 네이버 로그인 페이지 레이아웃 설계하기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/2 (3) Issue3) 웹 표준 및 접근성에 기반한 기존 로그인 페이지의 취약점 분석 및 개선안 정리하기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/3 (4) Issue4) BEM 방식으로 element 클래스 이름작성하기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/4 (5) Issue10) Initial project setup → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/10 (6) Issue12) 네이버 로그인 페이지 기본 레이아웃 작성하기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/12 (7) Issue14) 작성한 기본 레이아웃에 스타일 적용시키기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/14 🗓️ 2021.03.01(Mon) (8) Issue16) 로그인 상태 유지 및 IP 보안 ON/OFF 체크박스 수정하기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/16 (9) Issue5) 개선안 3을 위한 CSS 이벤트 처리관련 조사 및 구현 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/5 (10) Issue19) 반응형 웹 페이지 구현하기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/19 (11) Issue21) Project Reflection 작성하기 → https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility/issues/21 레이아웃 설계Contents의 논리적 흐름을 나타내는 논리 구조도와 레이아웃 요소 배치도 작성 < 네이버 로그인 화면 레이아웃 설계 > 웹 표준 및 접근성에 기반한 취약점 분석 및 개선안 개선안 1) 현재 페이지의 header 부분이 native HTML5 element인 header 태그를 사용하지 않고 div태그에 id로 “header”를 지정하고 있다. 이런 경우에는 native HTML5 element인 header를 사용하거나 div태그내에 ARIA Landmark role로 “banner” 속성값을 지정해주는 것이 좋을 것 같다. https://www.w3.org/TR/wai-aria-practices/examples/landmarks/HTML5.html 개선안 2) 기존 네이버 로그인 페이지의 contents의 논리적 흐름을 살펴보면 하단의 표에서 가장 좌측(개선안 적용 전)과 같은 순서로 이동하는 것을 알 수 있다. 각 contents의 순서상의 개선안과 태그 속성상의 개선안을 생각해보았다. 생각해 본 개선안을 적용한 후의 결과는 하단의 표에서 가장 우측(개선안 적용 후)와 같은 결과로 변경된다. 개선안 3)하나의 input tag에는 하나의 의미있는 label을 mapping 시켜주도록 개선하자. https://www.w3.org/TR/WCAG20-TECHS/H44.html 현재 네이버의 로그인 화면에서 id와 password 입력란을 보면, &lt;label&gt; 태그에는 z-index값이 8, &lt;input&gt; 태그에는 z-index값이 9 로 지정되어 있다. 따라서 화면 배치상 &lt;label&gt; 태그는 항상 &lt;input&gt; 태그의 뒤에 위치하게 되므로 &lt;label&gt; 태그에 표시된 내용은 의미가 없다. 결국에 &lt;input&gt; 태그의 placeholder에 넣어준 텍스트만 &lt;input&gt; 태그에 표시되는 것이다. 이렇게 되면, 경우에 따라 스크린 리더가 input태그의 뒤에 숨겨진 label의 텍스트와 placeholder 텍스트를 연달아서 두 번 읽어주게 된다. 조사를 해보니, JAWS와 NVDA와 같은 스크린 리더의 경우에는 placeholder를 읽어주지 않는다고 한다. 지금은 단순히 아이디와 패스워드를 입력하는 &lt;input&gt;태그를 다루고 있지만 만약에 구체적인 format의 데이터 입력을 요구하는 경우에는 반드시 시각장애인이 해당 입력 태그에 어떤 format의 데이터를 입력해줘야 하는지를 알려줘야 한다. 스크린 리더에 따라 읽어주기도 하고, 읽어주지 않기도 한다면 안정적으로 placeholder가 아닌 label에 입력해야하는 데이터에 대한 정보를 넣어주고, &lt;input&gt;태그의 위에 중첩시켜서 마치 placeholder와 같이 표시해준다음 입력 태그에 input cursor가 focus되는 경우, label 텍스트를 축소시켜서 입력태그의 상단에 표시될 수 있도록 CSS 효과를 주는 것도 좋은 방법인 것 같다. (구체적인 데이터의 입력 format은 경우에 따라 placeholder에 입력해서 처리해주면 된다) 개선안 적용 전) Label CSS 스타일 개선안 적용 전) input 태그 스타일 BEM 방식으로 element의 클래스 이름 작성하기 < BEM 방식으로 클래스 이름 정하기 > 네이버 로그인 페이지 기본 레이아웃 작성하기 기본 레이아웃에 스타일 적용하기 개선안 3) 하나의 input tag에는 하나의 의미있는 label을 mapping 시켜주도록 개선한다.입력 태그에 CSS 이벤트 구현 기존에는 label 태그를 input 태그의 뒤에 z-index 속성을 사용하여 보이지 않도록 처리하였다.개선한 부분은 input 태그의 placeholder에 표시한 hint 텍스트를 label 태그에 표시하도록 수정하였다. 표시된 label 태그는 input 태그가 focus 상태일 때와 valid 상태일 때, out-focusing 상태일 때 보다 글자 크기를 작게 하여 좌측 상단에 배치될 수 있도록 이벤트 처리하였다. 이렇게 되면 기존에는 경우에 따라 label의 텍스트와 placeholder의 텍스트 모두 스크린 리더가 읽어주는 경우가 생기는데 이를 해결할 수 있으며, 태그와 placeholder의 역할을 분리해야 되는 경우(입력해야 되는 데이터의 format을 알려줘야 하는 경우)를 해결할 수 있다고 생각한다. < 초기 로그인 페이지 상태 > < input 태그(아이디 입력)에 focus가 된 경우 > < input 태그(비밀번호 입력)에 focus가 된 경우 > < input 태그(아이디 & 비밀번호 입력)가 valid state인 경우 > 반응형 웹 페이지 구현(Responsive web page) < 데스크탑(너비가 769px이상) 대응 > < 모바일(너비가 768px이하) 대응 >","link":"/2021/03/01/202103/210301-Naver-login-page/"},{"title":"Baekjoon Online Judge 17609번  회문&#x2F;유사회문 문제 (다각도에서 문제 바라보기)","text":"백준 저지 17609번 회문/유사회문 문제 Pseudo code + Python code문제풀이 접근 방식 : 재귀호출 Pseudo code #1 RecursionError 본 코드 #1123456789101112131415161718192021222324252627shift_flag = Falsedef palindrome(input_str): global shift_flag if len(input_str) &lt;= 1: if shift_flag: return 1 return 0 if input_str[0] != input_str[-1]: if input_str[0] == input_str[-2]: shift_flag = True input_str = input_str[0:-1] elif input_str[1] == input_str[-1]: shift_flag = True input_str = input_str[1:] else: return 2 return palindrome(input_str[1:-1])n = int(input())for _ in range(n): input_str = input() print(palindrome(input_str)) 첫 번째 재귀함수를 사용한 풀이 방법에는 문제가 있다. 파이썬의 최대 재귀 깊이(maximum recursion depth)가 1000으로 정해져 있다.따라서 문제의 조건 주어지는 문자열의 길이는 3 이상 100,000 이하이고, 영문 알파벳 소문자로만 이루어져 있다.라는 조건에 있어서 최장 문자열의 길이에 대한 적절한 해결책이 되지 못한다. 최소한으로 재귀호출을 해서 문제를 해결해야 한다. Pseudo code #2 최소 재귀호출유사 재귀호출의 경우에만 재귀호출 형태로 함수를 호출하고 회문인 경우에는 조건처리가 끝나면 자동으로 회문인 것으로 간주한다. 본 코드 #2123456789101112131415161718192021222324252627282930# 유사회문 검사 functiondef inner_palindrome(input_str, start, end): while start &lt; end: if input_str[start] == input_str[end]: start += 1 end -= 1 else: return False return Truedef palindrome(input_str, start, end): while start &lt; end: if input_str[start] == input_str[end]: start += 1 end -= 1 else: fc = inner_palindrome(input_str, start, end-1) sc = inner_palindrome(input_str, start+1, end) # fc와 sc 둘 중에 하나를 만족한다면, 유사 회문이기 때문에 값 1을 반환해준다. if fc or sc: return 1 else: return 2 return 0test_case = int(input())for _ in range(test_case): input_str = input() print(palindrome(input_str, 0, len(input_str)-1))","link":"/2021/03/02/202103/210302-Algorithm_baekjoon_17609/"},{"title":"(Feedback)반응형 네이버 로그인 페이지 구현 및 웹 표준 &amp; 접근성에 기반한 취약점 분석 및 개선안 적용","text":"프로젝트 Repository : https://github.com/LeeHyungi0622/responsive-naver-login-page-with-web-standards-and-accessibility 이번 3.1절 연휴때 했던 과제에 대해서 어제(03월 02일) 데레사 강사님으로부터 피드백을 받았다. 어제 속이 별로 안좋아서 밥을 안먹고 있다가 마침 강사님이 계셔서 과제에 대한 피드백을 부탁드렸었는데 흔쾌히 해주시겠다고 하셔서 너무 감사했다. 쉬셔야 되는 시간인데 이렇게 소중한 시간을 내주셔서 피드백을 주신 강사님께 다시 한 번 감사의 말씀을 드리고 싶다. 자 그럼 어제 받은 피드백에 관한 내용을 정리해보자.과제를 수행하는 것도 중요하지만 받은 피드백을 통해 어느 부분이 개선이 필요한지 파악해서 내 것으로 만드는 것이 중요하다. (화면 설계 및 개발 관련 피드백) main 태그 내부의 form 태그main 태그에 바로 form 태그를 넣어줘도 되지만, form 태그를 section이나 div 태그로 한 번 감싸서 넣어주도록 하자. 그리고 main 태그에 heading 태그를 넣어서 주도록 하자. label 태그와 controller 태그의 배치경우에 따라 label 태그가 controller 태그의 다음에 위치하기도 하지만 보통은 label 태그 다음에 controller 태그가 위치하는 것이 일반적이다. 사용 할 태그를 선정할 때에는 구체적으로 어떤 action을 위한 부분인지 우선 분석하도록 하자.예를들어, 네이버 로그인 페이지에서 IP 보안 ON, OFF 표시 부준을 보면 표면적으로는 IP 보안은 ON, OFF를 위한 Label로 보일 수 있다. 하지만 IP 보안 부분은 클릭이 되는 부분이고, 별도의 페이지로 이동을 하게 하는 링크 객체이기 때문에 Label 태그가 아닌 anchor 태그로 처리해주는 것이 좋다. 이렇게 되면 ON/OFF(controller)는 mapping되는 label이 없기 때문에 이런 경우에는 ON/OFF(controller 태그)에 aria-label로 label 속성을 넣어서 처리해줘야 한다. tabindex에 대한 이벤트 처리키보드로 focus가 되지 않는 요소에 대해서 태그 속성에 tabindex=0을 넣어주게 되는데, 이런 경우에는 별도로 script단에서 해당 키 값에 대한 이벤트 처리를 해줘야 한다. BEM 방식으로 클래스명을 짓는 연습을 꾸준히 하자.처음에는 Block Element Modifier 단위로 구분하는 것이 어렵게 느껴지지만 나중에 Sass로 스타일링을 할때 BEM 방식은 매우 유용하다.Sass는 실제 태그의 구조와 같이 nested된 방식으로 스타일링을 할 수 있기 때문에 BEM 방식으로 클래스명을 작성하게 되면 각 Block, Element로 구분이 되어 있기 때문에 클래스명만 보더라도 레이아웃의 구조를 쉽게 파악할 수 있다. 개발을 할 때에는 mobile-first 방식으로 개발을 하도록 하자.이 부분에 대한 조언은 클래스의 구체성에 대한 이야기를 할 때 강사님께서 해주셨던 조언이었다.웹 페이지를 개발할때 모바일 개발을 우선 고려해서 개발을 진행하게 되면 나중에 데스크탑 기반의 웹 개발과의 구체성 문제를 사전에 예방할 수 있다. (접근성 관련 피드백) JAWS/NVDA 스크린 리더가 무조건 placeholder를 읽어주지 않는 것은 아니다.이는 브라우저의 조합에 따라 읽어주기도 하고, 읽어주지 않기도 한다. 스크린 리더와 브라우저의 궁합에 대해서는 접근성 연구소 유튜브 채널에 관련 영상이 있다.(아래 첨부)브라우저와 스크린 리더의 궁합 관련 영상 : https://youtu.be/ja0RalXx_VA 사용자 브라우저와 스크린 리더의 궁합국외 접근성 연구/조사 관련 기관 홈페이지: https://webaim.org/projects/practitionersurvey2/접근성 연구소 유튜브 채널의 영상을 통해 배운 내용에 대해서 간단하게 정리를 해보자. 브라우저 점유율Chrome &gt; Firefox &gt; IE &gt; Safari 사용자 브라우저의 점유율은 장애인과 비 장애인 모두 비슷한 브라우저 점유율을 보여주고 있다. 스크린 리더 점유율NVDA &gt; JAWS &gt; VoiceOver &gt; Narrow NVDA는 무료 스크린 리더이면서 사용자가 사용하기 편한 UI를 가지고 있기 때문에 기존 독과점을 했던 JAWS 스크린 리더의 1위 자리를 차지하였다.그 외 Window에서 무료로 제공해주는 Narrow 스크린 리더는 독자적인 특별한 기능을 제공해주기 때문에 사용자가 증가되고 있는 추세이다. 스크린 리더 사용자의 웹 브라우저와 스크린 리더의 조합스크린 리더와 웹 브라우저의 궁합에 따라 호환성을 극대화 시킬 수 있다. 특정 스크린 리더와 웹 브라우저가 만나면 호환성을 극대화 시킬 수 있다. 이 호환성이 극대화되면 스크린 리더가 브라우저 상의 콘텐츠를 잘 해석해주고, 인지해줘서 스크린 리더의 사용자가 빠르게 이해할 수 있도록 도와준다. 스크린 리더의 명세는 W3C 만큼 빠르게 업데이트 되는 구조가 아니기 때문에 이러한 이유로 특정 스크린 리더에 대한 모든 브라우저의 호환성이 보장되지 않는다. 2019.09 기준 21.4% JAWS W/Chrome 19.6% NVDA W/Firefox 18.0% NVDA W/Chrome 11.5% JAWS W/IE 9.1% VoiceOver W/Safari 유효성 검사에 따른 화면의 에러 메시지 처리(feat. aria-live)아이디와 비밀번호의 입력과 관련한 화면의 에러 메시지 표시 부분에는 태그 속성에 aria-live=&quot;assertive를 넣어주도록 하자. 그러면 에러 메시지가 보여졌을때 스크린 리더가 해당 에러 메시지를 읽어주게 된다.aria-live의 속성값으로는 다른 옵션들도 있기 때문에 아래 공식 Reference를 참고하도록 하자. https://developer.mozilla.org/ko/docs/Web/Accessibility/ARIA/ARIA_Live_Regions Light house 명도/채도에 대한 문제는 디자이너와 협업하도록 한다.디자인 시안대로 디자인을 했는데 접근성 검사(Accessibility check)에서 명도/채도 상의 문제가 발견되었다면, 이 부분에 대해서는 디자이너와 협의를 통해 조율을 해야 한다.","link":"/2021/03/03/202103/210303-Naver-login-assignment-feedback/"},{"title":"210302 HTML&#x2F;CSS TIL - 레이아웃 구성 순서, 태그 배치도를 그릴때는 태그의 특성이 보이게 그리기, 요소배치는 최대한 논리적 순서를 유지한 상태에서 하기, figure 태그, anchor 태그의 대체 텍스트, :root{}를 활용한 스타일 속성의 변수선언(재사용성), CSS 전처리기의 종류 및 장&#x2F;단점, 실무에서의 스타일링 방법, positioning에 따른 element의 성격 변화, rgba()와 hsla()의 사용법, 1px보다 얇은 디자인, 스프라이트 기법 (CSS Sprites), aria-label과 IR 기법","text":"HTML/CSS 여섯 번째 수업오늘은 HTML/CSS 여섯 번째 수업날이었다. 오늘도 많은 유익한 내용들을 배울 수 있던 날이었다. 이 모든 것들을 내 것으로 만들기 위해 끊임없이 노력하자. 레이아웃 구성할 때는 콘텐츠의 논리적 순서 흐름을 파악하고 나서 구체적인 태그의 배치(시맨틱 마크업)를 고려한다.이 내용에 대해서는 HTML/CSS 수업 초반에 강사님께서 강조해주셨던 내용이다. 지금은 개인적으로 레이아웃 설계 연습을 할 때도 항상 되뇌이면서 연습을 하고 있다. 오늘 이 부분에 대해서 강사께서 어쩌면 실무에서 일을 하면서 이러한 콘텐츠의 논리적 순서를 고려하지 않고 레이아웃을 단순 디자인적 관점에서 보게 될 수도 있고, 상황에 따라서는 타협을 해야 될 때가 온다고 하셨다. 이럴때에는 최소한의 접근성을 고려해야 되는 요소만은 지켜서 레이아웃을 설계하면 된다고 하셨다. 콘텐츠의 논리적 순서 → 시맨틱 마크업 → Class 네이밍 구체적인 태그를 배치할 때에는 Block element는 길게, inline element는 짧게 그려서 각 태그의 속성을 가시적으로 구분할 수 있도록 하자.태그의 배치도를 그릴 때 앞에서 명시한대로 Block element와 inline element를 구분해서 그려주게 되면, 가시적으로 각 element의 특성을 구분해서 화면 배치를 할때 처리해야 될 스타일링 요소도 같이 고려를 할 수 있다. 화면의 요소를 배치할때 논리적 순서를 유지할 수 있는 부분은 최대한 유지하고 배치하도록 하자.첫 번째 노트를 살펴보자. article 부분을 보면 Thumbnail Image와 title, date, content를 배치해야 되는 레이아웃이다. 처음 이 구조를 보았을 때에 전체적인 article의 position 속성을 relative로 처리하고 내부의 각 요소들의 position을 absolute로 처리해서 요소 배치를 하려고 하였다.하지만, Thumbnail Image를 제외한 나머지 부분을 보면 기존에 작성한 태그들의 논리적 흐름을 유지한다.따라서 article 요소의 position 속성은 relative로 주고, article 내의 요소들을 배치할때에는 Thumbnail Image만 position을 absolute로 하고, top, left 속성을 주어 요소 배치를 하면 쉽게 배치를 할 수 있다. 그리고 추가적으로 늘어날 가능성이 있는 요소의 경우, position 속성을 사용해서 화면에 배치하지 않도록 하는 것이 좋다. 예를 들어 콘텐츠의 내용은 얼마든지 늘어날 가능성이 있는 부분이기 때문에 이 부분은 되도록이면 position 속성을 지정해서 화면에 배치하지 않는 것이 좋다. figure 태그를 사용해서 이미지와 이미지 캡션을 배치하자.이미지와 이미지 캡션을 배치할 때에는 별도의 div태그를 사용해서 wrapping해주지 말고(이미지의 크기를 손쉽게 조정하기 위한 목적으로 wrapping해준다) figure 태그를 사용해서 이미지와 이미지 캡션을 배치해주도록 하자.이 figure 태그를 사용하게 되면 리스트 형태로 출력되는 기사 리스트 콘텐츠의 각 각의 아이템을 손쉽게 구현할 수 있다.figcaption을 사용해서 이미지 캡션을 넣어주게 되면, img태그의 대체 텍스트(alt) 속성값을 생략할 수 있다. 하지만, 대체 텍스트를 생략하게 되면, SEO 검색률이 떨어진다. anchor 태그의 label은 아래 두 가지 경우로 대체할 수 있다. anchor 태그의 title 속성 별도의 h2 태그에 label을 작성해주고, id 속성을 준 뒤에 anchor 태그의 속성에 aria-labelledby=[h2의 id] 를 넣어서 처리해준다. Title 속성과 h2로 별도의 label을 mapping해주게 되면 필요이상의 선언으로 보여지지만, 부족한 정보보다는 필요이상의 정보가 낫다고 생각하는 개발자도 있고, 그보다 좀 부족해도 괜찮다는 생각을 가진 개발자도 있다. aria-labelledby 속성 활용하기만약 첨부된 파일 리스트가 있다고 가정했을때 첨부 파일 링크가 이미지로 처리되어 있고, 구체적으로 어떤 타입의 파일이 첩부되어 있는지 모를 경우, 첨부된 파일의 타이틀에 할당된 id 속성(동적할당-파일타입)을 첨부된 파일 다운로드 링크 태그(anchor)의 aria-labelledby속성으로 넣어주면 해당 태그가 어떤 종류의 파일인지 알 수 있다. (두 번째 노트 참고) CSS의 :root{ }로 스타일 속성 값을 변수로 선언해서 재 사용성을 높일 수 있다. 스타일 속성 값을 변수로 선언하기12345:root { --primary-color: #ce4827; --secondary-color: #eaac2e; --silver: #aaa;} 선언한 스타일 속성 변수를 호출해서 사용하기12345.news-heading { margin: 0; font-size: 1.5rem; color: var(--primary-color);} CaniUse 사이트에서 각 브라우저 별로 버전 호환성 체크해보기 (var())https://caniuse.com/?search=var() Sass와 Less 그 외의 CSS 전처리기대표적인 CSS 전처리기에는 Sass와 Less가 있다. Sass의 경우에는 Less에 비해서 초기 셋팅해야 되는 부분이 많지만 가장 대중적으로 사용되는 라이브러리이다. 간단하게 전처리기들을 비교해서 정리해보자. Sass Less Stylus - CSS 전처리 라이브러리로 가장 오래되었으며, 대중적으로 사용되고 있다. - 막강한 내장기능을 가지고 있으며 Compass와 병용해서 사용하면 리소스 경로를 참조하는 것이 가능해서 특정 폴더 내의 이미지의 크기와 위치정보를 참조할 수도 있다. - 브라우저에 내장된 JS Interpreter로만 Compile이 가능하기 때문에 dependency로부터 자유롭게 사용될 수 있따. - Sass 다음으로 활발하게 사용되고 있으며, 관련 library나 mixins 구현과 관련된 내용들을 쉽게 찾아 볼 수 있다. - 상대적으로 프로그래밍 언어의 특징을 많이 포함하고 있는 라이브러리이다. - 첫 번째 이유 덕분에 CSS 프로퍼티 내에서 연산자나 함수, 루프 등을 비교적 자유롭게 사용할 수 있다. - 하지만 문법이 혼재되어 있어서 처음 전처리기를 사용하는 사람에게는 상대적으로 어렵게 느껴질 수 있다. 실무에서의 태그 스타일링 방법직접 특정 태그에 스타일링을 해줘도 되지만, 반복적으로 적용되는 스타일이라면, 별도의 스타일 관련 클래스로 스타일을 지정해놓고 재사용하는 편이 좋다. 예를들어, 박스에 그림자 효과를 주는 경우나 여러 컴포넌트에서 똑같은 그라데이션 효과를 주는 경우, 일회성이 아닌 다회성으로 스타일링을 해야 될 때에는 해당 스타일을 정의해서 재사용성을 높이도록 한다. positioning에 따른 element의 성격 변화일반적으로 inline element와 block element를 display 속성으로 그 본래 가지고 있는 성격을 바꿔 줄 수 있다는 것은 알고 있지만, position 에 따라서도 element가 본래 가지고 있는 성격을 바꿔줄 수도 있다.우선 inline-element의 position 속성값을 absolute로 주게 되면, 해당 요소은 기존의 콘텐츠의 논리적 흐름에서 벗어나게 되면서 block element과 같은 성격을 갖게 된다. 기존의 inline element의 경우에는 width 속성과 상하 padding 속성을 갖지 못하지만, 이렇게 absolute로 positioning된 inline element의 경우에는 block element와 같이 기존에 갖지 못했던 너비와 상하 패딩 속성을 지정해 줄 수 있기 된다. (display: block 지정과 동일 효과) 두 번째 예시로 inline element에 position으로 relative 속성값을 주는 경우이다. 이러한 경우에는 기존의 inline element가 inline-block의 성격을 갖게 된다. (inline element와 inline-block의 차이에 대해서는 별도의 category로 분류해서 정리해두자) DOM + CSSOM Render Tree페이지에 표시된 요소들이 rendering되기 위해서는 DOM요소와 CSSOM 요소가 준비되어야 한다. (구체적인 내용에 대해서는 공부해서 별도의 포스팅으로 정리를 해두도록 하자.) 기존에 Presentation 특성의 태그가 HTML5부터 구체적인 의미를 갖게 되었다.&lt;i&gt; 태그의 경우 기존에는 presentation 태그의 특징으로써 기울림체를 표현했었지만, HTML5에서부터 선박에서의 사용이나 여러 구체적인 의미를 가진 태그로써 추가 정의되었다. 이외에도 &lt;b&gt;, &lt;em&gt; 등 presentation 성격의 태그들이 구체적인 의미를 담은 시맨틱 태그로써 재 정의되었다. rgba()와 hsla()의 사용법1234567891011121314/* rgba()를 사용해서 빨간 색상 표현 *//* rgba([Red],[Green],[Blue],[Alpha]) *//* 빨간색에 0.3 alpha 값 적용 */.title { background-color: rgba(255, 0, 0, 0.3);}/* hsla()를 사용해서 밝은 초록색 표현 *//* hsl(hue, saturation, lightness) *//* hue는 color wheel에서 선택한 색상의 degree값으로, 범위는 0 부터 360까지 이다. 0(또는 360)은 빨간색이며, 120은 초록색, 240은 파란색이다.*//* saturation은 채도 값으로 % 단위로 표현한다. *//* lightness는 명도 값으로 채도 값과 같이 % 단위로 표현한다. */.title { background-color: hsl(120, 100%, 75%);} 1px보다 얇은 디자인 (이듬의 브런치 글귀)시간날때 이듬의 브런치에서 1px보다 얇은 디자인에 대한 글을 읽어보도록 하자.실무에서 디자이너가 디자인 시안상에 1px보다 작은 0.5px로 디자인을 해서 넘겨주게 되는 경우가 있는데, 이러한 경우 원래 0.5px이라는 개념은 없지만 어떻게 트릭을 써서 1보다 작은 소수점 단위의 px로 만들 수 있는지에 대한 글이다. → https://brunch.co.kr/@euid/6 시맨틱적인 이름이 아닌 디자인적인 이름으로 클래스 이름을 작성해서 공통 CSS 스타일을 모듈화 하기아래와 같이 반복적으로 사용할 스타일을 별도의 디자인적인 이름으로 클래스 이름을 지정해서 작성해준 다음에 스타일을 적용할 태그의 클래스로 넣어서 사용하도록 한다. style.css 123.point-color { color: #ce4827;} index.html 1&lt;span class=&quot;point-color&quot;&gt;사이트&lt;/span&gt; 스프라이트 기법 (CSS Sprites)화면에 보여지는 아이콘 이미지들을 개별적으로 서버쪽에 요청을 해서 화면에 뿌려주게 되면 서버쪽으로 수없이 Request를 보내줘야 한다. 이렇게 되면 페이지를 렌더링할때 퍼포먼스상 좋지 않다.이러한 퍼포먼스 측면의 문제를 해결하기 위해서 사용되는 기법이 스프라이트 기법이다. 화면에서 보여지는 이미지들을 아래와 같이 통 이미지로 한 번에 서버로부터 받아 온 다음에 background-position을 사용해서 x, y축 기준으로 일부분 잘라내서 필요한 요소의 배경으로 처리해서 화면에 표시하는 기법이다.원래 게임 분야에서 사용되던 기법이었는데 웹에서 퍼포먼스상의 문제를 해결하기 위해서 사용되었다고 한다. JS에서 별도로 컨트롤할 element의 집합은 별도의 wrapper container로 묶어서 처리한다.별도의 wrapper container로 묶어서 처리하게 되면 나중에 해당 container의 id로 내부의 element에 접근하기가 용이해진다. img 태그에서의 대체 텍스트를 바라보는 시각 차이강사님께서 아시는 디자이너 지인 분의 아이패드 당첨 에피소드로 재미있게 이 대체 텍스트에 대해서 이야기를 해주셨다. 일반인의 시각에서는 배너광고를 통해 당첨 이벤트라는 것을 알 수 있지만, 시각 장애인의 입장에서는 배너광고의 이미지를 대체 텍스트를 통해 해당 배너 광고가 어떤 내용인지 알 수 있다.이 대체 텍스트를 바라보는 시각이 두 가지로 나뉘게 되는데, 첫 번째로 “대체 텍스트를 화면에 표시된 이미지 내의 텍스트 정보만으로 동일하게 담아서 처리를 하고, 부가 정보의 경우에는 title 속성에 담아준다.”라는 시각과 두 번째로 “대체 텍스트를 부가 정보의 개념으로 어떠한 목적으로 해당 이미지가 삽입되었는지에 대한 부가 정보를 이 대체 텍스트에 넣어 준다.” 라는 시각이 존재한다. 이는 개발자의 개별적인 시각에 따라 다르기 때문에 어떤 것이 맞고 틀린 것이 아니다. aria-label과 IR 기법으로 화면에 표시한 텍스트 숨김처리 하기IR 기법으로 화면에 표시한 대체 텍스트를 숨김처리하는 연습예제는 수업 실습 프로젝트의 ir.html 파일을 참고하고, 수업 노트는 6번째 노트 필기를 참고하도록 하자. 예전에는 aria-label이라는 속성으로 대체 텍스트를 주지 않고 별도의 태그에 대체 텍스트를 작성하고 이를 IR기법을 사용해서 가시적으로 보이지 않도록 처리를 하였다. IR기법의 종류(1) 대체 텍스트로 사용하는 태그를 indentation으로 일부러 overflow를 발생시킨 다음 overflow: hidden 처리를 한다.(2) 대체 텍스트로 사용되는 태그를 position: absolute로 이미지와 겹치도록 위치시킨 다음에 z-index값으로 태그의 뒷쪽으로 보이지 않도록 처리한다.(3) 가상 클래스를 사용해서 대체 텍스트로 처리할 태그를 작성해준다. remind를 위해 수업시간에 필기했던 노트를 첨부한다.","link":"/2021/03/02/202103/210302-html_css_til/"},{"title":"210303 HTML&#x2F;CSS TIL - 키워드 작성완료(세부내용 작성 예정)","text":"HTML/CSS 일곱번째 수업오늘은 HTML/CSS 일곱번째 수업날이었다. 오늘도 많은 유익한 내용들을 배울 수 있던 날이었다. 이번 HTML/CSS 수업이 끝난 다음에 회고록을 작성해보도록 하자.Animation 설계/디자인하기MDN/MSDN 공식 문서 찾아보는 습관 들이기MDN/MSDN 공식 문서 찾아보는 습관 들이기가상 클래스의 사용(:focus와 :hover) transition배경 이미지의 위치를 백분율로 지정하기li 항목에 숫자 넣어서 처리하기atomic CSS / Tailwind(Utility first framework)Slogan 영역 레이아웃 연습하기&lt;blockquote&gt;와 &lt;q&gt;태그의 사용inner/outer &lt;div&gt;태그를 사용해서 그림자 만들기anchor tag의 href 속성footer 영역 레이아웃 연습하기Grid를 사용해서 화면 요소 배치하기remind를 위해 수업시간에 필기했던 노트를 첨부한다.","link":"/2021/03/03/202103/210303-html_css_til/"},{"title":"210304 HTML&#x2F;CSS TIL - 키워드 &amp; 세부내용 작성중...","text":"HTML/CSS 여덟번째 수업오늘은 HTML/CSS 여덟번째 수업날이었다. 오늘도 새로운 것에 대한 배움에 즐거운 하루였다. 적응형 웹 디자인(AWD)과 반응형 웹 디자인(RWD)이번 수업의 시작은 반응형 웹 디자인을 실습하기 전에 반응형 웹과 관련된 이론적인 부분에 대해서 살펴보았다. 내용의 시작은 적응형과 반응형 웹에 대한 내용이었다.이전에 개인적으로 네이버 포털사이트의 도메인 주소가 모바일에서는 m.으로 시작되고 데스크탑의 웹 주소에서는 www.으로 시작되는 것에 대해서 강사님께 질문을 했던 적이 있는데 이는 네이버에서는 적응형으로 어플리케이션을 개발했기 때문이다. 적응형은 각 플랫폼에 맞게 개별화된 웹 어플리케이션을 만드는 것을 의미하고, 반응형의 경우에는 하나의 코드에서 스타일링만 다르게 적용해서 화면 사이즈에 따라 다르게 콘텐츠의 구조를 출력해주는 것을 말한다. 서버의 이원화앞서 언급한 네이버 포털사이트의 적응형 어플리케이션에 대해서 배워보았다. 그렇다면 batch 서버의 구조는 어떻게 되어있는지 궁금했다. 그래서 문득 같은 batch 서버를 공유하고 있지는 않을까 생각했는데 강사님께 여쭤보니 네이버는 mobile용 batch server와 데스크탑용 batch server가 이원화 되어 처리를 하고 있다고 설명해주셨다.그러고 보니 실제 모바일과 데스크탑 웹 브라우저가 서로 다르게 동작하는 것을 본 적이 있는데 이것이 바로 서버의 이원화로 인한 것이었다. AWD와 RWD의 장단점개발은 mobile-first 방식으로 구성하기개발을 할때에 작은 디바이스부터 대응하고 그 다음으로 큰 디바이스에 대해서 대응하도록 하자.실행의 측면에서 본다면 가장 화면에 렌더링 될 요소가 많은 페이지는 데스크탑용 레이아웃 페이지이다. 이 부분을 고려하더라도 처음에는 가장 가벼운 모바일 환경과 관련된 처리를하고 제일 마지막에 데스크탑 레이아웃과 관련된 처리를 해주면 된다. 관심있는 기업의 웹 페이지를 클론해보자.클론을 할 때에 단순 클론을 하지 말고, 기존 웹 페이지를 분석하고 개선점을 적용시켜보도록 하자. RWD Architecture (Target / Context = Result)반응형 웹 디자인은 화면의 viewport 크기에 따라서 요소들의 사이즈가 달라져야 되기 때문에 px/em, %, vw/vh와 같이 상대단위를 사용해서 요소의 너비(width)와 높이(height)의 사이즈를 정의해야 한다. px은 절대 단위가 아닌 상대단위다? (화면의 해상도에 따라 1px은 다르다. 구체적인 자료 찾아서 공부해보기) Media Quries viewport 크기에 따라 디자인을 다르게 적용 개발의 순서는 Mobile device → Tablet &amp; Desktop → Tablet device → Desktop device 순으로 고려를 해서 개발을 해야한다. Desktop device를 제일 마지막에 작성을 하는 이유는 Desktop device의 레이아웃이 가장 복잡하고 화면에 렌더링 되어야 하는 요소가 많기 때문이다. 개발을 할때에 Component 단위로 디자인을 해야 기기별로 레이아웃을 제대로 분리해서 개발을 할 수 있다) Retinasrcset과 sizesCanIuse 사이트 활용하기(Resources-poly pill)Picture 태그Device-Pixel-Ratio(SEO) Open graph/Twitter cardURL을 복사해서 카톡의 채팅방에 붙여넣기를 하면, 주소 이외에 Thumbnail 이미지가 같이 표시되는 경우가 있는데 Open graph와 Twitter card에 대한 처리를 head 태그에 작성해주었기 때문이다. 다음에 프로젝트를 할 때에 적용시켜보도록 하자. 이디야 카페 메인 페이지Native tag를 사용하지 않고 div 태그로 각 세션을 나눌 때에는 role=&quot;&quot; 속성을 넣어서 처리를 해주도록 하자.반응형 요소를 위한 스타일링 요소반응형 요소로 만들기 위해서는 최대 너비와 최대 높이에 대해서 아래와 같이 스타일링을 적용시켜줘야 한다.반복되는 스타일링의 경우에는 별도의 클래스로 정의해서 태그의 클래스 이름으로 작성해서 스타일을 적용시켜주면 된다. 1234.respons { max-width: 100%; max-height: auto;} title 속성은 장애인이 마우스를 화면의 element에 mouse over를 했을때에 읽어주는 부분이다.비트맵 이미지와 svg 이미지 적용의 차이요소의 전체화면 배치remind를 위해 수업시간에 필기했던 노트를 첨부한다.","link":"/2021/03/04/202103/210304-html_css_til/"},{"title":"Baekjoon Online Judge 10825번 국영수 문제","text":"백준 저지 10825번 국영수 문제 Pseudo code + Python code문제풀이 접근 방식 : 튜플과 리스트 자료형을 사용한 정렬 Pseudo code 본 코드 (통과/PyPy3 - 231312KB / 3372ms) (시간초과/Python3)시간초과가 발생하는 경우 PyPy3를 사용해서 코드를 테스트해보도록 하자.이 시점에서 간단하게 PyPy3에 대해서 살펴보자. 1234567891011n = int(input())student_list = []for _ in range(n): (name, korean, english, math) = input().split(' ') student_list.append((name, eval(korean), eval(english), eval(math)))sorted_list = sorted(student_list, key=lambda x: (-x[1], x[2], -x[3], x[0]))for student in sorted_list: print(student[0]) sys.stdin.readline() (통과/Python3 - 60320KB / 1820ms)이 시점에서 sys.stdin.readline()에 대해서 알아보자. 12345678910111213import sysn = int(sys.stdin.readline())student_list = []for _ in range(n): (name, korean, english, math) = sys.stdin.readline().split(' ') student_list.append((name, eval(korean), eval(english), eval(math)))sorted_list = sorted(student_list, key=lambda x: (-x[1], x[2], -x[3], x[0]))for student in sorted_list: print(student[0])","link":"/2021/03/05/202103/210305-Algorithm_baekjoon_10825/"},{"title":"210306 React TDD Practice Mini Project","text":"TDD방식으로 React 개발하기 위한 연습 프로젝트test의 필요성 documentation (문서화 작업)테스트 코드는 코드가 어떻게 동작을 해야 되는지에 대한 정의를 하고 있다. consistency (지속성)Software developer들이 팀을 위한 최선의 practices와 conventions을 따를 수 있게 도와준다. Confidence (자신감)warm blanket 코드를 작성할때 좀 더 자신감 있게 코드를 작성할 수 있도록 도와준다. Productivity (생산성)테스트는 높은 품질의 코드를 빠르게 작성할 수 있도록 도와준다 React에서의 단위 테스트React환경에서 어떤 동작을 검증해야 하는지에 대해서 생각해보면, 아래 네 가지 동작에 대해서 검증이 필요하다. Testing basic component rendering Testing props PropTypes를 이용해서 유효성 검사 component간에 주고 받는 props를 체크하여 사전에 발생하는 버그를 예방할 수 있다. props관련하여 상세한 Type을 정의하여 코드의 가독성을 향상시킬 수 있다. Testing state Testing event handlers test의 종류 (Types of tests) End-to-end실제 사람이 개발된 어플리케이션에서 performing tasks를 수행한다.가장 복잡하고 느린 테스트 방식이다. (많이 수행되지 않는 테스트 방식이다) Integration하나의 컴포넌트가 다른 여러 컴포넌트들과 연관되어 있을때 테스트하는 방식이다. Unit개별 function과 component에 대한 기능 확인 테스트를 하는 방식이다. Static (type checking) (highly recommended)코드를 작성하는 동안 오타와 에러를 잡아내는 테스트 방식이다. Jest vs Enzyme vs React-Testing-Library Jest react-testing-library Enzyme 개발자가 사용자의 입장에 놓일 수 있게 해준다. (사용자가 해당 component와 어떻게 상호작용을 하는지 정의) React 개발자들이 React components를 테스트 할 수 있게 도와준다. Jest는 Facebook에서 Jasmine을 기반으로 개발한 JavaScript Test Framework이다. light library module로써, refactoring시에 테스트 코드가 깨지는 것을 방지하기 위해 구성 요소의 기능을 테스트하기 위한 코드 작성을 권고한다. Airbnb에서 개발한 React를 위한 JavaScript Testing utility 이다. state, props를 체크하지 않고, 사용자에게 보여지는 dom 객체를 이용하여 검증한다. render 메소드만 제공된다. mount, shallow와 같은 rendering method를 제공한다. - shallow : 자기 자신만 렌더링하므로 독립적으로 특정 component만 독립적으로 테스트할 수 있다. - mount : 관련 components를 모두 rendering한다. component간의 관계를 테스트할 수 있다. - render : component를 static한 html로 rendering한다. 2021/03/07 update Issue 1234567You want to write maintainable tests for your React components. As a part ofthis goal, you want your tests to avoid including implementation details of yourcomponents and rather focus on making your tests give you the confidence forwhich they are intended.As part of this, you want your testbase to be maintainable in the long run sorefactors of your components (changes to implementation but not functionality)don't break your tests and slow you and your team down. 반응 구성 요소에 대한 유지 관리 가능한 테스트를 작성하려고 합니다. 이 목표의 일부로, 구성요소의 구현 세부 정보를 포함하지 않고 구성요소가 의도한 대로 신뢰할 수 있도록 테스트에 초점을 맞추려고 합니다.그 일환으로 테스트 기반을 장기적으로 유지할 수 있기를 원하므로 구성 요소의 리팩터(구현에 대한 변경 사항이 있지만 기능이 아님)가 테스트를 중단하지 않고 사용자와 팀의 작업 속도를 저하시키지 않습니다. Solution 123456The RTL(React Testing Library) is a very lightweight solution for testing Reactcomponents. It provides light utility functions on top of react-dom andreact-dom/test-utils, in a way that encourages better testing practices. Itsprimary guiding principle is:The more your tests resemble the way your software is used, the more confidence they can give you. RTL(React Testing Library)는 React Components를 테스트하기 위한 매우 가벼운 솔루션이다. 이 라이브러리는 더 나은 테스트를 장려하는 방식으로 react-dom과 react-dom/test-utils위에서 가벼운 유틸리티 기능을 제공한다. 주요 원칙은 아래와 같다. 테스트가 소프트웨어의 사용방식과 비슷할수록 더 많은 자신감을 줄 수 있다. Jest?RTL은 Jest의 대체 도구가 아니다. RTL과 Jest는 테스트를 위해 서로 필요한 관계이다. modern React의 테스트에 있어서 개발자들은 테스트를 위해 Jest를 회피하지 않은 것이다. 그 이유는 JS를 위한 가장 인기 있는 테스트 프레임워크이기 때문이다.Jest는 Facebook에서 Jasmine을 기반으로 개발한 JavaScript Test Framework이다.페이스 북이 서포트하고 매우 큰 커뮤니티를 형성하고 있다. React Testing Library (RTL)Kent C.Dodds.이 만들어졌다.Airbnb Enzyme를 대체하는 테스트 라이브러리이다.Enzyme은 React 개발자들이 내부의 React component들을 테스트 할 수 있게 해준다. 반면에 RTL은 개발자가 사용자의 입장에 놓일 수 있게 한다. (End user가 해당 component와 어떻게 상호작용하는지에 대해서 정의한다.) 연습용 프로젝트 준비하기TypeScript 기반으로 react-testing 프로젝트 생성하기 1$ yarn create create-app react-testing --template typescript configuration, babel, webpack 등의 기본 설정을 모두 포함시켜서 프로젝트를 생성할 수 있다. 설치가 끝난 뒤에 package.json을 살펴보면 기본적을 jest와 관련된 test dependencies(dev)가 설치되어 있는 것을 확인할 수 있다. 테스트와 TypeScript 관련 dependency는 dev dependency로 분류한다. 간단히 설명하자면 dependencies는 Production 단계에서 어플리케이션에 의해 요구되는 패키지를 말하며, devDependencies는 오직 로컬에서의 개발과 테스트를 위해 필요한 패키지를 말한다. 생성된 프로젝트에서 간단히 아래의 명령으로 테스트를 시작할 수 있다. a옵션을 선택하면 현재 프로젝트 내에 있는 테스트 파일들을 실행시켜서 테스트를 진행한다. 1$ yarn test (유용한 Extension) VScode의 extension중에 Jest를 다운받으면 테스트 파일에서 테스트 결과를 가시적으로 확인할 수 있다.우선 React component가 아닌 간단한 function를 위한 테스트 코드를 작성해보도록 하자. example.test.ts TDD의 원리에 대해서 간단하게 배워보고, 테스트 코드를 작성할 때 지켜야 할 convention에 대해서 알아보자.앞에서 말했듯이 테스트 코드는 실제 코드가 어떻게 동작하는지에 대한 정의를 담은 문서이다.다른 개발자들이 내가 작성한 테스트 코드를 보았을때 무엇을 위한 테스트 코드인지 알기 쉽게 convention을 지켜서 작성을 하도록 하자. 실제 실무에서는 TDD가 정의하는 work flow를 따르지 않는 경우도 있다. 때로는 테스트 코드의 작성을 우선하지 않고, 실제 코드를 작성하고 테스트 코드를 작성하는 경우도 있다. implicit assertion아래의 코드를 보면, 기존에 작성해보았던 테스트 코드에는 있었던 expect() statement와 toBe 구문인 assertion 부분이 없다. 이는 암묵적인 방식으로 App component에 대한 assertion을 처리한 경우이다. app.test.tsx 123456789import React from 'react';import { render, screen } from '@testing-library/react';import App from './App';describe('When everything is ok', () =&gt; { test('should render the App component without crashing', () =&gt; { render(&lt;App /&gt;); });}); RTL의 screen 활용12345678910import React from 'react';import { render, screen } from '@testing-library/react';import App from './App';describe('When everything is ok', () =&gt; { test('should render the App component without crashing', () =&gt; { render(&lt;App /&gt;); screen.debug(); });}); &lt;App&gt; component를 렌더링 한 뒤에 screen.debug()를 해주게 되면 캡쳐와 같이 검사하고자 하는 component를 &lt;body&gt;와 &lt;div&gt; 태그로 감싼 형태로 출력됨을 알 수 있다. 이는 JSX가 아닌 HTML로써 출력이 된 것이다.","link":"/2021/03/06/202103/210306-React_testing_mini_project/"},{"title":"210305 HTML&#x2F;CSS TIL - 키워드 작성완료(세부내용 작성 예정)","text":"HTML/CSS 아홉번째 수업오늘은 HTML/CSS 아홉번째 수업날이었다. 오늘도 새로운 것에 대한 배움에 즐거운 하루였다. Sass Sass 공부할 때 참고하면 좋은 사이트 Sass 사용 및 설치 Sass의 문법 변수 선언 사용 Typo in variable name 변수명을 선언한 별도로 Sass 파일 작성하기 use/forward 사용하기module 단위로 개발을 할 때 @import 대신에 @use, @forward로 개발을 한다. Sass 폴더 구조 #{} 문자 보간 Sass의 nesting styles Sass스럽게 nesting style Sass script entry command Sass build Sass auto-load 배포시 사용되는 command Sass 사용시 주의사항 Sass의 내장함수 rgba() percentage() @each Mixins Mixins 선언과 호출 Mixins 매개변수 사용 가변인수 ($매개변수…) @content 그 외의 다양한 Sass의 단축 표기법 Pacel, WebpackGulp/Gruntcalc() 함수mixins와 function@iframe을 반응형으로 표현하기 위한 작업Grid system calculator수치연산을 통한 반응형 페이지 요소 배치실습내용 화면 flex styling 부분을 mixins로 선언해서 재사용하기 반응형 페이지에서 &lt;iframe&gt;태그 sizing가능하게 만들어보기 (Youtube, GoogleMap) (Mixins) Sprite Image Extract remind를 위해 수업시간에 필기했던 노트를 첨부한다.","link":"/2021/03/05/202103/210305-html_css_til/"},{"title":"210306 HTML&#x2F;CSS 회고록","text":"조금씩 천천히 나아가자이전에 2월달에 React 관련 사이드 프로젝트를 진행하던 도중에 HTML 시맨틱 태그 구조에 대해 의문이 생겼었다. 화면에는 제대로 랜더링 되어 예상했던 화면이 출력은 되었지만, 과연 이렇게 HTML 시맨틱 태그를 작성하는 것이 맞는 것인가하는 의문은 쉽게 머리속에서 사라지지 않았다. 그러던 중에 우연치 않게 현재 수강하고 있는 수업의 커리큘럼 상 뒷쪽에 배치되었던 HTML/CSS 수업이 앞쪽으로 당겨지면서 Python 수업 다음으로 HTML/CSS 수업을 듣게 되었다.정말 절묘한 타이밍에 수업을 듣게 되어 내가 기존에 화면의 레이아웃을 구성하면서 들었던 의문 중에 거의 대부분이 해소되었다.그래서 이제는 좀 더 정돈된 마음으로 프로젝트를 진행해 볼 수 있을 것 같다는 생각이 들었다. 아직 9일간의 수업내용이 전부 내 것이 되지는 않았지만, 배웠던 내용들 중에 내 것이 된 녀석들은 나의 사이드 프로젝트에 최대한 녹여서 작업을 해보려고 한다. 물론 당장 배웠던 내용을 100% 녹여낼 수는 없겠지만, 매일 매일 배웠던 내용을 염두해두면서 추가적인 공부를 통해 코드 Refactoring 작업을 하며 연습을 한다면, 나의 좋은 습관 중 하나로 자리잡을 거라고 확신한다. 내가 이번 HTML/CSS 수업을 들으면서 배웠던 내용중에 가장 큰 핵심적인 내용은 HTML 시맨틱 태그의 사용과 화면 설계 그리고 웹 표준과 접근성을 고려한 개발이었다.누가보면 “그냥 HTML 레이아웃 잡고, 웹 표준에 맞춰서 개발을 하는 내용을 배웠구나”라고 생각을 할 수도 있다. 나도 이 수업을 듣기 전에는 같은 생각이었지만, 이 수업을 듣고 난 후의 나의 생각은 완전히 달라졌다.제대로 된 HTML/CSS의 Native 태그 사용과 웹 표준과 접근성을 고려한 화면 설계는 생각보다 고려해야 될 부분이 많고 공수시간도 많이 든다.앞서 말한 내용을 연습하면서 느낀점은 자꾸 이전에 잘못 잡혀있던 습관으로 되돌아 가려고 한다는 것이다. 아마 내가 이전에 갖고 있던 올바르지 않은 HTML/CSS와 화면 설계에 대한 생각과 습관이 자리잡고 있는 것 같다. 지금은 이러한 습관을 고치기 위해 시간이 걸리더라고 최대한 노력해서 교정을 하려고 하고 있다.지금은 익숙하지 않아 시간이 오래 걸리지만, 노력해서 제대로 습관으로써 자리 잡는다면, 웹의 접근성과 표준 그리고 제대로 HTML/CSS를 고려해서 코드를 작성하지 않으면 그것에 대해 오히려 불편함을 느끼는 좋은 개발자가 될 수 있을 것이라고 생각한다. 뭔가 파편된 조각들이 하나 하나 제 자리를 찾아가는 이런 느낌으로 하루하루 성장해가는 느낌이 너무 좋다. 현재에 안주하지 않고 끊임없이 자기개발을 하는 개발자가 되도록 노력을 하자.","link":"/2021/03/06/202103/210306_html_css_memoirs/"},{"title":"210307 HTML&#x2F;CSS Grid (작성중...)","text":"-","link":"/2021/03/07/202103/210307_html_css_grid/"},{"title":"210308 React 좋은 테스트의 조건과 효과 그리고 테스트 시나리오 작성법","text":"좋은 테스트의 조건과 효과좋은 테스트란 무엇일까? 무작정 테스트 코드를 작성해보려고 했지만 막상 좋은 테스트란 무엇인지 알지 못했다. 우선 첫 번째, 테스트의 의도가 명확해야 한다. 코드의 가독성은 중요하다. 좋은 코드는 기계가 아닌 사람이 읽기 쉬워야 한다. 누군가 내가 작성한 테스트 코드를 보았을때 한 눈에 어떤 내용을 테스트하고 있는지 파악할 수 있어야 한다. 테스트 코드가 너무 장황해지거나 불필요하게 복잡해진다면 별도의 함수를 만들어 추상화시켜주는 것이 좋다. 두 번째, 좋은 테스트란 빠른 피드백을 받을 수 있으며, 개발 속도를 빠르게 할 수 있도록 해야 한다. 테스트 결과를 보기 위해 오랫동안 기다려야 하는 테스트는 개발 과정에서 좋지 않다. 세 번째, 내부 구현을 변경했을때, 테스트가 깨지지 않도록 해야한다. interface 기준으로 테스트를 작성하거나 테스트하는 부분이 다른 테스트 부분과 서로 종속적인 관계가 되어서는 안된다. 또한 너무 작게 테스트 단위를 쪼개서도 안된다. 작은 refactoring에도 작성한 테스트가 깨진다면 신뢰성 있는 테스트가 될 수 없으며 테스트를 수정하는 비용이 발생되어 코드 개선에 오히려 방해가 된다. 네 번째, 버그를 검출할 수 있어야 한다. 잘못된 코드를 검증하기 위해 작성한 테스트 코드가 버그를 잡지 못하지 못한다면 이는 치명적이다. 모의(Mock)객체를 너무 과하게 사용하게 되면 의존성이 있는 객체의 동작이 바뀌었을때 버그를 검출할 수 없게 된다. 따라서 테스트 코드를 작성할 때에는 명세는 구체적으로 작성하며, 모의 객체를 생성해서 테스트 하는 것은 최대한 지양하는 것이 좋다. 다섯 번째, 안정적인 테스트 결과가 나와야 한다. 오늘성공했던 테스트 코드가 내일은 실패하거나 특정 기기에 한정되서 테스트가 통과된다면 이는 신뢰할 수 없는 테스트 코드이다. 외부 환경의 영향을 최소화해서 언제 어디서든지 동일한 결과를 보장할 수 있도록 해야 한다. Test scenario BDD(Behavior Driven Development)를 기준으로 테스트 코드 작성 테스트 시나리오는 Given(Set up initial state) / When(Perform action(s)) / Then(Check end state) 세 가지 스탭으로 구상한다. 각 테스트는 다른 테스트에 의존적이지 않아야 한다. API, LocalStorage관련한 테스트는 Mock, Dummy를 사용하여 대체/작성한다. 모든 테스트 코드는 명확한 의도가 드러나게 작성해야 한다. 테스트 코드만 보더라도 테스트하고자 하는 Component가 어떤 기능을 담당하고 있는지 파악할 수 있어야 한다.","link":"/2021/03/08/202103/210308-React_testing_mini_project/"},{"title":"210308 Rubber Duck Team activity 준비","text":"본 포스팅은 월, 수, 금 아침 시간을 활용한 팀원들과의 activity를 위하여 정리한 개발과 관련된 용어입니다. runtime왜 1byte가 8bit인가?동적 타입 언어와 정적 타입 언어 IPO(Input Process Output) 모델Web site와 Web application의 차이 DOMECMA Script(ES)JavaScript Engine ASCII codeUnicode CRUDlegacy codehoisting변수var, let, const식별자선언 또는 선언문할당 또는 할당문초기화파싱","link":"/2021/03/08/202103/210308-javascript-rubber_duck/"},{"title":"210307 HTML&#x2F;CSS Flex (작성중...)","text":"우선 flex에 대해서 알아보기 전에 classic한 방식으로 block 요소를 배열하는 방법에 대해서 간단하게 알아보자. block과 inline-block왼쪽은 display 속성이 block인 요소의 배열을 나타낸다. 배열의 형태에서 유추해 볼 수 있듯이 block요소의 옆에는 어떠한 element도 올 수 없다. 그럼 div 요소의 display 속성을 inline-block으로 바꾸게 되면 어떻게 될까? 바로 오른쪽 캡쳐와 같이 일렬로 배열되는 것을 확인할 수 있다. inline-block으로 display 속성을 바꾸게 되면, 서로 옆으로 정렬된 형태로 box들이 정렬된다. 그렇다면 inline-block과 inline의 차이는 무엇일까? inline은 유동적이기 때문에 너비와 높이가 없는 요소 이다. 대표적인 inline 요소는 anchor 태그가 있다. 이러한 box 요소를 화면에 유연하게 배치할 수 있도록 등장한 modern한 방법이 바로 flex와 grid이다. 우선 간단히 flex에 대해서 알아보도록 하겠다. Flexbox의 규칙Flexbox의 첫 번째 규칙은 Don’t talk to children 이다. flex로 display 속성을 줘서 요소들을 배치하기 위해서는 child 요소를 아우르는 부모 요소의 display 속성에 display 속성 값을 넣어줘야 한다.예를들어 앞서 살펴본 div 박스의 화면구성에서 div박스들을 정렬하기 위해서는 모든 div 박스들의 부모 요소인 body에 display: flex 속성을 넣어줘야 한다. Main Axis=’row’(justify-content) and Cross Axis(align-items)flex-direction의 default 값은 row이다. 포스팅의 main 이미지를 살펴보면 flex구조에서의 main axis와 cross axis의 방향 구조에 대해서 이해할 수 있다. default 상태일 때에는 justify-content 속성 값을 사용해서 수평축을 기준으로 flex children의 위치를 정렬할 수 있다. 그리고 align-items 속성을 이용해서는 수직축을 기준으로 flex children의 위치를 정렬할 수 있다. 만약에 flex-direction을 통해서 default인 row 값을 column으로 바꾸게 되면, default 상태일 때와는 반대로 정렬의 기준이 적용된다. justify-content: space-around + align-items: center align-items: stretch(no height property)","link":"/2021/03/07/202103/210307_html_css_flex/"},{"title":"210309 TDD study for React","text":"요 몇일동안 아니 저번달도 몇 일정도 이 TDD라는 녀석에 대해서 검색도 해보고, 영상도 보고 나름 이것저것 시행착오를 겪어가며 공부를 하였다. 처음에는 요즘 트렌드라고 하니깐, 그리고 완벽하게는 아니지만, 개발 도중에 발생하는 대다수의 버그를 잡아낼 수도 있다는 이야기를 듣고 무작정 해보려고 테스트 코드 작성법부터 찾아보았다. 하지만 왠지 제대로 작성하고 있는건지 의문이 들었었다. 그래서 그때부터 이론적인 부분부터 실무에서 직접 적용해 본 실무자들이 올린 블로그 글, 유뷰트 관련 영상 등등 이 곳 저 곳에 흩어져 있는 알짜배기 내용들을 읽고 정리하고 연습하기를 반복하였다. 가장 도움이 되었던 블로그 글은 네이버 기술 블로그에 프론트엔드 개발자 분이 올려주신 TDD관련 내용이었다. 그 분의 글을 보면서 실무에서는 어떤 방식으로 테스트 코드를 작성하고 어떤 테스트가 좋은 테스트인지에 대해서도 자세하게 알 수 있었다. 너무 내용이 좋아서 노트 필기를 하며 읽어 보았는데, 나중에 한 번 더 보고 싶을 때 참고할 수 있게 아래에 필기내용을 첨부하였다. 그 결과 처음과 비교했을때, 이 TDD라는 개발 방법론과 조금은 친해진 것 같다. 그래서 이번에 진행하려고 하는 프론트엔드 사이드 프로젝트에 한 번 적용해보려고 한다. 아래에서 정리한 내용 중에서 가장 좋았던 내용은 실용적인 프론트엔드 테스트를 위한 전략에 관한 내용과 테스트 도구에 관한 내용 그리고 테스트를 시각적 요소의 테스트와 외부 요소(입력, 서버 통신)으로 인한 상태 변화에 대한 테스트 이 두 가지로 분류하여 설명해주신 부분 그리고 이전에 들었었지만 스냅샷 테스트의 정의와 한계점에 대해서 설명해주신 부분이 너무나도 유익했다. 이전 TDD관련 포스팅에서 React에서 단위 테스트를 할때 고려해야 할 요소에 대해서 잠깐 언급한 적이 있는데, 세 번째 노트 필기에는 고려해야 할 요소들 중에 하나인 Testing event handlers에 대해 테스트 코드를 손으로 작성해보았다. jest에서 제공해주는 mock function(jest.fn())과 react-testing-library에서 제공해주는 fireEvent를 사용해서 테스트 코드를 작성해보았는데 두 번째 보게 되는 부분이라 처음보다 많이 익숙해진 것 같다. 좀 더 효율적인 테스트 코드 작성을 연습해서, 효율적인 테스트 코드를 작성하지 않으면 오히려 불편함을 느끼는 개발자가 되도록 노력을 해야겠다.","link":"/2021/03/09/202103/210309-React-tdd_study/"},{"title":"210311 JavaScript assignment","text":"1. 변수 x가 10보다 크고 20보다 작을 때 변수 x를 출력하는 조건식을 완성하라12345var x = 15;if (x &gt; 10 &amp;&amp; x &lt; 20) { console.log(x);} 1-1. 조건문을 사용하지 않고 삼항연산자로 풀이해보기1console.log(x &gt; 10 &amp;&amp; x &lt; 20 ? x : ''); 1-2. 조건문을 사용하지 않고 단축평가를 활용해서 풀어보기1console.log(x &gt; 10 &amp;&amp; x &lt; 20 &amp;&amp; x); 2. for문을 사용하여 0부터 10미만의 정수 중에서 짝수만을 작은 수부터 출력하시오.123for (var i = 0; i &lt; 10; i += 2) { console.log(i);} 3. for문을 사용하여 0부터 10미만의 정수 중에서 짝수만을 작은 수부터 문자열로 출력하시오.3-1. 문자열 변수로 연결해서 결과값 출력하기12345var result = '';for (var i = 0; i &lt; 10; i += 2) { result += i;}console.log(result); 3-2. 이 2) 결과값을 리스트에 담고 join()을 사용해서 문자열로 합치기 (파이썬과 동일 기능)12345var resultList = [];for (var i = 0; i &lt; 10; i += 2) { resultList.push(i);}console.log(resultList.join('')); 4. for문을 사용하여 0부터 10미만의 정수 중에서 홀수만을 큰 수부터 출력하시오.12345for (var i = 9; i &gt;= 0; i -= 1) { if (i % 2 != 0) { console.log(i); }} 5. while문을 사용하여 0부터 10미만의 정수 중에서 짝수만을 작은 수부터 출력하시오.12345678conditionalValue = 0;while (conditionalValue &lt; 10) { if (conditionalValue % 2 === 0) { console.log(conditionalValue); } conditionalValue += 1;} 6. while문을 사용하여 0부터 10 미만의 정수 중에서 홀수만을 큰수부터 출력하시오.12345678conditionalValue = 10;while (conditionalValue &gt; 0) { if (conditionalValue % 2 !== 0) { console.log(conditionalValue); } conditionalValue -= 1;} 7. for 문을 사용하여 0부터 10미만의 정수의 합을 출력하시오.12345sum = 0;for (var i = 0; i &lt; 10; i++) { sum += i;}console.log(sum); 8. 1부터 20미만의 정수 중에서 2 또는 3의 배수가 아닌 수의 총합을 구하시오.1234567sum = 0;for (var i = 1; i &lt; 20; i += 1) { if (i % 2 !== 0 &amp;&amp; i % 3 != 0) { sum += i; }}console.log(sum); 9. 1부터 20미만의 정수 중에서 2 또는 3의 배수인 수의 총합을 구하시오.1234567sum = 0;for (var i = 1; i &lt; 20; i += 1) { if (i % 2 === 0 || i % 3 === 0) { sum += i; }}console.log(sum); 10. 두 개의 주사위를 던졌을 때, 눈의 합이 6이 되는 모든 경우의 수를 출력하시오.1234567for (var i = 1; i &lt;= 6; i += 1) { for (var j = 1; j &lt;= 6; j += 1) { if (i + j === 6) { console.log(`[${i}, ${j}]`); } }} 11. 삼각형 출력하기 - pattern11234567891011121314// 높이(line)가 5for (var i = 1; i &lt;= 5; i += 1) { var line = ''; for (var j = 1; j &lt;= i; j += 1) { line += '*'; } console.log(line);}//output:// *// **// ***// ****// ***** 12. 삼각형 출력하기 - pattern212345678910111213141516for (var i = 5; i &gt;= 1; i -= 1) { var line = ''; for (var k = 0; k &lt;= 5 - i; k += 1) { line += ' '; } for (var j = 1; j &lt;= i; j += 1) { line += '*'; } console.log(line);}// output:// *****// ****// ***// **// * 13. 삼각형 출력하기 - pattern312345678910111213for (var i = 5; i &gt;= 1; i -= 1) { var line = ''; for (var j = 1; j &lt;= i; j += 1) { line += '*'; } console.log(line);}// output:// *****// ****// ***// **// * 14. 삼각형 출력하기 - pattern412345678910111213141516for (var i = 1; i &lt;= 5; i += 1) { var line = ''; for (var k = 0; k &lt;= 5 - i; k += 1) { line += ' '; } for (var j = 1; j &lt;= i; j += 1) { line += '*'; } console.log(line);}// output:// *// **// ***// ****// ***** 15. 정삼각형 출력하기1234567891011121314151617181920var count = 0;for (var i = 1; i &lt;= 10; i += 1) { var line = ''; if (i % 2 !== 0) { count += 1; for (var j = 0; j &lt;= 5 - count; j += 1) { line += ' '; } for (var k = 1; k &lt;= i; k += 1) { line += '*'; } console.log(line); }}// output :// *// ***// *****// *******// ********* 16. 역정삼각형 출력하기1234567891011121314151617181920var count = 5;for (var i = 10; i &gt;= 1; i -= 1) { var line = ''; if (i % 2 !== 0) { for (var j = 5 - count; j &gt;= 0; j -= 1) { line += ' '; } for (var k = i; k &gt;= 1; k -= 1) { line += '*'; } count -= 1; console.log(line); }}// output :// *********// *******// *****// ***// *","link":"/2021/03/11/202103/210311-javascript-assignment/"},{"title":"210312 Rubber Duck Team activity 준비","text":"본 포스팅은 월, 수, 금 아침 시간을 활용한 팀원들과의 activity를 위하여 정리한 개발과 관련된 용어입니다. 리터럴 원시타입 vs 객체타입symbol type이 사용된 예표현식과 값 연산자의 부수효과(전위/후위 연산자)선언과 정의의 차이null 타입은 언제 사용되나? 언매니지드 언어와 매니지드 언어가비지 컬렉터참조 (값 참조와 주소 참조) 제어문조건문(switch문 포함)단축평가반복문블록문/코드블록변수의 유효범위(스코프)전역변수의 문제점break","link":"/2021/03/12/202103/210312-javascript-rubber_duck/"},{"title":"210313 JavaScript TIL 1일차 - 변수의 선언과 정의, Hoisting, var, let, const, TDZ","text":"이번주 월요일부터 새롭게 시작한 자바스크립트 강의를 들으면서 데레사 강사님으로부터 HTML/CSS 수업을 들었을 때와 마찬가지로 내가 제대로 JavaScript를 알지 못했었구나라는 생각이 들었다. 그래서 강사님이 말씀하시는 내용 하나 하나 놓치지 않으려고 집중하며 들어보았다. 상대적으로 다른 언어에 비해 러닝커브가 낮기 때문에 처음 배울때에는 마냥 쉽게만 느껴지는 자바스크립트지만, 제대로 그 언어의 특성을 알고 배우기란 여간 쉬운 일이 아니다. 그래서 수업시간에 배웠던 내용 중에 내가 정말 나중에 개발자로서 성장을 하면서 다시금 돌아보면 유익할 것 같은 내용을 위주로 개인적인 생각을 담아 정리를 해보려고 한다. 지금 알고 있는 것이 전부라고 생각하지는 않는다. 다만 현재의 지식의 한계를 인지하고 현재 알고 있는 부분에 대해 블로그에 정리를 해두고 나중에 알게 된 내용에 대해서 새롭게 업데이트를 할 생각으로 블로그 글을 남겨놓는다. Hoisting?첫 번째 수업때 가장 기억에 남는 개념은 바로 이 Hoisting이라는 개념이었다. 우선 Hoisting에 대해서 살펴보기 이전에 변수의 선언과 정의에 대해서 이야기해보려고 한다. 변수의 선언이란 컴파일러가 참조할 식별자(변수의 타입과 함수의 인수목록)와 이름(변수, 함수, 클래스의 이름)을 알리는 것을 의미한다. 이 과정에서는 별도의 메모리 영역에 영향을 주지 않기 때문에 여러번 중복되어도 문제가 되지 않는다. 변수의 정의란 식별자와 이름으로부터 코드를 생성하여 함수가 호출되거나 변수를 사용할 때 생성된 코드를 참조하는 것을 말한다. 이렇게 변수의 사용을 선언과 정의 두 가지로 나눠서 살펴 볼 수 있다.실제로 우리가 var a = 10으로 a라는 변수를 10이라는 값으로 초기화해서 사용을 한다면 눈에는 하나의 문(statement)으로 보이지만, 실제로는 선언과 정의, 두 개의 파트로 나눠져 있는 것이다. 12var a;a = 10; 앞에서 설명한 내용을 토대로 이제 hoisting에 대해서 살펴보자. 이 hoisting이란 컴파일 단계에서 컴파일러가 파싱을 하면서 선언에 해당하는 내용을 스코프 내의 최상단으로 끌어 올리는 것을 말한다.이러한 이유로 코드 구현시에 아래와 같은 코드 구조(선 정의 후 선언)가 가능하다. 12a = 10;var a; 2021.03.14 update함수는 선언된 위치와 상관없이 동일하게 호출되어 사용된다. 12345greet('World'); // 'Hello, World!'function greet(who) { return `Hello, ${who}!`;}greet('Earth'); // 'Hello, Earth!' 함수 선언문과 함수 표현식의 Hoisting함수 선언문으로 정의한 함수를 함수 선언문 이전에 호출하게 되면, 함수 호이스팅에 의해 호출이 가능한 상태가 되지만, 함수 표현식으로 함수를 정의하면 함수 호이스팅이 발생하는 것이 아니라 변수 호이스팅이 발생한다. 123456789101112firstHello();secondHello();// 함수 선언문function firstHello() { console.log('first hello');}// 함수 표현식var sh = function secondHello() { console.log('second hello');}; JavaScript의 Parser 내부의 Hoisting의 결과 12345678910111213var sh; // 함수 표현식의 변수값 (선언)// 함수 선언문function firstHello() { console.log('first hello');}firstHello(); // OKsecondHello(); // 에러발생sh = function secondHello() { console.log('second hello');}; 위와같이 hoisting은 함수 선언문과 함수 표현식에서 서로 다르게 동작하기 때문에 주의해야 한다. 그렇다면, 이제 다음주 월요일에 배우게 될 변수 사용시에 사용할 키워드인 let과 const는 hoisting이 일어나지 않는 것일까? 이 부분에 대해서 집에 돌아가는 길 지하철 안에서 궁금증을 참지 못하고 이래 저래 구글링을 해보았다. 우선 정답은 let과 const로 선언된 변수 또한 var로 선언된 변수와 같이 hoisting이 발생한다. 하지만 var의 경우 정의 전에 해당 변수에 접근하려고 하면 undefined을 반환하지만 let/const의 경우에는 정의 전에 해당 변수에 접근하려고 하면, ReferenceError가 발생한다. var와 let/const의 선언에서 차이가 나는 이유는 바로 TDZ(Temporal Dead Zone)에 의한 제약 때문이다.갑자기 뜬금없지만 TDZ라는 용어를 보자마자 생각났던 것이 군 복무시에 잠깐 파견을 나갔었던 DMZ(Demilitarized Zone)였고, 두 용어 모두 Zone이라는 공통점이 있다. 이렇게 나의 경험과 학습을 연관시키려는 이유는 HTML/CSS의 데레사 강사님의 수업방식으로부터 배웠다. 강사님은 어떤 예시를 설명해주실때 직접 경험하신 검험담으로 어려운 내용도 좀 더 확장해서 학습할 수 있도록 구체적으로 설명해주셨다. 그래서 나도 되도록이면 나의 경험과 연결시켜서 학습해보려고 하고 있다. 자 이제 돌아와서 TDZ와 DMZ는 Zone이라는 공통점을 가지고 있고, 뭔가 한정된 영역이라는 느낌을 준다. 자 그럼 이런 느낌으로 한 번 TDZ(Temporal Dead Zone)에 대해서 살펴보자. 앞서 이미 언급을 했지만, let과 const도 hoisting이 발생하지만, 선언 이전에 정의(엑세스)를 하려고 하면 에러가 발생한다. 이는 선언 이전에 정의(엑세스)를 하려는 영역은 TDZ에 의해 관리가 되고, ReferenceError를 반환하는 것이다. TDZ 시맨틱은 선언 전에 정의(엑세스)를 금지한다. 1234567// TDZ(Temporal Dead Zone)name; // ReferenceError (managed by TDZ(Temporal Dead Zone))const name = 'Lee Hyungi'; // Declaration &amp; Initializationname; 앞서 작성한 코드에서 name은 const라는 키워드로 선언이 되기 전까지 TDZ라는 영역에 있다고 볼 수 있다. TDZ에 영향을 받는 구문은 앞서 살펴본 const 변수와 let 변수, class 구문, 생성자 내부의 super(), 기본 함수 매개변수가 있다. 이 중에서 생성자 내부의 super()와 기본 함수 매개변수에 대해서 좀 더 자세히 알아보자. 생성자(constructor()) 내부의 super()부모 클래스를 상속받았다면 생성자 안에서 super()를 호출하기 전까지 this binding은 TDZ 영역에 있다. 이전 파이썬 수업때 엘리베이터 추상 클래스를 구현하고, 추상클래스를 상속받아 엘리베이터 클래스를 구현해보았는데, 그때를 생각하며 아래의 Elevator 클래스를 상속받는 HydraulicElevator(유압식 엘리베이터) 클래스를 만들어서 클래스 상속의 예시를 들어보았다. 12345678class HydraulicElevator extends Elevator { constructor(speed, capacity) { this.capacity = capacity; super(speed); }}const myElevator = new HydraulicElevator(100, 10); 위와같이 코드를 작성함게 되면 결과적으로 객체 인스턴스를 생성하는 시점에서 ReferenceError가 발생을 하게 된다. 그 이유는 앞에서 이미 언급을 했듯이 상속을 받았다면 상속을 받은 클래스 내부의 생성자에서는 super가 호출되기 전까지 this를 사용할 수 없다. (super()를 호출하기 전까지 this binding은 TDZ 영역에 있다)따라서 아래와 같이 우선 상속받은 부모 클래스의 생성자를 호출하고 this를 사용해야 한다. 12345678class HydraulicElevator extends Elevator { constructor(speed, capacity) { super(speed); this.capacity = capacity; }}const myElevator = new HydraulicElevator(100, 10); 기본 함수 매개변수아래와 같이 기본 함수의 매개변수를 작성해주게 되면 TDZ에 의해 ReferenceError가 발생하게 된다. 그 이유는 기본 함수의 매개변수의 Scope에 대해서 알아보면 이해할 수 있다.잠깐 이게 왜 에러가 나는지 이해가 되지 않았었는데, 기본 함수의 매개변수의 스코프는 전역(global)과 함수(function)스코프의 중간에 위치한다. 따라서 지금 함수의 외부에 선언된 b와 기본 매개변수를 초기화하기 위해 사용한 b는 서로 다른 변수이며, 기본 매개변수로써 사용한 b를 초기화되지 않은 변수 b를 사용해서 초기화시키려고 하고 있기 때문에 ReferenceError가 발생하게 되는 것이다. 123456const b = 1;function plusDouble(b = b) { return b + b;}plusDouble(); 따라서 위의 문제를 해결하기 위해서 아래와 같이 기본 매개변수의 이름과 외부에 선언한 기본 매개변수의 초기화를 위한 변수의 이름은 다르게 지정해서 사용하도록 한다. 123456const init = 1;function plusDouble(b = init) { return b + b;}plusDouble();","link":"/2021/03/13/202103/210313-javascript-basic/"},{"title":"210314 JavaScript TIL 2일차&#x2F;3일차 - 원시타입과 객체 타입, 값&#x2F;참조에 의한 전달, 유사배열 객체 String","text":"원시타입과 객체 타입원시타입 변수의 경우 값을 재정의 하는 경우, 기존에 사용하던 메모리 위치에 다시 덮어쓰지 않고, 다시 새로운 메모리 위치에 새로운 값을 저장한다. 반면 객체 타입 변수의 경우, 객체를 저장하고 있는 공간의 참조값(주소)을 저장하고 있는 공간과 객체의 값을 저장하고 있는 공간, 두 공간을 갖는다.따라서 객체 값을 저장하고 있는 변수의 값을 변경하는 경우, 메모리상에 저장되어있는 객체의 값을 수정하게 되는 것이다. 값에 의한 전달만약 변수에 원시 값을 갖는 변수를 할당하는 경우, 할당되는 변수에 원시 값이 복사되어 전달된다. (다른 메모리 주소 공간에 복사) 참조에 의한 전달객체는 원시 값과 달리 확보해야 할 메모리 공간의 크기를 사전에 정해 둘 수 없다. (프로퍼티의 수가 정해지지 않고 동적으로 추가/삭제)객체를 할당한 변수는 참조 값(Reference value)을 값으로 갖는다. (객체가 저장된 메모리 공간의 주소) 해당 메모리 공간은 실제 객체의 값을 저장하고 있는 또 다른 메모리 공간을 참조한다.따라서 아래와 같이 참조 값을 복사해서 또 다른 변수에 할당하는 경우, 참조 값(주소)을 저장하고 있는 메모리 공간이 복사되어, 원본과 사본 모두 같은 객체를 공유한다. 따라서 어느 한 쪽에서 객체를 변경하는 경우 두 변수의 값이 모두 영향을 받는다. 12345678910var human = { welcome: 'Hello'};var copyHuman = human;copyHuman.welcome = 'Halo';console.log(human);console.log(copyHuman); output: 12VM280:9 {welcome: &quot;Halo&quot;}welcome: &quot;Halo&quot;__proto__: ObjectVM280:10 {welcome: &quot;Halo&quot;}welcome: &quot;Halo&quot;__proto__: Object String은 유사배열 객체 (Array-like-Object)이면서 반복 가능한 객체 유사배열 객체가 되기 위해서는 우선 length 속성이 필요하고, index가 0부터 시작하여 1씩 증가해야 한다.String 타입은 유사배열 객체로 반복문을 통해 순회가 가능하다. (iterable)","link":"/2021/03/14/202103/210314-javascript-basic/"},{"title":"210315 Rubber Duck Team activity 준비","text":"본 포스팅은 월, 수, 금 아침 시간을 활용한 팀원들과의 activity를 위하여 정리한 개발과 관련된 용어입니다. 암묵적 타입 변환 vs 명시적 타입 변환객체 리터럴객체 리터럴에 의한 객체 생성Object 생성자 함수생성자 함수 프로퍼티 (cf. 프로퍼티 키 &amp; 프로퍼티 값)메서드인스턴스프로퍼티 접근법 2가지; 마침표 표기법 대괄호 표기법 프로퍼티 값의 참조프로퍼티의 동적 생성/추가/삭제유사 배열 객체프로퍼티 축약 표현계산된 프로퍼티 이름참조에 의한 전달참조값을 복사 - 얕은복사와 깊은 복사","link":"/2021/03/15/202103/210315-javascript-rubber_duck/"},{"title":"210213 React TIL - React가 만들어진 배경, 컴포넌트, Yarn, JSX의 규칙, Props, 조건부 렌더링, useState를 통한 동적 상태 관리, input 상태관리(단일 입력, 복수 입력)","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. 클래스 컴포넌트에서 Hooks + 함수형 컴포넌트리액트가 만들어진 배경 JavaScript를 사용한 DOM 조작만약에 어플리케이션의 규모가 커지게 되면, 상태와 이벤트 관리를 위한 코드가 복잡해진다. 따라서 Ember, Backbone, AngularJS라는 프레임워크가 등장을 해서, 자바 스크립트의 특정 값이 바뀌면 특정 DOM의 속성도 바뀌도록 규칙을 가지고 설계되었다.하지만 React의 경우에는 특정 값의 상태가 바뀌었을때 기존의 View를 날려버리고 다시 화면에 보여지는 View를 생성하는 형태로 동작하도록 구상되어 만들어졌다. 이런 방식으로 동작을 하게 되면, 특정 규칙에 의해 DOM속성을 조작하지 않아도 되기 때문에 개발면에서는 편하지만, 작은 어플리케이션의 경우에는 괜찮지만, 어느정도 규모가 있는 어플리케이션이라면 동적인 화면 표현에 있어, 성능상 문제가 된다.하지만 React는 Virtual DOM이라는 것을 사용했기 때문에 성능을 지켜가면서 가능하도록 설계되었다.Virtual DOM이란 말 그대로 가상 돔으로 브라우저에서 실제로 보여지는 DOM이 아니라 그냥 메모리에 가상으로 존재하는 DOM으로써 JavaScript 객체이기 때문에 작동 성능이 실제 브라우저에서 DOM을 보여주는 것보다 속도가 훨씬 빠르다. React에서는 상태가 업데이트되면, 업데이트가 필요한 UI를 메모리에 있는 Virtual DOM에 렌더링을 하게 된다.그 다음으로 React 팀이 개발한 비교 알고리즘을 통해서 실제 브라우저 상에서 보여지는 DOM과 메모리에 존재하는 Virtual DOM을 비교하고 차이점을 감지하고 나서 실제 DOM에 이를 패치(필요한 변화만 반영)를 해주게 된다. UI를 어떻게 보여줄지에 대해 집중컴포넌트 = UI 조각NodeJS자바스크립트를 브라우저 환경이 아닌 곳에서 사용할 수 있도록 하는 자바스크립트 런타임이다. (webpack, babel과 같은 도구들은 이 NodeJS를 기반으로 만들어졌다) YARN자바스크립트 패키지를 관리하기 위한 도구이다. yarn이 npm보다 패키지 설치시에 빠르다. JSX의 기본 규칙JSX는 React에서 컴포넌트의 생김새를 나타낼때 사용된다.JSX는 자바스크립트이다. 얼핏보면 HTML 태그처럼 보이지만, JSX 코드는 Babel을 통해서 자바스크립트 코드로 변환된다. 매번 React.createElement를 사용해서 화면에 보여질 컴포넌트를 만들 수 없기 때문에 JSX라는 문법을 사용해서 XML형태로 컴포넌트를 작성해주면, Babel이 JavaScript로 변환을 해준다. JSX 문법으로 XML형태로 컴포넌트 작성 123&lt;div&gt; &lt;b&gt;Hello,&lt;/b&gt; &lt;span&gt;React&lt;/span&gt;&lt;/div&gt; Babel이 JSX 문법으로 작성한 컴포넌트를 JavaScript로 변환 12345678910'use strict';/*#__PURE__*/React.createElement( 'div', null, /*#__PURE__*/ React.createElement('b', null, 'Hello,'), ' ' /*#__PURE__*/, React.createElement('span', null, 'React')); 태그는 반드시 닫혀있어야 한다.(opening tag - closing tag/self closing tag) 두 개이상의 태그는 반드시 하나의 태그로 감싸야 한다.불필요한 div태그를 사용하지 않고, &lt;&gt;&lt;/&gt; (fragment)를 사용해서 감싸줄 수 있다. fragment 태그는 실제 화면에 출력된 DOM에서 확인되지 않는 태그이다. 그리고 괄호(())로 감싸는 것은 가독성을 위한 처리이다. JSX 내부에서 자바스크립트 값을 표현하기중괄호({})로 감싸서 자바스크립트 값을 표현할 수 있다. 스타일과 클래스 이름 스타일기존에 css 스타일 속성에서 -(dash)로 이름이 표현되어있는 것은 camelCase로 속성의 이름을 바꿔서 지정해주면 된다. 클래스 이름JSX에서는 class이름을 지정해줄때 class가 아닌 className의 속성으로 지정해준다. 주석처리주석을 작성할때에는 주석처리한 내용을 중괄호({/* */})로 감싸주도록 한다. 태그의 내부에서는 single comment (//)로 주석처리를 해서 작성을 한다. 123456789101112131415161718192021222324252627import React from 'react';import Hello from './Hello';function App() { const name = 'react'; const style = { backgroundColor: 'black', color: 'aqua', fontSize: 24, padding: '1rem' }; return ( &lt;&gt; {/* Hello component */} &lt;Hello /&gt; &lt;div // div태그 내부에 스타일 지정 style={style} &gt; {name} &lt;/div&gt; &lt;div className=&quot;my-style&quot;&gt;&lt;/div&gt; &lt;/&gt; );}export default App; Props 부모 컴포넌트에서 자식 컴포넌트로 특정 속성을 전달하고자 할때 사용된다. 부모 컴포넌트로부터 props를 전달받은 자식 컴포넌트에서는 비구조 할당/구조분해를 통해 아래와 같은 형태로 전달받은 props를 사용할 수 있다. App.js 12345678import React from 'react';import Hello from './Hello';function App() { return &lt;Hello name=&quot;react&quot; color=&quot;red&quot; /&gt;;}export default App; Hello.js 1234567import React from 'react';function Hello({ name, color }) { return &lt;div style={{ color }}&gt; 안녕하세요. {name}&lt;/div&gt;;}export default Hello; Props 값의 기본값(default value) 설정하기123Hello.defaultProps = { name: '이름없음'}; Props ChildrenWrapper component를 만들어서 다른 component들을 감싸줄때 내부에 포함된 component들을 보여주기 위해서 props의 children을 사용해줘야 한다. 123456789101112import React from 'react';function Wrapper({children}) { const style = { border: '2px solid black', padding: 16 }; return &lt;div style = { style }&gt; {children} &lt;/div&gt;}export default Wrapper; 조건부 렌더링 삼항연산자(내용이 달라지는 경우)와 단축평가(출력되거나 출력이 안되는 경우)를 이용한 조건부 렌더링하기 123456789function Hello({ name, color, isSpecial }) { return ( &lt;div style={{ color }}&gt; {isSpecial ? '*' : ''} {isSpecial &amp;&amp; &lt;b&gt;*&lt;/b&gt;} 안녕하세요. {name} &lt;/div&gt; );} JSX에서 null, false, undefine을 렌더링해주게 되면, 아무것도 렌더링되지 않는다.(예외적으로 0은 출력된다) 만약에 props를 전달할때 아무런 값을 전달해주지 않으면 true를 전달하는 것과 같은 효과가 나타난다. useState를 통한 동적 상태 관리사용자와의 interaction을 통해서 화면에 표시된 값이 변화해야 될 때 어떻게 처리해야 되는지에 대해서 알아보자. 이전 React 16.8 이전 버전에서는 함수형 컴포넌트 내에서 상태를 관리할 수 없었다. 그런데 이후에 hooks라는 개념이 도입이 되면서 함수형 컴포넌트에서 상태관리가 가능해졌다. 카운터 구현Counter.js 12345678910111213141516171819202122232425import React, { useState } from 'react';function Counter() { // 구조분해를 통해서 아래와 같이 선언을 할 수 있다. // 현재상태와 현재상태를 바꾸는 함수로 구성 const [number, setNumber] = useState(0); const onIncrease = () =&gt; { setNumber(number + 1); }; const onDecrease = () =&gt; { setNumber(number - 1); }; return ( &lt;div&gt; &lt;h1&gt;{number}&lt;/h1&gt; &lt;button onClick={onIncrease}&gt;+1&lt;/button&gt; &lt;button onClick={onDecrease}&gt;-1&lt;/button&gt; &lt;/div&gt; );}export default Counter; &lt;br/&gt; - #### useState에서 함수형 업데이트하기 `리액트 컴포넌트를 최적화하는 과정에서 필요하다(성능 최적화)` 위의 코드에서 onIncrease 증가버튼의 이벤트 함수에서 useState의 상태변화 함수를 사용할때 내부에 현재 상태값을 받아서 처리했지만, 이를 현재 상태값이 아닌 함수로 받아서 처리를 해줄 수 있다. (함수로 처리하면 구체적으로 해당 이벤트 함수가 실행되는 로직을 작성해줄 수 있다) 12345...const onIncrease = () =&gt; { setNumber(prevNumber =&gt; prevNumber + 1);}... React에서 input 상태 관리하기입력 태그의 상태를 관리하기 위해서는 입력태그의 속성에 onChange, value의 속성값으로 input 태그에 입력된 값이 업데이트 되었을때 상태 업데이트 메서드로부터 현재 상태값을 업데이트 될 수 있도록 처리를 해야 한다. `InputSample.js` 1234567891011121314151617181920212223242526import React, { useState } from 'react';function InputSample(){ const [text, setText] = useState(''); const onChange = e =&gt; { setText(e.target.value); } const onReset = () =&gt; { setText(''); }; return( &lt;div&gt; &lt;input onChange={onChange} value={text}/&gt; &lt;button onClick={onReset}&gt;초기화&lt;/button&gt; &lt;div&gt; &lt;b&gt;값: &lt;/b&gt; {text} &lt;/div&gt; &lt;/div&gt; )}export default InputSample; 여러개의 input 상태 관리하기각 각의 input태그 속성에 name 속성으로 값을 넣어준다. 태그에 입력된 값에 변화가 생겼을때 어느 입력태그에서 변화가 생겼는지 구분해주기 위해서 name태그를 넣어 구분을 지어준다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import React, { useState } from 'react';function InputSample(){ // 객체 형태의 상태로 관리하기 const [inputs, setInputs] = useState({ name: '', nickname: '', }); // name, nickname을 사용할 수 있도록 const {name, nickname} = inputs; const onChange = e =&gt; { const { name, value } = e.target; // 객체 상태를 업데이트 할때에는 객체를 복사하고 업데이트된 속성값을 덮어써줘야 한다. // 객체를 업데이트 (기존의 객체를 복사(스프레드 문법)) // name 속성에 따른 value값을 덮어쓰기 // 아래 코드를 생략하고 위와같이 name을 대괄호로 묶어서 표기한다. // name은 실제 값으로써 추가되지 않고, value를 추가하기 위한 키 값으로만 활용된다. setInputs({ ...inputs, [name]: value }); // nextInputs[name] = value } const onReset = () =&gt; { setInputs({ name: '', nickname: '' }) }; return( &lt;div&gt; &lt;input name=&quot;name&quot; placeholder=&quot;이름&quot; onChange={onChange} value={name} /&gt; &lt;input name=&quot;nickname&quot; placeholder=&quot;닉네임&quot; onChange={onChange} value={nickname} /&gt; &lt;button onClick={onReset}&gt;초기화&lt;/button&gt; &lt;div&gt; &lt;b&gt;값: &lt;/b&gt; {name} {nickname} &lt;/div&gt; &lt;/div&gt; )}export default InputSample; onChange 함수 부분만 아래와 같이 간소화시켜서 작성해줄 수 있다. 1234567891011121314const onChange = e =&gt; { const { name, value } = e.target; // 객체를 업데이트 (기존의 객체를 복사(스프레드 문법)) // name 속성에 따른 value값을 덮어쓰기 // 아래 코드를 생략하고 위와같이 name을 대괄호로 묶어서 표기한다. // name은 실제 값으로써 추가되지 않고, value를 추가하기 위한 키 값으로만 활용된다. setInputs({ ...inputs, [name]: value }); // nextInputs[name] = value}","link":"/2021/02/13/202103/210316-React-review_study/"},{"title":"210316 JavaScript TIL 4일차 - 함수, 함수의 정의, 함수 객체 생성, 프로토 타입, 프로토 타입 멤버 속성 추가&#x2F;삭제, 객체의 동결, 즉시 실행함수, 중첩 함수, 콜백 함수, 순수 함수, 스코프, 전역변수 사용하지 말기, 일단 const로 변수 선언하기","text":"Preview오늘은 내일 있을 자바스크립트 수업을 위해서 12장 함수 ~ 15장 let, const와 블록 레벨 스코프에 해당하는 교재의 내용을 읽어보고 개인적으로 궁금한 내용들을 찾아보며 공부해보았다. 우선 강사님이 함수 부분이 가장 중요하다고 하셔서 이 부분을 읽을때 좀 더 집중해서 읽어보았다.다시 한 번 읽어보고 싶은 내용에 대해서 다시 한 번 정리를 해두었다. Review이번 수업에서 가장 중요한 개념은 중첩함수와 콜백함수의 개념이였다. 함수(fuction) 자바스크립트에서 함수는 객체이다.자바스크립트의 함수객체는 다른 일반 객체와는 다르다. 일반 객체는 호출할 수 없지만, 함수 객체는 호출할 수 있다. 자바스크립트에서 함수는 객체이기 때문에 함수 객체만이 가질 수 있는 고유한 속성(property)를 가지고 있다.아래 첨부한 노트의 첫 번째를 참고하자. 첫 번째 노트의 우측 상단부를 보면, 함수 객체가 가지는 고유속성 5가지(arguments, caller, length, name, prototype)가 있는 것을 알 수 있다. 순차적으로 언급한 순서로 함수 객체의 고유 속성에 대해서 알아보도록 하자. arguments : 함수의 인수로 iterable한 유사배열 객체이다. 함수 내부에서 사용되는 지역변수이며, 가변 인자((넘겨받을 인수의 갯수가 정해지지 않았을때 함수 내부에서 사용될 수 있다)과거에 가변 인자의 사용으로 인해 argument라는 속성을 사용했지만, ES6에서 rest parameter의 개념이 도입되었다. (…args) caller : ECMAScript 사양에 포함되지 않은 비표준 property이다. length : 함수로 전달된 매개변수(parameter)의 갯수 prototype : proto type object에 접근이 가능하게 해준다. 함수는 객체 타입의 값이다. 함수 선언문과 함수 리터럴 함수 리터럴에는 표현식이 아닌 문(함수 선언문)과 표현식인 문(함수 리터럴 표현식)으로 구분단독으로 사용된 함수 리터럴은 함수 선언문으로 해석되며, 그룹 연산자()내에 있는 함수 리터럴은 함수 리터럴 표현식으로 해석된다. 함수 리터럴에서 함수 이름은 함수 몸체 내에서만 참조할 수 있는 식별자이다. (함수를 가리키는 식별자가 없다. 즉 외부에서 함수 선언문을 호출할 수 없다) 함수 선언문에서 함수 이름은 자바스크립트 엔진이 암묵적으로 같은 이름으로 식별자로서 생성한다. 따라서 외부에서 함수 선언문의 함수 이름을 통해서 호출이 가능하다.함수는 함수의 이름으로 호출되는 것이 아니라 함수 객체를 가르키는 식별자로 호출되는 것이다. 자바스크립트의 함수는 일급 객체(first-class object) 무명의 리터럴로 생성이 가능하다. (선언과정 없이 정의만으로 함수를 생성하는 것이 가능하다는 말 (정의는 Runtime 과정에서 발생한다)) 변수나 객체/배열 등의 자료구조에 담을 수 있다. 함수의 매개 변수로 전달할 수 있다. 함수의 반환값으로도 사용할 수 있다. 함수 리터럴로 생성한 함수 객체를 변수에 할당할 수 있다.함수 리터럴의 함수 이름은 생략할 수 있다. (익명함수) 함수 객체 생성함수 선언문을 해석해서 함수 객체를 생성한다.(첨부한 첫 번째 노트의 우측 상단을 참고)구체적으로 어떤 흐름으로 자바스크립트에서 함수객체를 만들어내는지 알아보도록 하자.우선, 자바스크립트가 프로토타입 기반의 언어라는 것을 이해해야 한다. 프로토타입 기반이라는 말은 객체를 만들때 자신을 만드는데 사용된 원형인 프로토 타입 객체를 이용하여 객체를 만든다는 말이다.다른 언어에는 클래스 개념이 있기 때문에 클래스라는 하나의 틀을 만들어서 인스턴스 변수의 생성을 통해 객체 인스턴스를 만들어낸다.하지만 자바스크립트에는 클래스가 없다. 따라서 기본적으로 상속기능도 없다고 할 수 있다. 그냥 클래스와 상속을 흉내낸 것이다. 그럼 ES6의 표준에서 Class 키워드를 사용한 문법이 추가되었는데 클래스 기반 언어로 변화된 것일까? 아니다! 여전히 프로토 타입 기반의 언어이며, 문법만 새롭게 추가된 것이다. 함수 객체 생성에 대해서 알아보기 전에 함수의 정의에 대해서 우선 알아보자.함수가 정의되면 어떤 일이 일어날까?아래 첨부한 노트의 첫 번째 페이지 하단의 내용을 같이 참조하도록 하자.함수를 정의하게 되면 우선 기본적으로 함수의 내부에는 prototype이라는 함수객체 고유의 속성을 갖게 된다. 동시에 이 prototype 속성은 Prototype Object(프로토 타입 객체)를 가르키게 되고, Prototype Object 내의 생성자(constructor)는 prototype을 속성으로 가지는 정의한 함수를 가르키게 된다. 프로토 타입 객체의 내부에는 생성자와 __proto__라는 모든 객체가 가지고 있는 속성을 가지게 된다. (함수의 proto type object 생성 및 연결)__proto__ 라는 속성은 원형 프로토 타입 객체를 사용해서 생성한 객체의 내부에도 존재한다. 이 속성은 객체가 만들어지기 위해 사용된 원형인 프로토타입 객체를 숨은 링크로 참조하는 역할을 한다.(이 숨겨진 참조링크를 프로토 타입이라고 정의한다) prototype 객체에 동적으로 런타임에 멤버를 추가12345678function Person() {}Person.prototype.eyes = 2;Person.prototype.nose = 1;Person.prototype.getType = function () { return 'human';};console.log(Person.prototype); // Object{eyes: 2, nose: 1, getType: f} 추가된 prototype 멤버 속성은 같은 원형을 복사해서 생성한 모든 객체에서 공유할 수 있다. (멤버 속성을 추가하기 전에 생성한 객체에서도 새롭게 추가된 멤버 속성을 사용할 수 있다) 어떻게 prototype object에 존재하는 eyes 속성의 참조가 가능한 것인지?(두 번째 필기노트 참조)원형 프로토 타입 객체를 복제해서 새로 객체를 생성하게 되면, 새로 생성된 객체의 속성에는 모든 객체가 가지고 있는 proto라는 속성을 갖게 된다. 앞서 이미 설명했듯이 proto라는 속성은 객체를 생성할때 복제한 프로토 타입 객체 원형을 숨은 링크로 참조하는 역할을 하게 되므로, 프로토 타입 객체에 추가한 멤버 속성을 참조할 수 있는 것이다. 객체를 생성할때 복제한 프로토타입 객체의 proto 속성은 최상위 Object prototype object를 가르킨다.(첨부한 두 번째 필기 노트 참고)복제한 프로토 타입 객체의 proto 속성이 최상위 Object prototype object를 가르키고 있기 때문에 기본적으로 모든 객체는 Object prototype object이 가지고 있는 모든 속성을 사용할 수 있다. 예를 들어 toString()과 같은 함수가 있다. proto는 객체가 생성될 때 조상이었던 함수의 prototype object를 가르킨다. 아래에 첨부한 두 번째 필기 노트를 보면 Person이라는 함수를 통해서 Lee라는 새로운 객체를 만들었다. Lee 객체는 Person 함수로부터 생성되었기 때문에, Person함수의 prototype object를 가르키고 있다. (프로토타입 체인) 객체의 동결 Object.freeze()아래와 같이 객체 리터럴로 객체를 생성하고 const 키워드를 사용하여 상수 변수로 선언해준다고 하더라도 객체의 속성의 추가/삭제는 가능하다. 따라서 수정할 수 없는 객체를 만들어주기 위해서는 Object.freeze()를 사용해서 값을 동결시키고 싶은 객체를 얼릴 수 있다. 참고 : https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/freeze 즉시 실행 함수그룹 연산자 내부에서 함수 리터럴 뒤에 괄호를 써서 처리해주면, 즉시 실행할 수 있는 함수가 된다.(연산자 내부에는 값만 올 수 있다. 따라서 연산자 내부에서 함수 선언문을 작성하면 함수 리터럴이 된다) 중요한 이유는 클로저(Closure) 개념을 이해하기 위해서 알아야 하는 개념이기 때문이다. 화살표 함수가 등장하면서 붙여주는 괄호의 위치가 그룹 연산자 내부가 아닌, 그룹 연산자 외부에 붙여주도록 한다. 123(function (){ ....})() 중첩 함수외부 함수(outer)에서 중첩 함수(inner)는 외부 함수의 내부에서만 사용할 수 있도록 정의된 함수이다. 정보은닉의 측면에서 실제 사용되는 범위(스코프)내부에서 정의하도록 한다. 중첩함수를 개별 함수의 형태로 작성하는 이유는 중첩함수의 이름을 통해서 구체적으로 어떤 처리를 하는 부분인지 유추할 수 있기 때문이다. 로직/행위의 재사용을 위해 함수를 작성한다. 예시코드 작성 콜백 함수어떤 일에 대한 추상화는 함수를 통해 구현한다. 재사용 가능한 함수를 구현할때에 인자로 넘겨주는 함수 인자를 callback(나중에 호출되는 함수)라고 정의한다. 용어 살펴보기콜백함수를 인자로 받아주는 함수를 고차 함수(Higher-Order Function, HOF)라고 정의한다. 여기서 고차 함수란 콜백함수를 인자로 받거나 함수를 결과값으로 반환해주는 함수를 정의하는 용어이다. (프론트엔드 개발자)배열의 고차함수를 자유자재로 사용할 줄 알아야 한다.map함수에서 numbers의 갯수만큼 인자로 받은 callback함수(화살표 함수)를 map이 반복 실행한다. 12// 배열의 고차함수result = numbers.map((item) =&gt; item + 1); 순수함수 함수는 최대한 단순하고 기능은 최소한으로 해서 작성을 해줘야 한다.테스트하기 쉬운 함수를 작성하기 위해서는 순수함수를 작성해야 한다.항상 일관된 결과를 출력해야 테스트하기에 용이하다.비순수 함수의 경우, 예를들어 입력되는 날짜에 따라 출력되는 결과가 다를경우, 이러한 함수를 비순수 함수라 정의한다. 스코프전역코드가 실행되면 전체 스코프가 생성이 된다.(Lexical Environment)함수가 실행이 되면, 함수 스코프가 생성이 된다. (함수가 호출된 다음에 해당 함수의 스코프가 생성이 된다)함수 내부에서 사용중인 변수가 해당 내부 스코프에 존재하지 않는다면 상위 스코프로 이동해서 해당 변수를 찾는다.(우선적으로 자신이 속해있는 스코프에서 탐색을 시작한다) 12345678var x = 1;function foo(a) { var y = 2; console.log(x);}foo(100); 함수 스코프에서 상위 스코프(전역 스코프)로 이동해서 탐색을 하기 위해서는 스코프 간의 관계가 성립되어 있어야 한다.(단방향 연결 리스트(링크드 리스트) 구조 - 스코프 체인) (하위 스코프로는 이동을 하지 않는다) 스코프 생성 순서 (구체적으로)제일 먼저 전역 스코프에 있는 변수와 함수의 선언과 정의(런타임)가 발생된다. 내부 함수가 호출/실행된 뒤에 내부에 선언된 함수의 스코프 내부에 존재하는 변수와 함수의 선언과 정의(런타임)가 발생한다.내부 함수의 스코프 상에서 사용된 변수를 탐색할 때에는 우선적으로 내부 함수 스코프를 탐색하고 존재하지 않을 경우에는 외부 함수로 탐색을 이어간다. 외부에도 탐색하는 변수가 존재하지 않을 경우, Reference Error가 발생된다. 함수 레벨 스코프 (var키워드로 선언된 변수의 스코프)var 키워드로 변수를 선언할 경우, if 문 내에 존재하는 var 키워드 선언 변수일지라도 if문 내부 블럭에 존재하는 변수가 아닌 외부 스코프에 존재하는 변수이다. 블럭 레벨 스코프 (let, const 키워드로 선언된 변수의 스코프)let, const로 변수를 선언하게 되면 if 문 내에 존재하는 변수의 경우, if문 내부 블럭에만 허용되는 변수로써 존재한다. 12345// 변수 a가 덮어써진다.var a = 10;if (true) { var a = 10;} 상위 스코프의 결정 (함수가 정의된 위치를 기준으로 상위 스코프 결정**)함수가 정의된 위치를 기준으로 상위 스코프를 결정한다.(렉시컬 스코프 방식)아래의 코드에서 boo()함수의 상위 스코프는 foo()함수의 지역 스코프가 아닌 전역 스코프이다. 123456789var x = 20;function foo() { var x = 10; boo();}function boo() { console.log(x);} 함수 호출이 종료되면(return-반환) 정의되었던 스코프가 사라진다.어플리케이션의 성능면을 고려했을때, 전역변수를 사용하지 않고 지역변수로 처리를 해서 함수의 생명주기를 짧게 만들어줘야 한다. (전역변수의 경우 브라우저와 동일한 생명주기를 갖는다)또한 함수를 만들때에는 하나의 동작만 하도록 작성을 해줘야 좋은 함수이다. cf) 동적 스코프 방식 : 어디서 호출되었느냐를 기준으로 스코프를 결정한다.ex) Perl 렉시컬 환경(Lexical Environment) https://meetup.toast.com/posts/86 전역변수를 사용하지 않는 방법전역변수는 되도록 사용하지 않도록 해야한다.즉시 실행함수로 감싸서 전역변수를 처리한다.(지역 변수화 - classic한 방법)ES6의 module 방식으로 코드를 작성한다.(modern한 방식)모듈을 사용하는 것이 권장되는 방식이다. 사용되는 변수는 일단 const로 선언하고 재할당이 필요하다고 인지했을때, let으로 변경의외로 재할당을 할 필요가 없다. 따라서 자바스크립트에서는 let보다는 const를 사용한다. Remind를 위해서 예습하면서 필기했던 노트를 첨부한다.","link":"/2021/03/16/202103/210316-javascript-basic/"},{"title":"210316 JavaScript TIL 5일차&#x2F;6일차 - ES6 함수의 추가 기능과 배열","text":"Preview이번 예습 범위는 26장 ES6 함수의 추가 기능과 27장 배열이었다. 이전 예습범위를 공부하면서 자바스크립트가 프로토 타입 기반의 언어라는 부분과 함수형 객체 인스턴스가 어떻게 생성이 되는지에 대해 좀 자세히 공부를 했었는데, 그 덕분에 여러가지로 수월하게 26장의 내용을 이해할 수 있었던 것 같다. 이번 기회로 프로토 타입과 관련한 내용을 개인적으로 찾아가며 학습할 수 있었던 좋은 기회였던 것 같다.콜백함수와 고차 함수의 개념, 프로토 타입 메서드, 정적 메서드 등의 개념을 이해하고 27장을 보니 이전에는 단편적으로만 보였던 메서드들이 다양하게 구분되어 보이기 시작했다. 역시 아는만큼 보이는 것 같다.아직 개념적으로 공부해야 될 부분이 많기 때문에 지금 느끼는 재미로 좀 더 확장성 있게 공부하도록 해야겠다. 정적 메서드, 프로토타입 메서드, 인스턴스 메서드 얕은 복사(shallow copy), 깊은 복사(deep copy) spread 연산자와 concat() 함수를 통한 얕은 복사(shallow copy) 결론부터 말하면 실무에서 객체를 복사(얕은 복사)할때 spread 연산자를 사용해서 작성을 한다. 실제로 객체를 요소로 갖는 리스트를 얕은 복사를 하게 되면 리스트 내부의 모든 객체들을 복사하지 않는다. 모든 객체를 복사한다는 것은 메모리상 부담이 되고, 퍼포먼스상 문제가 된다.얕은 복사의 경우에는 원본과 사본의 식별자를 서로 비교할 경우 참조주소는 다르기 때문에 비교 연산자를 이용해서 비교를 하게 되면 서로 다름을 확인할 수 있다. 하지만 객체 속성 자체를 비교 연산자를 이용해서 비교하게 되면 서로 같는 결과를 확인할 수 있다.반면에 깊은 복사를 하는 경우에는 속성 자체를 비교 연산자를 이용해서 비교해도 다르다는 결과가 나온다.깊은 복사를 할때에는 리스트 내부에 있는 객체의 속성 중에서 원시타입이 아닌 객체 타입을 별도로 구분해서 복사를 하게 된다. Lodash clone deep→ https://lodash.com/docs/ Review","link":"/2021/03/16/202103/210316-javascript-preview/"},{"title":"210318 React TIL - React hook &#x2F; useEffect, useMemo, useCallback, React.memo, useReducer","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. useEffect를 사용하여 mount/unmount시 작업설정useEffect를 사용하여 생성한 컴포넌트가 처음 화면에 나타났을때와 화면에서 사라지게 될 때 특정 작업을 수행하도록 만들 수 있다. 그리고 추가적으로 컴포넌트의 특정 props나 상태가 업데이트 될 때 마다 전/후에 특정 작업을 하게 만들 수도 있다.우선 컴포넌트가 mount될 때 실행하는 callback 함수의 경우에는 useEffect의 첫 번째 인자로는 실행할 callback 함수를 넣어주고, 두 번째 인자로는 deps(dependency)로 의존되는 값을 넣어준다. deps에 빈 배열을 넣어주는 경우, 컴포넌트가 처음 화면에 나타날때만 해당 callback 함수가 실행된다.컴포넌트가 unmount될 때 실행하는 callback함수는 useEffect의 callback함수 내부에서 반환문(return)으로 작성을 해준다. UserList.js 12345678910function User({ user, onRemove, active, onToggle }){ const {username, email, id} = user; useEffect(() =&gt; { console.log('컴포넌트가 화면에 나타남'); return () =&gt; { console.log('컴포넌트가 화면에서 사라짐'); } },[]); ...... 그렇다면 구체적으로 컴포넌트가 mount될때 추가하는 처리에는 어떤 것들이 있으며, 컴포넌트가 unmount될때에 추가하는 처리에는 어떤 것들이 있는지 구체적으로 알아보자.일반적으로 mount의 경우에는 컴포넌트의 특정 props로 받은 값을 컴포넌트의 state 값으로 설정, 외부 API요청(REST API), 라이브러리(D3, Video.js)를 사용할때의 처리, setInterval, setTimeout관련 처리를 한다. useEffect()에서 호출되는 시점에서는 UI가 화면에 나타난 이후이기 때문에 DOM 객체에 접근을 할 수 있다.unmount의 경우에는 setInterval, setTimeout을 사용해서 등록했던 작업을 제거할때 (clearInterval, clearTimeout), 라이브러리 인스턴스 제거 작업 등 뒷정리하는 작업을 한다. deps만약에 빈 배열이 아닌 특정값을 넣어주게 되면 어떻게 될까? 바로 deps로 넣어준 값이 설정되거나 바뀔때마다 useEffect의 callback함수가 실행이 된다.(값이 업데이트된 이후에 콜백함수 실행)(빈 배열을 넣어서 처리를 해주는 경우에는 mount/unmount시에만 실행이 된다) 값이 변경되었을 경우, useEffect() 내부의 cleaner 함수가 우선 실행이되고(변경 전) 값이 업데이트 된 후에는 useEffect의 첫번째 인자로 넣어준 콜백함수가 실행이 된다.(cleaner 함수의 경우에는 값이 업데이트되기 이전과 unmount되기 직전에 호출이 된다)deps에 인자값을 넣어준 경우에는 빈 배열을 넣어줬을때와 같이 컴포넌트가 처음 화면에 나타날때도 실행이 된다. 만약에 useEffect내의 callback 함수 내부에서 컴포넌트의 props로 받아오는 값을 참조하거나 useState에서 관리하고 있는 값이 있는 경우 반드시 deps의 요소로 넣어줘야 한다.특별히 에러가 발생하거나 그런건 아니지만, useEffect내부에서 참조하고 있는 컴포넌트의 props나 useState의 값이 최신 상태로 업데이트되어 참조해야 되기 때문에 반드시 넣어서 처리해주도록 한다. ESLint를 사용하게 되면 deps에 값을 넣어주지 않는 경우, 별도의 경고를 확인할 수 있다. 그럼 deps의 인자로 아무것도 안넣어준다면 화면이 Re-rendering될때마다 useEffect()내의 콜백함수가 매번 호출이 된다.그 이유는 리액트 컴포넌트에서는 부모 컴포넌트가 Re-rendering되면 자식 컴포넌트도 Re-rendering된다. 그렇다고 해서 실제 브라우저에서는 업데이트된 내용이 반영이 되지만, virtual DOM 상에서는 화면의 모든 컴포넌트를 렌더링하고 나서 비교를 해서 바뀐 부분만 적용을 하고 있다. 그런데 대부분의 경우 바뀌지 않은 내용에 대해서 vitual DOM상에서 렌더링을 한다고 해서 느려지거나 하지 않지만, 항목이 많다면 느려지게 될 가능성이 있다. 따라서 virtual DOM에서 렌더링되는 리소스 조차 아껴서 작업을 해줘야 한다.이러한 작업을 컴포넌트 Re-rendering 최적화라고 한다. 예시로 블로그에서 포스팅 글을 볼때를 살펴보자. URL상 뒷쪽 값(URL Slug)을 props로 받아서 component가 mount될때 포스팅에 대한 정보를 읽어서 화면에 출력해주고, 다른 포스팅을 열 경우에는 주소가 바뀌게 되므로, 바뀐 URL상의 props 값을 이용해서 component의 useEffect()상에서 새로운 포스팅에 대한 정보를 읽어서 화면에 rendering \b해주게 된다. 여기서 deps 값으로는 URL의 주소가 될 수 있고, 이 주소가 업데이트 될때마다 useEffect()의 첫 번째 인자로 넣어준 callback함수를 실행하도록 처리를 해주면 된다. (업데이트 시에는 cleaner함수가 업데이트 직전에 실행이되고나서 useEffect의 첫 번째 인자로 넣어준 callback함수가 실행이 된다) 123useEffect(() =&gt; { loadPost(username, urlSlug);}, [username, urlSlug]); useMemo를 사용하여 연산했던 값 재사용 이전에 연산된 값을 재사용, 즉 성능을 최적화할때 사용되는 hook 함수이다.만약에 현재 화면에 표시된 사용자 정보중에 활성화 상태인 사용자 객체의 갯수를 확인하고자 한다면, 아래와 같이 filter 함수를 사용해서 사용자 중에서 활성화 상태인 사용자 객체의 갯수를 확인할 수 있다. 12345678910function countActiveUsers(users){ console.log('활성 사용자 수를 세는 중...'); return users.filter(user =&gt; user.active).length;}function App(){ ... const count = countActiveUsers(users); ...} 하지만 이렇게 작성을 해주게 되면 컴포넌트가 Re-rendering될때마다 작성해준 countActiveUsers()가 호출이 된다. username, email 입력태그에서 값을 입력을 할때마다 onChange 함수가 실행이 되어 Re-rendering이 되고, 해당 함수가 매번 호출되고 있음을 확인할 수 있다. 이런 경우에 사용을 하는 것이 useMemo라는 hook 함수이다. 이 함수를 사용하면 특정 값이 변경되었을때에만 함수가 호출될 수 있도록 처리할 수 있다. 1const count = useMemo(() =&gt; countActiveUsers(users), [users]); 위와같이 useMemo() hook 함수로 감싸서 첫 번째 인자로 기존의 countActiveUsers() 함수를 화살표 함수의 형태로 넣어주고, 두 번째 인자로 deps, users 리스트 정보를 넣어준다. useEffect()에서와 같이 deps로 넣어준 값에 변화가 생기게 될때 마다 넣어준 callback 함수가 호출이 되어 실행되게 된다. useCallback를 사용하여 함수 재사용useCallback 함수를 사용해서 기존에 만들었던 함수를 새로 만들지 않고 재사용할 수 있다.useCallback 함수는 useMemo() 함수와 동일한 효과를 주는데 그 대상이 함수이다.component가 Re-rendering될때마다 내부에서 만들어 준 함수들을 다시 만들어주고 있다. 이렇게 함수를 다시 만들어준다고 메모리/CPU상에서 많은 리소스를 차지하는 작업은 아니지만 한 번 만든 함수를 재사용하는 것이 좋다.그 이유는 컴포넌트의 Re-rendering 최적화 작업을 위해서이다. 컴포넌트에 전달되는 props가 변경되지 않았다면 virtual DOM상에서 수행되는 Re-rendering을 안되게 끔 만들기 위해서는 우선적으로 선행되어야 하는 것이 각 컴포넌트 내부에서 생성한 함수들을 재사용 가능한 상태로 만들어줘야 하는 것이다. 예를들어 입력태그에서 속성으로 사용되는 onChange의 함수를 useCallback을 사용해서 처리해주기 위해서는 아래와 같이 기존의 함수 인자와 반환 부분 전체를 useCallback으로 감싸주고, 두 번째 인자로 deps를 넣어준다. 12345678910const onChange = useCallback( (e) =&gt; { const { name, value } = e.target; setInputs({ ...inputs, [name]: value }); }, [inputs]); 위와같이 작성을 해주게 되면, inputs의 값이 바뀌었을때에만 작성해준 onChange 함수가 재생성이 되고, 그렇지 않은 경우에는 기존에 생성한 함수를 재사용하게 된다. 만약에 함수내부에서 컴포넌트의 props로 넘겨받은 또 다른 함수를 실행하는 경우에는 이 props로 넘겨받은 함수 또한 deps의 인수로써 작성을 해줘야 한다. 내부에서 사용되고 있는데 deps에 넣어주지 않는 경우에는 최신 상태 값을 보장해 줄 수 없기 때문에 반드시 신경써서 deps에 넣어줘야 한다. 이렇게 useCallback만을 사용해서 컴포넌트 내부의 함수의 재사용성을 가능하게 해줬다고 눈에 띄는 성능 최적화가 된 것은 아니다. 추가적으로 컴포넌트의 Re-rendering 최적화 작업을 해줘야 비로소 성능 최적화가 되었다고 할 수 있다. React Developer ToolsReact Dev Tools를 사용해서 현재 rendering되고 있는 컴포넌트를 Smart하게 확인할 수 있다. Chrome Inspector에서 설치한 React Dev Tools를 사용하면 현재 작업중에 rendering되고 있는 component를 가시적으로 Highlight해서 확인할 수 있다.(Chrome Inspector → Components → 톱니바퀴(Setting) → Highlight updates when components render 체크) 위에 첨부한 캡쳐를 보면 입력태그에 텍스트를 입력하게 되면 아래 사용자 리스트 값의 변화가 없는데도 Re-rendering되고 있음을 확인할 수 있다. 그럼 이런 경우에는 어떻게 컴포넌트를 관리해야 할까? 바로 아래에 정리한 React.memo를 사용해서 컴포넌트의 Re-rendering을 방지하면 된다. 좀 더 자세한 내용은 아래의 내용을 참고하자. React.memo를 사용한 컴포넌트 리렌더링 방지컴포넌트의 Re-rendering 최적화React.memo를 사용해서 불필요한 컴포넌트의 리렌더링을 방지할 수 있다.사용법은 아래와 같이 매우 간단하게 사용할 수 있다. 1export default React.memo(CreateUser); React.memo([Component], [prevProps와 nextProps를 비교])만약에 두번째 인자의 prevProps와 nextProps를 비교 연산자로 비교했을때 True값이 반환되면 리렌더링 방지를 해주고, False값이 반환되면 리렌더링을 하도록 처리해준다. 1234567891011121314151617181920function UserList({ users, onRemove, onToggle }) { return ( &lt;div&gt; {users.map((user) =&gt; ( &lt;User user={user} key={user.id} onRemove={onRemove} active={user.active} onToggle={onToggle} /&gt; ))} &lt;/div&gt; );}export default React.memo( UserList, (prevProps, nextProps) =&gt; prevProps.users === nextProps.users); 위의 UserList 컴포넌트를 살펴보면 props로 받는 onRemove()와 onToggle() 두 함수는 초기에 함수를 생성한 뒤에 재사용을 할 것이기 때문에 users 값만 전/후 값을 비교해서 리렌더링할 것인지 결정해주면 된다. 따라서 export해주는 구문에서 React.memo의 두 번째 인자로 해당 조건 콜백함수를 넣어서 처리를 해주면 된다. 컴포넌트를 export해주는 구문에서 컴포넌트 이름을 React.memo()로 감싸주면 된다. 이렇게 해주게 되면 해당 컴포넌트에 전달되는 Props의 값이 바뀌지 않으면 렌더링하지 않는다.함수 선언문으로 선언된 함수의 경우에는 화살표 함수를 사용하여 기존의 함수 선언문을 표현식으로써 작성을 한다음에 표현식 전체를 React.memo()로 감싸준다.(수정전) 코드 1234567891011121314151617const onRemove = useCallback( (id) =&gt; { setUsers(users.filter((user) =&gt; user.id !== id)); }, [users]);const onToggle = useCallback( (id) =&gt; { setUsers( users.map((user) =&gt; user.id === id ? { ...user, active: !user.active } : user ) ); }, [users]); 하지만 입력태그에 값을 입력할때에는 해당 입력 태그만 리렌더링되지만, 실제 새로운 항목이 리스트에 추가되거나 일부 항목이 업데이트 되면, 형제 레벨에 있는 다른 자식 컴포넌트들이 모두 같이 리렌더링 되는 것을 확인할 수 있다.따라서 추가적인 작업이 필요하다.우선 그 원인은 onToggle() 함수를 예로들었을때 deps로 users 정보를 참조하고 있기 때문이다. 그렇기 때문에 부모 컴포넌트에서 자식 컴포넌트로(App → UserList → User) onToggle() 함수를 props로 전달을 할때에 users의 값이 바뀌었기 때문에 전달되는 props도 변경되었다고 인식되어, 전달받은 모든 컴포넌트(React.memo()로 처리)들이 다시 Re-rendering되는 것이다. 해결방법은 useState()의 함수형 업데이트이다. 기존 useCallback() hook 함수를 사용할때에는 첫 번째 인자로 넣어 준 콜백함수의 내부에서 사용되는 props나 상태 정보를 최신상태로 만들기 위해서 deps에 요소를 넣어주었는데 현재 문제는 이 deps에 넣어주었기 때문에 문제가 발생하였다.따라서 useCallback()의 첫번째 콜백함수의 내부에서 이 users라는 값을 deps의 요소로 넣어주지 않고, 해당 값을 사용하는 setter 부분에서 기존에 값으로써 넣어준 것을 함수형으로 변경해주면 된다. 1234// 변경 전) 코드setUsers(users.concat(user));// 변경 후) 코드setUsers((users) =&gt; users.concat(user)); 위와같이 useState() 함수형 업데이트를 해주게 되면, useCallback()의 deps로 넣어주지 않아도 parameter에서 항상 최신 users 정보를 참조하게 되므로 위의 문제를 해결할 수 있다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// 변경 전 onCreate() 함수const onCreate = useCallback(() =&gt; { const user = { id: nextId.current + 1, username, email }; // setUsers([...users, user]) setUsers(users.concat(user)); setInputs({ username: '', email: '' }); console.log(nextId.current); nextId.current += 1;}, [username, email, users]);const onRemove = useCallback( (id) =&gt; { setUsers(users.filter((user) =&gt; user.id !== id)); }, [users]);const onToggle = useCallback( (id) =&gt; { setUsers( users.map((user) =&gt; user.id === id ? { ...user, active: !user.active } : user ) ); }, [users]);// 변경 후 onCreate() 함수const onCreate = useCallback(() =&gt; { const user = { id: nextId.current + 1, username, email }; // setUsers([...users, user]) setUsers((users) =&gt; users.concat(user)); setInputs({ username: '', email: '' }); console.log(nextId.current); nextId.current += 1;}, [username, email]);// 함수가 처음 컴포넌트가 생성되었을때 한 번만 생성되고 그 이후로는 재사용된다.const onRemove = useCallback((id) =&gt; { setUsers((users) =&gt; users.filter((user) =&gt; user.id !== id));}, []);const onToggle = useCallback((id) =&gt; { setUsers((users) =&gt; users.map((user) =&gt; user.id === id ? { ...user, active: !user.active } : user ) );}, []); 중간정리 연산된 값을 재사용하기 위해서는 useMemo() 사용 생성된 함수를 재사용하기 위해서는 useCallback() 사용 렌더링된 컴포넌트를 재사용하기 위해서는 React.memo() 사용 무조건 useCallback을 사용한다고 성능이 개선되는 것은 아니다. 사용함으로써 오히려 더 많은 코드를 실행하게 되는 결과를 낳을 수 있다. useCallback(), useMemo, React.memo()를 사용할때에는 반드시 최적화를 할 수 있는 상황을 판단해서 사용을 해야 한다. 또 다른 컴포넌트의 상태관리 방법 = useReducer컴포넌트의 현재상태 값을 업데이트 해 줄 때에는 여러가지 방법이 있는데 그 중에 하나인 useReducer()를 사용하는 방법에 대해서 정리해보려고 한다. 기존에 상태값을 업데이트할때 사용한 useState() hook 함수는 상태 값을 업데이트 시켜 줄때 다음 상태값을 직접 지정해주는 형태로 상태를 업데이트해주었지만, useReducer는 action 객체 기반으로 상태를 업데이트 해준다.action 객체는 업데이트할때 참조하는 객체로, type이라는 값을 참조해서 어떤 상태값을 업데이트를 할지 선택할 수 있다.업데이트할때 참조할 다른 값이 있다면 추가적으로 객체 내부에 key: value의 형태로 추가해줄 수 있다. 1234dispatch({ type: 'INCREMENT', diff: 4}); useReducer() hook 함수를 사용하게 되면 기존에 컴포넌트 내부에 존재하던 상태 업데이트 로직을 컴포넌트의 밖으로 분리시키는 것이 가능하다.심지어 다른 파일에 작성해서 불러올 수도 있다. reducer: 상태를 업데이트하는 함수로 인자로 넘겨받은 action 객체의 type을 참조하여 구체적으로 어떤 현재 상태 값을 업데이트해 줄 것인지, 지정해서 처리할 수 있다. 12345678910function reducer(state, action) { switch (action.type) { case 'INCREMENT': return state + 1; case 'DECREMENT': return state - 1; default: return state; }} 그럼 userReducer() hook 함수는 어떻게 사용할까?사용법은 useState와 비슷하다. number는 현재 상태를 의미하고, dispatch는 액션을 발생시키는 함수이다.useReducer()함수의 인자로는 첫번째 인자는 앞서 작성해준 reducer함수를 넣어주고 두번째 인자로는 상태값의 초기 값을 넣어주도록 한다. 1const [number, dispatch] = useReducer(reducer, 0); 기존에 작성했던 Counter 컴포넌트의 상태값을 업데이트 하는 로직을 reducer라는 별도의 외부 함수로 구현을 하고, 내부에서는 각 버튼의 액션을 useReducer() hook 함수의 dispatch를 사용해서 상태변화 이벤트가 일어날 수 있도록 작성하였다. Counter.js 1234567891011121314151617181920212223242526272829303132333435363738import React, { useReducer } from 'react';function reducer(state, action) { switch (action.type) { case 'INCREMENT': return state + 1; case 'DECREMENT': return state - 1; default: throw new Error('Unhandled action'); }}function Counter() { const [number, dispatch] = useReducer(reducer, 0); const onIncrease = () =&gt; { dispatch({ type: 'INCREMENT' }); }; const onDecrease = () =&gt; { dispatch({ type: 'DECREMENT' }); }; return ( &lt;div&gt; &lt;h1&gt;{number}&lt;/h1&gt; &lt;button onClick={onIncrease}&gt;+1&lt;/button&gt; &lt;button onClick={onDecrease}&gt;-1&lt;/button&gt; &lt;/div&gt; );}export default Counter; useReducer() hook 함수의 적용순서순서가 딱히 있는건 아니지만 순차적으로 적용하는 방법을 익혀서 useReducer()함수 사용법에 익숙해지도록 하자. (1) 컴포넌트의 초기 상태값에 해당하는 변수를 컴포넌트의 외부에 선언을 해준다. (2) reducer(state, action) 함수 만들기 (내부로직은 아직) (3) 컴포넌트 내부에 const[state, dispatch] = useReducer(reducer, [initial state]) 선언 (4) 비구조 할당문을 통해서 state에 있는 내부 값을 분리해서 필요한 컴포넌트로 props로 형태로 전달 (5) 태그의 속성에 넣어 줄 함수는 처음에 작성할때부터 useCallback을 사용하여 wrapping해서 작성을 해준다. (초기 생성하고 이후에는 계속해서 재사용하는 함수라면, 빈 배열([])을, 내부에 props나 상태값이 참조되고 있다면 해당 상태값을 최신 상태로 업데이트해주기 위해서 deps의 인자로 넣어서 처리해준다. (6) 컴포넌트 내부에서 작성한 이벤트 함수 내부에서 type과 전달할 상태값에 대한 정보를 key:value의 형태로 작성을 해준다. (7) 이제 6번 항목에서 작성해준 type에 관한 내용을 reducer() 함수의 내부에서 action 객체로부터 type 값에 접근/사용하여 조건 처리를 해주도록 한다. 해당 타입값인 경우에 업데이트해서 반환할 상태값에 대한 부분은 내부의 반환문(return)에서 작성을 해주도록 한다. useReducer VS useState딱히 정해진 것은 아니지만, 컴포넌트에서의 상태값 관리가 단순한 경우에는 useState를 사용하고, 상태값의 관리가 복잡한 경우에는 useReducer를 사용하도록 한다. setter를 한 함수에서 여러번 사용되는 경우가 생긴다면, useReducer를 사용을 고려해보는 것이 좋다. 12345setUsers((users) =&gt; users.concat(user));setInputs({ username: '', email: ''}); 커스텀 Hook 만들어서 사용하기","link":"/2021/03/18/202103/210318-React-review_study/"},{"title":"210317 React TIL - React Hook - useRef 함수의 사용, 배열의 효율적인 렌더링, 배열에 새로운 항목 추가, 배열에 항목 제거, 배열에 항목 수정","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. React Hooks - useRef함수의 사용 useRef로 특정 DOM 선택VanillaJS에서는 getElementBy나 querySelector를 사용해서 특정 DOM 객체를 조작한다. React에서도 특정 DOM 객체를 조작해야되는 경우가 있는데, element의 크기나 위치, Scroller bar의 위치, focus 설정하는 경우 등이 있다. 추가적으로 video.js, JWS HTML5 video관련 라이브러리의 사용이나 D3, chart.js와 같은 그래프 관련 라이브러리를 사용할때에도 적용할 특정 DOM을 선택해서 적용시켜야 하는 상황이 생긴다.React에서는 React Hook의 useRef라는 함수를 사용해서 변수를 생성해주고, 조작하고자 하는 특정 태그의 ref 속성값으로 생성한 useRef 변수를 넣어서 생성한 useRef 식별자를 통해 특정 DOM 객체에 접근한다.그외에 class형 컴포넌트에서는 React.createRef()를 사용해서 특정 DOM 객체를 선택하고, callback 함수를 사용해서도 가능하다. input 입력 태그의 값을 초기화 시켜주고, cursor의 focus를 특정 input입력태그로 이동을 시킨다. 12345678910111213141516171819const nameInput = useRef();const onReset = () =&gt; { setInputs({ name: '', nickname: '' }) nameInput.current.focus();};...&lt;input name=&quot;name&quot; placeholder=&quot;이름&quot; onChange={onChange} value={name} ref={nameInput}/&gt;... useRef로 컴포넌트 내의 변수 생성만약에 함수 컴포넌트 내에서 let을 사용해서 변수를 선언하게 되면 렌더링 될때마다 값이 초기화된다.그렇다면 변수의 값 상태를 유지하면서 관리하려면 어떻게 해야할까? state값으로써 관리를 할 수도 있겠지만, state 값의 경우에는 값에 변화가 생기는 경우, 화면이 다시 렌더링되는 결과를 낳기 때문에 효율적인 변수 관리 방법이 아니다.만약에 화면의 렌더링에는 영향을 주지 않고, 독립적으로 변수의 값도 유지를 하면서 관리하기 위해서는 useRef()함수를 사용해서 변수를 관리해줘야 한다. 그렇다면 어떤 경우에 useRef()를 사용해서 변수를 생성해서 관리를 해주게 될까?예를들어, setTimeout, setInterval의 id, 외부라이브러리를 사용해서 생성된 인스턴스, Scroll 위치를 알고 있어야 할때 등의 상황에서 useRef()를 사용하여 특정 변수의 값을 관리한다. 12345678910111213141516171819202122232425262728293031323334353637383940import React, { useRef } from 'react';import Counter from './Counter';import InputSample from './InputSample';import UserList from './UserList';function App() { const users = [ { id: 1, username: 'a', email: 'a@gmail.com' }, { id: 2, username: 'b', email: 'b@gmail.com' }, { id: 3, username: 'c', email: 'c@gmail.com' }, { id: 4, username: 'd', email: 'd@gmail.com' } ]; const nextId = useRef(4); const onCreate = () =&gt; { console.log(nextId.current); nextId.current += 1; }; return &lt;UserList users={users} /&gt;;}export default App; 배열의 효율적인 렌더링 배열을 사용해서 컴포넌트를 렌더링할때에는 map 함수를 사용한다. 렌더링되는 자식 컴포넌트에는 key속성을 넣어줘야 효율적으로 화면을 Re-rendering해줄 수 있다. 컴포넌트의 key라는 속성의 역할은 각 원소들마다 고유값을 할당함으로써 re-rendering성능을 최적화해줄 수 있다. key값은 unique한 값으로 넣어줘야 한다. (아래의 예시에서는 객체의 id값)만약에 특정할 고유값이 존재하지 않는다면, 배열 객체를 map으로 처리할때 두 번째 인자인 index를 받아서 key값으로 설정해줘도 된다. 만약 데이터가 10, 20개 정도의 소량의 데이터이면서 자주 업데이트되지 않는 데이터라면, 퍼포먼스상 문제가 되지 않지만 대량의 배열 데이터를 표시하면서 데이터가 자주 업데이트된다면, 이 경우에는 기존의 배열 데이터에 데이터를 추가, 삭제시에 퍼포먼스상(Re-rendering) 문제가 된다.따라서 되도록이면 index값을 자식 컴포넌트의 key값으로 넣어주지 않도록 해야한다. 컴포넌트가 key값을 가지고 있어야만 특정 컴포넌트가 리스트의 어떤 데이터를 가르키고 있는지 알 수 있다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445import React from 'react';function User({ user }) { const { username, email } = user; return ( &lt;div&gt; &lt;b&gt;{username}&lt;/b&gt; &lt;span&gt;{email}&lt;/span&gt; &lt;/div&gt; );}function UserList() { const users = [ { id: 1, username: 'a', email: 'a@gmail.com' }, { id: 2, username: 'b', email: 'b@gmail.com' }, { id: 3, username: 'c', email: 'c@gmail.com' }, { id: 4, username: 'd', email: 'd@gmail.com' } ]; return ( &lt;div&gt; {users.map((user, index) =&gt; ( &lt;User user={user} key={user.id} /&gt; ))} &lt;/div&gt; );}export default UserList; 배열에 항목 추가기존의 배열(원본)에는 데이터를 추가하지 않고, 기존의 배열(원본)을 복사해서 변화된 데이터 정보를 반영한 또 다른 배열 데이터를 생성하도록 한다.push/splice/sort 함수는 원본 데이터에 영향을 주기 때문에 되도록 사용하지 않도록 한다. 배열의 불변성을 지키면서 기존의 배열에 데이터 추가하는 방법앞서 언급한대로 push/splice/sort 함수를 사용해서 변수를 수정할 경우, 원본 데이터 값이 수정이 된다. 배열의 불변성을 지키면서 기존 배열 데이터에 새로운 데이터를 추가하기 위해서는 아래 두 가지 방법으로 하기를 권장된다.첫 번째, spread 연산자를 사용해서 원본 데이터를 복사해서 사용한다.spread 연산자를 사용해서 기존의 배열 데이터를 복사하고, 새롭게 추가하는 배열 데이터의 정보를 추가해주면 기존 원본 데이터에 변화를 주지 않는다.두 번째, concat() 함수를 사용해서 원본 배열 데이터에 데이터를 추가한다.인수로는 배열이 아닌 값을 넣어줘도 기존의 원본 배열 데이터에 값이 추가된다. 배열에 항목 제거filter() 함수를 사용하여 특정 조건에 맞는 배열의 요소만으로 새로운 배열을 생성한다. 123const taskNotDone = todos.filter((todo) =&gt; todo.done === false);// todos 리스트의 객체 값 중에서 done 속성이 false인 것만 따로 추출해서 새로운 배열로 만든다.console.log(taskNotDone); onClick 속성으로 화살표 함수로 함수 정의해서 넣어주기만약에 버튼의 onClick 속성값으로 함수를 넣어서 처리할 때에는 원형 화살표 함수로 함수를 만들어서 넣어줘야 한다. 배열에 항목 수정배열 안에서 특정 항목을 수정할 때에는 map 함수를 사용한다.객체 리스트가 있다고 가정했을때 리스트 내에 있는 객체 하나 하나를 순회하면서 할당된 id의 값이 일치하는 객체의 특정 속성값을 변경해줄때에는 아래와 같이 map 함수로 리스트를 순회하면서 조건처리를 해준다. 12345678const onToggle = (id) =&gt; { setUsers( // 원본 user의 값을 복제하고 해당 user의 속성 중에 active의 값을 기존의 값을 반전시킨 값으로 업데이트 한다. users.map((user) =&gt; user.id === id ? { ...user, active: !user.active } : user ) );}; useEffect를 사용하여 mount/unmount시 작업설정useMemo를 사용하여 연산했던 값 재사용useCallback를 사용하여 함수 재사용React.memo를 사용한 컴포넌트 리렌더링 방지useReducer","link":"/2021/02/17/202103/210317-React-review_study/"},{"title":"210319 기본기가 있는 개발자가 되기 위한 준비","text":"기본기가 있는 개발자가 되기 위한 준비요즘과 같이 IT 변화의 흐름이 빠른 상황에서 새로운 기술이 나왔을때 빠르게 적응하기 위해서는 특정 프레임워크를 잘 쓰는 개발자가 아닌 근간이 되는 기술들의 기본기가 잘 다져진 개발자가 되어야 한다고 생각한다.또한 개발이란 혼자서 하는 것이 아닌 여러 사람들과 하는 것이기 때문에 커뮤니케이션에서 기본이 되는 정확한 용어와 명확한 근거를 가진 개발자가 되어야 한다. 3주전부터 읽고 있는 타입스크립트 책(o’reilly typescript)를 읽으면서 책의 중간 부분에 책의 저자가 개발자들 사이에서 통용되는 용어를 사용하여 책의 내용을 작성하였다 라고 명시하는 부분이 있다. 이 부분을 읽고나서 느낀점은 실무에서 여러 사람들과 특정 문제에 대해서 해결점을 찾아가는 과정에서 여러 대화가 오가는데 그 과정에서 개발자들 사이에서 암묵적으로 약속된 용어를 사용해서 대화를 한다면 좀 더 원활한 커뮤니케이션이 될 것 같다고 생각했다.무언가가 안되는 상황에서 구체적인 상황을 설명할때에도 정확한 용어와 명확한 근거를 기반으로 설명을 한다면 다른 개발자분들에게 협업을 잘하는 개발자라는 이미지를 줄 수 있을 것이라고 생각한다. 이러한 정확한 용어를 익히기 위해서는 우선 영어 원문과 발음을 틈틈이 찾아보고, 처음부터 공부할때 확실하게 용어와 개념을 공부하는 습관을 들여야 겠다고 느꼈다. 그 다음으로 이 명확한 근거는 어떤 기술 스택을 선택할때에는 명확하게 어떠한 이유로 해당 기술 스택을 선택했는지에 대해 설명할 수 있어야 한다. 그냥 “요즘 뜨고 있어서” 혹은 “유명해서”가 아닌 구체적으로 해당 기술이 다른 특정 기술과는 어떠한 차별점이 있고 현재 진행하려는 프로젝트에서 어떤 부분과 더 부합한지에 대해서 설명할 수 있어야 한다.몇 몇 유명 회사의 기술 블로그에 들어가서 블로그 글을 읽어보다보면 이 기술을 도입하게 된 이유와 배경에 대한 내용을 심심치 않게 볼 수 있다.이처럼 실무에서는 어떠한 기술 스택을 선택하고 사용할때 왜 이 기술을 이 프로젝트에서 사용해야 되는지구체적인 근거를 가지고 사용하는 것 같다.따라서 내가 지금 특정 프레임워크나 기술을 배우고, 사용하고자 한다면 그것도 내가 왜 해당 기술을 배워서 사용하려고 하는지에 대한 구체적인 이해와 설명할 수 있는 근거를 기반으로 학습을 해야 된다고 생각한다.","link":"/2021/03/19/202103/210319-frontend-basic_til/"},{"title":"210319 JavaScript와 친해지기 - 조금은 낯설은 스코프에 대한 이야기","text":"이번 포스팅에서는 아직은 조금 낯설은 스코프(Scope)에 대해서 정리를 해보려고 한다. ECMAScript의 자바스크립트 명세를 보면 자료형, 문법, 연산자, 실행코드와 실행컨텍스트 등 다양한 명세로 구분이 되어있는데 그 중에서 살펴 볼 파트는 9장 실행코드와 실행 컨텍스트(Executable Code and Execution Contexts)이다. ft ECMA-262/March 18, 2021 ECMAScript 2022 Language Specification(Chapter.9)https://tc39.es/ecma262/#sec-executable-code-and-execution-contexts 스코프(Scope) 스코프는 왜 등장했는가? 스코프는 프로그램상의 식별자(이름)의 충돌을 막기 위해서 등장하였다. 초창기 프로그래밍 언어에서는 모든 식별자(이름)을 하나의 대응표에서 관리를 했는데, 사용되는 식별자(이름)의 충돌(collision)로 인해 스코프(Scope)라는 개념이 등장했다. 스코프는 이러한 충돌 문제를 해결하기 위한 규칙으로써 정의된다. 대응표의 역할과 등장배경프로그래밍에서는 변수와 함수에 식별자(이름)을 부여하여 메모리상의 주소와 매핑해서 변수 및 함수의 재사용성을 가능하게 한다. 따라서 프로그램에서는 이름과 값을 1:1로 매핑한 대응표를 만들어서 관리를 한다. 이 대응표의 이름을 통해 값의 조작이 용이해진다. 이 대응표는 프로그램내에 존재하는 이름(식별자)를 관리하는데 사용되는데 만약 하나의 대응표로 모두 관리를 한다면 이름(식별자)들 사이에서 충돌이 발생한다. 따라서 스코프라는 규칙을 적용하여 대응표를 작성하여 관리한다. 스코프와 함수의 관계스코프의 동작 방식은 함수와 밀접한 관련이 있다. 1급 객체로서의 함수는 특징에 대한 명세의 내용을 보면, 스코프의 특징의 전반적인 내용에 대해서 명시하고 있다. 스코프의 규칙최신 버전의 자바스크립트(ES6)에는 함수 레벨과 블록 레벨의 렉시컬 스코프 규칙을 따른다. 스코프 레벨 (함수 레벨과 블록 레벨)우선 함수레벨 스코프에 대해서 살펴보자.자바스크립트는 전통적으로 함수 레벨의 렉시컬 스코프 규칙을 지원했다. 전통적인 자바스크립트 변수 선언방식을 보면 var 키워드를 사용해서 선언을 하는데, 아래와 같이 특정 함수내의 if조건문 블럭에서 var 키워드로 변수를 선언하게 되면, 함수 레벨의 렉시컬 스코프 규칙을 갖기 때문에 함수 내부라면 어디서든 해당 변수의 값을 참조할 수 있다. 함수 레벨 스코프 예시) 12345678function getName() { if (true) { var name = 'Lee Hyungi'; console.log(name); } console.log(name);}getName(); 그렇다면 블록 레벨의 렉시컬 스코프 규칙은 어떻게 다를까?우선 ES6의 let, const 키워드를 사용해서 변수를 선언하게 되면 블록레벨 스코프를 생성해준다. 따라서 if 선언문 블럭 내부에서 선언해준 변수는 해당 블럭이 종료되는 시점에 파괴되어 외부에서 참조가 불가능한 변수가 된다. 블록 레벨 스코프 예시) 12345678function getName() { if (true) { const name = 'Lee Hyungi'; console.log(name); } console.log(name); // ReferenceError(참조에러)}foo(); 스코프를 결정하는 규칙이제 스코프에 대한 기본적인 개념과 스코프 레벨(함수 레벨 스코프와 블록 레벨 스코프)에 대해서 살펴보았으니, 스코프를 결정하는데 사용되는 스코프 규칙에 대해서 정리해보려고 한다.스코프 규칙에는 크게 동적 스코프(Dynamic scope)와 정적 스코프(Static scope)로 분류할 수 있다.동적 스코프란 런타임 도중의 실행 컨텍스트(Execution Context)나 호출 컨텍스트에 의해서 스코프가 결정되는 것을 말한다.반면, 정적 스코프(Static scope)란 렉시컬 스코프(lexical scope)라고도 불리며, 소스코드가 작성된 문맥에 의해 스코프가 결정된다. 실행 컨텍스트(Execution Context)그럼 잠깐 여기서 실행 컨텍스트(Execution Context)에 대해서 알아보자.실행 컨텍스트란 코드가 실행되는 위치를 설명한다는 의미에서 Execution Context라고 한다. 부가적으로 설명하자면 자바스크립트 엔진이 코드를 실행하기 위해서는 코드에 대한 다양한 정보(변수, 함수, 스코프, this, arguments 등)가 필요한데 이러한 정보들을 묶어서 코드가 실행되는 위치를 설명하는 것을 Execution Context라고 정의한다.간단히 말해 코드들이 실행되기 위한 환경으로 이해할 수 있다.자바스크립트 엔진은 Execution Context를 객체로 관리하고, 작성한 코드를 Execution Context 내에서 실행한다. 그럼 Execution Context는 어떻게 생겼을까? ExecutionContext는 아래와 같이 LexicalEnvironment와 VariableEnvironment, 두 가지로 구성이 되어 있다. 12345678910ExecutionContext :{ LexicalEnvironment:{ Environment Records, Reference to the outer environment, }, VariableEnvironment:{ Environment Records, Reference to the outer environment, }} Lexical Environment와 VariableEnvironment의 상세 내용과 관계에 대한 내용은 실행 컨텍스트(Execution Context)의 상단에 첨부한 노트 필기를 참고하도록 하자. 그럼 Execution Context에는 어떤 종류가 있을까? Execution Context 종류 (1) Global Execution Context 전역 실행 컨텍스트는 코드를 실행하는 역할을 하며, 단 한 개만 정의되는 Context이다. 전역 실행 컨텍스트는 Call Stack에 가장 먼저 추가되며 어플리케이션이 종료될 때 삭제된다. 전역 실행 컨텍스트는 global object를 생성하고 this 값에 global object를 참조한다. (2) Functional Execution Context 함수가 실행 될 때 마다 정의되는 Context이다. 전역 실행 컨텍스트가 단 한 번만 정의되는 것과 달리 함수 실행 컨텍스트는 매 실행시마다 정의된다. 함수 실행이 종료(반환문)되면 Call Stack에서 제거된다. (3) Eval Context eval() 함수로 실행한 코드의 Context이다. 보안상 취약한 점이 있어 권장되지 않는 함수이다. 그럼 Execution Context는 어떻게 관리가 되는지 구체적으로 알아보자. Execution Context 관리 앞서 Execution Context의 종류에 대해서 설명을 할때 언급을 했지만 자바스크립트 엔진이 각 각의 Execution Context들을 객체로써 Call Stack(호출스택) 내에서 관리한다.자바스크립트는 단일 스레드(Single Thread)이기 때문에 Runtime에 단 하나의 Call Stack만 존재한다. 우선 제일 먼저 자바스크립트 엔진은 전역 범위의 코드를 실행하며 전역 실행 컨텍스트(Global Execution Context)를 생성해서 Call Stack(호출 스택)에 push를 한다.그 다음으로 함수가 함수가 실행 또는 종료 될 때마다 Global Execution Context 위로 Functional Execution Context를 추가(push)했다가 제거(pop)하는 작업을 반복합니다. 이 Call Stack은 최대 stack 사이즈가 정해져있기 때문에 Call Stack에 쌓인 Context이 최대치를 넘을 경우, RangeError: Maximum call stack size exceeded라는 에러가 발생한다.이 에러를 Stack Overflow라고 한다. Call Stack은 선입 후출(FILO-First In Last Out)로 내부 데이터가 추출되며, Global Execution Context은 가장 먼저 push되어 어플리케이션이 종료되는 시점에 가장 마지막으로 pop된다. 123456789let name = 'Lee Hyungi';function firstFn() {}function secondFn() { console.log(`Hi! Nice to meet you! My name is ${name}`);}firstFn(); 실행 컨텍스트의 작동순서 (1) 코드의 전역 범위가 실행된다. Global Execution Context를 Call Stack에 push한다. (2) firstFn이 실행된다. (3) firstFn의 Functional Execution Context가 Call Stack에 push된다. (4) secondFn이 실행된다. (5) secondFn의 Functional Execution Context가 Call Stack에 push된다. (6) console.log가 실행된다. (7) secondFn내의 console.log의 Functional Execution Context가 Call Stack에 push된다.(Call Stack에 쌓여있는 Context 실행) (8) console.log의 실행이 완료되며 console.log의 Functional Execution Context가 Call Stack으로부터 pop됩니다. (9) secondFn의 실행이 완료되며 secondFn의 Functional Execution Context가 Call Stack으로부터 pop된다. (10) firstFn의 실행이 완료 firstFn의 Functional Execution Context가 Call Stack으로부터 pop된다. (11) 어플리케이션이 종료될 때에 Global Execution Context가 pop된다.","link":"/2021/03/19/202103/210319-javascript-basic_til/"},{"title":"210319 기술 스택의 선택과 이유","text":"왜 굳이 이 라이브러리 이 프레임워크를 사용했나요?사이드 프로젝트를 하기로 결심했다면 각자 어떤 라이브러리를 사용할지 또는 어떤 프레임워크를 사용할지 결정했을 것이다. 그런데 만약에 그 프로젝트가 완성이 되었다고 가정하고, 누군가 그 프로젝트에 대해서 구체적으로 왜 그 라이브러리(혹은 프레임워크)를 사용했나요? 라고 묻는다면 뭐라고 대답할지 생각해본 적이 있는가?나 역시도 이전에는 그냥 요즘 많이 언급되고 실무에서 많이 쓰인다는 이유로 학습을 한 경우가 많았다. 물론 뭔가가 이전에 이미 존재했던 것들에 비해 나은 점이 있으니 각광을 받고 있는 것이다. 하지만 지금 이 시점 이후에 하려는 사이드 프로젝트는 사용하려는 기술에 대해 제대로 이해하고 누군가가 물어봐도 확실하게 대답할 수 있을 정도로 알고 있다는 전제하에 시작하려고 한다. 사용하려는 기술의 선택과 이유간단하게 어떻게 기술스택을 선택하고 선택한 이유에 대해서 어떻게 설명해야되는지 적어보겠다. React를 사용한 이유?요즘에는 사용자들이 3~4초 정도 페이지 로딩이 지연되어도 해당 웹 어플리케이션을 이용하지 않는다고 합니다. 이처럼 React는 사용자가 빠른 interaction을 필요로 하기 때문에 선택을 하게 되었습니다.react의 사용은 사용자로 하여금 웹 어플리케이션이 아니라 마치 모바일 앱을 사용하는 것과 같은 사용자 경험을 주기 위해 사용합니다. React를 사용함으로써 생기는 문제React는 CSR 방식으로 초기 로딩시에 모든 페이지에 대한 정보를 내려받습니다. 그리고 각 화면에 필요한 데이터가 있는 경우, 백엔드 서버에 데이터를 요청해서 동적으로 DOM을 구성해서 Re-rendering을 하게 됩니다.첫 번째, 작은 규모의 어플리케이션의 경우에는 문제가 안되지만 규모가 큰 어플리케이션의 경우에는 초기 로딩시에 모든 페이지에 대한 정보를 내려받는 것이 큰 부담이 됩니다.두 번째, 검색엔진이 초기에 보여지는 페이지가 로딩페이지인 경우 아직 미완성의 페이지로 인식하고 낮은 순위로 검색결과에 노출시키게 됩니다. 하지만 구글 검색엔진의 경우에는 자바스크립트까지 크롤링을 하기 때문에 검색엔진 최적화에 따른 검색결과 노출이 가능합니다. 위에서 언급한 문제에 대해서 해결책을 알고 있는지?우선 첫 번째로 말씀드린 문제의 경우에는 code splitting으로 해결할 수 있습니다. 초기 로딩시에 모든 페이지에 대한 정보를 내려받게 되면 초기 로딩에 대한 시간 지연으로 이어질 수 있기 때문에, 페이지 방문시에 필요한 파일들만 분리해서 내려받음으로써 해결할 수 있습니다.두 번째로 말씀드린 SEO(검색엔진 최적화) 문제의 경우에는 기존의 SPA이 채택하고 있는 CSR 방식을 SSR방식으로 변경해서 구현함으로써 해결할 수 있습니다. 그럼 본 프로젝트에서 위의 문제들을 해결하기 위해 어떻게 구현을 했나요?저는 NextJS라는 React 프레임워크를 사용해서 프로젝트를 만들었습니다. 물론 React 자체로도 프레임워크를 사용하지 않고도 SSR을 구현할 수도 있지만, 좀 더 쉽게 구현할 수 있도록 도와주는 NextJS라는 프레임워크를 사용하기로 했습니다. 그럼 프로젝트의 모든 페이지를 NextJS 프레임워크를 적용해서 구현했나요?아닙니다. 각 페이지를 구현할때 해당 페이지가 code splitting이나 server side rendering이 필요한 페이지인지 우선 생각을 한 뒤에 필요하다고 판단된 페이지에만 NextJS 프레임워크를 적용했습니다.만약에 관리자 페이지(admin page)의 경우에는 검색엔진의 노출을 위한 SEO가 불필요하기 때문에 SSR방식이 불필요하고, 속도도 빠르게 할 필요성(code splitting)이 없다고 생각되어 단순하게 React로만 구현을 하였습니다. 만약에 개발하려는 어플리케이션이 BtoC 서비스인 경우 관리자 페이지도 NextJS를 사용해서 구현할 것을 고민할 필요는 있을 것 같습니다. code splitting과 Server Side Rendering의 효과에 대해서 구체적으로 설명해주세요.Code splitting은 일괄적으로 코드를 내려받는 방식이 아닌 각 페이지별로 필요한 코드들을 분리시켜서 초기 로딩시에 일괄적으로 코드들을 내려받지 않고, 각 페이지 전이시에 필요한 코드들만 내려받게 함으로써 초기 로딩 속도를 높일 수 있습니다.SSR의 경우에는 SEO(검색엔진 최적화)와 밀접한 관련이 있습니다. 구글 검색 엔진의 경우에는 자바스크립트까지 크롤링하기 때문에 문제없지만, 일반적인 검색엔진의 경우에는 초기에 빈 페이지만을 로딩하는 SPA의 특성상 초기 HTML페이지에 아무것도 없기 때문에 미완성 페이지로 인식하고, 검색 노출 순위를 낮게 책정하게 됩니다. 이러한 문제는 CSR방식이 아닌 SSR방식으로 구현함으로써 해결할 수 있습니다. 실무에서는 대부분의 서비스가 검색엔진에서의 노출과 빠른 응답속도를 요구하기 때문에 code splitting은 필수입니다. NextJS의 특징에 대해서 설명해보세요.NextJS는 첫 로딩, 페이지 새로고침, 검색엔진으로부터 접속, 직접 주소를 쳐서 웹 페이지 접속할때에는 SSR방식(browser - front - back - DB - back - front - browser 순)으로 페이지를 렌더링하게 됩니다. (Caching 기능으로 인해 새로고침된 페이지의 경우 빠르게 표시됩니다)또한 Pre-fetching 기능을 제공하기 때문에 로드될 페이지에 다른 페이지로 이동하는 링크가 존재하는 경우에는 링크들에 해당하는 코드들을 미리 내려받습니다. 따라서 이렇게 미리 내려받은 코드와 관련된 페이지로 이동시에는 back-end에서 데이터만 받아오게 됩니다. (CSR) 메인 페이지에 있는 메뉴를 클릭해서 해당 메뉴의 세부 페이지를 로딩하는 경우, 데이터만 비어있는 skeleton frame이 보이다가 잠시후에 데이터가 채워져서 보이는 경우가 CSR의 예시이다. 아무런 지연없이 HTML과 데이터가 완전히 결합된 상태로 빠르게 페이지에 표시되는 경우가 SSR의 예시이다. 위와같이 어떤 기술을 선택해서 사이드 프로젝트를 할때에는 구체적으로 왜 해당 기술스택을 사용했는지 구체적인 이유와 설명을 정리해보고 말로 설명해보는 연습이 필요하다. 그래야 나중에 실무에서 일을 할때에도 왜 내가 해당 기술 스택을 가지고 개발을 해야 되는지에 대한 깊은 이해를 통해 프로젝트에 사용된 기술 스택의 특징을 잘 살려서 견고한 어플리케이션을 개발할 수 있을 것이다.","link":"/2021/03/19/202103/210319-used_technical_stack/"},{"title":"JavaScript로 코딩테스트 준비하기 - 입출력에 대한 이야기","text":"자바스크립트에서 입력받기Python으로는 간단하게 input()을 사용해서 키보드의 입력을 받아서 처리할 수 있었다. 하지만 이번에 VSCode에서 JavaScript로 알고리즘 문제를 풀면서 키보드로 받은 입력 값을 처리하려고 했는데 입력 이벤트를 계속 받고는 있지만 입력 이벤트가 끝나지 않았다.왜 이런지 이해가 되지 않아서 방법을 찾아보던 도중에 해결방법을 찾았다.바로 입력이 끝났다면 ctrl + D를 눌러서 입력 종료를 알려주는 것이다. 예상하지 못한 JavaScript 입/출력 부분의 문제로 계획하지 않은 입출력 관련된 내용으로 포스팅을 하게 되었다. JavaScript에서 입출력은 fs(file system) 모듈을 사용한다. fs모듈의 readFileSync() 함수를 사용해서 파일이나 표준 입출력을 입력받게 되는데, 아래 예시 코드에서 0을 입력해주는 이유는 표준입력(stdin: standard input)이 파일 설명자로 0이기 때문이다. nodejs에서 File system에 관한 공식문서 내용은 아래 링크를 참조하도록 하자.→ https://nodejs.org/dist/latest-v14.x/docs/api/fs.html#fs_file_system 따라서 별도의 파일을 읽지 않고 표준 입력을 받는 경우에는 내부에 0이라는 인수를 넘겨준다. 0과 함께 encoding을 명시해줘야 하는데 별도로 명시하지 않고 표준입력의 설명자 0만을 넘겨준 경우에는 toString()함수를 사용해서 별도로 String 타입으로 변환을 해줘야 한다.(변환을 안해주게 되면 &lt;Buffer 31 30 0a&gt;와 같은 raw buffer가 결과값으로 나온다) 123456789const fs = require('fs');const inputWithNoEncoding = fs.readFileSync(0).toString().split('\\n');const inputWithEncoding = fs.readFileSync(0, 'utf8').split('\\n');const cvtInputToNumber = fs.readFileSync(0, 'utf8').split('\\n');// ['10', '']와 같은 배열의 형태로 값이 반환되기 때문에 [0]번째 인덱스 값을 가져온다.console.log(Number(cvtInputToNumber[0])); 참고로 readFileSync()의 내부에 작성해준 /dev/stdin은 백준 알고리즘 문제 풀이에서 입력 예제를 넣고 그 파일을 읽어 실행하게 만들기 위해서 작성해준 것이다. JavaScript에서 입력받는 방법은 앞서 설명한 fs(File System) 모듈을 사용한 방법과 readline 모듈을 사용한 방법이 있다.readline 모듈은 process.stdin이나 file stream과 같은 Readable stream에서 line by line으로 데이터를 읽어들이기 위한 interface를 제공한다. 123456789const readline = require('readline');const rl = readline.createInterface({ input: process.stdin, output: process.stdout});rl.on('line', (input) =&gt; { console.log(`received: ${input}`);}); 위에서 작성한 ‘line’이벤트는 input stream에서 줄 바꿈관련 입력을 받을때마다 발생한다.(사용자가 Enter키나 Return을 입력한 경우에 발생)콜백함수는 사용자로부터 입력받은 한 줄의 값을 인자로 받아서 처리한다. 그렇다면 모든 입력이 완료가 되었을때 어떤 방식으로 처리를 해야할까?바로 close관련 이벤트를 발생시키면 된다. input stream 도중에 close 이벤트를 발생시키기 위해서는 앞에서 찾아보았던 Ctrl + D (EOT: End Of Transmission) input stream이 end event를 받은 경우 rl.close()함수가 호출되어 readline.Interface instance가 입출력 스트림의 권한을 포기한 상태인 경우 리스너 함수는 처음에 아무런 인자를 넘겨받지 않은 상태로 호출이 되고, readline.Interface object는 close event가 발생된 후에 완료된다. readline module을 사용하여 복수의 입력을 받아 저장하는 경우에는 각 line이 입력될때마다 line event를 발생시켜서 해당 입력값을 별도의 변수에 저장하는 형태로 작업을 해줘야 한다.입력값이 전부 저장이 된 후에는 close 이벤트을 발생시켜서 저장된 변수를 사용해서 계산을 진행하게 된다. 1234567891011const readline = require('readline');const rl = readline.createInterface({ input: process.stdin, output: process.stdout});rl.on('line', (line) =&gt; { // 입력받은 값을 별도의 변수에 저장}).on('close', () =&gt; { // 입력받은 값을 담고 있는 변수를 사용하여 계산한 뒤에 결과값 출력}); 여지까지 JavaScript를 사용해서 입력을 받기 위한 fs와 readline, 두 가지 모듈에 대해서 알아보았다. 상대적으로 fs 모듈을 사용해서 코드를 작성하는 것이 간결한 느낌이 있기 때문에 나는 JavaScript를 사용해서 입력을 받을때에는 fs모듈을 사용하기로 결정했다. fs 모듈을 사용한 입력 및 활용 연습(연습예제 : 백준 알고리즘/10828) 1234567891011121314151617181920212223242526272829303132333435363738394041424344const fs = require('fs');const getInput = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `14push 1push 2topsizeemptypoppoppopsizeemptypoppush 3emptytop`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; getInput[line++];})();// Stack에 대한 정보를 담기 위한 배열const stack = [];// 결과값을 담을 배열const result = [];// 첫번째 입력값 테스트 케이스 갯수로 정수형으로 변환const numOfTestCase = parseInt(input());const stackAction = { push: (value) =&gt; stack.push(value), pop: () =&gt; (stack.length ? result.push(stack.pop()) : result.push(-1)), size: () =&gt; result.push(stack.length), empty: () =&gt; result.push(stack.length ? 0 : 1), top: () =&gt; result.push(stack.length ? stack[stack.length - 1] : -1)};for (let i = 0; i &lt; numOfTestCase; i++) { const [method, value] = input().split(' '); stackAction[method](value);}console.log(result.join('\\n')); 백준 알고리즘에서는 테스트 케이스 수에 대한 입력을 직접 사용자로부터 받고 코드 실행 도중에도 interactive하게 사용자로부터 테스트 케이스 수 만큼 입력을 받고 처리를 하기 때문에 파이썬으로 입력을 받는 것에 비해 자바스크립트로 입력을 받는 것이 어렵게 느껴졌다.하지만 실제 코딩테스트에서는 모든 입력을 받은 뒤에 계산을 하고 결과값을 출력하기 때문에 코드가 실행되는 도중에 사용자로부터 interactive하게 값을 입력받는 일은 없다고 한다. 따라서 event driven 방식의 복잡한 readline module을 사용할 필요는 없다.","link":"/2021/03/24/202103/210324-algorithm_javascript_input/"},{"title":"210325 Web browser와 친해지기 1탄 브라우저의 동작과정","text":"이번 포스팅에서는 브라우저의 동작과정에 대해서 정리를 해보려고 한다.웹 개발자가 좋은 소프트웨어를 개발하기 위해서는 그 Platform이 되는 웹 브라우저에 대한 정확한 이해가 필요하다. 웹 어플리케이션의 성능향상은 곧 웹 브라우저의 동작과 연관되어 있기 때문이다. 우리가 무심코 사용하는 웹 브라우저는 어떻게 동작을 할까? 한 번 자세하게 분석해보자. 자 그럼 웹 브라우저에 대해서 조금씩 알아가보자.우리가 특정 웹 페이지에서 다른 페이지로 이동을 할때 아래 첨부한 Processing model의 과정의 순서로 새로운 웹 페이지가 로드된다. 웹 브라우저의 Processing model출처 : W3C Navigation Timing Level 2 Spec https://www.w3.org/TR/2015/WD-navigation-timing-2-20150717/ (위에 첨부한 사진에서 괄호로 표기된 이벤트는 optional한 이벤트이다)특정 웹 페이지에서 다른 페이지로 이동을 할때, 가장 먼저 unload과정이 발생한다. unloadEventStart에서는 window: beforeunload의 이벤트가 발생을 하게 된다. 내가 저번에 티스토리 블로그에서 특정 블로그 글을 작성하는 도중에 다르 웹 페이지로 이동하려고 하면, 아래와 같이 기존 페이지를 떠나겠냐는 확인 메시지가 담긴 창이 팝업되었었는데 이런 창이 바로 window: beforeunload 이벤트로써 발생하는 이벤트이다. 그 다음으로는 Redirect 과정으로 서버에서 redirect 신호를 보낸 경우 발생을 한다. 위에 첨부한 그래프에서 노란색 부분(백엔드에서 담당-캐시정책, 최적화 방법은 공부하기)은 웹페이지에서 벗어난 뒤에 본격적으로 페이지를 읽어들이기 전이기 때문에 Network level에서 발생하는 과정들이다. 따라서 별도의 자바스크립트 이벤트는 없다. 그 다음은 AppCache로, 실제 서버에서 데이터를 불러오기 이전에 Browser cache에 저장된 데이터가 있는지 확인하는 작업을 한다.그 이후의 DNS, TCP, Request, Response 과정은 전부 네트워크 단계로 Name server를 조회해서 서버의 실제 IP를 받아오고, 실제로 서버에 요청을 보내게 된다. 그 후에 웹 페이지에 필요한 파일들(HTML, CSS, JavaScript, 이미지 파일)을 받아오게 된다. 이제 본격적으로 Processing 단계에서는 서버로부터 받아 온 웹 페이지에 필요한 파일들을 parsing하고 rendering하는 과정까지 포함한다. 이 Processing 과정의 시작과 끝은 domInteractive와 domComplete이벤트로 구성되어있는데 구체적인 내용은 아래 공식문서를 확인하도록 하자. [domInteractive]https://developer.cdn.mozilla.net/en-US/docs/Web/API/PerformanceTiming/domInteractive [domComplete]https://adeveloper.cdn.mozilla.net/en-US/docs/Web/API/PerformanceTiming/domComplete 아래 첨부한 노트에는 processing과 load단계에서 구체적으로 어떤 일들이 일어나고 processing 단계에서 세부적으로 어떤 이벤트가 발생을 하는지에 대해서 대해서 작성해보았다. 간단하게 정리를 해보면 Processing과정은 웹 페이지에서 필요한 파일들(HTML, CSS, JavaScript, Image)을 Parsing하고 rendering하는 과정을 포함한다. document: DOMContentLoaded단계에서는 JavaScript로 DOM에 기능을 추가할 수 있는 단계이다.이전에 HTML 파일을 다 읽고 파싱한 다음에 DOM을 그리고 JavaScript에서 DOM Tree를 그린 다음에 화면에 그리기 전 단계라고 이해하면 된다. 그리고 브라우저의 주요 구성요소와 구조, 그리고 실제 웹 페이지를 표현하는 역할을 하는 Rendering Engine에 대해서도 간단하게 정리를 해보았다. 렌더링 엔진(Rendering Engine)의 주된 역할은 요청 받은 내용을 브라우저 화면에 표시하는 것이다. 렌더링 엔진의 종류로는 Firefox 브라우저에서 사용하는 게코(Gecko)엔진(Mozila에서 직접 만듦)과 Safari, Chrome 브라우저에서 사용하는 웹킷(Webkit)엔진이 있다. 렌더링 엔진의 렌더링 과정렌더링 엔진이 요청 받은 내용을 브라우저 화면에 표시하는 과정은 총 5단계로 구분해서 살펴볼 수 있다. (1) HTML, CSS, JS 파싱 : 브라우저가 문서를 읽어서 문법을 분석하고 코드를 이해하는 과정이다. (2) DOM, CSSOM으로 변환 : 브라우저에서 실제로 사용할 수 있도록 HTML, CSS 코드를 객체의 형태로 변환한다. HTML을 객체의 형태로 변환한 객체가 DOM이고, CSS를 객체의 형태로 변환한 객체가 CSSOM이다.우리가 실제로 JavaScript를 이용해서 HTML 태그를 조작하거나 CSS를 속성으로 접근할 수 있는 것은 HTML과 CSS가 DOM과 CSSOM이라는 객체의 형태로 변환되었기 때문에 가능한 것이다. (3) 렌더 트리 구축 : 2번 과정에서 만든 DOM, CSSOM 객체를 결합해서 화면에 어떻게 렌더링할 것인지에 대한 내용을 담은 렌더트리를 구축한다. (4) 렌더 트리 배치(Layout 단계) : 3번 과정에서 만든 렌더트리를 실제로 그리지 않고 픽셀로써 각 요소의 크기와 위치를 계산한다. 브라우저 내부에는 포토샵과 같이 여러개의 layout으로 구분되어 구성되어 있다.특정 웹 페이지의 레이아웃이 어떻게 구성되어 있는지 가시적으로 확인하기 위해서는 page inspector의 More Tools의 Layers 메뉴를 확인하면 된다.(Layout의 전체 갯수는 30개 이하로 하는 것이 웹 페이지 성능상 좋다고 한다) (5) 렌더 트리 그리기 요소를 실제로 픽셀로 변환해서 화면에 그린다. (Paint/Rasterize)cf) Rasterize : 텍스트와 이미지를 프린트 가능한 형태로 전환시키는 것을 말한다. 픽셀로 그려진 여러 레이어를 합성한다. (compositing 단계)실제로 viewport 범주에서 보여지는 것이 우리가 브라우저에서 보는 화면이다. 실제로 화면에 있는 레이어의 크기는 viewport의 크기보다 크다. [참고사이트](1) NAVER D2 https://d2.naver.com/helloworld/59361 (2) Google https://developers.google.com/web/updates/2018/09/inside-browser-part1?hl=ko https://developers.google.com/web/updates/2018/09/inside-browser-part2?hl=ko https://developers.google.com/web/updates/2018/09/inside-browser-part3?hl=ko","link":"/2021/03/25/202103/210325-web_browser/"},{"title":"210327 Basic Algorithm 문자열 압축","text":"기본 알고리즘 문제 Pseudo code + JavaScript code 처음 이 문제를 보았을때 생각났던 구현방법으로 Set()자료형이 생각이 났다. 우선 주어진 문자열을 압축하는 것이기 때문에 반복되는 문자를 생략해서 표기해야 되기 때문이다.그래서 아래에 첨부한 코드와 같이 처음에 입력받은 문자열을 Set() 객체로 만들어서 중복되는 문자를 제거해줬다. 12345678910111213141516171819202122const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `KKHSSSSSSSE`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})(){ const inputWord = input(); let noRepeatWord = new Set(inputWord); let noRepeatWordList = [...noRepeatWord]; for (let i = 0; i &lt; noRepeatWordList.length; i++) { let repeatCount = 0; for (let w of inputWord) { if (noRepeatWordList[i] === w) repeatCount += 1; } if (repeatCount &gt;= 2) noRepeatWordList[i] += repeatCount; } console.log(noRepeatWordList.join('')); 1234567891011121314151617function solution(s) { let answer = ''; let cnt = 1; s = s + ' '; for (let i = 0; i &lt; s.length - 1; i++) { if (s[i] === s[i + 1]) cnt++; else { answer += s[i]; if (cnt &gt; 1) answer += String(cnt); cnt = 1; } } return answer;}let str = 'KKHSSSSSSSE';console.log(solution(str));","link":"/2021/03/27/202103/210327-Algorithm_basic_compress_string/"},{"title":"210327 Basic Algorithm 가장 짧은 문자거리","text":"기본 알고리즘 문제 Pseudo code + JavaScript code 오랜만에 이런저런 생각을 해 볼 수 있었던 알고리즘 문제인 것 같아 블로그에 포스팅 글로 남기기로 했다. 표면적으로는 매우 간단해보이는 문제이지만 그 접근방식에는 다양한 것 같다. 처음 이 문제를 보았을때 생각났던 구현방법으로 split()과 재귀호출 이 두 가지 방법이 생각이 났다.주어진 문자열에서 특정 문자를 기준으로 구분되는 각 문자열들의 특정 문자로부터의 거리를 구하는 프로그램을 작성하는 문제인데 처음에는 아래와 같은 방식으로 Pseudo code와 JavaScript 코드를 작성해보았다. 결과값은 정답과 근사하게 출력이 되지만 정답이 되는 코드는 아니다. 다만 이 코드를 첨부하는 이유는 나의 문제해결 접근방식과 또 다른 문제해결 접근방식을 비교하기 위한 목적에 있다. 12345678910111213141516let word = 'teachermode';let resultArr = Array.from(word.length).fill(0);const splitWord = word.split('e');console.log(splitWord);const result = splitWord.map((word) =&gt; { const wordLength = word.length; for (let i = 0; i &lt; Math.floor(wordLength / 2); i++) { if (wordLength % 2 !== 0) { resultArr[Math.ceil(wordLength / 2)] = i + 1; } resultArr[i] = i + 1; resultArr[wordLength - i - 1] = i + 1; } return resultArr.join('');});console.log(result); 우선 위의 코드에서 생각한 방식으로 코드를 구현해서 정답을 구할 수도 있다고 생각한다. 하지만 재귀호출 방식을 사용했기 때문에 시간 복잡도를 고려했을때 효율적이지 않은 코드가 된다. 그 다음으로 주어진 문자열을 왼쪽 끝에서 오른쪽 끝으로, 오른쪽 끝에서 왼쪽 끝으로 완전 탐색을 하면서 문제를 해결하는 방식으로 코드를 작성해보았다. 이 방식의 시간 복잡도는 O(N)이다. for문을 두번 사용하기때문에 O(N)만큼의 시간복잡도를 갖게 된다. 123456789101112131415161718192021222324252627282930313233const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `teachermode e`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ const iv = input().split(' '); const inputWord = iv[0]; const searchWord = iv[1]; let p1 = 1000; let answer = []; for (let w of inputWord) { if (w === searchWord) p1 = 0; else p1 += 1; answer.push(p1); } let p2 = 1000; for (let i = inputWord.length - 1; i &gt;= 0; i--) { if (searchWord === inputWord[i]) { p2 = 0; } else { p2 += 1; answer[i] = Math.min(answer[i], p2); } } console.log(answer.join(' '));} 주어진 문자로부터 얼마나 떨어졌는지 최소거리를 표현해야 되기 때문에 처음 왼쪽 끝에서 오른쪽 끝으로 순회를 할때에는 주어진 특정 문자와 다른 경우에는 0으로 초기화를 하고, 그렇지 않은 경우에는 1++로 누적한 값으로 해당 위치의 값을 넣어준다. 그 다음에 오른쪽 끝에서 왼쪽 끝으로 순회를 할때에는 기존에 있는 값이 순회하면서 1++로 누적한 값보다 크다면 작은 쪽의 값으로 초기화를 시켜준다.","link":"/2021/03/27/202103/210327-Algorithm_basic_shortest_word_distance/"},{"title":"210327 Algorithm problem solving challenge 1주차 회고록","text":"1일 10문제 10일 100문제 챌린지 도전오늘은 3일전부터 시작한 1일 10문제 10일 100문제 챌린지를 시작한지 4일째 되는 날이다. 오늘 풀 예정인 10문제를 시작하기 이전에 간단하게 3일동안 진행한 알고리즘 문제풀이에 대해서 간단하게 회고를 하려고 한다. 무엇이든 혼자하게 되면 피드백이라는 과정을 간과하게 되는데 이렇게 회고를 남기면서 부족했던 부분과 개선해야 되는 부분에 대해서 정리를 하면 나 스스로에게 피드백이 되기 때문에 다른 사람에게 피드백을 별도로 받을 수 없는 경우에 좋은 것 같다. 지난 3일동안 기본 알고리즘 문제를 27문제 풀었다. 원래 계획대로라면 30문제 정도 풀이를 했어야 했는데 그래도 나름 성과가 있는 3일이었다. 사실 이전에 백준 알고리즘 사이트에서 파이썬으로 대략 40문제정도 문제 풀이를 했었는데 프론트엔드 개발 포지션에 지원하기 위해서는 코딩테스트 문제를 자바스크립트로 응시해야 되는 경우가 많다고 해서 이번 알고리즘 문제풀이는 자바스크립트로 시작했다. 개발자로서 성장하기 위해 꾸준히 해나가야 되는 공부도 있고 단기에 최대한 역량을 끌어올리는데 필요한 공부도 있는 것 같다. 뭐 이런 모든 공부를 하기 위해서는 진짜 흥미를 가지고 있다는 전제가 되어야 한다고 생각한다. 다시 본문으로 돌아와서 이번 알고리즘 문제 풀이는 자바스크립트로 진행을 했다. 자바스크립트로 알고리즘 문제풀이를 하면서 좀 어렵다고 느꼈던 부분은 테스트 케이스에 대한 입력을 받는 부분이었다.파이썬은 단순히 input()으로 입력을 받을 수 있었는데, 자바스크립트에서는 별도의 입력을 받으려면 생각보다 작성해줘야 하는 부분이 많았다. (뭐 프로그래머스와 같은 알고리즘 문제풀이 사이트에서는 매개변수로 입력을 받아서 처리해주지만, 백준 알고리즘의 문제풀이에서는 별도의 입력을 받아줘야 하고, VS Code에서는 입출력 stream에 대한 제어가 필요하다) 자바스크립트를 이용한 입출력에 관련한 내용은 아래의 블로그 포스팅 글에 정리를 해두었다. JavaScript로 코딩테스트 준비하기 - 입출력에 대한 이야기 https://leehyungi0622.github.io/2021/03/24/202103/210324-algorithm_javascript_input/ 지금은 난이도가 쉬운 문제를 위주로 자바스크립트로 알고리즘 문제풀이를 하면서, 자바스크립트라는 언어의 사용에 익숙해지기 위한 시간이라고 생각한다. 지하철을 탈때나 버스안에서 그리고 쉴때 가끔 한 문제 한 문제씩 풀어가면서 눈에 익히다보니 27문제라는 문제를 3일동안 풀었고, 어느정도 알고리즘 문제 풀이에 있어 자바스크립트를 사용하는 것에 익숙해진 것 같다. 아마 난이도가 어느정도 올라가면 하루에 10문제를 풀기 힘들겠지만 그래도 나와의 약속이기 때문에 계속 끊임없이 문제풀이를 할 것이다. 그리고 오늘작성한 챌린지 시행 첫 주차 회고록이 끊임없이 지속하는데 하나의 나침반이 될 것이라고 믿는다.그럼 오늘은 첫주차 알고리즘 문제 풀이를 마무리하는 의미에서 2문제를 풀고 다음주 월요일부터 본격적으로 알고리즘 10문제 풀이를 진행해봐야겠다.^^","link":"/2021/03/27/202103/210327-algorithm_memoirs/"},{"title":"210329 JavaScript와 친해지기 - 이벤트 전파(Event propagation)이벤트 버블링(Bubbling)과 캡쳐링(Capturing)에 대한 이야기","text":"이번 포스팅에서는 이벤트 전파(Event propagation), 캡쳐링(Capturing)과 버블링(Bubbling)에 대해서 정리를 해보려고 한다. 이벤트 전파(Event propagation)?HTML태그는 중첩된 구조로 되어있다. 따라서 이벤트 또한 중첩적으로 장착될 수 있기 때문에 이러한 경우에 어떠한 순서로 이벤트 핸들러들을 호출(실행)할 것인가에 대한 순서에 대한 규약이 바로 이벤트 전파라고 할 수 있다. 만약 상위 HTML 요소에 이벤트를 주고 그 하위 요소들에도 이벤트를 주었을때 상위 HTML 요소에 적용한 이벤트 핸들러가 가장 먼저 호출이 되고 점점 내부로 들어오면서 가장 하위 요소(target element)의 이벤트 핸들러가 호출되는 형태가 된다면 이를 이벤트 캡처링(Event Capturing)이라고 한다. 그리고 가장 하위(target element) 요소에 적용한 이벤트 핸들러가 가장 먼저 호출되고, 그 상위 element에서 그 상위 element로, 그리고 상위 element로 타고 올라가면서 이벤트가 호출되는 것을 이벤트 버블링(Event Bubbling)이라고 한다. 캡처링(Capturing)과 버블링(Bubbling)앞서 이벤트 전파의 정의에서 캡처링과 버블링의 방향성에 대한 정의에 대해서 간단하게 살펴보았다.웹 브라우저의 이벤트 모델은 캡처링과 버블링, 모두를 지원한다.(하지만 이벤트 캡처링은 예전 IE버전에서는 지원되지 않고 있다) 캡처링과 버블링은 서로 비슷하지만 정반대의 방향성을 가지며, 버블링이 이벤트 전파에서 실세라고 할 수 있다. 캡처링을 사용하는 것은 매우 드문 편이지만 이벤트가 전달되는 흐름에 있어 유용한 개념이기 때문에 알아두면 유용하다. 그럼 캡처링과 버블링 방식은 어떻게 결정될까?이 캡처링과 버블링은 이벤트 적용시에 addEventListener의 세번째 인자값을 다르게 함으로써 방식을 다르게 적용할 수 있다.addEventListner의 세번째 인자 = Use capturing을 의미한다. 두 번째 인자로 넣은 event handler가 capturing의 방식으로 동작하길 원한다면 true로 설정해주고, 만약에 bubbling의 방식으로 동작하길 원한다면 false를 주거나 아무 인자값도 넣어주지 않는다면(default) bubbling의 방식으로 적용한 event handler가 동작하게 된다. 버블링은 모든 브라우저에서 지원이 되지만, 캡처링은 과거의 브라우저에서 지원이 되지 않기 때문에 왠만하면 사용을 하지 않는 것이 권장된다. 만약에 이벤트의 전파를 특정 지점에서 막고자 한다면 어떻게 해야될까?만약 이벤트가 전파되는 과정에서 특정 HTML element의 내/외부로는 이벤트 전파가 일어나지 않도록 하려면 어떻게 해야될까? 바로 이벤트 전파의 마지막 요소의 이벤트 핸들러에 event.stopPropagation();를 넣어주는 것이다.이렇게 되면 해당 HTML element의 이후로는 더 이상의 이벤트 전파가 발생하지 않는다. 이벤트 전파(Event propagation)예시index.html 12345678...&lt;body&gt; &lt;fieldset&gt; &lt;legend&gt;event propagation&lt;/legend&gt; &lt;input type=&quot;propagation&quot; id=&quot;target&quot; value=&quot;target&quot; /&gt; &lt;/fieldset&gt;&lt;/body&gt;... app.js 1234567891011121314151617181920212223242526272829303132function handler(event) { // event.eventPhase : // 1 = capturing(Starting from window, document and the root element, the event dives down through ancestors of the target element), // 2 = target element(The event gets triggered on the element on which the user has clicked), // 3 = bubbling(The event bubbles up through ancestors of the target element until the root element, document, and window.) var phases = ['capturing', 'target', 'bubbling']; console.log( event.target.nodeName, this.nodeName, phase[event.eventPhase - 1] );}function stopHandler(event) { // event.eventPhase : // 1 = capturing, // 2 = target element, // 3 = bubbling var phases = ['capturing', 'target', 'bubbling']; console.log( event.target.nodeName, this.nodeName, phase[event.eventPhase - 1] ); event.stopPropagation();}document.getElementById('target').addEventListener('click', handler, false);document .querySelector('fieldset') .addEventListener('click', stopHandler, false);document.querySelector('body').addEventListener('click', handler, false);","link":"/2021/03/29/202103/210329-javascript-basic_til/"},{"title":"210330 JavaScript와 친해지기 - 이벤트 위임(Event delegation)에 대한 이야기","text":"이번 포스팅에서는 앞서 포스팅한 이벤트 전파(Event propagation), 캡쳐링(Capturing)과 버블링(Bubbling)과 매우 밀접한 관련이 있는 이벤트 위임(Event delegation)에 대해서 정리를 해보려고 한다. 이벤트 위임(Event delegation)?이벤트 위임은 Vanilla JS로 웹 앱을 구현할때 자주 사용하게 되는 코딩 패턴이다.이벤트 위임은 간단하게 말하자면 '하위요소에 각각 이벤트를 붙이지 않고 상위 요소에서 하위 요소의 이벤트들을 제어하는 방식이라고 정의할 수 있다. 아래에 간단한 TO DO LIST 예시 코드를 작성해보았다. 1234567891011&lt;h1&gt;TO DO LIST&lt;/h1&gt;&lt;ul class=&quot;itemList&quot;&gt; &lt;li&gt; &lt;input type=&quot;checkbox&quot; id=&quot;item1&quot; /&gt; &lt;label for=&quot;item1&quot;&gt;Learn about event delegation&lt;/label&gt; &lt;/li&gt; &lt;li&gt; &lt;input type=&quot;checkbox&quot; id=&quot;item2&quot; /&gt; &lt;label for=&quot;item2&quot;&gt;Event delegation pattern&lt;/label&gt; &lt;/li&gt;&lt;/ul&gt; 123456const inputs = document.querySelectorAll('input');inputs.forEach((input) =&gt; { input.addEventListener('click', (event) =&gt; { console.log('clicked'); });}); 만약에 아래와 같이 새로운 리스트 아이템을 추가해준다면, 이전의 input태그는 정상적으로 이벤트가 동작하지만 새롭게 추가된 input태그는 이벤트가 적용되지 않은 것을 볼 수 있다. 이벤트를 적용할 당시에 단 두개의 input 태그만 존재했기 때문이다. 12345678910111213const todoList = document.querySelector('.todosList');const li = document.createElement('li');const input = document.createElement('input');const label = document.createElement('label');const labelText = document.createTextNode('new todo item');input.setAttribute('type', 'checkbox');input.setAttribute('id', 'item3');iabel.setAttribute('for', 'item3');label.appendChild(labelText);li.appendChild(input);li.appendChild(label);todoList.appendChild(li); 한 두개의 새로 추가될 li 요소에 대한 이벤트 추가는 괜찮지만, 만약에 추가해야 될 리스트 아이템이 엄청 많다면 이는 엄청 번거로운일이 아닐 수 없다.이러한 번거로운 상황을 해결해 줄 해결책이 바로 이벤트 위임(Event delegation)코딩 패턴이다. 아래의 예시코드는 각 li 태그의 삭제 버튼에 대한 이벤트를 일일이 주지 않고, li태그의 부모 요소인 ul태그에 onclick 이벤트를 주고 클릭된 요소가 삭제 버튼(.todos-list &gt; .todo-item &gt; .remove-todo)인 경우에만 클릭된 자식요소의 부모노드로부터 id값을 취득해서 삭제하는 함수의 인수로 넣어 함수를 호출하는 방식으로 작성하였다. 이와같이 상위 요소에 이벤트를 달아놓고 하위에서 발생한 클릭 이벤트를 감지하는 것을 이벤트 버블링(Event bubbling)이라고 한다. 이와 같은 기본적인 브라우저의 이벤트 감지 방식은 상식으로 알고 있어야 한다. 123456789101112// Remove button event functionconst deleteTodoItem = (id) =&gt; { todos = todos.filter((todo) =&gt; todo.id !== +id); render();};// Remove button event 처리 (Event delegation - Event bubbling)todosList.onclick = (event) =&gt; { if (!event.target.matches('.todos-list &gt; .todo-item &gt; .remove-todo')) return; const deleteTodoId = event.target.parentNode.id; deleteTodoItem(deleteTodoId);};","link":"/2021/03/30/202103/210330-javascript-basic_til/"},{"title":"210331 Basic Algorithm 소수 구하기","text":"기본 알고리즘 문제 Pseudo code + JavaScript code 내가 처음 풀이한 코드인데 소수인지 아닌지의 판단을 1부터 자기자신의 숫자 범위내의 숫자로 나눴을때 0으로 나누어 떨어지는 경우의 수가 2인 경우(1과 자기자신)에 결과 리스트에 담아서 정답을 출력하였다. 나의 풀이에서는 1부터 자기자신의 숫자까지 모두 순회를 하였는데 이렇게 순회를 할 필요가 없다.2부터 자기자신의 제곱근까지의 범위의 숫자로 나눠서 0으로 떨어지는 경우가 있다면 이는 소수(Prime number)가 아니기 때문에 이런 식으로 문제를 해결할 수도 있다. Solution 1 12345678910111213141516171819202122232425262728293031323334353637383940414243444546const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `932 55 62 20 250 370 200 30 100`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ let numOfNumbers = 0; let numberList = []; for (let i = 0; i &lt; 2; i++) { if (i === 0) numOfNumbers = Number(input()); if (i === 1) numberList = input() .split(' ') .map((number) =&gt; Number(number)); } console.log(numberList); // reversed된 number값을 기존의 numberList에 업데이트 해주기 위해서 // index로 접근 for (let i = 0; i &lt; numberList.length; i++) { let reversedNumber = ''; let temp = numberList[i]; while (temp) { reversedNumber += temp % 10; temp = Math.floor(temp / 10); } numberList[i] = Number(reversedNumber); } let primeNumberList = []; for (let number of numberList) { let zeroRemainderCount = 0; for (let i = 1; i &lt;= number; i++) { if (number % i === 0) { zeroRemainderCount += 1; } } if (zeroRemainderCount === 2) primeNumberList.push(number); } console.log(primeNumberList.join(' '));} Solution 2123456789101112131415161718192021222324function isPrime(num){ if (num === 1) return false; for (let i = 2; i &lt;= parseInt(Math.sqrt(num); i++) { if(num % i === 0) return false; } return true;}function solution(arr) { let answer = []; for(let x of arr) { let res = 0; while(x) { let t = x % 10; res = res * 10 + t; x = parseInt(x / 10); } if(isPrime(res)) answer.push(res); } return answer;}let arr = [32, 55, 62, 20, 250, 370, 200, 30, 100];console.log(solution(arr));","link":"/2021/03/31/202103/210331-Algorithm_basic_prime_number/"},{"title":"210331 Basic Algorithm brute force(블루트 포스) 대표유형 문제","text":"기본 알고리즘 문제 Pseudo code + JavaScript code 이번에 풀어 본 기본 알고리즘 문제는 완전탐색 알고리즘으로 이름에서 유추해 볼 수 있듯이 가능한 모든 경우의 수를 비교해서 풀이하는 방법의 알고리즘 기법이다. 이전에 이 Brute force 알고리즘에 대해서 포스팅한 적이 있다. 오늘 풀어 본 멘토링 문제도 이 완전탐색을 기반으로 풀어야 해결할 수 있는 문제였다. 문제의 조건은 다음과 같다. 총 4명의 학생이 N번의 테스트를 응시해서 각 테스트 별로 등수가 매겨진다. A와 B학생이 있다고 가정했을때, A학생이 B학생의 멘토가 되기 위해서는 시행된 모든 테스트에서 A학생은 B학생보다 등수가 우위에 있어야 한다.이러한 조건으로 각 테스트별 해당 학생의 등수가 비교되는 학생보다 등수가 우위에 있는지 비교를 하고 우위에 있다면 count값을 증가시켜 count값이 응시하는 테스트 수와 같다면 멘토, 멘티의 관계가 성립되기 때문에 결과로 출력할 answer값을 증가시켜 값을 누적하도록 하면 해결할 수 있는 문제였다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `4 33 4 1 24 3 2 13 1 4 2`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ let firstInput = input(); const numOfStudents = firstInput.split(' ')[0]; const numOfTests = firstInput.split(' ')[1]; let testList = []; let answer = 0; for (let k = 0; k &lt;= numOfTests; k++) { if (k &gt; 0) testList.push( input() .split(' ') .map((number) =&gt; Number(number)) ); } console.log(testList); for (let i = 1; i &lt;= numOfStudents; i++) { for (let j = 1; j &lt;= numOfStudents; j++) { let cnt = 0; for (let k = 0; k &lt; numOfTests; k++) { let pi, pj = 0; for (let s = 0; s &lt; numOfStudents; s++) { // 등수 넣기 if (testList[k][s] === i) pi = s; if (testList[k][s] === j) pj = s; } if (pi &lt; pj) cnt++; } if (cnt === +numOfTests) { answer++; } } } console.log(answer);} 1234567891011121314151617181920212223242526272829303132333435function solution(test) { let answer = 0; m = test.length; n = test[0].length; for (let i = 1; i &lt;= n; i++) { for (let j = 1; j &lt;= n; j++) { let cnt = 0; // k: test 시행 수 for (let k = 0; k &lt; m; k++) { let pi = (pj = 0); // s: 등수 for (let s = 0; s &lt; n; s++) { // i, j번째 학생의 등수 결정 if (test[k][s] === i) pi = s; if (test[k][s] === j) pj = s; } // pi: mentor, pj: mentee // mentor, mentee 성립 if (pi &lt; pj) cnt++; } // 만약에 테스트 시행 수와 cnt된 값(mentor, mentee성립 수)이 일치한다면 // mentor, mentee가 될 수 있는 필요충분 조건을 만족한다. if (cnt === m) answer++; } } return answer;}let arr = [ [3, 4, 1, 2], [4, 3, 2, 1], [3, 1, 4, 2]];console.log(solution(arr));","link":"/2021/03/31/202103/210331-Algorithm_basic_brute_force_problem/"},{"title":"210331 Node.js TDD Practice","text":"Express.js + MongoDB 이전에 배웠던 NodeJS의 내용을 복습하기 위한 목적으로 간단한 앱을 만들어 본 적이 있는데 만든지 시간이 좀 지나서 다시 상기시킬 겸 Express.js + MongoDB의 형태를 TDD 방식으로 연습을 해보려고 한다. React도 TDD 방식으로 개발하는 방법에 대해서 공부를 해보며 실습을 해보았는데, NodeJS도 JavaScript 기반이기 때문에 Jest라는 공통 라이브러리를 사용해서 테스트 코드를 작성한다. 이미 React를 위한 단위 테스트 코드를 작성해 본 경험이 있기 때문에 테스트 코드 작성의 개념적인 부분과 작성방법은 다시 복습하는 느낌이었다. 처음에 테스트 코드를 작성했을때에는 이전에 개발을 했을때 다른 언어로 단위 테스트 코드를 작성해 본 경험이 있었음에도 생각보다 익숙해지는데 여러번 반복이 필요했다.요즘 새로운 것들을 스스로 학습하면서 느끼지만, 이 프로그래밍이라는 언어는 사람관계와 비슷한 점이 많은 것 같다. 새로운 사람을 만날때 누군가에 의해 소개로 알게 되더라도 개인적으로 계속 만나고 이야기를 해야 관계가 깊어지듯이, 이 프로그래밍이라는 것도 내가 지속적으로 관심을 갖고 계속 눈과 손에 익히고 연습을 해야 친해질 수 있는 것 같다. 아직 연습이 많이 필요하지만 요즘에 점점 프로그래밍과 친해지고 있다는 느낌이 든다. 아래에 내가 실제 실습을 하며 작성하였던 코드를 업로드한 GitHub Repository를 첨부한다. 실습 Repository → https://github.com/LeeHyungi0622/TDD-Practice-NodeJS 나중에 공부한 내용을 상기시키기 위해서 공부하면서 필기했던 노트를 첨부한다.","link":"/2021/03/31/202103/210331-nodejs-tdd-practice/"},{"title":"210401 Node.js TDD Practice 두 번째 이야기 - jest.fn() and node-mocks-http","text":"jest.fn() Mock function jest.fn()이번 포스팅에서는 jest에서 제공해주는 Mock 함수(jest.fn())에 대해서 정리를 해보려고 한다.mock은 한글로 직역하면 ‘모의’라는 의미를 가지며 가짜, 흉내내는 이라는 뜻을 가지고 있다. Mock 함수는 단위 테스트를 작성할 때에 테스트하려는 코드가 의존하고 있는 부분을 가짜로 대체하는 일을 해준다. 여기서 의존하는 부분을 가짜로 대체하는 이유는 무엇일까?의존하는 부분을 가짜로 대체하는 이유는 우선 첫 번째, 의존적인 부분을 개별적으로 구현하기 까다로운 경우가 있다. 그리고 두 번째, 의존적인 부분의 상태에 따라 테스트 결과가 다르게 나온다면 안되기 때문에 의존적인 부분에 의해 테스트 결과가 영향을 받지 않도록 해야한다. 데이터베이스의 데이터 조작을 테스트데이터베이스의 데이터를 추가하는 부분을 테스트한다고 가정했을때, 실제 데이터베이스를 가지고 테스트를 한다면 Network, I/O task, Transaction creation, Query transmission etc...의 다양한 작업과 데이터 베이스에서 변경된 데이터를 직접 원상복귀하거나 Transaction rollback 해야하는 경우에 데이터 베이스에 데이터를 저장하는 부분 테스트에 고려해야 되는 작업이 많아 비효율적인 방법이 될 것이다.혹여나 테스트 도중에 실 데이터베이스가 죽어버린다면 테스트 결과에 영향을 미치게 될 것은 뻔하디 뻔한 상황일 것이다. 그래서 이러한 의존적인 부분을 jest.fn()을 이용해서 가짜 함수를 생성함으로써 해결할 수 있다. 이 jest.fn() 함수는 매우 유용하게 사용될 수 있는데, 생성한 가짜 함수에 어떤 일들이 발생했는지, 다른 코드들에 의해서 어떻게 호출이 되는지 기억해주는 역할을 해주기 때문이다. 함수가 내부적으로 어떻게 사용되는지 검증할 수 있다. 구체적인 mock 함수 사용은 아래 첨부한 노트 1~2을 참고하도록 하자. 데이터베이스에 request객체의 body 속성으로 받은 실제 데이터를 추가해주는 부분을 테스트하기 위해서는 reqest와 response 객체가 필요한데 이 경우에는 node-mocks-http module을 사용해서 아래와 같이 request, response 객체를 초기화시켜줄 수 있다. 해당 부분에 대한 내용은 아래 첨부한 세 번째 노트를 참고하도록 하자 /test/unit/products.test.js 12345const httpMocks = require('node-mocks-http');let req = httpMocks.createRequest();let res = httpMocks.createResponse();let next = null; /test/data/new-product.json 12345{ &quot;name&quot;: &quot;Gloves&quot;, &quot;description&quot;: &quot;good to wear&quot;, &quot;price&quot;: 15} /test/unit/products.test.js 1234567891011121314151617181920212223242526272829303132333435const { describe, test, expect } = require('@jest/globals');const productController = require('../../controllers/products.controller');// Create, Update, Delete를 하기 위해 필요한 modelconst productModel = require('../../models/Product');// 실제 데이터를 추가할때 사용할 req, res 객체 생성을 위한 node-mocks-httpconst httpMocks = require('node-mocks-http');// req.body에 추가해줄 newProduct json 데이터const newProduct = require('../data/new-product.json');// 단위 테스트이기 때문에 model에 직접적으로 영향을 받으면 안된다.(mock 함수 사용)// mock 함수를 사용해서 호출되는 함수를 정의해주면,// 어떤 것에 의해서 호출되는지, 어떤 것과 함께 호출이 되는지 알 수 있다.// 아래에서 productController.createProduct()가 호출되었을때, productModel.create가// 호출이 되었는지 안되었는지 spy해서 추적할 수 있다.productModel.create = jest.fn();describe('Product Controller Create', () =&gt; { test('should have a createProduct function', () =&gt; { expect(typeof productController.createProduct).toBe('function'); }); test('Should call ProductModel.create', () =&gt; { // product data를 데이터베이스에 추가할때 필요한 product객체를 // 넘겨받을 req 객체 let req = httpMocks.createRequest(); let res = httpMocks.createResponse(); let next = null; req.body = newProduct; // createProduct() 함수가 호출이 될때, // 위에서 httpMocks로 생성해준 req, res, next를 인수로 넘겨서 // createProduct함수를 호출해준다. productController.createProduct(req, res, next); // productModel의 create 메소드가 같이 호출되는지 확인 expect(productModel.create).toBeCalledWith(newProduct); });}); 실습 Repository → https://github.com/LeeHyungi0622/TDD-Practice-NodeJS 나중에 공부한 내용을 상기시키기 위해서 공부하면서 필기했던 노트를 첨부한다.","link":"/2021/04/01/202104/210401-nodejs-tdd-practice/"},{"title":"210402 Algorithm Efficiency (Two pointer algorithm, Sliding window, Hash)에 대한 이야기","text":"알고리즘 효율성 (Two pointer algorithm, Sliding window, Hash) 이번 포스팅에서는 알고리즘의 효율성에 대해서 이야기해보려고 한다. 아래의 코드는 주어진 두 배열을 합쳐서 합쳐진 배열을 오름차순으로 정렬하는 문제를 구현한 코드이다.처음에 이 문제를 보았을때 들었던 생각은 spread 문법을 사용해서 주어진 두 배열을 하나의 배열로써 unpacking한 다음에 sort() 함수를 사용해서 오름차순 정렬하는 방법을 생각했다. (solution 1) 그런데 sort()함수를 사용해서 N개의 숫자를 정렬하는 경우에는 시간복잡도 O(NlogN)만큼의 시간 복잡도를 갖는다. 그렇다면 좀 더 시간복잡도상 효율적인 코드를 작성할 수는 없을까? 단순히 결과값이 나왔다고 좋아하지 말고 좀 더 효율적인 코드를 작성할 수 있도록 생각하고 또 생각해야 한다. 그 해결책으로는 Two Pointer Algorithm가 있다. arr1, arr2 두 개의 배열이 주어지고, 두 개의 배열을 합친 후 오름차순으로 정렬을 해야 되는 문제가 주어졌다고 가정하자.이런 경우에 각 배열의 start index에 해당하는 p1, p2 변수를 0으로 초기화 해서 선언을 한다.arr1[p1]과 arr2[p2]의 값을 서로 비교해서 작은 값을 새로운 배열 변수(answer)에 담아주고 작은 값이 arr1에 있는 값인 경우에는 p1++, arr2에 있는 값인 경우에는 p2++를 해주도록 한다. 이 일련의 과정은 p1, p2가 arr1.length, arr2.length보다 작을때까지 looping한다. 구체적인 내용은 아래 첨부한 필기노트를 참고하자. Solution 1(use spread and sort) - O(nlogn) Time complexity1234567891011121314151617181920212223const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `31 3 552 3 6 7 9`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ const n = Number(input()); const nList = input().split(' ').map(number =&gt; Number(number)); const m = Number(input()); const mList = input().split(' ').map(number =&gt; Number(number)); // n개의 숫자를 sort함수를 사용해서 정렬을 하면, nlogn만큼의 시간 복잡도를 갖는다. // Two pointer 알고리즘을 사용하면, n 만큼의 시간 복잡도를 갖는다. // 두 리스트를 순회한다고 하더라도 (n+m) 만큼의 시간 복잡도를 갖기 때문에 sort함수를 사용해서 // 해결하는 것보다 시간 복잡도상 더 효율적이다. console.log([...nList, ...mList].sort((f, s) =&gt; f - s).join(' ')); Solution 2(Two pointer algorithm) - O(n) Time complexity1234567891011121314151617function solution(arr1, arr2) { let answer = []; let n = arr1.length; let m = arr2.length; let p1 = (p2 = 0); while (p1 &lt; n &amp;&amp; p2 &lt; m) { if (arr1[p1] &lt;= arr2[p2]) answer.push(arr1[p1++]); else answer.push(arr2[p2++]); } while (p1 &lt; n) answer.push(arr1[p1++]); while (p2 &lt; m) answer.push(arr2[p2++]); return answer;}let a = [1, 3, 5];let b = [2, 3, 6, 7, 9];console.log(solution(a, b).join(' ')); 두 배열의 공통원소를 찾아서 정렬하는 문제도 Two pointer algorithm을 사용해서 구현할 수 있다. solution 1 (Set 자료구조와 sort() 활용)123456789101112131415161718192021222324252627282930const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `51 3 9 5 253 2 5 7 8`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ const n = Number(input()); const nList = input() .split(' ') .map((number) =&gt; Number(number)); const m = Number(input()); const mList = input() .split(' ') .map((number) =&gt; Number(number)); const nSet = new Set(nList); const mSet = new Set(mList); const intersectionSet = new Set( [...nSet].filter((number) =&gt; mSet.has(number)) ); console.log([...intersectionSet].sort((f, s) =&gt; f - s).join(' '));} solution2 (Two pointer algorithm 활용)12345678910111213141516171819function solution(arr1, arr2) { let answer = []; arr1.sort(); arr2.sort(); let p1 = (p2 = 0); while (p1 &lt; arr1.length &amp;&amp; p2 &lt; arr2.length) { if (arr1[p1] === arr2[p2]) { answer.push(arr1[p1++]); p2++; // 값이 작은 쪽의 index(p1, p2)를 증가시킨다. } else if (arr1[p1] &lt; arr2[p2]) p1++; else p2++; } return answer;}let a = [1, 3, 9, 5, 2];let b = [3, 2, 5, 7, 8];console.log(solution(a, b)); 연속 부분수열 문제Two pointer algorithm 대표유형 위에서 풀이한 알고리즘 문제는 주어진 숫자배열에서의 숫자들의 합이 특정 숫자 m이 되는 경우의 수를 구하는 문제를 Two pointer algorithm으로 풀이한 문제이다. Two pointer algorithm에 대해서 설명을 할 때에는 위의 문제를 활용해서 설명하는 경우가 많기 때문에 위에 첨부한 노트의 좌측에 있는 코드를 제대로 이해하고 있어야 한다. 1234567891011121314function solution(m, arr) { let lt = 0; let sum = 0; let count = 0; for (let rt = 0; rt &lt; arr.length; rt++) { sum += arr[rt]; if (sum === m) count++; while (sum &gt;= m) { sum -= arr[lt++]; if (sum === m) count++; } } return count;} 아래의 코드는 초기에 내가 직접 Two pointer algorithm을 활용하여 풀이했던 코드이다. numberCombine과 sumNumberList는 더해준 숫자 요소의 그룹을 확인하기 위한 목적으로 사용한 변수이다. 내가 처음에 작성한 코드의 경우, sum값이 target sum value(m)보다 작거나 같은 경우와 큰 경우로 나눠서 p1값이 주어진 배열의 길이(numOfNumbers)의 값보다 작은 경우에 반복 순회하도록 작성하였다. 위의 대표유형 코드와 다른 부분은 대표유형 코드의 경우, sum값이 target sum value(m)보다 크거나 같은 경우에 다른 포인트 (lt)를 index로 하는 배열의 위치값을 빼주었지만, 내가 작성한 코드에서는 커지면 p2의 값을 1증가시키고, 증가시킨 p2의 값을 기존의 p1의 값으로 대체하고 누적 합(sum)의 값을 0으로 초기화시켜주었다. 12345678910111213141516171819202122232425262728293031323334353637383940414243const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `8 61 2 1 3 1 1 1 2`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ const firstInput = input().split(' '); const numOfNumbers = Number(firstInput[0]); const sumOfNumbers = Number(firstInput[1]); const numberList = input() .split(' ') .map((number) =&gt; Number(number)); let p1 = (p2 = 0); let sum = 0; let count = 0; let sumNumberList = []; let numberCombine = []; while (p1 &lt; numOfNumbers) { if (sum &lt;= sumOfNumbers) { if (sum === sumOfNumbers) { sumNumberList.push(numberCombine); numberCombine = []; count++; } numberCombine.push(numberList[p1]); sum += numberList[p1++]; } else { numberCombine = []; sum = 0; p2++; p1 = p2; } } console.log(count); console.log(sumNumberList); // [ [ 2, 1, 3 ], [ 1, 3, 1, 1 ], [ 3, 1, 1, 1 ] ]} 2021.04.03 업데이트 Sliding window 기법Sliding window 기법이란 정해진 구간을 정해진 방향으로 밀고 나가면서 정해진 구간만큼의 값을 계산하는 것을 말한다.Sliding window라는 말도 창문이라는 정해진 너비만큼 옆으로 이동하는 모습에서 착안한 이름이 아닐까 생각된다. 주어진 N개의 숫자를 K개씩 묶어서 더할때 최대합을 구하는 문제로 Sliding window 기법을 정리해본다. 초기 합은 주어진 숫자 리스트의 초기 K개의 합으로 변수를 초기화한다.그리고나서 numberList[i]의 값을 더하고 numberList[i-k]의 값을 빼주는 과정을 반복한다. 이중 for문을 사용해서 위와같은 과정을 구현할 수도 있지만, 이중 for문을 사용하게 되면 O(N^2)만큼의 시간 복잡도를 갖기 때문에 효율적이지 않다.따라서 아래와 같이 Sliding window 기법으로 코드를 작성하도록 하자. 1234567891011121314151617181920212223242526272829const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `10 312 15 11 20 25 10 20 19 13 15`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ const [n, k] = input() .split(' ') .map((n) =&gt; +n); const numberList = input() .split(' ') .map((n) =&gt; +n); let answer = 0; let sum = 0; for (let i = 0; i &lt; k; i++) sum += numberList[i]; // 첫 번째 3일을 더한 값을 초기화 answer = sum; for (let i = k; i &lt; numberList.length; i++) { sum += numberList[i] - numberList[i - k]; answer = Math.max(answer, sum); } console.log(answer);}","link":"/2021/04/02/202104/210402-Algorithm_Efficiency/"},{"title":"210401 JavaScript의 Set()과 Map()에 대한 이야기","text":"알고리즘 문제 풀이를 하면서 중복된 요소를 제거하기 위해서 Set 자료구조를 자주 사용하였다.그런데 이 Set 자료구조의 사용법에 대해서 자세히 설명하라고 하면 설명할 수 없을 것 같다는 생각에 자바스크립트에서 자주 사용되는 자료구조인 Set과 Map 자료구조의 사용법에 대해 비교해서 정리해가며 공부를 해보려고 한다. SetSet은 value들로 이루어진 집합이다. Array와는 다른점은 같은 value를 두 번 포함할 수 없다는 것이다. 따라서 보통 중복을 제거하고자 할때 이 Set 자료구조를 많이 사용한다. (1) has([value]) : 주어진 Set 내부에 value값이 존재하는지, 존재하지 않는지 확인할 수 있다.(true/false 반환) (2) add([value]) : 지정한 value 값을 추가할 수 있다. (2) delete([value]) : \b지정한 value 값을 제거한다. (3) clear() : Set 내부의 모든 데이터를 삭제할 수 있다. 123456789let testSet = new Set();testSet.add('a').add('b');console.log(testSet.size); // 2console.log(testSet.has('a')); // trueconsole.log(testSet.has('z')); // falsetestSet.delete('a');console.log(testSet.has('a')); // falsetestSet.clear();console.log(testSet.size); // 0 union(합집합), intersection(교집합), difference(차집합)Set 자료구조를 사용하여 합집합, 교집합, 차집합을 구현해보려고 한다. 123456789101112131415161718192021222324252627282930let setA = new Set([1, 2, 3, 4, 5]);let setB = new Set([4, 5, 6, 7, 8]);// 합집합let unionSet = new Set([...setA, ...setB]);for (let v of unionSet) { console.log(v); // 1, 2, 3, 4, 5, 6, 7, 8}// 교집합let intersectionSet = new Set([...setA].filter((v) =&gt; setB.has(v)));for (let v of intersectionSet) { console.log(v); // 4, 5}// 차집합let differenceSet = new Set([...setA].filter((v) =&gt; !setB.has(v)));for (let v of differenceSet) { console.log(v); // 1, 2, 3}// 대칭차 집합(SymmetricDifference)let symmetricDifferenceSet = new Set([ ...[...setA].filter((v) =&gt; !setB.has(v)), ...[...setB].filter((v) =&gt; !setA.has(v))]);for (let v of symmetricDifferenceSet) { console.log(v); // 1, 2, 6, 7} MapMap은 JavaScript에서 Key와 Value가 한 쌍(pair)을 이루는 Collection이다.map.keys()와 map.values()는 모두 map의 key와 value값들을 iterable한 객체로써 반환한다. 123let testMap = new Map().set('a', 1).set('b', 2);console.log([...testMap.keys()]); // ['a', 'b']console.log([...testMap.values()]); // [1, 2] map.entries()를 사용하면, map 안의 모든 entries를 순회할 수 있는 iterable한 객체로 반환해준다. 12345let person = new Map().set('a', 1).set('b', 2);let iterPerson = person.entries();console.log(iterPerson.next()); // {value: ['a', 1], done: false}console.log(iterPerson.next()); // {value: ['b', 2], done: false}console.log(iterPerson.next()); // {value: undefined, done: true} for-of와 forEach로 map객체 순환하기 123456789101112131415let person = new Map().set('a', 1).set('b', 2);// for-of로 map 객체 순환하기for (let [key, value] of person) { console.log(key, value); // a 1 // b 2}// forEach에서는 key value의 순서가 다르다.person.forEach((value, key, map) =&gt; { console.log(key, value, map); // a 1 Map { 'a' =&gt; 1, 'b' =&gt; 2 } // b 2 Map { 'a' =&gt; 1, 'b' =&gt; 2 }}); 자바스크립트의 배열 메서드에만 존재하는 map, filtermethod는 Map()에 존재하지 않지만, 아래와 같이 우회 사용이 가능하다. 12345678910let person = new Map().set('a', 1).set('b', 2);// value가 1인 entry만 filterlet filteredPerson = new Map([...person].filter(([k, v]) =&gt; v === 1));console.log(filteredPerson.entries()); // { [ 'a', 1 ] }let appendCharPerson = new Map([...person].map(([k, v]) =&gt; [k + 'K', v + 1]));console.log([...appendCharPerson.entries()]); // [ [ 'aK', 2 ], [ 'bK', 3 ] ]","link":"/2021/04/01/202104/210401-javascript_map_and_set/"},{"title":"210402 Node.js TDD Practice 세 번째 이야기","text":"beforeEach, response status code and value, mockReturnValue(), _isEndCalled(), _getJSONData() beforeEach() 활용해서 공통 코드 처리하기여러개의 테스트 코드를 작성하면서 공통된 코드가 있다면 beforeEach 안에 작성을 해서 불필요한 코드의 반복을 줄여줄 수 있다.beforeEach의 위치는 describe의 내부와 외부 모두 가능하다. describe 단위로 공통된 코드는 describe 내부에 beforeEach를 작성해서 공통된 코드를 처리해주고, 모든 describe에 공통적으로 참조해야하는 공통 코드가 있다면 이는 describe 외부에 작성을 해서 작성한 모든 테스트 케이스에서 공통 코드를 참조할 수 있도록 해야한다.(아래 첨부한 첫 번째 노트를 참고) response 객체를 통해 상태값 전달하기request 객체의 body 속성으로부터 저장할 데이터에 대한 정보를 받아 데이터베이스에 저장을 했다면 이제 제대로 저장이 되었는지, 제대로 저장이 되었다면 상태값에 대한 정보를 보내줘야 한다.상태값은 res.status(201)과 같이 response 객체의 status로 상태코드를 인수로 넘겨준다. 테스트 코드에서는 mock response 객체의 statusCode 속성을 참고해서 전달된 상태값을 확인할 수 있다.expect(res.statusCode).toBe(201) response 객체를 통해 결과값 전달하기앞서 response 객체의 status로 상태코드를 인수로 넘겨서 상태값을 전달하였다.그렇다면 추가적으로 결과값을 전달해야될 때에는 어떻게 해야할까?(아래 첨부한 세 번째 노트 필기 참고) 테스트 코드에서는 mock response 객체의 _isEndCalled()메서드를 사용(node-mocks-http 제공)해서 res.status(201).send()에서 send()나 json()과 같이 추가적인 결과값이 전달되고 있는지 확인할 수 있다. 테스트 코드에서 전달된 결과값은 mock 함수의 mockReturnValue를 사용해서 반환되는 값을 임의로 지정해줄 수 있다. 반환값을 지정하고, mock request, response, next 객체를 create함수의 인수로 넣어 호출한 뒤에 mock response 객체의 _getJSONData()를 통해 전달한 JSON 타입의 결과값을 참조할 수 있다. ex) expect(res._getJSONData()).toStrictEqual([JSONData]) 실습 Repository → https://github.com/LeeHyungi0622/TDD-Practice-NodeJS 나중에 공부한 내용을 상기시키기 위해서 공부하면서 필기했던 노트를 첨부한다.","link":"/2021/04/02/202104/210402-nodejs-tdd-practice/"},{"title":"210403 Algorithm Consecutive number subsequence와 Number subsequence에 대한 이야기","text":"Consecutive number subsequence(연속 부분수열)과 Number subsequence(부분수열)이번 포스팅에서는 연속 부분수열과 부분수열에 대해서 이야기해보려고 한다. 위 두 개념에 대해서는 알고리즘 문제풀이를 하면서 접하게 되었는데, 그 풀이방법에 대해서 왠지 정리해두면 나중에 유용할 듯 싶어 블로그 포스팅하기로 했다. 우선 연속 부분수열에 대한 문제풀이에서 사용한 코드 패턴을 살펴보자.이름에서 예상할 수 있듯이 연속된 수들의 부분집합으로 이해할 수 있다. 만약에 N개의 숫자가 주어졌을때 수들의 합이 주어진 값인 S와 같은 연속 부분수열의 갯수를 구해야 한다면 어떻게 코드 구현을 해야할까?바로 아래와 같이 코드를 구현할 수 있다. 123456789101112131415161718const n = 8;const arr = [1, 2, 1, 3, 1, 1, 1, 2];const s = 6;let lt = 0;let sum = 0;let answer = 0;for (let rt = 0; rt &lt; n; rt++) { sum += arr[rt]; if (sum === s) answer += 1; while (sum &gt;= s) { sum -= arr[lt++]; if (sum === s) answer += 1; }}console.log(answer); 그렇다면 연속 부분수열이 아닌 부분수열이라면 어떤 식으로 코드를 구현해야 될까?이 부분은 백준 알고리즘의 1182번 문제를 풀이해보면서 코드를 작성해보았다. 1234567891011121314151617181920212223242526272829303132333435363738const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `5 0-7 -3 -2 5 8`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ const firstInput = input() .split(' ') .map((number) =&gt; +number); const n = firstInput[0]; const s = firstInput[1]; const numberList = input() .split(' ') .map((number) =&gt; +number); let answer = 0; let dp = new Array(n).fill(null).map((v, i) =&gt; [numberList[i]]); for (let i = 1; i &lt; n; i++) { for (let j = 0; j &lt; i; j++) { for (let k = 0; k &lt; dp[j].length; k++) { dp[i].push(dp[i][0] + dp[j][k]); } } } dp.forEach((row) =&gt; { row.forEach((col) =&gt; { if (col === s) answer += 1; }); }); console.log(answer);}","link":"/2021/04/03/202104/210403-Algorithm_consecutive_number_subsequence_and_number_subsequence_problem_solving/"},{"title":"210404 JavaScript module pattern","text":"이번 포스팅에서는 JavaScript에서 문제해결 패턴으로 가장 많이 사용되는 module pattern에 대해서 포스팅을 하려고 한다.이 module pattern은 JavaScript의 코드 관리 기법 중 하나로 함수로 데이터를 감추고, 모듈 API를 담고 있는 객체를 반환하는 형태로 코드를 작성하며, JavaScript의 특성상 객체를 핸들링하기 위한 방법론 중 하나이다.JavaScript의 모듈 패턴은 유효범위를 지정하는 언어와 같이 private, public 등의 캡슐화를 사용하는 방법이라고 볼 수 있다.그리고 module pattern에는 두 가지 패턴이 있는데 임의 모듈 패턴과 즉시 실행 함수(IIFE) 모듈 패턴(Singleton instance)이 있다. 아래 임의 모듈 패턴과 즉시 실행함수 모듈 패턴(IIFE)를 보면 가장 첫 줄에 namespace pattern이 사용이 되었는데 이는 자바스크립트에서 함수 또는 변수 객체를 다룰때 중복된 이름의 사용으로 인한 문제를 방지하기 위한 것이다.global영역에 객체 고유의 영역을 지정하고 변수와 함수의 할당을 해당 namespace의 하위로 두게 해서 중복된 이름으로 인한 오류를 예방하는 방법이다.(즉 모듈패턴이란 이 namespace pattern에 언어적 유효범위를 추가해놓은 것이라고 이해하면 된다) namespace 패턴1234567var App = App || {}; // declare namespaceApp.getName = function () { return 'hyungilee';};App.hello = function () { return 'hello';}; 임의 모듈 패턴 : 임의 함수를 호출하여 생성하는 모듈여러 객체가 필요한 경우에 사용되는 패턴 방식이다. 12345678910111213141516171819202122// namespace 만들기var App = App || {};// namespace에 함수를 추가. 의존성있는 God function을 주입// Person이라는 module(함수)를 넣어준다.App.Person = function (God) { // God module이 name을 생성하는 역할을 한다. // God module을 통해 생성한 name값을 변수에 초기화한다. var name = God.makeName(); // API 노출 (getter, setter 함수를 가지는 객체를 반환) return { getName: function () { return name; }, setName: function (newName) { name = newName; } };};const person = App.Person(God);person.getName(); 즉시 실행 함수(IIFE) 모듈 패턴(Singleton instance) : 즉시 실행 함수(IIFE) 기반의 모듈singleton일 경우(단일 객체가 필요한 경우)에 사용되는 패턴 방식이다. 즉시 실행 함수(IIFE) 모듈 패턴(싱글톤 인스턴스) 예제1 12345678910111213var App = App || {};App.Person = (function () { let name = ''; return { getName(God) { name = name || God.makeName(); return name; }, setName(newName) { name = newName; } };})(); //함수 선언 즉시 실행한다. singleton 즉시 실행 함수(IIFE) 모듈 패턴(싱글톤 인스턴스) 예제2 12345678910111213141516171819var Person = { name: 'hyungi', greeting: 'hi' };var App = (function (person) { var sayHi = person.greeting; var sayHiToFriend = person.greeting + ' ' + person.name; var printHi = function () { return sayHi; }; var printHiToFriend = function () { return sayHiToFriend; }; return { sayHi: sayHi, sayHiToFriend: printHiToFriend };})(Person);console.log(App.sayHi());console.log(App.sayHiToFriend()); 위와같이 모듈을 생성하는데에는 두 가지 원칙이 필요하다. 단일 책임 원칙에 따라 모듈은 한 가지 역할만 한다.해당 역할만 집중함으로서 모듈을 더욱 튼튼하고 견고하게 만들고 테스트하기 쉬워진다. 모듈 자신이 사용할 객체가 있다면 의존성 주입형태로 제공한다.팩토리 주입형태로 제공하기도 하며 테스트하기 쉬워진다. 모듈을 작성함으로써 반환하는 return문을 통해 공개될 영역과 내부적으로 처리할 영역을 구분할 수 있게 된다.","link":"/2021/04/04/202104/210404-js_module_pattern/"},{"title":"210404 Sliding window + Map + Two pointer algorithm 문제풀이","text":"이번 포스팅에서는 Sliding window, Map, Two pointer algorithm을 복합적으로 사용하여 풀이한 알고리즘 문제에 대해서 정리를 해보려고 한다. 효율성을 전혀 고려하지 않은 이중 for문의 사용을 자제하고 앞의 세 가지 개념을 활용하여 코드를 구현한다면 효율성을 극대화시켜서 코드를 작성할 수 있을 것이라고 생각한다. 우선 하나의 문제를 예시로 내가 처음에 구현한 코드와 다른 사람이 구현한 코드 두 가지를 비교분석해보자. 문제는 문자열 S와 T가 주어졌을때 문자열 S에서 T문자열과 아나그램이 되는 부분 문자열의 갯수를 출력하는 문제이다. 이 문제를 처음 보았을때 Sliding window, Map, Slice method를 사용하여 문제를 풀이해야 겠다고 생각했고 아래와 같이 코드를 작성했다. 우선, 아나그램인지 아닌지 판별하기 위한 개별함수를 선언(checkAnagram() function)한다.그 다음에 초기에는 입력받은 문자열(is)를 검색 문자열(ss)의 길이만큼 slice해서 확인용 문자열 변수에 별도로 저장을 해준다. 저장된 문자열 변수와 검색 문자열은 만들어 둔 checkAnagram() function에 인수로 넣어 아나그램인지 아닌지 판별한다.이제 입력받은 문자열의 검색 문자열의 길이부터 입력 문자열(is)의 길이까지 순회를 하며 추가적으로 문자를 붙여주고, 붙인 다음에는 첫 번째 문자를 제외한 나머지를 문자열을 slice해서 checkAnagram() function을 통해 아나그램인지 아닌지 판별한다. 나는 처음에 이런 방식으로 코드 구현을 해보았다. 1234567891011121314151617181920212223242526272829303132333435363738394041const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `bacaAacbaabc`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ // input stirng const is = input(); // search string const ss = input(); const checkAnagram = (f, s) =&gt; { const fm = new Map(); for (let w of f) { if (fm.has(w)) fm.set(w, fm.get(w) + 1); else fm.set(w, 1); } for (let w of s) { if (!fm.has(w) || fm.get(w) === 0) return false; else fm.set(w, fm.get(w) - 1); } return true; }; let count = 0; let checkString = is.slice(0, 3); // console.log(checkString); if (checkAnagram(ss, checkString)) count += 1; for (let i = ss.length; i &lt; is.length; i++) { checkString += is[i]; const slicedString = checkString.slice(i - ss.length + 1, i + 1); //console.log(slicedString); if (checkAnagram(ss, slicedString)) count += 1; } console.log(count);} 그 다음에는 다른 사람이 작성한 코드인데, 이 사람은 내가 사용한 slice() method를 사용하지 않고, Two pointer algorithm을 사용하였다.각 function과 코드가 어떤 의도로 작성되었는지 분석을 하면서 comment를 작성해보았다.이 코드는 compareMaps()라는 별도의 함수를 만들어서 아나그램인지 아닌지 판별을 하였고, 비교 문자열에 문자를 추가하고 비교하고 빼는 과정을 반복 순회하며 처리를 하였다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 두 개의 map을 인자로 받아서 사이즈를 비교한다. 두 map의 key속성 길이(size)가 다르다면// 비교하는 것이 의미가 없기 때문에 false를 반환한다.// 길이가 같다면 첫 번째 인자로 받은 map1의 key, value를 순회하며 map2에 같은 key가 존재하는지,// 존재한다면 해당 key값이 가지는 value가 map2에서의 key값의 value와 일치하는지 확인한다.function compareMaps(map1, map2) { if (map1.size !== map2.size) return false; for (let [key, val] of map1) { if (!map2.has(key) || map2.get(key) !== val) return false; } return true;}function solution(s, t) { let answer = 0; let tH = new Map(); let sH = new Map(); for (let x of t) { if (tH.has(x)) tH.set(x, tH.get(x) + 1); else tH.set(x, 1); } let len = t.length - 1; // s문자열에서 index 0부터 비교문자열 t의 길이-1만큼(우선 앞의 2개만) sH에 업데이트해준다. for (let i = 0; i &lt; len; i++) { if (sH.has(s[i])) sH.set(s[i], sH.get(s[i]) + 1); else sH.set(s[i], 1); } let lt = 0; // 빼고 =&gt; 추가하고 =&gt; 비교 (반복) for (let rt = len; rt &lt; s.length; rt++) { // 새로 추가하는 문자(처음에는 문자 'c')가 기존의 sH에 존재한다면 해당 요소의 값을 1증가 시켜준다. if (sH.has(s[rt])) sH.set(s[rt], sH.get(s[rt]) + 1); // 그렇지 않다면 default value를 1로 해서 요소를 추가한다. else sH.set(s[rt], 1); // compareMaps에 sH와 tH를 인수로 넘겨서 호출을 해서 true를 반환할 경우 answer값을 1증가 시킨다. if (compareMaps(sH, tH)) answer++; // sH에서 lt index위치에 해당하는 key의 값에서 1만큼 빼준다. (오른쪽으로 sliding window) sH.set(s[lt], sH.get(s[lt]) - 1); // 만약에 값을 뺀 뒤에 해당 lt index위치의 값이 0이 되는 경우, sH Map에서 해당 값을 delete해준다. if (sH.get(s[lt]) === 0) sH.delete(s[lt]); // lt index의 값을 1 증가시켜준다. lt++; } return answer;}let a = 'bacaAacba';let b = 'abc';console.log(solution(a, b));","link":"/2021/04/04/202104/210404-sliding_window_and_map_and_two_pointer_algorithm/"},{"title":"210404 Vanilla JavaScript TDD Practice","text":"210405 Update 이번 포스팅에서는 TDD방식으로 Vanilla JavaScript를 사용해서 개발을 하는 방법에 대해서 실습한 내용을 기반으로 블로그 포스팅을 해보려고 한다. 처음에는 이 TDD방식이 익숙하지 않아서 느릴지 모르지만 이런 과정이 견고한 Logic을 만들게 되고, 결국에는 개발속도가 빨라지는 결과를 낳게 된다고 한다.여기서 포인트는 익숙해지는 것! 개발자 커뮤니티에서 어떤 개발자 분이 말씀하셨는데 &quot;개발의 가장 좋은 실력향상 방법은 반복&quot;이라고 하셨다. 누군가로부터 무언가 새로운 것을 배웠을 당시에 이해를 한 것처럼 착각하고 마치 내 것이 된거마냥 생각하는 사람들이 많은 것 같다. 이러한 개인적인 반복학습 없이 한 두 번 더 본다고 절대 내 것이 되었다고 볼 수 없다. 내가 이렇게 블로그에 배웠던 내용을 정리하고, 필기했던 노트를 첨부하는 이유는 반복학습을 위해서다.이 TDD 방식 또한 처음에 적용할때 시간이 오래 걸린다고 생각하지 말고 스스로 생각하고 반복을 하면서 내 것으로 만들도록 노력을 해야겠다. 본론개선전 코드아래의 코드에는 문제가 많다. Increase button에는 여러 관심사들이 혼재되어 있다. 단순히 화면에 버튼을 출력하지 않고, click event가 binding되어있다.또한 JavaScript 코드를 보면 count라는 변수를 전역 스코프에 작성을 하였다. 이는 변수이름이 충돌할 위험을 증가시킨다.그리고 카운트 수를 출력할 span 태그의 id를 hard coded한 방식으로 작성을 해서 DOM element를 가져오고 있다. 만약에 markup이 변경된다면 변경된 id에 맞춰서 작성한 id를 JavaScript 코드에서 수정해줘야 한다. 아래의 간단해보이는 코드에는 위와같은 복합적인 문제들이 혼재되어있다. 123456789&lt;button onclick=&quot;counter++; countDisplay()&quot;&gt;Increase&lt;/button&gt;&lt;span id=&quot;counter-display&quot;&gt;0&lt;/span&gt;&lt;script&gt; var count = 0; function countDisplay() { var el = document.getElementById('counter-display'); el.innerHTML = count; }&lt;/script&gt; 개선후 코드 그럼 아래의 개선된 코드를 살펴보자. index.html 12345678910111213&lt;button id=&quot;btn-increase&quot;&gt;Increase&lt;/button&gt;&lt;script src=&quot;ClickCounter.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;ClickCounterView.js&quot;&gt;&lt;/script&gt;&lt;script&gt; (() =&gt; { const clickCounter = App.ClickCounter(); const updateEl = document.querySelector('#counter-display'); const triggerEl = document.querySelector('#btn-increase'); // 두 객체에 대해 역할 위임을 하였다. const view = App.ClickCounterView(clickCounter, { updateEl, triggerEl }); view.updateView(); })();&lt;/script&gt; ClickCounter.js 기존에 전역변수로 선언해서 변수이름이 충돌할 위험을 높였던 count 변수를 ClickCounter 함수내에서 value함수로 선언하였다.이렇게 하면 value 변수는 ClickCounter라는 함수 스코프내에서만 유효한 변수이기 때문에 이름이 충돌할 위험은 적어진다. 123456789101112var App = App || {};App.ClickCounter = () =&gt; { let value = 0; return { getValue() { return value; }, increase() { value++; } };}; 그리고 기존에 hard coded한 방식으로 span 태그의 id를 작성한 방식을 options라는 객체를 parameter로 넘겨받아서 접근하고 있기 때문에 만약에 markup에서 id에 대한 변경이 있다고 하더라도 ClickCounterView.js 파일의 JavaScript 코드를 변경하지 않아도 된다.이는 화면과 JavaScript 코드가 분리되었기 때문에 가능하다. (유연한 코드) 아래의 view 객체 안의 updateView(), increaseAndUpdateView(), event binding 부분을 보면, 각 각 단일 책임의 원칙에 따라 기능이 분리되어 작성되어있음을 알 수 있다.이렇게 코드를 작성하게 되면 유지보수가 간결해진다.ClickCounterView.js 12345678910111213141516171819202122232425262728293031var App = App || {};App.ClickCounterView = (clickCounter, options) =&gt; { if (!clickCounter) throw new Error(App.ClickCounterView.message.noClickCounter); if (!options.updateEl) throw new Error(App.ClickCounterView.message.noUpdateEl); if (!options.triggerEl) throw new Error(App.ClickCounterView.message.noTriggerEl); const view = { updateView() { options.updateEl.innerHTML = clickCounter.getValue(); }, increaseAndUpdateView() { clickCounter.increase(); this.updateView(); } }; options.triggerEl.addEventListener('click', () =&gt; { view.increaseAndUpdateView(); }); return view;};App.ClickCounterView.message = { noClickCounter: 'clickCounter를 주입해야 합니다.', noUpdateEl: 'updateEl을 주입해야 합니다.', noTriggerEl: 'triggerEl을 주입해야 합니다.'}; 실습 Repository → https://github.com/LeeHyungi0622/TDD-Practice-VanillaJS 나중에 공부한 내용을 상기시키기 위해서 공부하면서 필기했던 노트를 첨부한다.","link":"/2021/04/04/202104/210404-vanilla_javascript_tdd/"},{"title":"210408 React 상태관리 (작성중...)","text":"React를 하면서 상태관리라는 말을 자주 접하게 된다.이번 포스팅에서는 상태란 무엇이고 그 상태를 관리한다는 것은 무엇을 의미하는지, 그리고 React에서 상태관리를 하는 다양한 방법들을 직접 사용해보고 그 특징들을 비교해서 정리해보려고 한다. 우선 상태과 상태관리에 대한 개념부터 이해해보자. 상태(Status)? 간단히 말하자면 상태란 데이터로써 이해할 수 있는 것 같다.객체지향 프로그래밍에서 프로그램의 기본 단위는 객체이고 객체간의 상호작용을 통해서 프로그램이 구현된다. 이때 각 객체들의 상호작용에 있어, 각 객체들이 가지고 있는 데이터가 전달되는데, 여기서 말하는 전달되는 데이터들을 상태로써 이해를 하면 좋을 것 같다. 상태관리(Status management)? 앞서 정리해 본 상태(Status)의 개념에서 비춰볼때 상태관리란 객체들간에 전달되는 데이터에 맞게 적절하게 UI를 설계하고 구현하는 것이 상태관리라고 볼 수 있을 것 같다.예를들어 하나의 웹 어플리케이션의 페이지에는 로그인 상태, 사용자 이름, 콘텐츠 정보 등 다양한 데이터들이 존재한다고 가정한다면, 이러한 다양한 데이터들을 UI에 따라 적절하게 배치하고 표시하는 것이 상태관리가 아닐까 생각된다. React를 사용해서 프론트엔드 개발을 할때 기본 단위는 재사용성 있는 컴포넌트인데, 이 컴포넌트 간의 상태 공유의 형태에 따라 local state와 global state를 적절하게 구분해서 설계해야 한다.설계를 할때에는 상태를 일관적인 형태로 유지해서 무결성을 지켜야 한다. 즉, 공유되는 특정 데이터가 업데이트된다면 해당 데이터를 참조하고 있는 다른 화면에서도 업데이트된 데이터를 표시해야 된다는 말이다. 상태의 저장과 사용 앞서 상태를 관리한다는 것의 의미가 각 객체들간에 전달되는 데이터에 맞게 적절하게 UI를 설계하고 구현하는 것이라고 했는데, UI에서 전달되는 데이터에 맞게 설계를 하고 구현하기 위해서는 상태를 어떻게 저장하고 사용할 것인지에 대한 판단도 중요하다. 전역적으로 redux와 같은 store에 저장 웹 소켓으로 관리 (Backend와의 실시간 소통이 중요한 경우) Cookie나 LocalStorage에 저장 (브라우저가 종료된 후에도 상태유지) 이후의 내용부터는 각 상태관리 방법을 다양하게 사용해보면서 구체적으로 어떤 차이점과 특징이 있는지 정리를 할 예정이다. 단순히 익숙함에 의해 특정 상태관리 방법을 고수한다면 프로젝트에 따라 적절한 상태관리 방법을 사용할 수 없다고 생각한다.구체적으로 어떤 상태관리 방법은 어떤 특징이 있으며 어떤 상황에서 사용하면 좋은지 구체적인 예시를 들어서 한 번 작성해보려고 한다.","link":"/2021/04/08/202104/210408-React-status/"},{"title":"210408 React project (작성중...)","text":"React를 사용하는 이유? 사용자 경험(UX)웹 페이지에서 모바일 앱과 같은 사용자 경험을 준다. 재사용 컴포넌트웹 사이트의 내부를 보면 구조상 중복되는 부분이 많다.이러한 중복된 구조를 코드로 공통화를 시켜서 내부 표시되는 콘텐츠만 바꿔주는 방식으로 처리한다. (유지보수의 측면에서도 좋다) 데이터 - 화면 일치데이터와 화면 페이지의 싱크를 맞추는 것이 어렵다. 페이스북과 같은 웹 서비스에서 좋아요 버튼을 클릭하는 경우, 해당 게시글을 전부 업데이트하지 않고, 좋아요 부분만 업데이트해야 퍼포먼스상 문제가 없다. 데이터를 화면에 반영을 할때 데이터와 화면의 데이터 Sync를 맞출때 좋다. Webpack우선 자바스크립트 파일에 대한 이해가 없기 때문에 Webpack이 마법을 부린다는 소리를 하는데 마법을 부리는 것이 아니라 단지 React를 자바스크립트 파일로 만들어주는 것이다.웹팩은 쪼개진 자바스크립트 파일을 HTML이 실행할 수 있는 자바스크립트 파일로 합쳐준다는 개념이다.이 Webpack이라는 것의 필요성에 대해서 이해하기 위해 CRA없이 React프로젝트를 구성해봐야 한다. 프로젝트 기본 폴더구조","link":"/2021/04/08/202104/210408-React_project/"},{"title":"210408 CRA없이 React 프로젝트 시작하기","text":"첫번째 코드에서는 React 관련된 라이브러리 두 개를 script로 포함시켜주었다.첫 번째로 import해 준 라이브러리는 React의 핵심부분과 관련된 라이브러리이며, 두 번째로 import해 준 라이브러리는 React 코드를 웹에 표시해주는 역할을 해주는 라이브러리이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot; /&gt; &lt;!-- React 관련 라이브러리 --&gt; &lt;!-- React의 핵심적인 코드 --&gt; &lt;script crossorigin src=&quot;https://unpkg.com/react@16/umd/react.development.js&quot; &gt;&lt;/script&gt; &lt;!-- React 코드를 웹에 붙여주는 역할 --&gt; &lt;script crossorigin src=&quot;https://unpkg.com/react-dom@16/umd/react-dom.development.js&quot; &gt;&lt;/script&gt; &lt;title&gt;Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;root&quot;&gt;&lt;/div&gt; &lt;!-- 결과 --&gt; &lt;script&gt; const e = React.createElement; class LikeButton extends React.Component { constructor(props) { super(props); this.state = { liked: false }; } // 버튼을 만들겠다. render() { // type, property, textNode return e( 'button', { onClick: () =&gt; { this.setState({ liked: true }); console.log('clicked'); }, type: 'submit' }, this.state.liked === true ? 'Liked' : 'Like' ); // &lt;button type=&quot;submit&quot; onclick=&quot;onclick&quot;&gt;Like&lt;/button&gt; } } &lt;/script&gt; &lt;script&gt; // 만든 버튼을 그린다. // 실제 화면에 렌더링을 해주는 역할(DOM) ReactDOM.render(e(LikeButton), document.querySelector('#root')); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 하지만 위의 코드에서 React.createElement로 버튼 요소를 만드는 부분은 가독성이 좋지 않다. 따라서 React 팀에서는 JavaScript에서 JSX를 사용할 수 있도록 개선을 하였다.JSX를 사용할 수 있게 되면서 아래의 코드와 같이 React.createElement를 사용해서 버튼을 생성하지 않고, 바로 &lt;button&gt;과 같은 HTML 태그를 사용하여 가독성 좋은 코드를 작성할 수 있게 되었다. 이러한 JavaScript 내에서 JSX문법을 사용하기 위해서는 선행되어야 하는 작업이 있는데 바로 Babel을 import시켜줘야 한다.원래 JavaScript에서 HTML태그를 쓰는 것은 문법상 맞지 않지만 Babel이 작성한 JSX문법의 코드를 createElement로 convert해준다. import한 babel을 사용하기 위해서는 script 태그의 type의 속성으로 text/babel을 넣어줘야 한다. 간단하게 JavaScript상에서 최신 문법을 사용하고 싶을 때에는 &lt;script src=&quot;https://unpkg.com/babel-standalone@6/babel.min.js&quot;&gt;&lt;/script&gt;를 넣어주면 된다. 이렇게 되면 JavaScript ES6 문법의 코드가 각 브라우저에서 동작할 수 있도록 코드 변환을 해준다. 다만, babel에 대한 세부설정을 하기 위해서는 별도의 Webpack이나 Babel 툴이 필요하다.그리고 부가적으로 최신 메서드나 객체를 사용하고 싶은 경우에는 babel polyfill을 추가해줘야 한다. &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/babel-polyfill/7.12.1/polyfill.min.js&quot; integrity=&quot;sha512-uzOpZ74myvXTYZ+mXUsPhDF+/iL/n32GDxdryI2SJronkEyKC8FBFRLiBQ7l7U/PTYebDbgTtbqTa6/vGtU23A==&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;!-- React 관련 라이브러리 --&gt; &lt;!-- React의 핵심적인 코드 --&gt; &lt;script crossorigin src=&quot;https://unpkg.com/react@16/umd/react.development.js&quot;&gt;&lt;/script&gt; &lt;!-- React 코드를 웹에 붙여주는 역할 --&gt; &lt;script crossorigin src=&quot;https://unpkg.com/react-dom@16/umd/react-dom.development.js&quot;&gt;&lt;/script&gt; &lt;!-- Babel --&gt; &lt;script src=&quot;https://unpkg.com/babel-standalone@6/babel.min.js&quot;&gt;&lt;/script&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;root&quot;&gt;&lt;/div&gt; &lt;!-- 결과 --&gt; &lt;!-- JavaScript 내부에서 실험적으로 html을 사용할 수 있도록 한다. --&gt; &lt;script type=&quot;text/babel&quot;&gt; const e = React.createElement; class LikeButton extends React.Component { constructor(props) { super(props); this.state = { liked: false, }; } // 버튼을 만들겠다. render() { // type, property, textNode // JS에서 html을 쓰는 것은 문법상 맞지 않다. // 따라서 babel을 사용해야 한다. (최신문법, 실험적인 문법을 자바스크립트에서 쓸 수 있도록 도와준다) return &lt;button type=&quot;submit&quot; onClick={() =&gt; { this.setState({ liked: true })}}&gt; {this.state.liked ? 'Liked' : 'Like'} &lt;/button&gt;; } } &lt;/script&gt; &lt;script type=&quot;text/babel&quot;&gt; // 만든 버튼을 그린다. // 실제 화면에 렌더링을 해주는 역할(DOM) ReactDOM.render(&lt;div&gt;&lt;LikeButton /&gt;&lt;LikeButton /&gt;&lt;LikeButton /&gt;&lt;LikeButton /&gt;&lt;/div&gt;, document.querySelector('#root')); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; (1) 실제로 실무에서는 생성자(Constructor)내부에서 state를 정의하지 않고, 별도로 분리를 해서 작성을 해준다. (2) class component의 render 함수의 내부에서는 하나의 component를 반환해줘야 한다. 여러 개의 component를 전달하는 경우에는 &lt;div&gt; 태그로 묶어서 반환을 해줘도 되지만 불필요한 &lt;div&gt; 태그로 묶어주면 실제 화면에 렌더링 될 때에 불필요한 &lt;div&gt;태그가 포함이 되기 때문에 좋지 않다.따라서 React에서 제공해주는 &lt;&gt;&lt;/&gt; or &lt;React.Fragment&gt;&lt;/React.Fragment&gt; (Fragment)를 사용하면 불필요한 태그의 사용없이 여러 개의 component를 묶어서 반환해줄 수 있다. (3) class 내에서 정의한 함수의 경우에는 함수 선언형으로 작성해주지 않고 화살표 함수(arrow function)을 써주도록 한다. 함수 내부에서의 this와 화살표 함수 내부에서의 this의 의미가 달라지기 때문이다.이전에 공부를 했을때 화살표 함수는 직접적으로 this를 binding할 수 없고 내부에 this를 정의해서 사용하는 경우에는 상위 스코프의 this를 가르키게 된다. (4) 현재 상태 값과 미래의 상태 값을 구분해주기 위해서 class component의 내부에 정의해준 setState의 내부에 callback 함수로 처리를 해준다.setState는 비동기이기 때문에 하나의 이벤트에서 여러 개의 setState가 실행이 되어도 일괄적으로 딱 한 번만 처리된다. 그래서 setState의 내부에 콜백함수를 넘겨서 처리해준다.만약에 setState의 내부에서 this.state.[상태요소]가 사용이 된다면 콜백함수로 처리하도록 한다.이렇게 처리를 하는 이유는 효율적으로 화면을 렌더링하기 위함이다. 12345this.setState((prevState) =&gt; { return{ console.log(prevState); }}); (5) 특정 DOM element를 선택하기 위해서는 선택하고자 하는 element의 속성으로 ref={(c) =&gt; {this.input=c}}와 같이 ref 속성을 넣어준다. (input은 클래스 컴포넌트의 내부에 선언해준 변수이다) (6) 함수형 컴포넌트는 본래 상태관리가 불필요한 Component를 만들때 사용했었다. 하지만 요청에 의해 함수형 컴포넌트의 내부에서도 상태관리가 가능하게 되었고, 그것이 바로 React Hooks이다. React.useState(상태값 관리), React.useRef(특정 DOM 요소 선택) 함수형 컴포넌트에서는 별도의 render함수가 없기 때문에 상태값이 바뀌게 되면 함수 자체가 통째로 다시 실행이 된다. 그래서 클래스 컴포넌트와 비교했을때 조금 느릴 수 있다.(4)에서 언급을 했듯이 setState는 비동기 함수이다. 하나의 이벤트 내에서 여러개의 setState 함수들을 사용해도 일괄적으로 딱 한 번만 실행된다. 만약에 이전 상태값의 참조가 필요한 경우에는 setState 함수 내부에서 함수형 업데이트를 해주게 되면 된다. 이전에 React를 공부하면서 효율적인 렌더링을 위한 useState()의 함수형 업데이트에 대해서 정리를 했었는데, 아래 포스팅 글을 같이 참고하도록 하자. https://leehyungi0622.github.io/2021/03/18/202103/210318-React-review_study/ (7) jsx 작성시에는 태그의 class속성을 넣어줄때 className으로, label의 for속성을 넣어줄때는 htmlFor속성으로 넣어준다. 이는 class 작성시에 사용되는 키워드와 for-loop의 키워드와 같은 이름이기 때문에 구분을 해주기 위함이다.","link":"/2021/04/08/202104/210408-Use_React_without_CRA/"},{"title":"210409 Webpack에 대해서 이해하기","text":"웹팩은 왜 사용하는 걸까?실무에서 개발을 할때에는 여러개의 컴포넌트들을 복합적으로 사용한다. 만약에 이러한 컴포넌트들을 하나의 자바스크립트 파일에 작성을 해준다면 어떻게 될까?아마 1000줄 10000줄? 그 이상이 될 것이다. 페이스북의 컴포넌트가 대략 2만개라고 하니 그 줄 수는 어마무시할 것이다.이렇듯 한 개의 자바스크립트 파일에 모든 컴포넌트들을 일괄작성하게 되면 나중에 유지보수도 어려워지기 때문에 각 각의 컴포넌트들은 개별 자바스크립트 파일로써 작성되어야 한다. 이렇게 개별로 작성된 자바스크립트 파일들은 HTML파일에 의해 읽혀질때에는 하나의 자바스크립트 파일로 합쳐져서 읽혀지게 되는데, 이러한 여러개의 자바스크립트 파일들을 하나의 자바스크립트 파일로 만들어주는 기술이 바로 웹팩(Webpack)이다. 12345&lt;body&gt; &lt;div id=&quot;root&quot;&gt;&lt;/div&gt; &lt;!-- 결과 --&gt; &lt;script src=&quot;./dist/app.js&quot;&gt;&lt;/script&gt;&lt;/body&gt; 위의 코드에서 dist라는 폴더 내부의 app.js파일을 script태그의 src 속성으로 불러오고 있는데, 이 파일이 바로 webpack 기술로 여러개의 자바스크립트 파일들을 합쳐서 만들어낸 하나의 자바스크립트 파일이다. 이 웹팩(Webpack)이라는 기술은 소크라라는 개발자에 의해 개발이 되었는데, babel도 적용을 할 수 있고, 쓸데없는 코드(console.log와 같은)도 제거를 할 수 있다. 이 웹팩을 이해하기 위해서는 node를 이해해야 하는데, node란 V8 자바스크립트 엔진에 내장된 자바스크립트 실행환경이다. 웹팩도 자바스크립트로 만들어졌기 때문에 웹팩을 돌리기 위한 이 자바스크립트 실행환경을 이해해야 되는 것이다. 웹팩의 사용 npm init npm i react react-dom앞서 실습을 해봐서 알겠지만 기본적으로 react를 사용하기 위해서는 react의 기본적인 것들을 담고 있는 라이브러리(react)와 작성한 컴포넌트를 화면에 그리기 위한 react-dom 라이브러리가 필요하다. npm i -D webpack webpack-cli프로젝트에서 webpack을 사용하기 위해서는 webpack과 webpack-cli를 설치해야 한다. webpack은 실제 서비스시에는 필요가 없고 개발시에만 필요하기 때문에 DevDependency로 설치를 해준다. webpack.config.js 파일 생성webpack.config.js 파일내부에 webpack에 대한 구체적인 설정에 대한 내용을 작성해준다. jsx, js 확장자의 차이 자바스크립트 파일 내에서 jsx 문법을 사용했으면 jsx로 작성을 해주고, 그렇지 않으면 js 파일 확장자를 붙여주도록 한다. 이러한 구분으로 확장자를 작성해주면, 개발자가 파일 확장자만 보더라도 해당 자바스크립트 파일이 jsx 문법이 포함되어있는 리액트 전용 파일이라고 이해를 할 수 있다. HTML에서 하나의 자바스크립트 코드웹팩을 사용하면 여러개의 자바스크립트 컴포넌트 파일들을 하나의 자바스크립트 코드로 만들어서 HTML에 넣어줄 수 있다. Node에서 경로를 쉽게 조작하기 위해 제공하는 pathNode에서 시스템의 경로를 쉽게 조작할 수 있도록 path라는 모듈을 제공해준다. webpack의 설정파일(webpack.config.js)에서 합쳐진 자바스크립트 파일을 생성할 폴더의 위치를 path 모듈을 사용해 손쉽게 지정할 수 있다. 1234output: { path: path.join(__dirname, 'dist'), filename: 'app.js'}, // 출력 (app.js) 웹팩 설정파일(webpack.config.js) - 실행 순서로 코드 작성1234567891011121314151617181920212223242526272829303132333435const path = require('path');module.exports = { // 웹팩 설정 이름 name: 'webpack setting name', mode: 'development', //실제 서비스 시에는 production으로 작성 devtool: 'eval', // 빠르게 실행하기 위한 코드, // 개발시에는 eval // 실제 서비스 시에는 hidden-source-map //entry section내에 작성할 파일들의 확장자 생략을 위해서 resolve 옵션을 사용 resolve: { extensions: ['.js', '.jsx'] }, // 이제 webpack을 이용해서 합칠 자바스크립트 파일들을 지정해준다. // 만약에 서로서로 포함관계에 있는 자바스크립트 파일이라면 가장 최상단의 파일만 작성해주면 된다. entry: { app: ['./client'] }, // entry에서 지정한 파일을 읽고 module에서 정의한 여러개의 규칙을 적용한다. module: { rules: [ { // 정규표현식으로 js와 jsx파일에 룰을 적용한다고 작성 test: /\\.jsx?/ } ] }, // 합칠 자바스크립트 파일들을 읽고 moudle에서 정의한 여러개의 규칙들을 적용하고 // 결과로 나온 파일을 app.js라는 이름과 확장자로 현재위치에서 dist 폴더를 생성해서 하위에 파일을 넣어준다. output: { path: path.join(__dirname, 'dist'), filename: 'app.js' }}; 위와같이 웹팩 설정파일을 작성해주면 jsx관련 에러가 발생한다. 이전에 babel이 jsx코드를 React.createElement로 코드변환을 해준다고 했는데, 웹팩에는 이 babel에 대한 설정이 없기 때문에 별도로 설정을 해줘야 한다. 우선 첫 번째로 필요한 library들을 설치해준다.(1) @babel/core : babel의 기본적인 것들을 포함하며, 최신문법을 변환해주는 역할을 한다.(2) @babel/preset-env : 현재 사용하는 브라우저의 환경에 맞게 최신 문법 코드를 변환해준다.(3) @babel/preset-react : jsx를 지원한다.(4) babel-loader : babel과 webpack을 연결시켜주는 역할을 한다. 위의 모듈들을 설치한 뒤에는 module의 rules의 내부에 추가 규칙을 작성해줘야 한다. 123456789101112.......module: { rules: [{ test: /\\.jsx?/, loader: 'babel-loader', options: { presets: ['@babel/preset-env', '@babel/preset-react'], plugins: ['@babel/plugin-proposal-class-properties'] } }]},....... 웹팩 실행 package.json에서 “scripts”에서 코드를 추가한다.npm run dev 123&quot;scripts&quot;: { &quot;dev&quot;: &quot;webpack&quot;} npx webpack을 사용해서 webpack 실행 실행의 결과로 output section에서 지정한 경로의 폴더에 지정한 파일명과 확장자로 파일이 생성된다.(파일의 내용은 난독화되어있다) 이렇게 직접 웹팩에서 필요한 부분만 최소한으로 작성을 해주면 굳이 CRA를 사용하지 않아도 React 프로젝트를 시작할 수 있다.최소한의 것들만 설치하고 에러가 나면 추가적으로 필요한 것을 설치해주는 것이 좋다. Preset 구체적으로 알아보기앞서 웹팩 설정을 작성할때 module section → rules section → options section → presets section을 살펴보면 아래와 같이 간략하게 배열의 내부에 설치한 모듈을 작성해주었다. 1presets: ['@babel/preset-env', '@babel/preset-react']; 하지만 preset은 여러개의 모듈이 모여서 구성되어 있기 때문에 구체적으로 옵션을 달아 지정해 줄 수 있다.예를들어 위에서 지정한 @babel/preset-env의 경우에는 다양한 브러우저의 버전 호환성을 지원하는 모듈이다.따라서 아래와 같이 구체적으로 어떤 버전의 브라우저의 호환성을 지원할 것인지 명시해 줄 수 있다. 123456789101112presets: [ [ '@babel/preset-env', { targets: { browsers: ['&gt; 5% in KR', 'last 2 chrome versions'] }, debug: true } ], '@babel/preset-react']; 구체적으로 지원할 브라우저의 종류를 지정해주는 이유는 구체적으로 지원할 브라우저를 지정하지 않을 경우, webpack에서 자체적으로 하는 작업량이 많아지기 때문에 어플리케이션 개발시에 회사에서 지원할 브라우저의 버전을 구체적으로 명시해주는 것이 좋다.browsers의 옵션은 아래의 사이트를 참고하도록 하자.https://github.com/browserslist/browserslist 웹팩의 설정은 흐름대로 기억하고 작성하자웹팩의 공식 홈페이지에 들어가보면 왼쪽 상단에 아래와 같은 순서로 카테고리가 적혀져 있다.https://webpack.js.org/concepts/ ConceptsEntryOutputLoadersPluginsModeBrowser CompatibilityEnvironment 웹팩 설정에 있어 가장 기본이 되는 항목들을 아래와 같은 기본 틀로써 기억하고 작성하도록 하자. 1234567891011121314151617181920212223242526272829303132const path = require('path');const webpack = require('webpack');module.exports = { // (1) 웹팩 기타 설정 name: '', mode: 'development', devtool: 'eval', // (2) 웹팩 entry 설정 entry: {}, // (3) 웹팩 module 설정 module: { rules: [ { test: '', loader: '', options: { presets: [], plugins: [] } } ] }, // (4) 웹팩 plugins 설정 (module 이외에 추가적으로 추가하고 싶은 plugins) plugins: [], // (5) 웹팩 output 설정 output: {}}; plugin이란 추가적으로 하고 싶은 작업에 대한 확장 프로그램을 의미한다.plugin의 종류가 많이 있다. 실무 코드에서는 10개 이상의 plugin이 추가되어 있는 경우도 있으며, 이런 코드에서는 plugin과 최소한의 rule만 남겨두고 실행하면서 에러 발생시에 필요한 plugin, rule을 추가해가며 해당 plugin이 어떤 역할을 하는지 파악하는 것이 중요하다. 아래에 sample로 작성해 본 웹팩 설정파일(webpack.config.js)은 참고하도록 하자. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657const path = require('path');const webpack = require('webpack');module.exports = { // 웹팩 설정 이름 name: 'webpack-setting', mode: 'development', // 실서비스: production // 빠르게 처리하기 위한 코드 devtool: 'eval', // 확장자 생략을 위해서 resolve 옵션을 사용 resolve: { extensions: ['.js', '.jsx'] }, // 이 부분이 중요 // 목표 : 작성한 JavaScript파일을 하나로 합쳐서 // html에서 스크립트를 실행하기 entry: { app: ['./client'] }, // 입력(client.jsx, WordRelay.jsx) module: { // 여러개의 규칙을 적용 rules: [ { // 정규표현식으로 js와 jsx파일에 룰을 적용하겠다. test: /\\.jsx?/, // babel을 적용해서 최신 문법의 코드를 구형브라우저에서도 돌아갈 수 있도록 // 호환되도록 바꿔주겠다. loader: 'babel-loader', options: { // presets: ['@babel/preset-env', '@babel/preset-react'], presets: [ [ '@babel/preset-env', { targets: { // 한국에서 점유율이 5% 이상인 브라우저와 크롬의 특정 버전을 지정(최신 크롬 2개 버전) browsers: ['&gt; 5% in KR', 'last 2 chrome versions'] }, debug: true } ], '@babel/preset-react' ], plugins: ['@babel/plugin-proposal-class-properties'] } } ] }, // 아래의 plugin의 경우에는 위에서 작성해준 Loader의 options에 대해서 모두 debug를 true로 설정해주겠다는 의미이다. plugins: [new webpack.LoaderOptionsPlugin({ debug: true })], output: { path: path.join(__dirname, 'dist'), filename: 'app.js' } // 출력 (app.js)}; 아래와 같이 preset-env의 DEBUG 옵션을 터미널에서 확인할 수 있다. 12345678910@babel/preset-env: `DEBUG` optionUsing targets:{ &quot;chrome&quot;: &quot;88&quot;, &quot;edge&quot;: &quot;89&quot;, &quot;ie&quot;: &quot;11&quot;, &quot;ios&quot;: &quot;14&quot;, &quot;samsung&quot;: &quot;13&quot;} 웹팩 Hot reload와 웹팩 dev Server이 웹팩의 Hot reload 기능을 추가하지 않은 상태에서는 작성하던 코드에서 변경사항이 생기면 npm run dev나 npx webpack 명령을 통해서 변경사항이 반영된 JavaScript 파일로 다시 생성해줘야 한다.하지만 매번 코드상에 변경사항이 생길때마다 수동으로 webpack을 실행시키는 것은 여간 귀찮은 일이 아닐 수 없다.따라서 이 Hot reload라는 설정을 추가해서 자동으로 코드상에서 변경점이 생겼을때 이를 감지해서 자동으로 webpack을 실행시켜줄 수 있다. webpack-cli의 버전이 4버전으로 업데이트 되면서 기존에 hot reload기능을 위해서 사용하였던 react hot loader가 react refresh로 업데이트 되었다. Hot reload 사용을 위한 설치 라이브러리Hot reload 기능을 사용하기 위해서는 아래 3가지 라이브러리를 설치해줘야 한다. 12$ npm i -D react-refresh @pmmmwh/react-refresh-webpack-plugin$ npm i -D webpack-dev-server webpack-dev-server는 프론트 개발의 편의를 위한 개발 서버이다. package.json의 script 수정 및 webpack 설정파일(webpack.config.js) 업데이트앞서 hot reload를 위한 라이브러리 설치가 완료되었다면 package.json의 script 수정 및 webpack 설정파일을 수정해줘야 한다.package.json 123&quot;scripts&quot;: { &quot;dev&quot;: &quot;webpack serve --env development&quot;} webpack.config.js웹팩 설정파일에서는 설치한 @pmmmwh/react-refresh-webpack-plugin 라이브러리를 import해서 output section 이전의 plugins section에 넣어줘야 한다. 123456const RefreshWebpackPlugin = require('@pmmmwh/react-refresh-webpack-plugin');......plugins: [ new RefreshWebpackPlugin()],...... 그리고 babel-loader의 옵션에 있는 plugins에 react-refresh/babel을 추가해줘야 한다.이렇게 작성을 해주게 되면, babel이 최신 문법의 자바스크립트 코드를 구형 자바스크립트 코드로 변환을 해줌과 동시에 hot reload기능도 같이 추가를 해준다. 1234567891011........loader: 'babel-loader',options: { presets: [ ...... ], plugins: [ 'react-refresh/babel' ]}........ dev server 설정여기서 설정하는 개발 서버는 프론트 개발의 편의를 위해 두는 서버이다.기존의 웹팩 설정에서 output section의 아래에 개발 서버에 대한 설정을 작성하도록 한다.이 개발서버의 역할은 webpack 설정파일에 작성한대로 build를 해주고 /dist/ 폴\b더에 결과물을 저장, index.html 파일에 반영을 해주는 역할을 한다. hot reloading기능은 코드상에 변경점이 생겼을때 이를 자동으로 감지하여 설정해둔 개발서버가 앞서 설명한 일련의 과정을 처리할 수 있게 도와준다. 12345devServer: { // output과 동일 publicPath: '/dist/', hot: true} reloading은 기본적으로 지원하지만 hot reloading이 reloading과 다른점은 기존의 데이터를 유지하느냐 안하느냐에 있다.hot reloading은 기존의 데이터의 상태를 유지하면서 변경된 코드에 맞게 화면을 새로고침해준다.예를들어 1단계부터 4단계까지의 작업을 하는 도중에 3단계에서 코드를 변경할 필요성이 생겼다고 가정하자.hot reloading 기능을 사용하지 않는다면, 3단계에서 코드를 변경하면 기존의 상태 데이터도 같이 초기화가 되어 다시 1단계부터 데이터의 상태를 업데이트 해줘야 한다.하지만 hot reloading 기능을 사용한다면, 3단계에서 코드 변경을 해도 기존의 상태 데이터는 유지를 시켜준다. 따라서 즉각적으로 작업을 이어갈 수 있다. hot reload관련 로그 확인하기WDS(Webpack Dev Server), HMR(Hot Module Replacement) 어떤 컴포넌트가 바뀌어서 업데이트가 되었는지 확인한다. WDS가 실제 변경사항을 받아서 업데이트를 해준다. path와 publicPath의 차이웹팩 설정 파일의 output과 devServer 설정 부분을 보면 path와 publicPath에 대한 속성값이 있는 것을 볼 수 있다.여기서 path는 실제경로를 의미하고 publicPath는 가상경로를 의미한다.cf) NodeJS에서 app.use('/dist', express.static(__dirname, 'dist'))를 작성해주는 부분이 있는데 앞의 ‘/dist’ 부분이 publicPath(가상경로)를 의미하고, 두 번째 인수가 path(실제경로)를 의미한다. 참고를 위해 작성한 웹팩 설정파일을 첨부해둔다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869const path = require('path');const webpack = require('webpack');const RefreshWebpackPlugin = require('@pmmmwh/react-refresh-webpack-plugin');module.exports = { // 웹팩 설정 이름 name: 'webpack-setting-with-hot-reloading', mode: 'development', // 실서비스: production // 빠르게 처리하기 위한 코드 devtool: 'eval', // 확장자 생략을 위해서 resolve 옵션을 사용 resolve: { extensions: ['.js', '.jsx'] }, // 이 부분이 중요 // 목표 : 작성한 JavaScript파일을 하나로 합쳐서 // html에서 스크립트를 실행하기 entry: { app: ['./client'] }, // 입력(client.jsx, WordRelay.jsx) module: { // 여러개의 규칙을 적용 rules: [ { // 정규표현식으로 js와 jsx파일에 룰을 적용하겠다. test: /\\.jsx?/, // babel을 적용해서 최신 문법의 코드를 구형브라우저에서도 돌아갈 수 있도록 // 호환되도록 바꿔주겠다. loader: 'babel-loader', options: { // presets: ['@babel/preset-env', '@babel/preset-react'], presets: [ [ '@babel/preset-env', { targets: { // 한국에서 지원률이 5% 이상인 브라우저와 크롬의 특정 버전을 지정 browsers: ['&gt; 1% in KR', 'last 2 chrome versions'] }, debug: true } ], '@babel/preset-react' ], plugins: [ '@babel/plugin-proposal-class-properties', 'react-refresh/babel' ] } } ] }, plugins: [ new webpack.LoaderOptionsPlugin({ debug: true }), new RefreshWebpackPlugin() ], output: { path: path.join(__dirname, 'dist'), filename: 'app.js', publicPath: '/dist/' }, // 출력 (app.js) devServer: { // output와 동일 publicPath: '/dist/', hot: true }};","link":"/2021/04/09/202104/210409-Use_webpack/"},{"title":"210409 JavaScript의 this binding에 대한 이야기","text":"이번 포스팅에서는 JavaScript에서의 this binding에 대해서 정리를 해보려고 한다.이는 중요한 개념이기 때문에 나의 말로써 다른 사람에게 설명할 수 있을 정도로 연습을 해둬야 한다. 그럼 JavaScript에서 this란 무엇일까?갑자기 누군가 JavaScript에서 this가 무엇이냐고 물어본다면 간단하게 현재 실행되는 코드의 실행 컨텍스트를 가르킨다라고 대답할 수 있다.JavaScript는 Script 언어로, interpreter가 코드를 라인단위로 읽고 해석한 뒤에 실행을 시킨다.이때 interpreter에 의해 현시점에서 실행되는 JavaScript 코드의 환경(스코프)를 실행 컨텍스트(execution context)라고 정의한다.JavaScript에서는 내부에서 이런 실행 컨텍스트를 Call stack에서 관리를 하고 실행되는 시점에 자주 변경되는 실행 컨텍스트를 이 this라는 녀석이 가르키고 있다. 그럼 this는 구체적으로 무엇을 가르키는가?this binding(this에 binding되는 값)은 함수 호출 방식(함수가 어떻게 호출되었는지에 따라)에 따라 동적으로 결정된다. 이전에 스코프에 대한 블로그 포스팅을 했을때 JavaScript에서는 함수의 상위 스코프를 경정하는 방식인 Lexical scope는 함수 정의가 평가되어 함수 객체가 생성되는 시점(함수가 선언되는 지점)에 상위 스코프를 결정한다.하지만 this binding은 함수의 호출 시점에 의해 결정이 된다. 여기서 주의해야 될 것은 동일한 함수도 다양한 방식으로 호출이 될 수 있으며, 경우에 따라 this에 binding되는 값도 달라진다는 것이다. 첫 번째, default binding기본적으로 this는 전역 객체를 가르킨다. Node환경에서는 global 객체를, Browser에서는 window 객체를 가르키게 된다. Browser 환경에서의 this NodeJS 환경에서의 this 123456789101112131415Welcome to Node.js v12.16.0.Type &quot;.help&quot; for more information.&gt; console.log(this);Object [global] { global: [Circular], clearInterval: [Function: clearInterval], clearTimeout: [Function: clearTimeout], setInterval: [Function: setInterval], setTimeout: [Function: setTimeout] { [Symbol(util.promisify.custom)]: [Function] }, queueMicrotask: [Function: queueMicrotask], clearImmediate: [Function: clearImmediate], setImmediate: [Function: setImmediate] { [Symbol(util.promisify.custom)]: [Function] }} 두 번째, 함수의 호출 방식에 따른 this binding일반 함수 호출일반 함수를 호출하는 경우에는 기본적으로 this에 전역 객체(global object)가 binding된다. 123456function checkThisInNormalFunc() { console.log(this); //window}checkThisInNormalFunc();console.log(this); //window 만약에 일반 함수의 내/외부에서 strict mode를 사용한다면, 함수 내부에서의 this는 전역객체를 binding하지 않는다. 1234567function checkThisInNormalFunc() { 'use strict'; console.log(this === window); //false}checkThisInNormalFunc();console.log(this === window); //true 일반 함수에서의 this는 객체의 property나 method를 참조하기 위한 자기 참조 변수이기 때문에 객체를 생성하지 않으면 일반 함수에서 this는 의미가 없다. 따라서 위와같이 일반 함수 내부에서 'strict mode'를 사용하게 되면 this는 전역 객체가 아닌 undefined가 binding된다. 만약에 메서드 내에서 정의한 중첩 함수를 일반 함수로 호출을 하게 되면 중첩 함수 내부의 this에는 전역 객체가 binding된다. 1234567891011121314151617181920212223242526// var 키워드로 선언된 전역 변수의 경우에는 전역 객체의 프로퍼티가 된다.var value = 1;// 단 const 키워드로 선언한 전역 변수의 경우에는 전역 객체의 프로퍼티가 아니다.const obj = { value: 100, foo() { console.log(&quot;foo's this: &quot;, this); // { value: 100, foo: f } console.log(&quot;foo's this.value: &quot;, this.value); // 100 // method 내에서 정의한 중첩 함수 function bar() { console.log(&quot;bar's this: &quot;, this); //window console.log(&quot;bar's this.value: &quot;, this.value); // 1 } // method 내에서 정의한 중첩 함수도 일반 함수로 호출되면 중첩 함수 내부의 this에는 전역 객체가 바인딩 된다. bar(); setTimeout(function () { console.log(&quot;callback's this: &quot;, this); // window console.log(&quot;callback's this.value: &quot;, this.value); // 1 }, 100); }};obj.foo(); 어떤 함수라도 일반함수로 호출(중첩함수, 콜백함수 포함)이 되면, this에는 전역 객체가 바인딩된다.이 부분은 본래 중첩 함수나 콜백 함수가 외부 함수를 돕는 헬퍼 함수의 역할(외부 함수의 일부 로직을 대신 처리)을 한다는 것을 고려했을때, 외부 함수인 method와 중첩 함수 또는 콜백 함수의 this가 일치하지 않는 것은 중첩 함수 또는 콜백 함수가 헬퍼 함수로써 동작하기 어렵다는 의미가 된다. 앞서 예시로 작성한 setTimeout()함수에 전달된 callback 함수내의 this에는 전역 객체가 binding되기 때문에 this.value는 obj의 value 프로퍼티가 아닌 전역 객체의 value 프로퍼티를 참조한다.var 키워드로 선언한 전역 변수는 전역 객체의 프로퍼티가 되기 때문에 window.value는 1로써 참조된다. method 내부의 중첩 함수나 콜백 함수의 this binding을 method의 this binding과 일치시키기 위해서는 아래와 같이 method내에서 this를 특정 변수에 담고 해당 변수를 이용해서 콜백함수 내부에서는 method내의 this를 참조할 수 있다. 123456789101112var value = 1;const obj = { value: 100, foo() { const that = this; setTimeout(function () { console.log(that.value); // 100 }, 100); }};obj.foo(); 위와같이 method 내의 this를 별도의 변수에 담아 method 내부의 중첩 함수나 콜백 함수에서 해당 변수를 통해 method에서의 this를 참조하는 방식 이외에도 명시적으로 binding할 수 있는 Function.prototype.apply, Function.prototype.call, Function.prototype.bind 메서드를 활용한 방식들이 있다. 12345678910111213141516var value = 1;const obj = { value: 100, foo() { // callback함수에서 명시적으로 this를 binding한다. setTimeout( function () { console.log(this.value); // 100 }.bind(this), 100 ); }};obj.foo(); 화살표 함수를 사용해서 this를 바인딩화살표 함수(arrow function)에서는 this를 직접적으로 binding하지 않고 상위 스코프의 this를 가르키기 때문에 가능하다. 1234567891011var value = 1;const obj = { value: 100, foo() { // 화살표 함수 내부의 this는 상위 스코프의 this를 가르킨다. setTimeout(() =&gt; console.log(this.value), 100); // 100 }};obj.foo(); 메서드 호출메서드 내부에서의 this는 메서드를 호출한 객체(메서드를 호출할 때 메서드의 이름 앞의 마침표(.) 연산자 앞에 기술된 객체)가 바인딩된다. (메서드를 소유한 객체가 아닌 호출한 객체에 바인딩이 된다) 123456789const person = { name: 'Lee', getName() { // 메서드 내부의 this는 메서드를 호출한 객체에 binding된다. return this.name; }};// 메서드 getName()을 호출한 객체는 person이다.console.log(person.getName()); // Lee 위에서 정의한 getName() 메서드는 person 객체의 메서드로써 정의가 되어있다. 메서드는 프로퍼티에 바인딩된 함수이다. 따라서 person 객체의 getName 프로퍼티가 가르키고 있는 함수 객체는 person 객체에 포함된 것이 아닌 독립적으로 존재하는 별도의 객체이다.getName 프로퍼티가 함수 객체를 가르키고 있는 것이다. 이 말은 즉슨 getName 프로퍼티가 가르키는 함수 객체인 getName 메서드는 다른 객체의 프로퍼티에 할당해서 다른 객체의 메서드가 될 수도 있고 일반 변수에 할당해서 일반 함수로써 호출될 수도 있다. 12345678910111213141516const anotherPerson = { name: 'Kim'};// getName method를 anotherPerson 객체의 메서드로 할당anotherPerson.getName = person.getName();console.log(anotherPerson.getName()); // Kim// getName 메서드를 변수에 할당해서 일반함수로써 호출const getName = person.getName;console.log(getName()); // ''// 일반 함수로 호출된 getName 함수 내부의 this.name은 브라우저 환경에서의 window.name과 같다.// 브라우저 환경에서 window.name은 브라우저 창의 이름을 나타내는 built-in propert이고, default value는 ''이다.// Node.js 환경에서의 this.name은 undefined이다. 메서드 내부의 this는 프로퍼티로 메서드를 가지고 있는 객체와는 관계가 없고 메서드를 호출한 객체에 바인딩이 된다. 프로토 타입 메서드 내부에서의 this프로토 타입 메서드 내부에서의 this 또한 앞서 살펴본 일반 메서드와 마찬가지로 해당 메서드를 호출한 객체에 바인딩된다. 1234567891011121314151617function Person(name) { this.name = name;}Person.prototype.getName = function () { return this.name;};const me = new Person('Lee');// getName method를 호출한 객체는 me이다.console.log(me.getName()); // LeePerson.prototype.name = 'Kim';// getName method를 호출한 객체는 Person.prototype이다.console.log(Person.prototype.getName()); // Kim 생성자 함수 내부에서의 this생성자 함수 내부의 this에는 생성자 함수가 생성할 객체 인스턴스가 바인딩된다.생성자 함수는 객체(인스턴스)를 생성하는 함수이다. 1234567891011121314// 생성자 함수function Circle(radius) { // 생성자 함수 내부의 this는 생성자 함수가 생성할 객체 인스턴스를 가르킨다. this.radius = radius; this.getDiameter = function () { return 2 * this.radius; };}const circle1 = new Circle(5);const circle2 = new Circle(10);console.log(circle1.getDiameter()); // 10console.log(circle2.getDiameter()); // 20 앞서 bind 메서드를 사용해서 콜백 함수의 메서드 내부에 this를 바인딩해주었다.bind이외에도 apply, call 등의 메서드가 있는데, 이 메서드들은 Function.prototype의 메서드이기 때문에 이 메서드들은 모든 함수들이 상속받아 사용할 수 있다. apply와 call 메서드는 모두 함수를 호출하는 기능을 가진다.apply와 call 메서드는 ㅎ마수를 호출하면서 첫 번째 인수로 전달한 특정 객체를 호출한 함수의 this에 binding한다. apply와 call 메서드는 호출할 함수에 인수를 전달하는 방식만 다르고 동일한 동작을 한다. 1234567891011121314151617function getThisBinding() { console.log(arguments); return this;}// this로 바인딩할 객체const thisArg = { a: 1 };// apply 메서드는 호출할 함수의 인수를 배열로 묶어서 전달한다.console.log(getThisBinding.apply(thisArg, [1, 2, 3]));// Arguments(3) [1, 2, 3, callee: ƒ, Symbol(Symbol.iterator): ƒ]// {a: 1}// call 메서드는 호출할 함수의 인수를 쉼표로 구분한 리스트 형식으로 전달한다.console.log(getThisBinding.call(thisArg, 1, 2, 3));// Arguments(3) [1, 2, 3, callee: ƒ, Symbol(Symbol.iterator): ƒ]// {a: 1} apply와 call 메서드의 대표적인 용도는 argument 객체와 같은 유사 배열 객체에 메서드를 사용하는 경우이다.argument 객체는 배열이 아니기 때문에 slice와 같은 배열의 메서드를 사용할 수 없다. 하지만 apply나 call 메서드를 이용하면 가능하다. 123456789101112function convertArgsToArray() { console.log(arguments); // arguments 객체를 배열로 변환 // Array.prototype.slice를 인수없이 호출하면 배열의 복사본을 생성한다. const arr = Array.prototype.slice.call(arguments); // const arr = Array.prototype.slice.apply(arguments); console.log(arr); return arr;}convertArgsToArray(1, 2, 3); // [1, 2, 3] Function.prototype.bind 메서드는 apply와 call 메서드와 달리 함수를 호출하지 않고 this로 사용할 객체만 전달한다. 12345678910function getThisBinding() { return this;}// this로 사용할 객체const thisArg = { a: 1 };console.log(getThisBinding.bind(thisArg));// bind 메서드는 함수를 호출하지 않으므로 명시적으로 호출해줘야 한다.console.log(getThisBinding.bind(thisArg)()); // {a: 1} 앞서 실습해본 것 처럼 bind 메서드는 메서드 내부의 중첩 함수 또는 콜백 함수의 this가 불일치하는 문제를 해결하기 위해서 유용하게 사용된다.","link":"/2021/04/09/202104/210409-javascript_this_binding/"},{"title":"210410 React TIL - import와 require 비교, React component의 key 속성, React component의 props, Top-Down &amp; Bottom-Top, Redux와 Context, Class내에서 화살표 함수가 아닌 일반함수의 형태로 메서드를 정의하는 경우","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. import와 require 비교간단하게 설명하면 import 구문은 리액트 코드 작성시에 사용하고 require는 Node 관련 코드를 작성시에 사용한다. 첫 번째, import 구문은 ES2015 module 문법이다. export 구문은 아래와 같이 두 가지 형태로 export를 해줄 수 있는데, 각 각 다른 형태로 import해야한다.export default 구문은 파일에서 한 번만 쓸 수 있고, export const 구문은 제한 없이 여러번 쓸 수 있다. 작성 예시 12export const hello = 'hello'; // import { hello } from './Hello';export default Hello; // import Hello from './Hello'; 두 번째, require 구문은 Node의 module 문법 = CommonJS이다. import와 export 구문은 아래와 같이 작성을 해주면 된다.Webpack 설정 파일의 내용도 node가 실행시켜주기 때문에 import 구문을 사용할 수 없다. 작성 예시 123const React = require('react');exports.hello = 'hello';module.exports = Hello; babel이 ES2015 module 문법을 Node의 module 문법으로 바꿔준다. 리액트 컴포넌트의 key 속성리액트 컴포넌트 내의 key 속성은 컴포넌트 요소가 추가/삭제될때 참조되는 속성이다. 이 부분은 성능 최적화와 관련이 있으며 단순 index값을 key값으로 넣어주는 것은 좋지 않은 방법이다.따라서 index가 아닌 unique한 값을 할당해줄 수 있도록 해야 한다. 리액트 컴포넌트의 props부모 컴포넌트에서 자식 컴포넌트로 속성값을 넘겨 줄 수 있는데 이를 props라 한다. Redux, Context부모 컴포넌트에서 자식 컴포넌트로 속성값을 넘겨주는데 그 깊이가 깊어지는 경우, 불필요한 props의 전달이 이루어지기 때문에 이런 경우에는 Redux, Context를 사용하도록 한다.Context가 Redux보다 나중에 등장했지만, Context는 Redux보다 간단한 작업을 하는데 도움을 주고, react-redux의 형태로 Redux 내부에서 Context를 사용한다. 리액트 컴포넌트의 작성순서리액트에서 컴포넌트를 작성할때, 큰 하나의 컴포넌트를 작성한 다음에 작은 컴포넌트 단위로 쪼개서 분리하는 것을 Top - Down방식이라고 하며, 작은 컴포넌트를 우선적으로 만든 다음에 큰 컴포넌트를 만드는 것을 Bottom - Up방식이라고 한다. class 내에서 함수를 화살표 함수가 아닌 프로토타입 메서드로 정의하는 경우클래스 내에서 함수를 화살표 함수(arrow function)가 아닌 프로토타입 메서드로 작성하게 되면 메서드내에서 this를 참조하는 경우 문제가 생긴다.아래의 코드를 참조해보자. 화살표 함수의 경우 자체적으로 this를 바인딩하지 않고, 상위 스코프의 this를 바인딩하기 때문에 화살표 함수내의 this는 클래스 객체 자체를 가르킨다.하지만 클래스 내부의 메서드를 화살표 함수가 아닌 프로토타입 메서드로 작성해주게 되면, 생성자(constructor)에서 별도로 클래스 내부의 프로토타입 메서드에 this를 바인딩해줘야 한다. 이는 클래스 내부의 프로토타입 메서드의 this는 메서드를 호출한 객체를 바인딩하기 때문에 별도의 호출없이 프로토타입 메서드 안의 this를 확인해보면 undefined가 출력됨을 확인할 수 있다. 123456789101112131415161718192021222324252627class SampleClass extends Component { constructor(props) { super(props); this.state = { result: '', value: '' }; // class 내부 매서드에 this 바인딩 this.onSubmitForm = this.onSubmitForm.bind(this); this.onChangeInput = this.onChangeInput.bind(this); } // 프로토타입 메서드 onSubmitForm(e) { e.preventDefault(); this.setState({ result: }) } // 프로토타입 메서드 onChangeInput(e) { this.setState({ value: e.target.value }); }}","link":"/2021/04/10/202104/210410-React-review_study/"},{"title":"210411 React TIL - 상태의 불변성, React DevTools, 배포모드, 성능에 대한 이야기, React.createRef, props와 state 연결하기","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. 상태의 불변성push를 이용해서 컴포넌트의 상태를 직접 바꿔주게 되면, React가 구체적으로 무엇이 바뀌었는지 감지하지 못한다. 따라서 원본은 그대로 두고, 바뀌는 상태 데이터는 원본을 복사해서(얕은 복사) 상태를 업데이트 해주도록 한다. React의 Rendering 기준은 이전 상태값과 현재 상태값이 다른 경우 Re-rendering을 해준다. arr1 === arr2 // false (Re-rendering) - 이전 상태와 현재 상태가 같지 않은 경우만 렌더링 arr1 === arr2 // true 내부에서 this를 사용하지 않는 함수의 경우에는 외부로 빼주도록 한다.setState에서의 함수형 업데이트상태를 업데이트 해주는 setState 함수는 비동기 방식이기 때문에 setState함수를 여러번 연달아 사용하는 경우 문제가 생길 여지가 있다. 따라서 setState 내부에서 또 다른 상태값을 참조해서 상태를 업데이트 시키는 경우에는 함수형으로 이전 상태값을 받아서 새로운 상태로 업데이트시켜줘야 한다.(class component, functional component(hooks) 둘 다 ) 123const [tries, setTries] = useState([]);setTries((tries) =&gt; [...tries, { try: value, result: 'good!' }]); 클래스 컴포넌트의 경우에는 states 전체를 이전상태로 받아오기 때문에 states.tries로 참조하고자 하는 특정 상태값을 참조하면 된다. React DevToolsReact 개발 툴(React, Redux)을 사용해서 화면의 각 컴포넌트가 어느 시점에서 렌더링이 되고 있는지 확인할 수 있다. 배포 모드로 바꿔주기개발모드에서 배포 모드로 바꿔주기 위해서는 웹 팩 설정파일에서 두 가지 부분을 수정해주면 된다.우선 첫 번째로 process.env.NODE_ENV = &quot;production&quot;를 작성해준다.두 번째로 기존의 mode: ‘development’를 mode: production으로 업데이트해준다. Redux를 사용할때에는 툴로 Redux의 구조가 노출되지 않게 처리하기Redux를 사용하는 경우에는 웹 어플리케이션에서 사용중인 데이터의 구조가 노출되지 않도록 처리를 해줘야 한다.포트폴리오 전용으로 React 프로젝트를 할때에는 현재 실제 서비스를 하고 있는 웹 어플리케이션을 분석해서 어떤 폴더구조로 작성을 했는지, 확인이 가능하다면 데이터 구조는 어떤 식으로 구성을 했는지 참고를 해서 만들어보도록 한다. 성능에 대한 이야기React의 class component에서는 state와 props가 바뀌었을때 그리고 setState가 호출되었을때 렌더링이 발생한다. 불필요한 컴포넌트의 렌더링을 제거하기 위해서는 render와 같이 기본적으로 제공되는 shouldComponentUpdate()함수의 내부에 렌더링되는 조건을 작성해서 렌더링하는 경우와 안하는 경우를 작성해줘야 한다. 1234567shouldComponentUpdate(nextProps, nextState, nextContext){ // 이전 state의 value값이 다음 state의 value값과 일치하지 않는 경우에 렌더링을 한다. if(this.state.value !== nextState.value){ return true; } return false;} 앞서 설명한 shouldComponentUpdate()가 복잡하다면, PureComponent를 사용하도록 한다.사용법은 class 작성시 기존에 상속받았던 Component 대신에 PureComponent를 상속받아서 구현을 한다. 그럼 PureComponent가 알아서 클래스 내부의 shouldComponentUpdate를 알아서 구현해준다.단, PureComponent를 사용하는 경우의 단점은 배열이나 객체와 같이 참조 관계가 있는 구조의 경우에는 PureComponent가 판단하기 힘들다. 배열A []와 배열B[]는 서로 메모리의 다른 곳을 참조하고 있기 때문에 다른 값으로 간주된다. 따라서 배열A에 배열B를 업데이트 하여도 다른 값으로 알고 해당 컴포넌트를 렌더링하게 된다.state에는 되도록 객체구조를 사용하지 않도록 하고, 배열 내부에 객체를 넣고 그 객체 안에 배열을 넣는 복잡한 구조를 사용하지 않도록 한다. (간단한 구조로 state를 관리)그리고 객체를 상태값으로 사용하는 경우에는 값을 업데이트할때 불변성을 지켜주도록 해야한다.좀 더 디테일한 조건부로 렌더링을 하고자 한다면 기존의 React.Component를 상속받아서 shouldComponentUpdate()에서 별도로 렌더링 조건을 작성해주도록 한다. 그렇다면 함수형 컴포넌트(functional component)의 경우에는 어떻게 처리해야 할까? 바로 React.memo를 사용하도록 한다.사용방법은 작성해준 함수형 컴포넌트의 몸체(인자와 반환부)를 그룹 연산자를 사용해서 React.memo로 감싸주면 된다. (구조 분해 문법을 사용해서 간단하게 memo로 작성해줄 수 있다)React.memo로 함수형 컴포넌트의 몸체를 감싸주면, props나 state가 바뀌었을때에만 렌더링을 해준다. 자식 컴포넌트들을 모두 PureComponent나 React.memo로 처리를 해주었다면, 부모 컴포넌트도 PureComponent나 memo로 처리하도록 한다. 성능최적화를 고려하여 Component 대신에 PureComponent를 사용하도록 한다. React.createRef의 사용이전에 React에서 특정 DOM Element를 선택하는 경우에 ref라는 속성을 사용하는 것에 대해서 배웠다. 사용법은 class component와 functional component, 두 가지 경우에 따라 달랐는데 사용법은 아래와 같다.class component 1234567inputRef;onInputRef = (c) =&gt; { this.inputRef = c;};&lt;input ref={this.onInputRef} /&gt;; functional component 1234567import React, { useState, useRef } from 'react';inputEl.current.focus();const inputEl = useRef(null);&lt;input ref={inputEl} /&gt;; class component와 functional component 두 경우에는 약간의 사용 방식에 차이가 있음을 알 수 있다.하지만 class component에서 React.createRef를 사용하게 되면 hooks와 같이 처리를 해줄 수 있다. class component - React.createRef 사용 1234567import React, { Component, createRef } from 'react';this.inputRef.current.focus();inputRef = createRef();&lt;input ref={ this.inputRef }&gt; 위의 React.createRef를 사용하게 되면, React Hooks의 useRef()와 통일성이 있는 코드를 작성할 수 있다. 위의 React.createRef를 사용하게 되면 좀 더 간단하게 DOM element의 선택이 가능하다. 하지만 좀 더 미세한 처리를 하고자 하는 경우에는 기존의 방식으로 함수 내에 미세한 처리에 대해 작성을 해줄 수 있다.같은 맥락으로 초기 state에 대한 작성에 있어, 생성자(constructor)내에 작성을 해줘도 되고 안해줘도 되는데 만약 초기 생성자 내에 작성해주는 상태값을 한 번 가공해서 넣어줘야 한다면, constructor 블럭 안에서 처리를 해서 초기화를 시켜 줄 수 있다. props와 state 연결하기부모 컴포넌트로부터 넘겨받은 props의 경우 자식 컴포넌트에서 수정할 수 없다. 만약 자식 컴포넌트에서 props를 수정해야되는 경우에는 자식 컴포넌트의 state에서 넘겨받은 props의 값을 상태의 초기값으로 받아 값을 수정해서 사용해야 한다.class component 1234state = { result: this.props.result, value: this.props.value}; functional component 123456789const Sample = ({ info }) =&gt; { const [result, setResult] = useState(info.result); return ( &lt;li&gt; &lt;div&gt;{result}&lt;/div&gt; &lt;/li&gt; );}; 컴포넌트 간의 props 전달 - Context API컴포넌트 간에 props를 전달하는 경우에 있어 컴포넌트간에 깊이가 깊어져서 불필요한 props의 전달이 발생하는 경우, Context API를 사용해서 불필요한 props의 전달 없이 props를 전달할 수 있다.Context가 props의 진화형으로 이해하면 된다.","link":"/2021/04/11/202104/210411-React-review_study/"},{"title":"210412 React TIL - setTimeout(), useEffect()&#x2F;useMemo()&#x2F;useCallback(), useState와 useRef의 차이, jsx에서 if&#x2F;for문 작성, React class&#x2F;function - 라이프 사이클","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. React에서 jsx를 구조있게 작성하기필요에 따라 jsx 작성 부분이 조건문이나 반복문으로 복잡해진다면, 별도의 함수나 자식 컴포넌트로 작성해주도록 한다. setTimeout() 사용법setTimeout의 첫 번째 인수로는 콜백 함수를 넣어주고 두 번째 인수로는 ms단위로 interval time을 적어준다. setTimeout()은 비동기 함수이기 때문에 외부에 선언한 변수를 참조할 경우, 클로저 문제가 생긴다.따라서 setTimeout()내에서 사용할 변수의 경우에는 setTimeout() 내에서 처리하도록 한다.setTimeout()을 해줬다면 clearTimeout()을 해줘야 한다. 만약 clearTimeout()을 해주지 않으면, setTimeout()을 처리한 컴포넌트가 제거되더라도 call stack에 남아 계속 실행될 수 있다. 12345678910// timeout변수 선언timeout;// setTimeout()timeout = setTimeout(() =&gt; { console.log('test');}, 1000);// setTimeout() 제거clearTimeout(this.timeout); setTimeout은 call stack으로 넘어가서 실행이 되는데, settTimeout은 call stack으로 넘어간다고 하더라도 clearTimeout으로 취소를 할 수 있다. useEffect()와 useMemo(), useCallback()이전에 공부를 하면서 별도로 정리를 해둔 포스팅이 있는데, 아래 링크의 내용을 참고하도록 하자. → https://leehyungi0622.github.io/2021/03/18/202103/210318-React-review_study/ 함수형 컴포넌트에서는 함수 내부에 선언된 함수들이 매번 상태가 업데이트 되면서 렌더링 될 때마다 다시 생성이 된다. 이미 한 번 생성된 함수들을 처음 한 번만 생성하고 이후에는 재사용하기 위해서는 useCallback()을 사용해야 한다. useState와 useRef의 차이useState는 호출될 경우(setState), return 부분이 다시 실행된다. 하지만 useRef의 경우에는 return 부분이 다시 실행되지 않는다. 특정 변수값이 바뀌어도 다시 화면을 렌더링하고 싶지 않은 경우에는 useRef를 사용하도록 한다.(useRef를 사용할때에는 항상 useRef 변수는 current로 접근해야 한다.) jsx 구문에서 if /for 문 작성해보기 jsx 구문에서 if와 for 문을 사용할 수는 있지만 이는 실무에서도 별로 사용되지 않는 코드 작성법이다. 단축표기법이나 삼항연산자, map과 같은 함수를 대체해서 사용하도록 하자. 아래와 같이 즉시 실행함수의 형태로 작성해주면 그룹연산자 내에 작성한 조건문 및 반복문을 실행할 수 있다. 하지만 가독성이 좋지 않기 때문에 좋지 않은 코드 작성법이다. 123456789return ( &lt;&gt; {(() =&gt; { if (result.length === 0) { } else { } })()} &lt;/&gt;); return 반환문 내부에 작성해주는 jsx 코드에서 component를 배열의 형태로 반환해줄 수도 있다. 1234567891011return ( &lt;&gt; {(() =&gt; { let array = []; for (let i = 0; i &lt; result.length; i++) { array.push(&lt;Child key={`${i + 'c'}`} /&gt;); } return array; })()} &lt;/&gt;); 위와 같이 배열을 반환해주게 되면, 아래와 같은 형태로 반환해주는 것과 동일한 형태가 된다.아래와 같이 작성을 해주는 경우, 반드시 리스트 내의 각 컴포넌트 요소들은 key값을 가져야 한다.하지만 이 또한 실무에서 사용되지 않는 방법이지만, 이러한 형태로 컴포넌트를 반환할 수 있다는 것도 알아두도록 한다. 12345return [ &lt;div key=&quot;A&quot;&gt;A&lt;/div&gt; &lt;div key=&quot;B&quot;&gt;B&lt;/div&gt; &lt;div key=&quot;C&quot;&gt;C&lt;/div&gt;] React class component - 라이프 사이클React의 클래스 컴포넌트에서 render 함수가 실행되면, react가 jsx를 DOM에 붙여주게 되는데, 붙여주게 되는 그 순간에 특정 동작을 할 수 있게 해 줄 수 있다. componentDidMount()render가 처음 실행되어 성공적으로 렌더링이 되었을때 componentDidMount() 함수가 실행이 된다. setState 함수로 인해 re-rendering되는 경우에는 componentDidMount()가 실행되지 않는다. componentWillUnmount()부모 컴포넌트에 의해서 자식 컴포넌트가 제거될 수 있는데, 이렇게 컴포넌트가 제거되기 직전에 실행되는 함수이다. componentDidMount()와 한 쌍으로, componentDidMount()에서 처리했던 작업을 종료시키는 역할을 한다. 예시 1234567891011interval;componentDidMount(){ this.interval = setInterval(() =&gt; { console.log('result'); }, 1000)}// setInterval을 종료시켜주지 않으면, component 생성될때마다 계속해서 setInterval이 생성이 되므로, 비동기 요청은 반드시 componentWillUnmount에서 종료를 시켜주도록 한다.componentWillUnmount(){ clearInterval(this.interval);} componentDidUpdate()setState 함수로 인해 컴포넌트가 re-rendering되는 경우에 실행되는 함수이다. 순서(1) constructor()(2) render()(3) ref (jsx 태그에 ref 속성을 넣은 경우)(4) componentDidMount()(5) (setState/props가 바뀌는 경우)(6) shouldComponentUpdate(→ true)(7) rendering(8) componentDidUpdate()(9) (부모 컴포넌트가 자식 컴포넌트를 제거하는 경우)(10) componentWillUnmount()(11) 소멸 고차함수(Higher-Order-Function)고차함수(HOF)란 함수를 인자로 받거나 반환값으로 갖는 함수를 말한다.만약에 아래와 같이 button의 onClick 속성에 함수 형태로 이벤트를 넣어주었다면, 아래와 같은 형태로 이벤트 함수를 정의해서 작성해 줄 수 있다. 123456const test = (value) =&gt; () =&gt; {}// &lt;button onClick={() =&gt; test('value')}&gt;&lt;button onClick={test('value')}&gt; React functional component - 라이프 사이클 그렇다면 클래스 컴포넌트에 있는 라이프 사이클이 함수형 컴포넌트에도 있을까? 별도의 라이프 사이클이 있는 것은 아니지만 라이프 사이클을 흉내낸 useEffect()가 있다. 이전에 공부를 하면서 별도로 정리를 해둔 포스팅이 있는데, 아래 링크의 내용을 참고하도록 하자. → https://leehyungi0622.github.io/2021/03/18/202103/210318-React-review_study/ 123456789101112const interval = useRef();useEffect(() =&gt; { //componentDidMount, componentDidUpdate의 역할 (1:1 대응은 아니다) interval.current = setInterval([function], 1000); return () =&gt; { //componentWillUnmount의 역할 clearInterval(this.interval.current); }},[ result ]) // result 상태값이 변화할때마다 re-rendering이 발생한다. class / hooks의 라이프 사이클 비교hooks에서 각 각의 state마다 다른 effect를 적용시키고 싶을때, useEffect()를 여러번 사용할 수 있다.반면 class의 경우에는 각 각의 state마다 다른 effect를 적용시키고 싶을때 componentDidMount나 componentDidUpdate에서 각 각의 state를 조건문으로 분기처리한다. 예를들어 a, b, c 세 개의 state가 있다고 가정하자. componentDidMount, componentDidUpdate, componentWillUnmount 이 세 개의 클래스 라이프 사이클 메서드에서는 세 개의 state 모두 동시에 처리한다.반면 hooks의 useEffect의 경우에는 [a], [a,b], [a,b,c] 로 각 각의 state를 그룹화해서 각기다른 effect를 처리할 수 있다.ex) 123456789101112131415161718// class component의 componentDidMountcomponentDidMount(){ this.setState({ a: 1, b: 2, c: 3 });};// functional component의 react hooks - useEffectuseEffect(() =&gt; { setA(); setB();},[a, b]);useEffect(() =&gt; { setC();},[c]); Q : hooks는 부모 컴포넌트에서 자식 컴포넌트를 가지고 있으면, 부모 컴포넌트가 렌더링 될때마다 무조건 자식 컴포넌트도 변경이 되는가? A.무조건 변경된다. 자식 컴포넌트가 전달되는 props가 바뀌면 re-rendering되기 때문에 React.memo로 함수 컴포넌트를 감싸주는 것이다. useLayoutEffect vs useEffectuseLayoutEffect는 화면의 크기를 resizing할때 useEffect가 발생하기 이전에 발생한다. useLayoutEffect는 화면 레이아웃의 변화를 감지할때 사용된다.useEffect의 경우에는 화면이 렌더링 된 다음에 실행되지만, useLayoutEffect의 경우에는 화면이 바뀌기 전에 발생을 한다.","link":"/2021/04/12/202104/210412-React-review_study/"},{"title":"210413 React TIL - PureComponent와 React.memo, useMemo와 useCallback, 자식 컴포넌트의 props로 함수를 넘겨줄때, hooks의 순서","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. PureComponent와 React.memo클래스형 컴포넌트에서는 PureComponent를 상속받아서 컴포넌트 최적화를 시켜줄 수 있고, 함수형 컴포넌트에서는 함수몸체를 React.memo로 감싸서 PureComponent와 같이 컴포넌트를 최적화 시켜 줄 수 있다. 이렇게 컴포넌트로 다른 컴포넌트를 감싸는 것을 HOC(Higher Order Component라고 한다. useCallback과 useMemo의 비교useCallback은 함수자체를 기억하고, useMemo는 함수의 반환값 자체를 기억한다.함수형 컴포넌트의 경우에는 재실행될 때 함수 전체가 재실행되기 때문에 한 번 생성한 함수를 재생성하지 않고 재활용하기 위해서는 useCallback을 사용한다.useCallback의 내부에서 사용되는 state 값이 있는 경우에는, 반드시 두 번째 인자인 []내에 작성해줘야 한다. 이렇게 되면 비효율적이기 때문에 특정 값이 변경될때에만 해당 함수가 호출되어 초기 상태값을 업데이트 할 수 있도록 useMemo를 사용해서 작성해줘야 한다.만약 특정 함수로부터 반환받은 값을 상태의 초기값으로 사용하는 경우, 함수 자체를 useState의 초기값에 넣어주게 되면, 해당 함수는 함수 컴포넌트가 실행될 때마다 재생성된다. 따라서 아래와 같이 useMemo를 사용해서 상태값을 갱신해서 사용해야 한다. 1234567891011function getStatusValue(){ ........ return values;}const TestComponent = () =&gt; { // refValues가 바뀌면 getStatusValue의 함수로부터 값을 가져와서 갱신한다. const statusValue = useMemo(() =&gt; getStatusValue(), [refValues]); const [winStatus, setWinStatus] = useState(statusValue); ........} 자식 컴포넌트의 props로 함수를 넘겨줄때자식 컴포넌트의 props로 함수를 넘겨줄때 반드시 넘겨주는 함수는 useCallback으로 처리해준다. useCallback으로 처리를 안해주게 되면, 매번 새로운 함수로 생성이되어 자식 컴포넌트에 전달될때 자식 컴포넌트가 새로운 함수를 props로 넘겨준다고 인식한다.(불필요한 re-rendering) hooks는 순서가 중요하다hooks는 순서가 중요하기 때문에 항상 함수의 최상위에 배치하도록 하고, 조건문이나 함수, 반복문 안에는 웬만하면 넣지 않도록 한다. useEffect, useMemo, useCallback 동작123useEffect('[f]', []); // [] 내의 deps가 바뀔때 [f]가 실행된다.useMemo('[f]', []); // [] 내의 deps가 바뀌기 전까지 [f]의 반환값을 기억한다.useCallback('[f]', []); // [] 내의 deps가 바뀌기 전까지 [f] 함수 자체를 기억한다. componentDidMount/componentDidUpdate와 useEffect를 비교useEffect의 두 번째 인자(deps)를 아무것도 주지 않으면 componentDidMount와 같이 실행하고, deps에 특정 상태값을 주게 되면, componentDidMount + componentDidUpdate의 효과를 준다. componentDidUpdate에서는 prevProps, prevState의 인자를 이용해서 함수 내에서 이전 상태와 현재 상태를 비교해서 분기문으로 처리를 해주면 되지만, useEffect에서는 state마다 다른 처리를 해주고자 한다면, useEffect를 여러번 사용해서 특정 상태값에 따른 다른 동작을 정의해주면 된다. 만약에 useEffect에서 componentDidUpdate와 같은 효과를 주기 위해서는 아래와 같은 패턴으로 코드를 작성해준다.처음 실행때는 useRef로 정의한 mounted의 값만 true로 바꿔주고, 그 이후에 대한 작업만 deps의 값이 바뀌었을때 실행되게 된다. 123456789const mounted = useRef(false);useEffect(() =&gt; { if (!mounted.current) { mounted.current = true; } else { // componentDidUpdate와 같은 처리 }}, [deps]);","link":"/2021/04/13/202104/210413-React-review_study/"},{"title":"210415 Algorithm javascript로 queue를 사용한 문제풀이","text":"큐를 활용한 문제풀이이번 포스팅에서는 자바스크립트로 큐(queue)를 이용한 문제풀이에 대해서 사용되는 배열의 메서드에 대해서 간단하게 정리하고 관련 문제를 풀이해보려고 한다. 우선 pop과 push의 경우에는 배열의 맨 마지막 요소를 빼거나(pop) 추가(push)할때 사용하고, shift는 배열의 맨 앞 요소를 빼고, unshift는 맨 앞에 요소를 추가할때 사용한다. 배열을 일정 길이로 초기화하는 두 가지 방법 1234// index를 값으로 갖는 길이가 10인 배열 만들기new Array(10).fill(null).map((v, i) =&gt; i);// Array의 정적 메서드 from을 사용하여 길이 및 내부 요소값 초기화하기Array.from({ length: 10 }, (v, i) =&gt; i + 1); 문제풀이길이가 N개인 배열에서 0번째 인덱스부터 M번째 떨어진 숫자를 배열내에 한 개의 숫자가 남을때까지 반복해서 제거한다고 했을때 최종적으로 남을 단 하나의 숫자는 무엇인가? 12345678910111213141516171819202122232425const fs = require('fs');const stdin = (process.platform === 'linux' ? fs.readFileSync('/dev/stdin').toString() : `8 3`).split('\\n');const input = (() =&gt; { let line = 0; return () =&gt; stdin[line++];})();{ const [n, m] = input().split(' ').map(n =&gt; +n); const queue = Array.from({ length: n, (v, i) =&gt; i + 1}); let answer; while(queue.length){ for(let i = 1; i &lt; m; i++){ queue.push(queue.shift()); } // 세 번째 요소는 그냥 shift queue.shift(); if(queue.length === 1) answer = queue.shift(); } console.log(answer);}","link":"/2021/04/15/202104/210415-Algorithm_queue_problem_solving/"},{"title":"210416 기본 정렬 알고리즘 자바스크립트로 구현하기 (선택정렬, 버블정렬, 삽입정렬)","text":"기본 정렬 알고리즘 구현하기이 기본 정렬 알고리즘은 직접 손 코딩 할 수 있을 정도로 코드를 익혀야 하기 때문에 반복 복습을 위해 블로그 포스팅을 해두기로 했다.코딩 테스트 문제를 풀때 내장된 sort 함수를 사용해서 간편하게 정렬을 해서 문제 해결을 하지만, 기존에 있는 정렬 알고리즘의 구현을 이해하고 필요에 따라 기존 정렬 알고리즘을 응용해서 새로운 알고리즘을 만들어 낼 수 있는 능력도 필요하다.그럼 자바스크립트 언어로 코딩테스트를 준비하고 있기 때문에 자바스크립트로 각 기본 정렬 알고리즘을 구현 및 정리를 해보겠다. 선택정렬(Selection sort) - O(N^2) 시간복잡도/정수 6만개 기준, 10.842(sec)우선 선택 정렬에 대해서 코드 구현을 하기 전에 간단하게 선택정렬을 어떻게 구현해야 되는지 설명을 하겠다.선택정렬은 버블정렬과 같이 1회 순회에 한 개 요소의 위치가 결정이 된다. (가장 작은 값(왼쪽부터 오른쪽으로) 결정)이중 for-loop의 형태로 i: 0 ~ arr.length, j: i+1 ~ arr.length로 순회를 한다.그리고 j의 범주로 순회를 하면서 가장 작은 값의 index를 찾는다. 가장 작은 값의 index를 찾았다면, 최소값의 index위치와 i번째 index위치를 바꿔준다. 이 과정을 i의 범주로 순회를 하면 정렬이 완료된다.그럼 코드로 한 번 구현을 해보겠다. 1234567for (let i = 0; i &lt; arr.length; i++) { let index = i; for (let j = i + 1; j &lt; arr.length; j++) { if (arr[j] &lt; arr[index]) index = j; } [arr[i], arr[index]] = [arr[index], arr[i]];} 버블정렬(Bubble sort) - O(N^2) 시간복잡도/정수 6만개 기준, 22.894(sec)버블정렬도 선택정렬과 같이 1회 순회에 한 개 요소의 위치가 결정된다. 하지만 선택정렬과 다른 점은 가장 큰 값(오른쪽부터 왼쪽으로) 우선으로 결정된다는 점이다. 따라서 이중 for-loop 형태로 반복 순회를 할때 내부 for-loop의 경우에는 완성된 오른쪽 요소의 수만큼 차감시켜서 순회를 해야한다. 1234567for (let i = 0; i &lt; arr.length; i++) { for (let j = 0; j &lt; arr.length - i - 1; j++) { if (arr[j] &gt; arr[j + 1]) { [arr[j], arr[j + 1]] = [arr[j + 1], arr[j]]; } }} 삽입정렬(Insertion sort) - O(N^2) 시간복잡도/정수 6만개 기준, 7.438(sec)삽입정렬의 경우에는 앞서 구현해본 1회 순회에 한 개 요소의 최종 위치가 결정되는 선택정렬, 버블정렬과 달리 정렬의 범주를 점진적으로 늘려가며 정렬을 하는 방식이다.가장 처음 요소는 정렬이 되어있다고 가정을 하고 두 번째 요소부터 배열의 길이만큼 순회를 하게 되는데, 가장 처음 요소는 정렬이 되어있다고 가정을 하였기 때문에 두 번째 요소를 key값으로 선언을 한다. key값으로 선언을 하는 이유는 j index의 값과 key값을 비교를 해서 j index의 값이 key값보다 큰 경우에 j+1위치의 값(key값 설정 위치)을 j위치의 값으로 덮어쓰는 형태로 진행이 되기 때문에 기존 j+1의 위치값을 저장해두기 위한 목적에서 key값으로써 별도로 저장해둔다. 코드구현시 주의해야 될 점은 i의 진행방향과 j의 진행방향이 반대라는 점이다. i는 왼쪽에서 오른쪽 방향으로 진행이 되고, j의 경우에는 오른쪽에서 왼쪽방향으로 진행이 된다. 구현 1 123456789for (let i = 1; i &lt; arr.length; i++) { let key = arr[i]; let j = i - 1; while (j &gt;= 0 &amp;&amp; arr[j] &gt; key) { arr[j + 1] = arr[j]; j--; } arr[j + 1] = key;} 구현 2 123456789for (let i = 1; i &lt; arr.length; i++) { let key = arr[i]; let j; for (j = i - 1; j &gt;= 0; j--) { if (arr[j] &gt; key) arr[j + 1] = arr[j]; else break; } arr[j + 1] = key;}","link":"/2021/04/16/202104/210416-Basic_sorting_algorithm/"},{"title":"210415 React TIL - useReducer, setState의 비동기적 처리, useReducer에서 dispatch를 통한 state 업데이트(비동기 처리), 성능 최적화","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. useReducerReact에서 Redux의 핵심 기능을 useReducer로써 구현하였다. useReducer + context API의 조합으로 Redux를 대체할 수 있을 것이라고 생각할 수 있다. 작은 앱의 경우에는 괜찮지만, 비동기 처리의 한계로 어플리케이션의 규모가 커지게 되면 Redux를 사용해서 구현하는 편이 좋다.state의 갯수가 늘어나게 되면, useState를 통한 state관리 코드가 많아진다. 이러한 경우 useReducer를 사용해서 하나의 state와 setState로 통합해 줄 수 있다. state 객체는 아무도 직접적인 수정이 불가하다. 다만 dispatch를 통해 action object를 인수로써 호출하게 되면 action object의 type에 정의된 action의 종류에 따라 reducder 함수에서 정의된 state 업데이트 방식에 의해 state가 업데이트된다. 12345// useReducer의 첫 번째 인수로는 정의한 reducer 함수를 넣어주고, 두 번째 인수로는 state 초기 상태를 정의해준다.const [state, dispatch] = useReducer(reducer, initialState);// event 처리부분dispatch({ type: 'SET_RESULT', result: 'ABC' }); reducer 함수 예시 123456789const reducer = (state, action) =&gt; { switch (action.type) { case 'SET_RESULT': return { ...state, result: action.result }; }}; 나중을 위해 action의 이름은 개별 상수로 따로 빼서 관리하도록 한다.추가적으로 관리해야 될 state가 있다면 action을 추가하고 해당 action에 대한 구체적인 state 업데이트 방식을 reducer 함수에 정의하도록 한다. 기존의 state를 업데이트할때 사용할 상태값은 reducer 함수의 두 번째 인자(action)로부터 넘겨준다. (Redux는 Facebook의 Flux와 함수형 프로그래밍에 영감을 받아 2015년 6월경에 Dan Abramov에 의해 개발되었다) React에서 setState의 비동기적 처리React에서는 setState가 비동기적으로 동작하기 때문에 동일 함수내에서 state를 업데이트하고 state를 바로 확인해보면 값이 업데이트되기 이전의 state로 확인이 된다.따라서 class component에서는 다음과 같이 callback function으로 해결을 할 수 있다. 1234567891011function changeValue(e) { this.setState({ value: e.target.value }); // 이전 value값 출력 console.log(this.state.value);}// solutionfunction changeValue(e) { this.setState({ value: e.target.value }, function () { console.log(this.state.value); });} 그렇다면 functional component에서는 어떻게 해결할 수 있을까?바로 Hooks의 useEffect를 사용하면 해결할 수 있다. 1234567891011121314const changeValue = (e) =&gt; { setValue(e.target.value); // 이전 value값 출력 console.log(value);};// solutionconst changeValue = (e) =&gt; { setValue(e.target.value);};useEffect(() =&gt; { console.log(value);}, [value]); value의 값이 변할때마다 useEffect내의 console.log()가 호출되기 때문에, changeValue 함수 내에서 setValue를 통해 값을 바꾸고 나서 value 값이 바뀐 것을 useEffect가 감지를 하고 console.log를 실행하기 때문에 제대로 바뀐 상태값 value를 console.log에 출력하게 된다. useReducer에서 dispatch를 통한 state 업데이트 - 비동기useReducer에서 dispatch를 통해 state 값을 바꿀때 비동기적으로 바뀐다. 따라서 dispatch를 통해서 state를 업데이트 할 때에는 useEffect내에서 처리하도록 한다.(참고로 Redux에서는 state가 동기적으로 바뀐지만, useReducer에서는 state가 비동기적으로 바뀐다.) 성능 최적화성능 최적화를 위해서는 어떤 요소가 불필요한 렌더링을 유발시키는지 확인을 해야 한다.파악하는 방법은 이전에 넘겨받은 props(useRef를 통해 기억)와 현재 넘겨받은 props를 비교해서 서로 다르다면 해당 요소때문에 렌더링이 발생하는 것이다. 12345678910const ref = useRef([]);useEffect(() =&gt; { console.log( rowIndex === ref.current[0], cellIndex === ref.current[1], dispatch === ref.current[2], cellData === ref.current[3] ); ref.current = [rowIndex, cellIndex, dispatch, cellData];}, [rowIndex, cellIndex, dispatch, cellData]); 가장 손쉽게 해결할 수 있는 방법은 React.memo로 함수 전체를 감싸서 처리해주는 것이다. 만약 React.memo를 적용했는데도 성능개선이 되지 않는다면, jsx 구문에서 반환하는 자식 component를 useMemo로 감싸서 component 자체를 기억시키도록 할 수도 있다. 12345678910111213141516171819return ( &lt;tr&gt; {Array(rowData.length) .fill() .map((td, i) =&gt; useMemo( () =&gt; ( &lt;Tr key={i} dispatch={dispatch} rowIndex={i} cellData={rowData[i]} /&gt; ), [rowData[i]] ) )} &lt;/tr&gt;); ContextAPI부모와 자식간의 관계가 깊어지면 props를 물려줄때 힘들기 때문에 contextAPI를 사용한다.사용법은 부모 컴포넌트에서 React.createContext()를 초기값과 함께 초기화를 시켜주고, 부모 컴포넌트에서 전달하는 값들을 전달받아서 사용할 자식 컴포넌트 전체를 작성한 createContext의 Provider로 감싸준다. 감싸준 태그에는 value의 속성으로 contextAPI를 통해 전달할 값을 객체형태로 전달을 해주면 된다. contextAPI - ParentComponent 123456789101112131415161718192021222324252627282930import React, { createContext, useReducer } from 'react';// createContext의 초기값은 값의 형태만으로 정의해준다.export const SampleContext = createContext({ data: [], dispatch: () =&gt; {},});const initialState = { data: []};const reducer = (state, action) =&gt; { switch(action.Type){ default: return state; }};// component내....const [ state, dispatch ] = useReducer(reducer, initialState);return ( &lt;SampleContext.Provider value={{ data: state.state }}&gt; &lt;FirstChildContext /&gt; &lt;SecondChildContext /&gt; &lt;ThirdChildContext /&gt; &lt;/SampleContext.Provider&gt;).... 위에서 작성한 코드와 같이 SampleContext.Provider의 value 속성으로 자식 컴포넌트에 넘겨 줄 객체 자체를 넣어주게 되면, 함수 컴포넌트가 재실행될 때 마다 value 속성으로 넣어준 객체가 재생성된다.이 말은 ContextAPI를 통해 부모로부터 값을 넘겨받는 자식 컴포넌트들도 모두 새로운 객체를 넘겨받는 것으로 인식을 해서 다시 렌더링된다는 의미이다.따라서 value의 값에 객체 그대로를 넣어주지 말고 반드시 useMemo를 사용해서 값을 기억하도록 작성해줘야 한다.dispatch는 절대로 바뀌지 않기 때문에 useMemo의 deps값으로 넣어주지 않아도 된다. ContextAPI 성능 최적화 12345678910import React, { useMemo } from 'react';....const value = useMemo(() =&gt; ({ data: state.data, dispatch }), [state.data]);return ( &lt;SampleContext.Provider value={ value }&gt; &lt;FirstChildContext /&gt; &lt;SecondChildContext /&gt; &lt;ThirdChildContext /&gt; &lt;/SampleContext.Provider&gt;) contextAPI - ChildComponent 123456import React,{ useContext } from 'react';import { SampleContext } from './ParentComponent';// component내....const { data, dispatch } = useContext(SampleContext);....","link":"/2021/04/15/202104/210415-React-review_study/"},{"title":"210416 JavaScript와 친해지기 - 클로저(Closure)와 즉시실행 함수(IIEF)에 대한 이야기","text":"이번 포스팅에서는 아직은 조금 낯설은 클로저(Closure)에 대해서 정리를 해보려고 한다. 클로저(Closure)클로저에 대한 정의는 ECMAScript에 없다. 다만 클로저는 자바스크립트가 채용하고 있는 기술적 기반/컨셉이다.자바스크립트는 클로저를 이용하여 스코프적 특징과 함수에 대한 명세를 구현하였다.다시 말해 클로저(Closure)는 ECMAScript의 명세에는 없지만 스코프와 스코프와 긴밀한 관련이 있는 1급 객체로서의 함수와 매우 관련이 깊다. 자 그럼 누군가 클로저(Closure)가 뭐냐고 물어본다면 뭐라고 대답할 수 있을까? inner function은 항상 outer function의 변수와 인자에 접근을 할 수 있다. 비록 outer function이 이미 호출되어 값이 반환되었을지라도 outer function의 변수 객체에 접근이 가능한 이유가 바로 클로저(Closure) 덕분이다. 아직 설명이 부족하기 때문에 아래 간단한 예시를 살펴보자. simple example(1) 1234567891011121314151617181920212223function add(a) { var x = '두 숫자의 합은 '; return function (b) { var sum = a + b; console.log(x + sum); };}var totalSum = add(10);totalSum(5);// output: 두 숫자의 합은 15add(10);/*ƒ(b){ var sum = a + b; console.log(x + sum);}*/add(10)(4);// output: 두 숫자의 합은 14 JavaScript에서 함수는 1급 객체(First-Class Objects)로써 정의되었기 때문에 위와같이 함수 내에서 함수를 반환할 수 있는 것이다. 자 그럼 위의 결과값부터 말을 하자면 두 숫자의 합은 15이다. 새로운 함수는 새로운 함수 실행 컨텍스트를 갖는다. 이전에 스코프에 대해서 블로그 포스팅을 했을때 이미 자세히 다뤘듯이 실행 컨텍스트 내부에는 코드가 실행되기 위한 다양한 정보(변수, 함수 등)들과 참조 속성에 대한 정보가 담긴다. 이 참조 속성이 상위 스코프의 실행 컨텍스트의 참조 속성으로 연결되어 변수를 참조하는 것을 스코프 체인(Scope chain)이라고 정의를 한다. 자 그럼 위의 예시 코드에서 totalSum이라는 변수에 10이라는 인수로 add함수를 호출한 결과를 담았다. add 함수를 호출하는 과정에서 이 함수 실행 컨텍스트는 전역 실행 컨텍스트의 위에 쌓이게 된다. 그리고 실행되고 나서 호출스택(Call stack)으로부터 pop되었다. 그 다음에 5라는 인수를 넣어 totalSum이라는 함수를 호출하고 있다.이미 앞에서 결과값을 말했듯이 출력내용은 두 숫자의 합은 15이다.여기서 의문이 드는 점은 add 함수에 대한 함수 실행 컨텍스트는 호출 스택으로부터 pop되었고, 그렇다면 Scope chain에서도 존재를 하지 말아야 되는데, 결과값으로 15가 나온다는 것은 totalSum에 5의 인수를 넣어 호출하기 이전에 10이라는 인수를 넣어 add 함수를 호출한 것에 대한 변수 객체는 여전히 존재를 한다는 말이 된다.스코프 체인상에는 add 함수의 변수 객체가 존재를 하고, 이후에 호출된 totalSum이라는 함수는 앞서 호출된 add 함수의 변수 객체를 참조할 수 있는 것이다. 이렇듯 호출 스택에서 실행 컨텍스트가 사라진 뒤에도 사라진 실행 컨텍스트의 변수 객체를 참조할 수 있는 것이 바로 클로저(Closure) 덕분이다. 두 번째 예시코드를 살펴보자. 앞서 살펴 본 예시에서 알 수 있듯이 Closure는 inner function으로부터 outer function의 스코프에 접근할 수 있도록 해주는 역할을 하며,closure를 이용하면 private variable을 만들 수 있다. 아래의 예시코드에서 newAccount의 내부에는 인수로 받은 initialBalance로 값을 초기화하는 balance라는 변수가 있다. 이 변수가 newAccount라는 outer function에 의해서 보호되는 private variable이다. simple example(2) 1234567891011121314151617// Closurefunction newAccount(name, initialBalance) { let balance = initialBalance; function showBalance() { console.log(`Hey ${name}, your balance is ${balance}`); } return showBalance;}newAccount('susan', 500)();const john = newAccount('john', 300);const bob = newAccount('bob', 1000);john();bob(); advanced example 12345678910111213141516171819202122232425262728293031function newAccount(name, initialBalance) { let balance = initialBalance; function showBalance() { console.log(`Hey ${name}, your balance is ${balance}`); } function deposit(amount) { balance += amount; showBalance(); } function withdraw(amount) { if (amount &gt; balance) { console.log(`Hey, ${name}, sorry not enough funds`); return; } balance -= amount; showBalance(); } return { showBalance: showBalance, deposit: deposit, withdraw: withdraw };}const john = newAccount('john', 1000);const bob = newAccount('bob', 1000);john.showBalance();john.deposit(400);john.deposit(1000);john.withdraw(400);john.balance = 10000;john.withdraw(2001);bob.showBalance();bob.deposit(400); 즉시 실행함수 표현(IIFE - Immediately-Invoked Function Expression)즉시 실행함수 표현을 사용함으로써 생기는 이점은 다음 두 가지가 있다.첫 번째, global scope pollution으로부터 회피할 수 있는 간단한 방법이다.두 번째, 함수와 함수내의 변수들의 스코프를 보호하기 위한 좋은 방법이다. 하지만 다소 오래된 접근방법이기 때문에 module이라는 새로운 방식으로 코드를 작성하도록 하자. 간단하게 아래의 코드를 살펴보자. 12345678const num1 = 30;const num2 = 50;function add() { console.log(`the result is : ${num1 + num2}`);}add(); 위의 코드는 모두 전역 스코프에 변수와 함수를 선언하고 실행하고 있다. 이렇게 작성을 해주게 되면 변수 이름간의 충돌이 발생할 수 있다. 따라서 아래와 같이 즉시 실행함수 표현으로 함수를 실행해주게 되면 함수내에 선언된 변수를 보호할 수 있고 전역 스코프가 오염되는 것을 방지할 수 있다. 12345(function () { const num3 = 30; const num4 = 50; console.log(`the result is : ${num3 + num4}`);})(); 즉시 실행함수를 통해 실행하는 경우 인수를 넣어줄 수도 있고 실행한 결과값을 별도의 변수에 담아줄 수도 있다. 123456const result = (function (num3, num4) { console.log(`the result is : ${num3 + num4}`); return num3 + num4;})(500, 900);console.log(result);","link":"/2021/04/16/202104/210416-javascript-basic_til/"},{"title":"210417 Mobile first 방식의 이점과 간단한 HTML 마크업과 CSS 스타일링 예시","text":"Mobile-First 방식 ?이번 포스팅에서는 모바일 우선(Mobile-First) 웹 디자인에 대해서 정리를 하려고 한다.요즘 간단한 화면의 구성요소들을 모듈형태로 나눠서 프로젝트식으로 만들어보고 있는데, 모두 Mobile-first 방식으로 웹 디자인을 하고 있다.이전에 반응형 웹 페이지를 만들때는 아무 생각없이 처음에 Desktop 기준으로 웹 페이지를 디자인하는 경우가 많았는데, 모바일 우선 방식으로 웹 페이지를 디자인 하는 것이 더 효율적이라는 것을 알게 된 후부터는 시작할때 모바일 기준으로 HTML 마크업을 하고 CSS 스타일링을 하고 있다. 이번 포스팅에서는 왜 모바일 우선으로 웹 페이지를 디자인해야 되는지에 대해서 공부한 내용을 기반으로 내용을 작성해보려고 한다. 우선 Desktop과 Mobile의 사용률을 보았을때 2014년도를 기점으로 Mobile 사용률이 Desktop 사용률을 넘어섰다. 단순히 이러한 사용률만의 문제가 아니라 모바일 우선 방식으로 웹 페이지를 디자인했을때 생기는 이점들에 대해서 정리를 해보겠다. 우선 첫 번째, 구글은 Mobile-First Index를 사용한다. 구글은 우리가 모두 다 아는 대규모 검색 엔진으로 구글이 모바일에 올인하고 있다. 구글의 검색 알고리즘이 모바일 버전 사이트의 콘텐츠를 사용하여 해당 사이트의 검색 노출 페이지 순위를 매긴다는 의미이다. 만약 본인이 개발한 웹 페이지가 구글에서 많이 노출되길 원한다면 웹 페이지를 모바일 우선 방식으로 디자인하는 것이 좋다. 두 번째, 좋은 사용자 경험(UX)을 줄 수 있다. 일반적으로 모바일 사용자는 빠른 웹 사이트 로드를 기대한다. 만약 웹 페이지가 모바일 우선 방식으로 디자인이 되어있다면, 모바일 사용자가 초기에 웹 페이지를 로드했을때 빠르게 페이지가 로드되어 사용자에게 좋은 경험을 줄 수 있다. 세 번째, 빠른 웹 사이트를 만들 수 있다. 두 번째 이점과 거의 같은 맥락의 이야기지만, 모바일 화면에서 보여지는 페이지의 요소는 상대적으로 데스크탑 웹 페이지에 비해 화면에 보여지는 요소가 적다. 만약 반응형 디자인으로 페이지가 디자인되어 있고, 모바일에서 해당 웹 페이지를 로드한다면, 초기에 로드되어야 하는 화면의 요소가 데스크탑 기준으로 우선 로드되고, 미디어 태그로 조건 처리된 모바일 기준 스타일이 다음으로 적용되어 상대적으로 느린 웹 페이지 로드를 보여 줄 수 있다. Mobile-First 방식의 HTML 마크업 및 CSS 스타일링 예시우선 웹 페이지를 디자인할때 mobile화면에서 어떻게 페이지가 출력이 되는지 구상한다.간단하게 네비게이션 바를 예로들어 설명을 해보겠다. 웹 페이지를 데스크탑과 모바일에서 보았을때 가장 눈에 띄게 다른 부분은 바로 이 네비게이션 바 부분이다. 보통 데스크탑 기준으로 웹 페이지의 네비게이션 바를 본다면 모든 메뉴들이 다 보여지고, 모바일 기준으로 화면의 viewport 사이즈를 줄였을 때 모든 메뉴들이 숨겨지고 toggle 버튼이 보여지게 된다. 간단하게 모바일 기준으로 네비게이션 바의 HTML 마크업을 해보면 아래와 같다.네비게이션 바의 구성은 로고 이미지와 토글버튼 그리고 메인 메뉴로 구성이 되어있다. 1234567891011121314151617181920212223242526272829&lt;!-- header --&gt;&lt;header id=&quot;home&quot;&gt; &lt;!-- navbar --&gt; &lt;nav id=&quot;nav&quot;&gt; &lt;div class=&quot;nav-center&quot;&gt; &lt;!-- nav header --&gt; &lt;div class=&quot;nav-header&quot;&gt; &lt;img src=&quot;./logo.svg&quot; alt=&quot;logo&quot; class=&quot;logo&quot; /&gt; &lt;button class=&quot;nav-toggle&quot;&gt; &lt;i class=&quot;fas fa-bars&quot;&gt;&lt;/i&gt; &lt;/button&gt; &lt;/div&gt; &lt;!-- links --&gt; &lt;div class=&quot;links-container&quot;&gt; &lt;ul class=&quot;links&quot;&gt; &lt;li&gt; &lt;a href=&quot;#home&quot; class=&quot;scroll-link&quot;&gt;home&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;#menu1&quot; class=&quot;scroll-link&quot;&gt;Menu1&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;#menu2&quot; class=&quot;scroll-link&quot;&gt;Menu2&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/nav&gt;&lt;/header&gt; 이제 위에서 마크업한 HTML 태그를 모바일 우선 방식으로 CSS 스타일링을 해보겠다. 세세한 스타일링 부분은 제외하고 화면 배치와 관련있는 스타일링을 위주로 작성해보았다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/* nav의 배경색과 padding 설정 */nav { background: var(--clr-white); padding: 1rem 1.5rem;}/* 로고와 토글버튼을 nav의 좌우 양쪽 끝에 배치 */.nav-header { display: flex; align-items: center; justify-content: space-between;}/* 토글버튼 스타일링 */.nav-toggle { font-size: 1.5rem; color: var(--clr-grey-1); background: transparent; border-color: transparent; transition: var(--transition); cursor: pointer;}/* 모바일 페이지에서는 메인 메뉴 부분의 부모 요소의 높이를 0으로 설정하고 부모 요소 내부의 자식요소를 숨기기 위해서 overflow: hidden 속성을 준다.*/.links-container { height: 0; overflow: hidden; transition: var(--transition);}/* 토글버튼이 클릭되었을때 메인 메뉴 부분의 부모 요소에 높이를 주기 위한 css class *//* 이 부분은 동적으로 height 속성을 javascript에서 인라인 스타일링한다.*/.show-links { height: 200px;}/* 메인 메뉴의 메뉴 &lt;a&gt; 태그에 대한 스타일링 (모바일 페이지 기준) */.links a { background: var(--clr-white); color: var(--clr-grey-5); font-size: 1.1rem; text-transform: capitalize; letter-spacing: var(--spacing); display: block; transition: var(--transition); font-weight: bold; padding: 0.75rem 0;}/* 데스크탑 기준 */@media screen and (min-width: 800px) { nav { background: transparent; } /* 토글 버튼 화면에서 표시 안함 */ .nav-toggle { display: none; } /* 자바스크립트에서 인라인 방식으로 스타일링하게 되면 우선순위가 CSS Stylesheet에 작성한 스타일링보다 높게 되어 토글 버튼 클릭(인라인 스타일링 적용)후에 viewport의 크기를 조정하여도 인라인 스타일링한 스타일 속성이 그대로 적용되어있어 화면 배치가 깨진다. 따라서 아래와 같이 !important로 이전의 스타일링을 무시하고 최우선으로 스타일을 적용할 수 있도록 하였다. */ .links-container { height: auto !important; } /* 메인메뉴의 출력 */ .links { display: flex; }}","link":"/2021/04/17/202104/210417_mobile-first/"},{"title":"210514 React TIL - ContextAPI의 사용, Custom Hook(useContext + React.createContext), Strict mode, action 객체와 reducer 함수의 작성, React에서 상태관리의 위치","text":"20210418 Update - Custom hooks - context.js20210514 Update - Custom hooks(input) 본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. 실습 Repository : https://github.com/LeeHyungi0622/react-basic-projects/blob/master/sidebar-modal/src/context.js ContextAPI의 사용 &amp; Custom Hook(useContext + React.createContext)여지까지 ContextAPI를 모든 하위 컴포넌트들의 상위 컴포넌트에서 정의해서 사용했었는데, 이 ContextAPI부분을 별도의 파일로 정의를 해서 사용할 수 있다.이렇게 별도의 context.js파일로 작성해서 관리를 하면 코드를 좀 더 가독성 좋게 만들 수 있는 것 같다. 그리고 ContextAPI에서 전달하는 값을 사용하는 하위 컴포넌트들에서는 매번 useContext를 import해서 인자로 React.createContext() 객체를 인자로 넣어서 값을 호출해서 사용하였는데 이 부분은 별도의 custom Hook으로 정의해서 사용을 할 수도 있다. (custom Hook을 만들어서 사용할때에는 함수의 이름에 접두사 use를 꼭 붙여줘야 한다.) context.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import React, { useState, useContext } from 'react';const AppContext = React.createContext();const AppProvider = ({ children }) =&gt; { // modal, sidebar state const [isSidebarOpen, setIsSidebarOpen] = useState(false); const [isModalOpen, setIsModalOpen] = useState(false); const openSidebar = () =&gt; { setIsSidebarOpen(true); }; const closeSidebar = () =&gt; { setIsSidebarOpen(false); }; const openModal = () =&gt; { setIsModalOpen(true); }; const closeModal = () =&gt; { setIsModalOpen(false); }; return ( &lt;AppContext.Provider value={{ isModalOpen, isSidebarOpen, openModal, openSidebar, closeModal, closeSidebar }} &gt; {children} &lt;/AppContext.Provider&gt; );};// Custom Hook(useContext + AppContext)export const useGlobalContext = () =&gt; { return useContext(AppContext);};// index.js에서 최상위 App.js를 감싸주기 위해 AppProvider를 export해준다. (index.js)export { AppProvider }; Strict 모드StrictMode는 프로그램에서의 잠재적인 문제점을 밝혀내기 위한 도구이다. Fragment와 같이 StrictMode는 가시적으로 UI에 렌더링되지 않는다. StrictMode는 하위 요소에 대해서 추가적인 검사 및 경고를 활성화한다. (개발모드에서만 실행, Production build에는 영향을 주지 않는다.) StrictMode를 사용하게 되면 아래의 항목들을 검사할 수 있다. 안전하지 않은 생명주기가 있는 구성 요소 식별 (레거시 라이프 사이클 메서드 - componentWillMount, componentWillReceiveProps, componentWillUpate) 레거시 문자열 참조 API 사용에 대한 경고React에서는 refs를 관리하기 위한 두 가지 방법을 제공했다. (legacy string ref API, callback API) ref API는 콜백 API보다 상대적으로 편리하지만 몇가지 문제점이 있기 때문에 공식적으로 콜백 API 사용을 권장하고 있다. React 16.3 버전부터 문제점이 개선된 ref API를 제공한다. 123456789101112131415class MyComponent extends React.Component { constructor(props) { super(props); this.inputRef = React.createRef(); } render() { return &lt;input type=&quot;text&quot; ref={this.inputRef} /&gt;; } componentDidMount() { this.inputRef.current.focus(); }} 예기치 않은 부작용을 감지 레거시 컨텍스트 API 감지 onContextMenu 속성 (마우스 오른쪽 클릭 이벤트 속성)보통은 마우스 오른쪽 클릭 이벤트가 사용되지 않지만 게임을 만들때 종종 사용된다. action 객체와 reducer 함수의 작성Action은 추상적으로 우선 작성하고 구체적인 구현은 reducer 함수에서 구현을 해준다. React에서 상태관리는 controller의 위치를 기준으로 한다React에서 상태관리는 controller의 위치를 기준으로 한다. (전역적으로 관리되는 상태의 경우에는 ContextAPI가 위치한 곳에서 정의) 예를들어 Home 컴포넌트와 Modal 컴포넌트가 있는데, modal 창이 열려있는지, 닫혀있는지에 대한 상태관리는 modal 창을 열고 닫게 하는 버튼이 위치한 Home 컴포넌트에서 관리를 한다. 20210514 Update 반복되는 input 부분을 custom hook으로 공통화 처리useInput.js 123456789import { useState, useCallback } from 'react';export default (initialValue = null) =&gt; { const [value, setter] = useState(initialValue); const handler = useCallback((e) =&gt; { setter(e.target.value); }, []); return [value, handler];};","link":"/2021/05/14/202104/210418-React-review_study/"},{"title":"210420 Front-end Interview 준비 (작성중...)","text":"오늘부터 여지까지 공부했던 내용들을 토대로 조금씩 프론트엔드 개발 기술면접 대비를 해보려고 한다. 아직 공부한 내용은 많지 않지만, 내가 공부했던 내용들 중에서 좀 더 보충해야 되는 부분이 있다면 내용을 보강해서 진행해보도록 하겠다.알고 있는 내용이라고 하더라도 막상 누군가로부터 질문을 받게 되면 제대로 대답을 못하게 되는게 일반적이라 평소에 조금씩 꾸준하게 준비해두려고 한다.면접 예상 질문은 구글링을 한 내용을 참고하였으며, 여지까지 공부했던 내용들과 가장 기본이 되는 내용들을 위주로 질문을 선별해서 작성하였다. ## interview Q1. 호이스팅(Hoisting)에 대해서 설명하세요. ## interview Q2. 이벤트 버블링(Event bubbling)에 대해서 설명하세요. ## interview Q2-1. 이벤트 위임에 대해서 설명하세요. ## interview Q3. document load 이벤트와 document DOMContentLoaded 이벤트의 차이점에 대해서 설명하세요. ## interview Q3-1. 왜 load 이벤트를 사용했나요? 그 이유를 설명하고 이 이벤트를 사용함으로써 생기는 단점에 대해서 설명하세요. 그리고 다른 대안을 알고 있다면 설명을 하고 왜 그 대안을 사용할 건지에 대해서 설명하세요. ## interview Q4. ==와 ===의 차이점에 대해서 설명하세요. ## interview Q5. 100까지 증가하면서 3의 배수에는 fizz를 출력하고, 5의 배수에는 buzz를 출력하고, 3과 5의 배수에는 fizzbuzz를 출력하는 for loop를 손코딩하고 설명하세요. ## interview Q6. 웹 사이트의 전역 스코프를 그대로 두고 건드리지 않는 것이 좋은 이유와 방법에 대해서 설명하세요. ## interview Q7. Promise와 그 Polyfill에 대해서 알고 있는대로 설명하세요. ## interview Q8. Callback 대신에 Promise를 사용할 때의 장점과 단점에 대해서 설명하세요. ## interview Q9. JavaScript 코드를 디버깅 하기 위해서 어떤 도구와 기술을 사용해봤는지 설명하세요. ## interview Q10. function foo() {}와 var foo = function() {} 사이에서 foo 사용의 차이점에 대해서 설명하세요. ## interview Q11. let,var,const를 사용하여 생성된 변수들의 차이점에 대해서 설명하세요. ## interview Q12. 화살표 함수 문법에 대한 사용예시를 들고, 다른 함수와는 어떤 차이가 있는지 설명하세요. ## interview Q13. 생성자 함수의 메서드에 화살표 문법을 사용하면 어떤 이점이 있는지 설명하세요. ## interview Q14. 고차 함수(Higher-Order Function)에 대해서 설명하세요. ## interview Q15. 객체나 배열에 대한 destructuring에 대해 예시를 들고 설명하세요. ## interview Q16. spread 문법을 사용할때의 이점은 무엇이며 rest 문법과 다르점은 무엇인지 설명하세요. ## interview Q17. 정적 클래스 멤버를 만드는 이유는 무엇인지 설명하세요. ## interview Q18. this가 JavaScript에서 어떻게 동작하는지 설명하세요. ## interview Q19. 클로저는 무엇이며 어떻게 사용하며 왜 사용을 해야 되는지 설명하세요. ## interview Q20. 익명 함수의 일반적인 사용 사례는 무엇인지 설명하세요. ## interview Q21. .call과 .apply 그리고 ‘.bind’의 차이점에 대해서 설명하세요. ## interview Q22. 데이터 바인딩 (단방향 바인딩, 양방향 바인딩)에 대해서 설명하세요.","link":"/2021/04/20/202104/210420-Front-end-interview-preparation/"},{"title":"210420 결정 알고리즘(Decision algorithm)을 활용한 문제풀이","text":"결정 알고리즘은 이분검색을 기반으로 한다.알고리즘 문제에서 주어진 조건에서의 최대값 혹은 최소값을 구하는 문제 중에 몇 몇 문제는 결정 알고리즘(Decision algorithm)을 활용해서 풀이해야 한다.결정 알고리즘은 이분 검색을 기반으로 하는 문제로, 전체 데이터를 순회하면 시간 복잡도 O(N)을 갖지만 이분 검색으로 데이터를 검색하면 O(logN)만큼의 시간 복잡도를 갖는다. 예시문제길이가 N인 숫자 리스트가 있다고 가정했을때, 총 T 개의 그룹으로 리스트를 나눠야 한다.T개의 그룹으로 리스트를 나눴을때 한 그룹당 최소 얼마의 숫자합이여야 하는지 구하시오. 1234567891011121314151617181920212223242526272829303132333435363738394041424344const numList = [1, 3, 2, 5, 4, 7, 6, 9, 8];// 숫자 리스트의 최소 합과 최대 합을 구한다.// 숫자 리스트에서 최대값이 최소합let lt = Math.max(...numList);// 숫자 리스트 전체 요소들의 합이 최대합let rt = numList.reduce((acc, n) =&gt; acc + n, 0);const target = 3;let answer = 0;// 총 몇 개의 그룹으로 나눠지는지 판별하는 함수function count(numList, mid) { // 처음 그룹 let cnt = 1; let sum = 0; for (let n of numList) { // sum + n의 값이 한계값을 초과하는 경우, if (sum + n &gt; mid) { // 더하는 그룹의 갯수를 1증가 cnt += 1; // 합계를 다음 더할 숫자 n으로 초기화 sum = n; } else { // 아직 합계가 한계값을 초과하지 않은 경우, // 계속해서 sum에 값 n을 누적한다. sum += n; } } return cnt;}while (lt &lt;= rt) { // 최소합 ~ 최대합의 중간 값부터 탐색을 시작 let mid = Math.floor((lt + rt) / 2); // 조건 그룹 갯수보다 작거나 같은 경우, if (count(numList, mid) &lt;= target) { // 해당 합계는 answer가 될 수 있는 조건을 충족 answer = mid; // 조건을 만족하기 때문에 mid - 1을 최대 범위로 업데이트 rt = mid - 1; } else { // 만약 조건을 만족하지 않은 경우, 최소값을 mid + 1로 업데이트 lt = mid + 1; }}","link":"/2021/04/20/202104/210420-Algorithm_decision_tree/"},{"title":"210421 TypeScript TIL - JavaScript와 TypeScript, TypeScript 설치, HTML 태그의 타입, 타입스크립트의 점진적 도입, 문법적 설탕(Syntactic sugar), tsconfig.json에서 자주 사용되는 옵션들, 공식문서 handbook 읽기","text":"JavaScript와 TypeScriptTypescript는 JavaScript의 superset이라고 불리며, 자바스크립트의 변수/함수의 매개변수와 리턴값에 명시적으로 타입을 지정한다. 항상 그러하듯이 공식 사이트를 참고하면서 공부하면 좋다. https://www.typescriptlang.org/ Typescript는 언어이자 라이브러리로, 프로그래밍 언어는 실행기가 필요하다. JavaScript의 실행기는 ‘Browser’와 ‘Node’가 있다. 이 실행기가 프로그래밍 언어를 해석해주는 역할을 해준다. 그렇다면 TypeScript의 실행기는 무엇일까? TypeScript의 실행기는 현재 개발중에 있는 Deno이다. 아직은 안정적인 버전이아니기 때문에 라이브러리가 TypeScript의 실행기이다. 실행기가 라이브러리라서 좋은 점은 빠르게 release할 수 있다는 점이 있지만 단점으로는 여러가지 버그가 존재한다는 것이다.TypeScript의 라이브러리(실행기)를 설치하기 위해서는 Node를 설치해줘야 되기 때문에 Node에 대해서 어느정도는 이해해야 한다.이 TypeScript 라이브러리가 TypeScript를 JavaScript로 코드 변환을 해주는데 이는 브라우저는 JavaScript 코드만 이해할 수 있기 때문이다. 특수기호가끔 특수기호에 대해서 말을 할때 어떻게 말을 해야되는지 모르는 경우가 있다.간단하게 몇 개정도만 정리를 해보겠다.(1) ^ : Circumflex, Caret(2) &amp; : Ampersand(3) ~ : Tilde TypeScript 설치1234# typescript 설치$ npm i typescript# typescript를 명령어로써 설치$ npm i -g typescript 이제 터미널에서 tsc명령을 통해서 _.ts 파일을 _.js 파일로 컴파일할 수 있다.컴파일을 하면 에러가 발생할 경우 터미널에서 TS XXXX 에러코드도 같이 표시가 되는데, 이 에러코드를 구글링해서 에러를 해결할 수 있다. HTML 태그의 타입아래와 같이 코드를 작성하고 변수이름에 마우스 포인터를 올려보면, HTMLDivElement 타입이라는 것을 알 수 있다. 12// HTMLDivElementconst word: HTMLDivElement = document.createElement('div'); 이는 타입스크립트에서 이미 만들어 놓은 것으로, 변수명에서 오른쪽 클릭한 뒤에 &quot;Go to type definition&quot;을 클릭해보면 *.d.ts 파일에 정의된 타입내용들을 확인할 수 있다._.d.ts 파일에는 타입들만 적어두었고, 이 _.d.ts 파일은 개발자가 필요에 의해 직접 작성을 하고 loading을 해서 _.ts 파일에서 사용을 할 수도 있다. loading을 하는 방법은 TypeScript에 대한 환경설정 파일인 tsconfig.json에서 --types나 --typeRoots의 옵션에 _.d.ts파일의 경로를 지정해주면 된다.tsconfig.json의 설정에서 자주 사용되는 옵션에 대한 내용은 별도의 category로 작성을 해보겠다. 123456789101112/** Provides special properties (beyond the regular HTMLElement interface it also has available to it by inheritance) for manipulating &lt;div&gt; elements. */interface HTMLDivElement extends HTMLElement { /** * Sets or retrieves how the object is aligned with adjacent text. */ /** @deprecated */ align: string; addEventListener&lt;K extends keyof HTMLElementEventMap&gt;(type: K, listener: (this: HTMLDivElement, ev: HTMLElementEventMap[K]) =&gt; any, options?: boolean | AddEventListenerOptions): void; addEventListener(type: string, listener: EventListenerOrEventListenerObject, options?: boolean | AddEventListenerOptions): void; removeEventListener&lt;K extends keyof HTMLElementEventMap&gt;(type: K, listener: (this: HTMLDivElement, ev: HTMLElementEventMap[K]) =&gt; any, options?: boolean | EventListenerOptions): void; removeEventListener(type: string, listener: EventListenerOrEventListenerObject, options?: boolean | EventListenerOptions): void;} interface는 객체로, 위에서는 타입관련된 내용을 interface로 객체로써 정의하였다. 타입에 대한 내용도 공통 타입속성에 대해서는 상속을 통해서 구현을 하고 있다.이렇게 이미 정의되어있는 타입에 대한 정의 파일로 인해 IDE를 사용하게 되면, 코드 작성시에 관련 메소드 및 속성에 대한 내용을 확인해가며 작업을 할 수 있기 때문에 실수를 줄이고 코드 작성에 있어 생산성을 높여줄 수 있다. 자신이 직접 타입을 정의해서 사용할 수도 있지만 다른 사람이 작성한 타입을 읽고 이해하는 것 또한 중요하다. 타입스크립트의 도입만약 개발중인 어플리케이션이 잠정적으로 시스템 확장을 고려해야 한다면, 간단하더라도 타입 스크립트를 도입해서 개발을 하는 것이 좋다.기존에 자바스크립트로 구현된 어플리케이션도 타입스크립트를 점진적으로 도입해가면서 업그레이드할 수 있다. 이 작업을 할때에는 tsconfig.json파일에서 --allowJs: true 옵션을 넣어줘야 한다. 이는 js 파일의 컴파일을 허용한다는 옵션으로, TypeScript에서는 기본적으로 *.ts 파일에 대한 컴파일만 허용하기 때문이다. 문법적 설탕(Syntactic sugar) ?문법적 설탕이란 사람이 프로그래밍 언어를 달달하게 사용할 수 있도록 도와주는 문법, 더욱 더 간결하고 명확하게 표현이 가능한 문법, 사람이 이해하고 표현하기 쉽게 디자인된 프로그래밍 언어 문법으로 정의한다.예를들어, 기존에 ES6이전의 문법에서는 자바스크립트로 클래스를 정의할때 prototype으로 상속을 구현해서 사용을 하였는데, ES6부터는 클래스 기반의 언어와 같이 클래스를 정의해서 손쉽게 상속을 구현해서 사용할 수 있다. (여전히 프로토타입 기반의 언어이지만 문법상 클래스 기반의 언어와 같이 클래스를 정의할 수 있다는 말) ^3.7.4의 의미package를 설치하고 나면 package.json 파일 안에서 각 dependency 항목의 버전들이 ^3.7.4로 표기되어 있는 것을 볼 수 있다. 이는 3.7.4 &lt;= x &lt; 4.0.0이라는 의미로, caret뒤의 숫자(3) 버전의 최신 버전을 설치하겠다는 의미이다.(버전 4는 미포함) 따라서 '^3.7.4'가 아닌 '^3'로만 적어줘도 된다. 그럼 정확히 어떤 버전이 설치되었는지는 어떻게 확인할 수 있을까?바로 package-lock.json 파일내에서 확인을 할 수 있다. 이 곳에서는 구체적으로 어떤 버전의 package가 설치되었는지 확인할 수 있다. 따라서 프로젝트 폴더를 Github에 업로드할때에는 반드시 package-lock.json파일을 함께 업로드 해줘야 한다. npx tsc 명령을 통한 컴파일$ npm i -g typescript를 통해 설치된 typescript의 버전과 프로젝트의 타입스크립트 버전이 일치하지 않는 경우 문제가 생길 여지가 있다. 따라서 타입스크립트 파일을 컴파일 할 때에는 $ npx tsc를 사용해야 한다. npx 명령을 쓰면 프로젝트에 설치된 타입스크립트의 버전으로 맞춰서 컴파일을 해준다. tsconfig.json 에서 자주 사용되는 옵션들tsconfig.json에는 TypeScript 코드에서 JavaScript 코드로 변환을 할때 필요한 설정에 대한 정보를 담고 있다.타입스크립트 파일과 컴파일된 자바스크립트 파일을 동시에 켰을때 타입스크립트 파일내의 코드에 에러표시가 나는 경우가 있는데, 열려있는 컴파일된 자바스크립트 파일을 닫았을때 해결이 되었다. 이는 tsconfig.json 파일에서 설정과 관련이 있다.아래에서 tsconfig.json 파일에서 자주 사용되는 옵션들을 위주로 살펴보도록 하겠다. 공식 사이트: https://www.typescriptlang.org/docs/handbook/tsconfig-json.html –allowJs*.js 파일의 컴파일을 허용할때 사용되는 옵션이다. (true) 기존에 JS로 작성된 어플리케이션을 TS로 점진적으로 업그레이드 하는 경우, 이 옵션을 true로 해서 넣어줘야 한다. typescript에서는 기본적으로 *.ts 파일에 대한 컴파일만 허용을 하기 때문이다. –baseUrl기본 경로 지정할때 사용되는 옵션이다. –checkJsallowJs 옵션과 같이 사용되는 옵션이다. –declaration / -d_.d.ts 파일을 작성하는 경우, true로 옵션을 활성화 시켜준다.주로 객체와 함수를 사용할때 type을 지정할 필요성이 생기기 때문에 _.d.ts 파일을 작성한다. 타입에 대한 정의 파일을 별도로 분리함으로써 다른 파일에서 해당 타입에 대한 정의파일을 재사용할 수도 있다. –esModuleInterop사용에 있어 주의가 필요한 옵션이다.원래 import * as React from 'react'를 사용하는 것이 공식적으로 맞지만,일반적으로 개발자들이 import React from 'react'를 사용할때 에러가 발생해서 이 옵션을 활성화시키는 경우가 있다. 이는 옳지 않으며 되도록 공식적으로 권장하는 방식으로 코드를 작성하도록 하자. **–emitDecorator ~ **데코레이터를 사용하는 경우에 사용하는 옵션이다. –inittsc init을 하면 tsconfig.json 파일을 자동으로 생성해주는 옵션이다. –jsxTypeScript는 ts 파일을 js 파일로 변환해 줄 뿐만 아니라, React나 React Native에서 tsx 파일을 jsx 파일로 변환해주기도 한다. 이러한 경우에 활성화시켜야 되는 옵션이다. –lib만약 ts 파일에서 자바스크립트의 최신 문법을 인식하지 못하는 경우, 아래와 같이 추가적으로 라이브러리를 추가해줘야 한다.ESNEXT (가장 최신 JS문법) 1&quot;--lib&quot;: [&quot;DOM&quot;, &quot;ES5&quot;, &quot;ES2015&quot;, &quot;ES2020&quot;] ES2015(ES6)와 ES2017(ES8)은 매우 중요하기 때문에 –lib 옵션을 사용하는 경우에 반드시 옵션 값으로 넣어줘야 한다. 1&quot;--lib&quot;: [&quot;ES6&quot;, &quot;ES2016&quot;, &quot;ES2017&quot;, &quot;ES2018&quot;, &quot;ES2019&quot;, &quot;ES2020&quot;] –outDir*.ts 파일과 컴파일된 *.js 파일의 경로를 다르게 지정하고 싶을때 사용되는 옵션이다. –target / -t기본적으로 타입스크립트가 JS ES3로 코드 변환을 시켜준다. (ES3는 IE8까지 지원)반면, Babel은 ES5로 코드 변환을 해준다. 따라서 과거에는 개발자들이 IE8까지 지원을 해야되는 경우에 어쩔 수 없이 타입스크립트를 사용하는 경우도 있었다고 한다. (ES5는 IE11까지 지원)만약에 컴파일되는 JS파일에서 변수의 선언이 var 키워드로 되는 것이 싫다면, &quot;--target&quot;: &quot;ES6&quot;로 해주도록 하자.하지만 “ES6”의 경우에는 IE에서 지원을 하지 않는다. 만약에 IE를 지원해야 되는 경우에는 Babel로 추가적으로 ES5로 컴파일해줘야 한다. –types / –typeRoots직접 작성한 *.d.ts 파일에 대한 경로를 지정할때 사용하는 옵션이다. *.ts파일에서 직접 작성한 *.d.ts 파일을 로드해서 사용해야되는 경우 필요한 옵션이다. –strictstrict나 noImplicit으로 시작되는 옵션은 모두 true로 해두는 것이 좋다. 이것은 TypeScript의 엄격한 장점을 사용하기 위한 옵션들이다. 123&quot;noImplicitAny&quot;: true&quot;noImplicitThis&quot;: true&quot;noImplicitReturn&quot;: true 초보자의 경우 &quot;strict&quot;: true 옵션만 넣어주더라도 TS의 대부분의 기능을 사용할 수 있다. –module / -m대부분 &quot;CommonJS&quot;을 사용하지만 주의해야 될 옵션은 --esModuleInterop 옵션이다.import React from 'react'와 import * as React from 'react'는 export default 할때 차이가 있기 때문에 주의해야 한다.만약 최신 문법만 지원을 하는 경우에는 &quot;ES6&quot;로 옵션 값을 지정해주면 된다.만약 IE를 지원해야 될 때에는 &quot;CommonJS&quot;로 옵션 값을 지정해주면 된다. –watch타입스크립트 파일에서의 변경사항을 자동으로 감지해서 자바스크립트 파일로 자동 컴파일 시켜주는 경우 사용되는 옵션 include: []컴파일할 파일들을 넣어주는 옵션이다. 만약 특정 파일만 컴파일하는 경우에는 아래와 같이 작성을 해준다. 1&quot;include&quot;: [&quot;main.ts&quot;] exclude: []include옵션과는 반대로 컴파일을 하지 않을 파일들을 넣어주는 옵션이다. 1&quot;exclude&quot;: [&quot;*.js&quot;] extends: “ “이 옵션은 하나의 폴더 안에 여러 개의 타입스크립트 프로젝트가 있고, 공통으로 사용할 tsconfig.json 파일과 개별 프로젝트별로 사용할 tsconfig.json 파일이 있는 경우, 각 각의 개별 tsconfig.json 파일 내에서 공통 tsconfig.json 파일을 확장해서 사용할 필요가 있는 경우, 사용될 수 있는 옵션이다. 공식문서의 Handbook 읽기공식문서에서 타입스크립트 핸드북을 처음부터 끝까지 읽어보자.타입스크립트에서 어떤 기능이 어떤 버전에서 추가가 되었는지 알 수 있다. https://typescript-kr.github.io/","link":"/2021/04/21/202104/210421-Typescript_TIL/"},{"title":"210422 스택 프레임(Stack frame)과 재귀함수(recursive function)","text":"재귀함수의 동작을 스택 프레임과 연관지어 이해아래에 작성한 간단한 재귀함수의 동작방식을 스택 프레임과 연관지어 설명해보려고 한다.이미 재귀함수는 문제풀이나 기본적인 개념이해는 되었지만, 스택과 연관지어 기본적인 동작방식을 설명해본적은 없기 때문에 이번 포스팅을 계기로 한 번 정리를 해보려고 한다. sample code 1 123456789101112131415161718function sampleRecursiveFunc(n) { function DFS(L) { if (L === 0) return; else { console.log(L); DFS(L - 1); } } DFS(n);}sampleRecursiveFunc(3);// output:/*321*/ sample code 2 123456789101112131415161718function sampleRecursiveFunc(n) { function DFS(L) { if (L === 0) return; else { DFS(L - 1); console.log(L); } } DFS(n);}sampleRecursiveFunc(3);// output:/*123*/ 자 위에 작성한 sample code 1과 sample code 2의 코드를 살펴보자.매우 간단한 코드이기 때문에 쉽게 결과를 예측할 수 있을 것이다.그렇다면 위의 코드가 실행되었을때 무슨일이 일어날까?바로 함수가 실행되면 실행된 해당 함수에 대한 스택 프레임(Stack frame)이 생성이 되고 LIFO Stack에 적재가 된다. 이 스택 프레임은 매개변수(Parameters)와 지역변수(Local variables) 그리고 복기주소(Return address)로 구성이 되어있는데, 이 복기 주소(Return address)가 스택의 상위 스택 프레임이 실행되고 나서 돌아갈 주소를 담고 있다. 자 그럼 기본적으로 함수가 실행되면 스택 프레임이 스택에 적재되는 과정을 이해했으니, sample code2의 코드를 이 기본적인 개념에 비춰 설명해보도록 하겠다. (1) 우선적으로 sampleRecursiveFunc 함수가 number 타입의 인자 3이 호출이 되고, 이 함수 실행에 대한 스택 프레임이 스택에 적재가 된다. 그리고 sampleRecursiveFunc 함수의 내부에 선언된 매개변수 3을 가진 DFS함수를 호출하게 된다. (DFS(3) 스택 프레임 적재) (2) number 타입의 인자 3을 매개변수로 갖는 함수는 실행이 되어 함수 몸체에 정의한 코드를 순차적으로 실행하게 된다. 초기 인자값은 3이기 때문에 조건 분기문에서 else 문이 실행되어 else 문 내부의 DFS(3-1) 함수가 호출이 된다. (재귀호출) (3) 이때 DFS(n-1) 이후의 코드는 실행이 되지 않으며, 스택 프레임 DFS(3)은 waiting 상태가 된다. 그리고 DFS(n-1)인 DFS(2)의 스택 프레임이 스택에 적재되게 된다. (4) DFS(2) 함수실행의 경우도 DFS(3)과 동일한 조건분기 처리를 거쳐 waiting 상태가 되고, DFS(1)에 대한 함수호출과 스택 프레임이 스택에 적재되게 된다. (5) 최종적으로 DFS(0)에 대한 스택 프레임이 스택에 적재되고 함수가 실행되게 되면, 조건 분기문의 if문에서 조건이 만족되어 return; (함수의 가장 마지막 하단으로 이동 및 실행 종료)가 된다. 이후에는 DFS(0)의 스택 프레임은 스택으로부터 pop되어 스택 프레임의 복기 주소(Return address)에 저장되어있는 곳으로 이동한다. (6) DFS(0)의 복기 주소에는 이전 DFS(1) 스택 프레임(waiting 상태)의 주소를 가르키고 있기 때문에 DFS(1)이 이전에 실행을 하다가 멈춘 시점 (DFS(n-1))의 이후 코드를 순차적으로 실행하게 된다. (7) DFS(1)의 함수 실행이 완료된 이후에도 순차적으로 각 실행함수의 스택 프레임의 복기 주소를 참조하여 다음, 그 다음에 실행될 함수들을 실행하게 된다. 이 Stack frame에 대한 내용은 완전탐색, DFS, Back tracking등의 알고리즘 개념을 이해하기 위한 초석으로 매우 중요한 개념이다.","link":"/2021/04/22/202104/210422-Algorithm_stack_frame_and_recursive_function/"},{"title":"210422 TypeScript TIL - 암묵적 타입 정의과 명시적 타입 정의, 배열과 튜플, 변수를 상수로 정의해서 사용하기, 객체의 타입 지정, enum 타입, void, Overloading과 ? 연산자, never과 any 타입, 정의된 타입 재정의, 기존 JavaScript를 TypeScript로 전환하는 경우와 처음부터 TypeScript로 작성하는 경우","text":"암묵적 타입 정의, 명시적 타입 정의아래와 같이 코드를 작성해주면 암묵적으로 num\b이라는 변수의 타입은 정의된 값 10에 의해 number 타입으로 타입지정이 된다. 1let num = 10; 그런데 만약에 값을 정의하지 않고 변수만 선언하는 경우, 이러한 경우에는 타입을 지정해서 해당 변수에 할당될 수 있는 값의 타입을 지정해주는 것이 좋다. 1let num: number; 간혹 타입을 지정할때 Number, String, Boolean, Object 등으로 작성을 해서 실수하는 경우가 있는데, 앞서 작성한 것들은 모두 자바스크립트에서 객체들이기 때문에 타입은 모두 맨 앞을 소문자로 표기해서 타입지정을 해주도록 하자. 배열과 튜플 배열 표기법123let arr1: Array&lt;string&gt; = ['1', '2', '3'];let arr2: (string | number | boolean)[] = [true, 2, '3'];let arr3: number[] = [1, 2, 3]; 타입스크립트에서 숫자타입의 배열을 명시적으로 작성할때 Array와 number[] 두 가지 방법으로 작성할 수 있다.두 번째 작성한 것과 같이 복합적인 타입 값들로 구성된 배열 타입으로 명시적으로 지정할 수도 있다. 엄격한 배열의 사용12let arr1: [boolean, number, string] = [true, 2, '3']; // tuplelet arr2: [boolean, 1, string] = [true, 1, '3']; // 두번째 인자를 고정된 상수 타입으로 지정할 수도 있다. 상수로 사용하기 as const아래 코드와 같이 정의된 값 뒤에 as const를 붙여서 상수로써 사용할 수 있다. 만약 변수의 선언을 const로 한 경우에는 효용성이 없고, 객체의 경우 유용하게 사용될 수 있다.객체는 const로 변수가 선언되어도 객체의 내부 속성값은 변경이 가능하다. 이러한 단점을 as const를 붙여줌으로써 객체의 내부 속성값이 바뀌지 않도록 할 수 있다. 12let arr = [true, 2, '3'] as const; // type이 (boolean, number, string)[]let str = 'hello' as const; // type이 'hello' 객체에서 as const의 사용 12345const obj1 = {a: 'b'};obj1.a = 'c'; // 객체의 속성 변경 가능const obj2 = {a: 'b'} as const;obj2.a = 'c' // 객체의 속성 변경 불가능 객체의 타입 지정하기 객체의 타입을 지정할때 아래와 같이 지정을 해 줄 수도 있다. 1const obj: object = { a: 'b' }; 하지만 타입을 object로 지정해주면 타입이 구체적이지 않기 때문에 아래와 같이 객체의 타입은 구체적으로 작성해주는 것이 좋다. 12const obj1: { a: string } = { a: 'b' };const obj2: { a: string, b?: number } = { a: '1', b: 2 }; 위와 같이 객체의 타입을 명시해주게 되면 중복되는 느낌과 코드가 길어져서 가독성이 안좋아진다. 따라서 이 타입에 대한 부분을 별도의 interface나 type으로 분리를 해서 작성을 해주기도 한다. 만약 b속성을 나중에 정의해주거나 a 속성만 존재하는 경우, 위와 같이 ? 연산자를 변수 뒤에 붙여줌으로써 표현할 수 있다.이 물음표 연산자의 위와같은 사용은 타입스크립트에서 생겼으며, 아래와 같이 객체 및 객체의 메서드에 접근시에도 사용될 수 있다. 12const a = obj?.nameconst b = obj.&lt;method&gt;?.() enum 타입열거형이란 해당 타입으로 사용할 수 있는 값을 열거하는 기법이다.이전에 타입스크립트 노루책으로 공부하며 정리했던 블로그 포스팅을 참고하도록 하자. https://leehyungi0622.github.io/2021/02/19/202102/210219-Typescript/ 1234567891011121314const enum Flippable { Burger = 'Burger', Chair = 'Chair', Cup = 'Cup', Skateboard = 'Skateboard', Table = 'Table'}function flip(f: Flippable) { return 'flipped it'}flip(Flippable.Chair)flip(Flippable.Cup) enum 타입을 사용하는 경우에는 안전하게 const enum을 사용해서 enum 타입을 정의하고 사용하도록 한다. 잘만 사용하면 코드를 줄여줄 수 있다. TypeScript에서 void의 사용123456const n = void 0; // n은 undefined type// 아무것도 반환하지 않는 함수의 반환 타입으로 void를 적어준다.function print(): void { console.log('print console');} TypeScript에서의 Overloading과 ? 연산자overloading이란 같은 이름의 함수를 매개변수만 다르게 해서 정의하는 것을 말한다.JavaScript에서는 타입이 없었기 때문에 overloading이라는 개념이 없었지만, TypeScript에서 타입이 등장하면서 overloading이라는 개념이 도입되었다.간단한 예시로 아래와 같이 매개변수에 ?(물음표 연산자)를 사용해서 오버로딩을 구현할 수 있다. 아래 함수에서 매개변수는 (a, b)또는 (a, b, c)가 될 수도 있다. 이처럼 같은 함수에서 서로 다른 매개변수의 형태를 갖을 수 있는 것이다. 123const add(a: number, b: number, c?: number): number { //........} TypeScript에서 never과 any 타입타입스크립트에서는 never와 any라는 타입이 있다. never 타입의 경우 실무에서 에러를 통해 접할 수 있다. 아래와 같이 타입을 실수로 빈 배열로 정의한다면, 이 변수는 아무것도 넣을 수 없는 never 타입으로 선언된다.따라서 number 타입의 값 3을 push하려고 하면 아래와 같은 에러가 발생한다.not assignable to parameter of type 'never' 12const arr: [] = [];arr.push(3); any 타입의 변수에는 아무 값이나 넣을 수 있다. 그러면 타입스크립트를 사용하는 것에 의미가 없기 때문에 불가피한 경우가 아니라면, 되도록 any 타입은 사용하지 않도록 한다.(만약 타입을 정의할때 너무 복잡해서 타입을 정의할 수 없는 경우, any를 사용하도록 한다.) 1const hi: any = []; 만약 다른 사람이 정의한 변수의 타입이 틀린 경우만약 같이 협업하는 사람이 작성한 코드에서 특정 변수의 타입이 틀린경우, 그리고 직접적으로 해당 변수에 정의한 타입을 바꿀 수 없다면 어떻게 해야될까? 위와같은 상황이라면 해결방법은 Type Casting이 있다. 만약 아래와 같이 string 타입으로 정의되어야 할 name이라는 변수가 number 타입으로 정의되어 있는 경우, 우리는 name이라는 변수의 타입을 string으로 cating해줘야 한다. 우선 타입 캐스팅 이전에 바꾸려는 타입과 기존의 타입의 관계부터 생각해봐야 한다.만약 기존의 타입과 바꾸려는 타입의 관계가 서로 겹치는 경우에는 아래와 같이 상위 개념의 타입으로 타입 캐스팅을 해주면 된다. 123// HTMLDivElement -&gt; HTMLElement로 타입변환const div = document.createElement('div');const a = div as HTMLElement; 그러면 기존의 타입과 바꾸려는 타입의 관계가 서로 겹치지 않는다면 어떻게 해야될까?바로 아래와 같이 두 가지 형태로 타입 변환을 해 줄 수 있다. 12345678910111213// hello.d.tsconst name: number;// main.tsimport name from 'hello';// name substring하기name.substr(1, 2); // number type은 substr 안됨// 강제로 name 변수의 타입을 변환// 방법 1(name as unknown as string).substr(1, 2);// 방법 2(&lt;string&gt;&lt;unknown&gt; name).substr(1, 2); 기존의 JavaScript를 TypeScript로 바꿔주는 경우와 처음부터 TypeScript로 작성하는 경우 기존의 JavaScript를 TypeScript 코드로 전환하는 방식최대한 타입을 안적고, 기본 타입이 문제가 되는 경우에만 type을 지정해주는 방식으로 type을 도입한다. JavaScript→TypeScript 전환시의 tsconfig.json 설정12345678910{ &quot;compileOptions&quot;: { &quot;strict&quot;: true, &quot;allowJs&quot;: true,# 자바스크립트를 타입스크립트로 모두 전환되었을 경우, allowJs는 꼭 비활성화 시켜준다. &quot;checkJs&quot;: true # 자바스크립트 에러 체크 # 만약 ts로 전환하는 과정에서 에러가 너무 많이 발생하는 경우, # 일단 &quot;checkJs&quot;는 비활성화한다. }, &quot;exclude&quot;: [&quot;*.js&quot;] # js 파일의 컴파일은 제외한다.} 처음부터 TypeScript 코드로 작성하는 방식남이 만든 것은 암묵적으로 타입을 추론할 수 있도록 두고, 내가 만든 객체 또는 함수의 경우에는 타입을 직접 정의해서 작성하도록 한다. Custom type1type stringOrNumber = string | number; 내가 만든 객체 또는 함수의 타입 작성하는 방법 *1)**별도의 *.d.ts 파일로 분리를 해서 type 작성하기\\.ts 파일내에서 interface와 type alias 방식으로 작성한 실제 코드에서는 역할이 딱히 없고 type정의 목적만 가지는 코드는 나중에 길어지게 되면, 별도의 *.d.ts 파일로 빼서 관리한다. 2)*.ts파일 내에 type 작성하기 123456789101112131415// (1)interface 방식으로 type 작성하기interface hello { a: string; b?: number;}// (2)type alias방식으로 type 작성하기type hello = { a: string, b?: number};// 아래와 같이 객체의 타입을 작성해주면 가독성이 좋지 않다.const hi: { a: string, b?: number } = { a: 'b' };// 따라서 아래와 같이 interface나 type으로 정의한 타입을 사용해서 변수의 타입을 정의해준다.const hi: hello = { a: 'b' }; interface로 타입을 정의해주면 다른 타입 interface를 상속받아서 타입을 정의할 수 있다.만약에 인터페이스 내부에서 정의한 함수의 타입정의에 있어 공통 매개변수를 가지고 있는 함수의 경우에는 ? 연산자를 사용해서 정의할 수 있다.하지만 만약에 같은 함수이름에 매개변수가 너무 다른 경우, 다른 매개변수의 형태로 두 번 정의해준다. 1234567891011interface hello { a: string; b?: number; // 함수 a의 매개변수의 형태가 너무 다른 경우, 두 번 정의해준다. a(f: number, s: number): number; a(f: string, s: boolean, t: number): string;}interface helloChild extends hello { c?: boolean;} TypeScript 자체는 별거아니다. TypeScript 자체는 별거 아니다. 다만 남이 작성한 type interface를 읽고 적용을 하는 것이 어렵다.따라서 지속적으로 다른 사람이 작성한 type interface를 읽고 해석하는 연습을 하는 것이 중요하다. 최대한 *.d.ts는 남이 만들어 놓은 것을 라이브러리로 불러서 사용하는 것이 좋다.*.d.ts 파일은 라이브러리로 남이 작성해놓은 것을 가져다가 쓰고 *.ts 파일의 코드작성에 집중한다. 다른 사람이 작성한 *.d.ts 파일 읽고 분석하기아래의 링크는 DefinitelyTyped의 fp.d.ts 파일의 링크이다. 많이 복잡해보이는 타입정의 인터페이스들로 구성이 되어있고, 대부분 제네릭으로 작성되어 있는 것을 볼 수 있다. https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/types/lodash/fp.d.ts 123456789interface LodashFlowRight { &lt;A extends any[], R1, R2, R3, R4, R5, R6, R7&gt;(f7: (a: R6) =&gt; R7, f6: (a: R5) =&gt; R6, f5: (a: R4) =&gt; R5, f4: (a: R3) =&gt; R4, f3: (a: R2) =&gt; R3, f2: (a: R1) =&gt; R2, f1: (...args: A) =&gt; R1): (...args: A) =&gt; R7; &lt;A extends any[], R1, R2, R3, R4, R5, R6&gt;(f6: (a: R5) =&gt; R6, f5: (a: R4) =&gt; R5, f4: (a: R3) =&gt; R4, f3: (a: R2) =&gt; R3, f2: (a: R1) =&gt; R2, f1: (...args: A) =&gt; R1): (...args: A) =&gt; R6; &lt;A extends any[], R1, R2, R3, R4, R5&gt;(f5: (a: R4) =&gt; R5, f4: (a: R3) =&gt; R4, f3: (a: R2) =&gt; R3, f2: (a: R1) =&gt; R2, f1: (...args: A) =&gt; R1): (...args: A) =&gt; R5; &lt;A extends any[], R1, R2, R3, R4&gt;(f4: (a: R3) =&gt; R4, f3: (a: R2) =&gt; R3, f2: (a: R1) =&gt; R2, f1: (...args: A) =&gt; R1): (...args: A) =&gt; R4; &lt;A extends any[], R1, R2, R3&gt;(f3: (a: R2) =&gt; R3, f2: (a: R1) =&gt; R2, f1: (...args: A) =&gt; R1): (...args: A) =&gt; R3; &lt;A extends any[], R1, R2&gt;(f2: (a: R1) =&gt; R2, f1: (...args: A) =&gt; R1): (...args: A) =&gt; R2; (...func: Array&lt;lodash.Many&lt;(...args: any[]) =&gt; any&gt;&gt;): (...args: any[]) =&gt; any; }","link":"/2021/04/22/202104/210422-Typescript_TIL/"},{"title":"210422 트리 순회(Tree Traversal)","text":"무엇을 하든 기초가 정말 중요하다. 이전에 한 번 이 트리순회를 공부했을때에는 단순히 코드작성과 단순 이해만 했기 때문에 문제에 제대로 응용이 되지 않았던 것 같다.그래서 이번에는 한 번 직접 손으로 스택 내부에 스택 프레임이 어떻게 쌓이는지 그려보며 내부의 재귀형태의 함수 호출은 어떻게 이루어지는지 살펴보았다. 또 스택과 함께 트리 구조도 어떻게 가지치기를 하는지 살펴보았다. 나중에 다시 복습을 위해 직접 그려 본 스택의 프레임과 트리구조를 아래에 첨부한다.","link":"/2021/04/22/202104/210422-Algorithm_tree_traversal/"},{"title":"210422 Are stack frame and execution context the same thing in JavaScript?","text":"이번 포스팅에서는 자바스크립트를 공부하다가 용어상 혼동되는 부분이 있어 간단하게 정리하고 넘어가고자 한다.계속 공부를 해나가면서 혹시 용어상 혼동되는 부분이 있으면 이 포스팅 글에 지속적으로 업데이트를 해나가도록 하겠다. Stack frame vs Execution context알고리즘 관련 공부를 하다가 함수 실행과 관련된 글을 읽던 중에 스택 프레임(Stack frame)이라는 말이 나왔다. 함수가 실행되면 함수의 매개변수와 지역변수, 복기주소로 구성된 스택 프레임이 스택에 쌓이게 되고, 함수가 최종적으로 실행이 되면 복기주소를 통해서 자기가 호출되었던 주소로 되돌아 간다고 한다.이전에 자바스크립트에서 스코프에 대해서 공부를 했을때, 실행 컨텍스트에 대해서도 공부를 했었는데 스택 프레임과 같은 개념인 것 같아서 찾아보니 용어만 다를 뿐 같은 것이라는 것을 알게 되었다. 이전에 스코프와 실행 컨텍스트에 대해서 공부하면서 정리했던 포스팅의 링크를 아래에 첨부해둔다.다시 한 번 반복해서 읽어보자.https://leehyungi0622.github.io/2021/03/19/202103/210319-javascript-basic_til/ Call stack vs Execution stack콜 스택(Call stack)과 컨텍스트 스택(Context stack)이 있는데 콜 스택은 자바스크립트 코드가 실행될때 생성된 실행 컨텍스트를 보관하는 LIFO 스택이라는 것은 공부를 통해 알고 있다. 그런데 다른곳에서는 같은 개념의 설명을 컨텍스트 스택이라고 하는 곳도 있었다.이 역시 스택 프레임과 실행 컨텍스트와 같이 다른 용어지만 같은 것이라고 한다.","link":"/2021/04/22/202104/210422-javascript_stack_frame_and_execution_context/"},{"title":"210423 TypeScript TIL-1 - as const와 readonly, interface와 type alias의 비교 및 사용, interface의 다양한 활용 및 문제해결","text":"as const와 readonly의 사용객체를 as const로 정의를 해주게 되면 객체내의 모든 타입 속성들이 readonly로 바뀌는 것을 확인할 수 있다. 만약 일부의 타입 속성들만 상수로써 바뀌지 않게 하고 싶다면 해당 타입 속성의 변수명의 앞에 readonly를 붙여주도록 한다.만약 변수의 타입이 상수 타입으로 고정되는 경우, 이 경우에는 별도로 as const나 readonly를 붙여주지 않아도 상관없다. interface와 type alias 비교 및 사용interface와 type alias를 사용해서 객체의 타입을 정의할때 그 용도를 일관되게 나눠서 사용하는 것이 좋다.예를들어, 객체의 타입은 interface를 사용해서 정의하고 custom type은 type alias를 사용해서 정의하도록 하는 것이다.interface와 type 모두 아래와 같이 객체의 타입을 정의할 수는 있지만 코드를 작성할때 일관성을 지켜서 코드를 작성하는 편이 좋다. interface의 특징 interface내에 정의한 타입 속성은 ,(콤마)나 ;(세미콜론), \\n(줄바꿈)으로 구분할 수 있다. 다른 type interface를 상속할 수 있다.12345678interface ParentExample{ readonly A: string, readonly B: string}interface ChildExample extends ParentExample{ readonly C: string} 같은 이름의 interface를 여러번 정의할 수 있다.interface에서는 위와 같이 동일한 이름으로 정의를 해 줄 수 있다. 이렇게 작성을 해주게 되면, 이름이 같은 두 interface는 서로 합쳐진다. 12345678interface Example { readonly A: string, readonly B: string}interface Example { readonly C: string} 위의 특징은 남이 작성한 라이브러리를 사용해서 타입을 정의하는데, 만약에 타입의 정의에 문제가 있어서 기존의 라이브러리에 있는 타입의 정의를 수정해서 사용하고 싶을때 라이브러리의 타입 interface와 동일한 이름으로 interface를 정의해서 타입을 추가해서 사용할 수 있다. type alias의 특징type alias는 interface보다 좀 더 넓은 범주이다. type alias는 합쳐지지도 상속되지도 않는다. type alias로도 객체의 타입을 정의할 수 있다.type alias로도 객체의 타입을 정의할 수는 있지만 되도록 일관성있게 객체의 타입은 interface로, custom type의 정의는 type alias를 사용하도록 한다. 1234type Example = { readonly A: string, readonly B: string} type alias로는 | 연산자를 사용할 수 있다.1234type ObjectOrString = { readonly A: string, readonly B: string} | string; type alias 방식은 주로 별칭으로 새로운 타입을 정의할때 사용된다.1type Example = string | number; // string이나 number 타입 interface의 다양한 활용 및 문제해결 keyof (interface name)와 (interface name)[typeof (interface name)]아래의 코드와 같이 interface 내에서 사용된 변수명 또는 변수의 타입이 특정 함수의 매개변수 혹은 반환값의 타입으로써 사용이 된다면, keyof 키워드를 사용해서 interface 내의 변수명과 변수의 타입을 재사용해서 코드의 중복없이 재정의할 수 있다. 123456789101112131415interface Example { readonly A: '0', readonly B: '-142px', readonly C: '-284px'}// exampleFunc의 인자 imgCoords가 Example interface의 type들인 경우 아래와 같이 풀어서 작성해 줄 수 있지만,function exampleFunc(imgCoords: '0'|'-142px'|'-284px' ): 'A'|'B'|'C' { ......}// interface의 정의를 재활용할 수도 있다.// 또한 반환값도 Example interface의 타입 속성의 이름이기 때문에 아래와 같이 keyOf Example로 대체할 수 있다.function exampleFunc(imgCoords: Example[keyof Example]): keyOf Example{ ......} interface의 타입속성이 확실하지 않을때 동적으로 속성을 지정해줄 수 있다.아래에서 Example interface에서는 속성값 a는 상수 3을 타입으로 갖고, b는 상수 7을 타입으로 갖는다고 정의하고 있다. 이를 이용해서 객체 example을 생성한다고 했을때, 만약 a, b이 외에 다른 number타입의 객체 속성을 넣어주고자 한다면 어떻게 해야할까? 바로 아래와 같이 대괄호 표기법으로 내부에 [key: string] 으로 정의해서 속성의 이름이 동적으로 정의될 수 있도록 할 수 있다.하지만 되도록이면 타입스크립트를 사용할때에는 객체의 타입을 엄격하게 정의해서 사용하는 것이 좋다. 1234567891011interface Example { a: 3; b: 7; [key: string]: number;}const example: Example = { a: 3, b: 7, d: 100}; 기본 *.d.ts 문제해결하기일반적으로 타입스크립트에서 기본적으로 정의되어 있는 *.d.ts를 사용해서 코드를 작성할때 문제가 되는 경우가 있다. 아래 예시를 살펴보자.아래의 코드에서는 Object.keys를 통해 타입을 정의한 interface에서 타입의 이름을 배열의 형태로 불러와서 Example을 타입으로 갖는 객체 변수 example의 키값으로 정의해서 매개변수 imgCoords와 엄격한 비교연산을 통해 비교하고 있다. 비교연산이 참인 경우의 키 값만을 반환하도록 하는 처리를 하고 있다. 1Object.keys(Example).find((k) =&gt; example[k] === imgCoords); 하지만 Object.keys가 정의되어있는 lib.es5.d.ts 파일을 살펴보면 아래와 같이 keys 메서드는 문자열 타입의 배열(string[])만을 반환하고 있다. 1234interface ObjectConstructor {......keys(o: object): string[];} 하지만 example 객체 변수는 interface Example을 타입으로 정의하고 있기 때문에 example[k]에서 k는 interface Example에서 정의하고 있는 타입의 이름이 되어야 한다.따라서 위에서 interface에 정의되어있는 타입이름을 배열값으로 갖는 배열을 참조하기 위해 의도적으로 위와같은 코드를 작성했지만 실제로는 string 타입의 배열이 반환되기 때문에 find의 콜백함수 k에는 type ‘string’이 반환된다.이러한 문제를 해결하고 위해서는 명시적으로 Object.keys를 통해 가져온 문자열 배열의 타입을 캐스팅해줘야 한다. 1(Object.keys(example) as ['A','B','C']).find((k) =&gt; example[k] === imgCoords); 위와같이 Example을 타입으로 갖는 객체변수 example의 키값을 배열로 호출했을때 단순 문자열 배열이 아닌 구체적으로 길이와 속성이 고정된 타입의 배열로써 호출이 된다. 자바스크립트 버전 호환문제 해결하기위의 예시에서 사용된 find 메서드는 ES6에서 새롭게 추가된 기능이다. tsconfig 에서 lib 옵션은 기본값으로 ES5를 갖기 때문에 find 메서드를 사용하기 위해서는 tsconfig.json의 lib옵션에 추가적으로 자바스크립트 라이브러리들을 추가해줘야 한다. ES2014 - ES5ES2015 - ES6ES2016 - ES7… 12345678910111213141516{ &quot;compilerOptions&quot;: { &quot;strict&quot;: true, &quot;lib&quot;: [ &quot;ES5&quot;, &quot;ES6&quot;, &quot;ES2016&quot;, &quot;ES2017&quot;, &quot;ES2018&quot;, &quot;ES2019&quot;, &quot;ES2020&quot;, &quot;DOM&quot; ] }, &quot;exclude&quot;: [&quot;*.js&quot;]} 타입 시스템과 프로그래머의 의도가 다른 경우1find(predicate: (value: T, index: number, obj: T[]) =&gt; unknown, thisArg?: any): T | undefined; 앞서 예시 코드에서 살펴본 find 메서드의 경우, T 또는 undefined을 반환한다고 정의가 되어있다.만약에 find의 콜백함수 내에서 사용된 매개변수가 특정 객체의 key값으로써 사용이 된다면 에러가 발생된다.따라서 이런 경우에는 명시적으로 프로그래머가 undefined가 발생하지 않는다는 것을 보증한다는 의미에서 아래와 같이 !를 붙여줘야 한다. 1(Object.keys(example) as ['A','B','C']).find((k) =&gt; example[k] === imgCoords)!; addEventListener에서 콜백함수addEventListener에서 콜백함수에 화살표 함수가 아닌 익명 함수(일반 함수)의 형태로 넣었을때 첫 번째 인자로 바인딩해줄 this를 넣어줄 수 있다. 화살표 함수로 콜백함수를 넣어주게 되면 자동으로 상위 스코프의 객체를 this에 바인딩해주기 때문에 마침표 연산자 앞에 호출한 객체를 가르키게 된다. 하지만 익명 함수(일반 함수)의 형태로 넣어주게 되면 this에는 전역 객체가 바인딩된다.따라서 addEventListener의 콜백함수로 넣어준 일반 함수내에서 this를 사용하는 경우에는 addEventListener의 콜백함수의 첫 번째 인자로 this를 정의(두 번째 인자는 event 객체)해서 사용해야 한다. 123456btn.addEventListener('click', function (this: HTMLButtonElement, e: Event) { // button의 textContent를 this로 가져오기 this.textContent; // button의 textContent를 event 객체로 가져오기 e.target.textContent;}); 또한 this.textContent를 통해 참조한 버튼의 TextContent는 타입스크립트에서는 string | null 로써 인식을 하기 때문에 만약 this.textContent의 값을 다른 곳의 인자로써 사용하고자 한다면 구체적으로 해당 참조값의 타입을 명시해줘야 한다. 1this.textContent as keyof Example; 타입스크립트를 사용할때에는 타입의 범위를 좁혀서 엄격하게 처리해줘야 한다.","link":"/2021/04/23/202104/210423-Typescript_TIL-1/"},{"title":"210423 TypeScript TIL-2 - TypeScript에서의 DOM element 조작, TypeScript 인라인 스타일, strictNullCheck 옵션, JavaScript에서의 클래스와 TypeScript에서의 클래스 정의, interface에서 객체내의 메서드 타입 정의, 제네릭(generic), 제네릭 예시 - addEventListener 함수","text":"TypeScript는 HTML을 인지하지 못한다.TypeScript는 HTML 자체를 인식하지 못하기 때문에 HTML 내의 요소를 참조할때 null로 인식하는 경우가 있다. 따라서 확실하게 HTML에 요소가 존재하는 경우에는 개발자가 !로 존재를 보증해주거나 if 문으로 조건처리를 해서 작성을 해줘야 한다. 123if (document.querySelector('#id')) { (document.querySelector('#id') as HTMLElement).style.background = '......';} TypeScript에서 inline styling타입스크립트에서 inline 스타일링을 하기 위해 HTML element를 참조하게 되면 .style을 하거나 .textContent를 하는 경우에 에러가 발생한다.그 이유는 아래와 같이 querySelector로 HTML 요소를 참조하게 되면 해당 참조 요소의 타입은 Element를 상속받는 제네릭 타입 E가 된다. 이 Element에는 style, textContent 속성이 없다. 1querySelector&lt;E extends Element = Element&gt;(selectors: string): E | null; 따라서 Element type을 HTMLDivElement로 타입의 범위를 좁게 잡아줘야 한다. 1234567document.querySelector('#id').style.background = '...'; // error// as로 Type casting(document.querySelector('#id') as HTMLDivElement).style.background = '...';// generic으로 Type castingdocument.querySelector&lt;HTMLDivElement&gt;('#id').style.background = '...'; tsc로 타입스크립트 파일을 컴파일할때에는 파일명을 넣지 않는다.타입스크립트 파일을 컴파일할때 npx tsc -w과 같이 컴파일할 파일을 명시하지 않도록 한다. 그 이유는 파일명을 구체적으로 명시해주게 되면, 프로젝트 폴더 내에 정의한 타입스크립트 설정파일(tsconfig.json)이 무시가 되서 설정된 사항들이 적용이 안될 수 있기 때문이다. 비슷한 속성의 객체는 같은 interface로 타입을 정의할 수 있다.타입스크립트의 장점은 interface에 작성해준 타입의 정의를 보고 프로그램의 구조에 대한 힌트를 얻을 수 있다. 아래에 사람(Person)과 레스토랑(Restaurant) 인터페이스를 작성해보았다. 아래 예시 코드를 통해 &quot;strictNullCheck&quot;: true일 경우의 null과 undefined에 대한 내용도 한 번 정리해보겠다. 1234567891011121314interface Person { name: string; age: number; address: string;}interface Restaurant { waiter: Person[]; table: HTMLDivElement; chair: HTMLDivElement; dish: HTMLDivElement; guestData?: Person[] | null; queueData?: Person[] | null;} 위의 코드를 보면 앞서 타입스크립트의 interface 정의를 통해 프로그램의 구조에 대한 힌트를 얻을 수 있다고 한 내용이 이해가 될 것이다. 레스토랑 interface를 살펴보면 waiter, table, chair, dish, guestData, queueData 등의 정보들로 구성이 되어있고 각 각의 타입을 통해 waiter는 Person타입의 배열이라는 것을 알 수 있다. 그리고 그외에 table, chair, dish는 HTML 상의 div 태그로써 존재한다는 것도 알 수 있다.따라서 코드를 분석할때 타입의 정의를 통해서 손쉽게 파악을 할 수 있다. tsconfig.json에서의 “strictNullCheck”: true타입스크립트의 컴파일 옵션에 대한 내용을 담고 있는 tsconfig.json파일의 내부에 이전에 &quot;strict&quot;: true를 작성해주었다. 이는 &quot;strictNullCheck&quot;: true에 대한 옵션도 포함하고 있는 옵션이다.여기서 &quot;strictNullCheck&quot;: true옵션을 활성화 해준다는 의미는 타입스크립트에서 undefined과 null 타입을 구분해주겠다는 의미이다.위의 레스토랑 interface에서 손님데이터(guestData)와 대기열데이터(queueData)에 대한 정보를 살펴보자.guestData 타입명뒤에 ? 는 undefined와 같은 의미이다. 따라서 아래와 같은 코드이다. 1guestData: Person[] | null | undefined 이처럼 &quot;strictNullCheck&quot;: true옵션이 활성화된 경우에는 null과 undefined를 구분해서 타입스크립트가 인식하기 때문에 빈값을 의도적으로 넣었다는 null에 대한 정의와 값의 부재를 의미하는 undefined을 각 각 개별적으로 정의를 하였다.그렇다면 &quot;strictNullCheck&quot;: false로 옵션을 비활성화 해주게 되면 어떻게 될까? 바로 null과 undefined를 구분하지 않기 때문에 null에 대한 정의를 생략해도 된다. 하지만 타입스크립트를 사용하는 이유는 엄격한 타입을 적용하기 위함이다. 따라서 옵션을 활성화시켜서 null과 undefined을 구분해서 작성해주도록 하는 것이 좋다. JavaScript의 클래스를 TypeScript의 클래스로 재정의하기Class in JavaScript 1234567891011class Example { first; second; #third; //private constructor(first, second, third) { this.first = first; this.second = second; this.third = third; }} 자바스크립트에서의 클래스에서 타입스크립트의 클래스로 재정의할때에는 클래스 내에서 'this.x'로 정의된 속성과 'constructor의 매개변수'에 대한 타입을 정의를 해줘야 한다. 타입스크립트의 장점 private, public, protected 접근 제어자 키워드를 제공한다.기존의 자바스크립트에서는 private 클래스 속성을 구현하기 위해서 즉시실행함수와 클로저개념을 사용해야만 했다. 최신 자바스크립트 문법에서 클래스 속성 이름 앞에 #을 붙여서 private 속성으로 만들어 줄 수 도 있지만 최신 문법이기 때문에 브라우저 간의 호환성 문제가 있다. 하지만 타입스크립트에서는 private, public, protected 접근 제어자 키워드를 제공한다. (1) public : 클래스 내부와 상속받는 자식, 생성한 객체 인스턴스를 통해 접근이 가능하다. (2) private : 클래스 내부에서만 접근이 가능하다. (3) protected: 클래스 내부와 클래스를 상속한 자식 클래스에서 접근이 가능하다. (자식 클래스에서 정의된 protected 클래스 속성은 부모 클래스에서 접근할 수 없다.) protected는 public 보다는 타이트하지만, private 보다는 느슨하다는 특징을 가지고 있다. Class in TypeScript 클래스 속성의 접근 제어자를 작성할때에는 처음에는 private으로 적은 다음에 점차 필요에 따라 protected에서 public 순으로 권한을 늘려주며 작성해주는 것이 좋다.실제로 아래와 같이 interface를 정의하는 이유는 class의 형태를 잡아주기 위한 목적으로 정의한다.class에서 무조건적으로 정의해야 되는 속성이 있다면 interface로 별도로 구현해서 class에서 상속받아서 작성해주도록 한다. 123456789101112131415161718// interface명의 맨 앞글자를 대문자로 하는 것은 일종의 convention이다.interface IExample { // private와 protected는 부모가 접근할 수 없는 요소이기 때문에 // interface에서는 public 속성만을 갖는다. first: number;}class Example implements IExample { public first: number; private second: string; protected third: boolean; constructor(first, second, third) { this.first = first; this.second = second; this.third = third; }} interface로 객체 말고 함수나 클래스를 정의할 수도 있다.interface로 객체 말고 함수나 클래스의 타입을 정의할 수도 있지만, interface로는 주로 객체의 타입을 정의하는데 사용하도록 하자. 123456789// interface로 함수의 타입 정의하기interface Example { (a: number, b: number): number;}// interface로 객체 정의하기interface Example { new(a: number, b: number): number;} 그렇다면 객체 내의 메서드를 interface 내에서는 어떻게 정의를 할까? 1234567891011121314151617interface Example { add: (a: number, b: number) =&gt; number;}const example: Example = { add(a, b) { return a + b; }};// or// 화살표 함수(Arrow function)로 객체의 메서드 정의const example: Example = { add: (a, b) =&gt; { return a + b; }}; 위와 같이 interface로 객체 내의 메서드의 타입을 정의 할 수 있다. 제네릭(generic)만약에 객체 obj에 더하기 메서드가 정의되어있다고 가정하자.이 더하기 메서드는 두 개의 매개변수를 갖는데, 이 두 매개변수의 타입은 number가 될수도 있고, string이 될 수도 있다. (단, 각 각의 두 매개변수의 타입은 number 또는 string으로 동일하다.)메서드를 두 개 작성하지 않고 하나의 메서드만 작성한다고 했을때 어떻게 작성할 수 있을까? 1234567891011121314151617interface obj&lt;T&gt; { add: (a: T, b: T) =&gt; T;}// number 타입의 두 매개변수를 더하는 메서드const a: obj&lt;number&gt; = { add: (a, b) =&gt; a + b;}// string 타입의 두 매개변수를 더하는 메서드const b: obj&lt;string&gt; = { add: (a, b) =&gt; a + b;}console.log(a.add(1, 2)); // 3console.log(b.add('a', 'b')); // ab 위에서 interface를 사용하여 각 객체의 타입을 정의할때 의 타입을 구체화해서 정의하고 있다. 제네릭(generic) 예시 - addEventListener 이벤트 핸들러를 DOM element에 적용할때 사용하는 addEventListener 함수의 타입 정의를 살펴보자. lib.dom.d.ts 123function addEventListener&lt;K extends keyof WindowEventMap&gt;(type: K, listener: (this: Window, ev: WindowEventMap[K]) =&gt; any, options?: boolean | AddEventListenerOptions): void;function addEventListener(type: string, listener: EventListenerOrEventListenerObject, options?: boolean | AddEventListenerOptions): void; addEventListener 함수는 각각 다른 매개변수로 함수 오버로딩(Overloading)되어 있다. addEventListener 함수 사용 예시 123456button.addEventListener('submit', () =&gt; { ...});button.addEventListener&lt;'submit'&gt;('submit', () =&gt; { ...}); 우리가 일반적으로 첫 번째와 같은 방식으로 addEventListener 함수를 사용해서 이벤트 핸들러를 DOM element에 적용을 하는데, 두 번째로 정의된 함수를 보면 addEventListener 함수의 뒤에 &lt;’submit’&gt;이 있다.사실은 우리가 무심결에 생략을 해서 사용을 하여도 실제로는 암묵적으로 추론을 하고 있는 것이다. lib.dom.d.ts의 첫 번째 예시 함수를 보면 &lt;K extends keyof WindowEventMap&gt;(type: K, listener:부분을 찾을 수 있는데, WindowEventMap이라는 interface의 key값으로 K를 제한하고 있고 매개변수 type이 이 K를 타입으로 정의하고 있기 때문에 매개변수 type에 입력된 값이 WindowEventMap의 키값으로 존재를 한다면 암묵적으로 추론을 할 수 있는 것이다.만약에 type에 입력된 값이 WindowEventMap이라는 interface의 key 값에 존재하지 않는다면, lib.dom.d.ts의 두 번째 예시 함수 function addEventListener(type: string,(제네릭 부분이 샹략된 함수)가 참조된다. 제네릭에서 extends는 타입을 제한하는 기능을 한다.1234567interface obj&lt;T&gt; { ....}interface obj&lt;T extends string&gt; {} 첫 번째 interface에서는 type T에 모든 타입의 정의가 가능했지만 두 번째 interface에서는 string이거나 string의 자식 혹은 string을 구현한 type만 타입으로써 정의가 가능하다. 이처럼 extends는 상속이 아닌 제한의 의미를 포함하고 있다. 제네릭 예시 - forEachlib.es5.d.ts 12345interface Array&lt;T&gt; { ... forEach(callbackfn: (value: T, index: number, array: T[]) =&gt; void, thisArg?: any): void; ...} lib.es5.d.ts파일에서 Array interface에서 정의된 forEach 함수를 보면 callback 함수의 value가 배열 interface에서 정의된 타입 T로 정의가 되어있는 것을 볼 수 있다. 실제로 마우스로 아래 코드에서 forEach의 콜백함수의 item에 마우스를 올려보면 타입이 number로 추론되고 있음을 확인할 수 있다.이처럼 대부분의 타입추론은 제네릭으로 작성이 되어있다. 123[1, 2, 3].forEach((item) =&gt; { console.log(item);}); 위의 배열을 [1, true, ‘a’]로 작성하게 되면 타입을 number | boolean | string 으로 추론한다. any보다 제네릭으로 타입을 정의하는 것이 정밀하게 타입을 잡아내는 것 같다. 제네릭으로 직접 구현해보는 forEach12345678910111213141516// 함수이름 뒤에 매개변수에서 사용할 제네릭 타입 T를 정의해준다.// 아래 매개변수 arr의 타입을 T[] 라고 작성할 수도 있으나 나중에 T를 다른 타입으로 제한을 하기 위해// extends를 하는 경우 가독성을 고려했을때 Array&lt;T&gt;로 표기해주는 것이 좋다.function forEach&lt;T&gt;(arr: Array&lt;T&gt;, callback: (item: T) =&gt; void): void { for (let i: number = 0; i &lt; arr.length; i++) { callback(arr[i]); }}// 직접 구현한 forEach 함수 사용forEach &lt; string &gt; (['1', '2', '3'], (item) =&gt; { console.log(item); }); 제네릭 예시 - findlib.es5.d.ts의 배열 interface에서 정의한 find 함수를 살펴보면 제네릭이 정의되어 있는 함수와 그렇지 않은 함수, 두 가지로 함수 오버로딩이 되어있다.제네릭이 정의되어 있는 함수와 제네릭이 정의되어 있지 않은 함수 모두 predicate 함수의 value 타입이 find 함수를 호출할때 참조한 배열 요소의 타입에 따라 정의가 되는 것을 볼 수 있다. 다른점은 predicate 함수의 첫 번째 인자로 this를 사용하는 경우에는 제네릭으로 타입을 정의해주고, 그렇지 않은 경우에는 제네릭으로 타입을 정의하지 않는다. 123456interface Array&lt;T&gt; { ... find&lt;S extends T&gt;(predicate: (this: void, value: T, index: number, obj: T[]) =&gt; value is S, thisArg?: any): S | undefined; find(predicate: (value: T, index: number, obj: T[]) =&gt; unknown, thisArg?: any): T | undefined; ...} callback과 predicatecallback함수들 중에서 true 혹은 false를 반환하는 함수를 predicate라고 한다.forEach함수에서 두번째 인자로 넣어주는 것은 callback 함수지만, find 함수의 콜백함수는 predicate라고 한다. 클래스에서의 제네릭123456class Example&lt;T&gt; implement IExample{ public first: T; public second: T; protected third: T; ....}","link":"/2021/04/23/202104/210423-Typescript_TIL-2/"},{"title":"210424 TypeScript TIL-1 - 구조분해 매개변수의 타입지정, 타입 가드(Type Guard), 객체의 타입이 복잡해지는 경우의 타입지정(interface)","text":"복잡한 인수를 넘겨줄때는 객체 타입으로 작성함수의 매개변수가 단순한 경우에는 그냥 각 각의 인수를 넣어서 호출해줘도 되지만, 만약에 함수의 매개변수로 넘겨줘야 되는 인수의 갯수가 많아지는 경우, 각 각의 인수가 어떤 역할을 하는지 파악하려면 다시 그 함수를 참조해야 한다.따라서 넘겨줘야 되는 인수의 갯수가 늘어나는 경우 객체 타입으로 작성해서 각 인수의 역할에 대해서 유추할 수 있도록 작성을 해주면 코드의 가독성이 높아진다. 1Example({ flag: true, id: '1' }); 함수의 구조분해 매개변수에 타입 지정해주기구조분해한 매개변수의 경우에는 구조분해한 변수의 뒤에 타입을 지정하지 않고 구조분해한 {}뒤에 콜론을 찍고 타입을 지정해준다. 123function Example({ flag, id }: { flag: boolean, id: string }) { ...} 타입 가드(Type Guard)타입 가드를 사용하면 조건문에서 객체의 타입을 좁혀나갈 수 있다.타입 가드란 함수의특정 인자가 어떤 타입인지 구분하기 위한 값을 반환하는 함수이다. 사용자 정의 Type Guard만약 Data 객체가 A 또는 B 객체로 구분된다면 아래와 같이 사용자 정의 Type Guards 방식으로 data 객체의 타입을 좁힐 수 있다. 12345678function isA(data: Data): data is A { // 만약 data 객체에 valueA 속성이 있다면, // data는 객체 A이다. if(data.valueA){ return true; } return false;} 12345678function isB(data: Data): data is B { // 만약 data 깨체에 valueB 속성이 있다면, // data는 객체 B이다. if(data.valueB){ return true; } return false;} 자바스크립트에서는 풍부한 런타임 내부검사 (runtime introspection support)를 지원하지 않는다. 그래서 자바스크립트 객체를 사용할 때에는 instanceof 나 typeof와 같은 연산자로 엑세스 불가능하다.그런데 타입스크립트에서는 사용자 정의 타입 가드를 사용해서 instanceof나 typeof와 같은 효과를 낼 수 있다. 12345678910111213141516171819202122232425interface Foo { foo: number; common: string;}interface Bar { bar: number; common: string;}// 사용자 정의 Type Guardfunction isFoo(arg: any): arg is Foo { return arg.foo !== undefined;}function doStuff(arg: Foo | Bar){ if(isFoo(arg)){ console.log(arg.foo); } else { console.log(arg.bar); }}doStuff({ foo: 123, common: '123' });doStuff({ bar: 123, common: '123' }); 리터럴 Type Guard리터럴 값의 경우 === / == / !== / != 연산자를 사용해서 타입을 구분할 수 있다. 1234567891011type TriState = 'yes' | 'no' | 'unknown';function logOutState(state: TriState) { if (state === 'yes') { console.log('사용자가 yes를 선택했습니다.'); } else if (state === 'no') { console.log('사용자가 no를 선택했습니다.'); } else { console.log('사용자가 아직 결정을 내리지 못했습다.'); }} 참고 : https://radlohead.gitbook.io/typescript-deep-dive/type-system/typeguard 객체의 타입이 복잡해지는 경우의 타입선언객체의 타입이 복잡해지는 경우 별도로 interface로 빼서 타입을 정의해주면 코드가 간결해진다. 123456789101112131415interface IExample { data: number, flag: boolean, result: string}// 매개변수의 타입지정으로 인해 코드의 가독성이 떨어진다.function getResult({ data, flag, result }: { data: number, flag: boolean, result: string }){ ......}// 구조분해 매개변수의 타입을 별도의 interface로 빼서 코드를 간결하게 작성한다.(코드의 가독성 좋아진다)function getResult({ data, flag, result }: IExample){ ......}","link":"/2021/04/24/202104/210424-Typescript_TIL-1/"},{"title":"210424 TypeScript TIL-2 - NodeJS의 모듈 시스템과 TypeScript의 모듈 시스템","text":"NodeJS의 모듈 시스템과 TypeScript의 모듈 시스템타입스크립트의 모듈 시스템을 이해하기 전에 자바스크립트의 모듈 시스템 아니 더 정확하게는 NodeJS의 모듈 시스템에 대해서 알아보도록 하겠다. NodeJS의 모듈 시스템 = commonJS TypeScript의 모듈 시스템 = JavaScript의 최신 문법(ES2015(ES6))을 모듈 시스템에서 그대로 계승 우선 module을 이해하기 이전에 스크립트와 모듈을 구분할 수 있어야 한다.만약에 자바스크립트 파일 안에 import와 export가 있는 경우, module이며, 없다면 script이다. 그렇다면 모듈이란 무엇일까? 모듈은 다른 자바스크립트 파일에서 특정 자바스크립트 파일을 사용할 수 있도록 만들어주는 것이다. 다시말해 파일을 잘개 쪼개는 효과를 줌으로써 코드의 가독성과 재사용성이 가능하도록 해주는 것이다. 자 이제 모듈의 개념에 대해서 간단하게 이해했으니 이제 NodeJS의 모듈 시스템과 TypeScript의 모듈 시스템의 차이에 대해서 살펴보도록 하겠다. commonJS 12345678910// module.jsexports.a = 'b';exports.b = false;module.exports = { a: 'b', b: false};// run.jsconst { a, b } = require('./module'); 우선 commonJS에서 exports와 module.exports는 같은 exports이다. 따라서 위와같이 module.js 파일에서 exports와 module.exports를 같이 작성하게 되면, module.exports에서 이전에 작성해준 exports를 덮어쓰게 되기 때문에 run.js파일에서 호출해서 사용할 수 없게 된다.따라서 하나의 파일에서는 둘 중에 하나만 사용하는 것이 좋다.또한 module.exports는 하나의 파일에서 한 번만 써야 한다는 제약조건이 있다. NodeJS 모듈 시스템에서 함수를 exports 123456// module.jsmodule.exports = function () {};// run.jsconst func = require('./module');func(); 자 그럼 TypeScript 모듈 시스템에 대해서 알아보자.이전에 NodeJS 모듈 시스템에서 exports와 module.exports가 같은 exports를 사용하기 때문에 같이 사용하게 되면 덮어써진다는 한계가 있다고 배웠다. 이러한 한계점을 극복하기 위해서 TypeScript의 모듈 시스템에서는 default 개념을 도입했다. JS 최신 문법(ES2015)을 계승한 모듈 시스템 123456789// module.jsexport const a = 'b';export const b = false;export default function () {}// run.jsimport func, { a, b } from './module';console.log(a, b);func(); 위와같이 타입스크립트의 모듈 시스템에서는 export와 export default에서의 export가 같지 않기 때문에 덮어쓰지 않아 export한 a, b, Func을 다른 파일에서 import 해서 사용할 수 있다. 아래와같이 export를 해줄수도 있다. 12345const a = 'b';const b = false;export { a };export { b }; TypeScript의 모듈 시스템을 이해하기 위해서는 commonJS를 이해해야 한다. 그 이유는 TypeScript는 ES2015(ES6) 문법만 지원하는 것이 아닌 commonJS 문법도 지원하기 때문이다. 따라서 TypeScript에서 모듈을 처리하는 방법이 나뉜다.만약에 특정 모듈이 commonJS로 작성(module.exports)이 되어있고 TypeScript 파일에서 이를 import해서 사용하려면 아래와 같이 작성을 해줘야 한다. 12345// module.jsmodule.exports = function () {}; // export default와 다르다.// 따라서 타입스크립트 파일에서 위의 module을 import할때는 아래와 같이 작성해줘야 한다.import * as func from './module'; commonJS로 작성된 module들은 꼭 위와같이 _ as 를 붙여서 import해야한다.tsconfig.json 파일에서 esModuleInterop 옵션을 활성화 시켜주면 _ as 를 빼고 import를 해 줄 수 있지만, 좋지 않은 방법이기 때문에 반드시 commonJS로 작성된 module을 TypeScript 파일에서 import할때에는 위와같은 방식으로 사용하도록 하자. 타입의 정의부분을 별도의 파일로 분리하기우리가 타입을 위해 선언해준 interface와 class, type alias는 보통 같은 파일에서 관리를 안하고 별도의 파일로 분리해서 재사용이 가능하게 만든다.(파일 안에서 export interface로 외부 파일에서 import해서 사용할 수 있도록 할 수도 있음) types.ts 1234567export interface IExample { ...}export class Example implements IExample { ...} commonJS 문법으로 작성된 JS파일을 타입스크립트 파일에서 사용하기 위한 *.d.ts 파일 정의common.js 123module.exports = function () { console.log('hi');}; TypeScript 파일에서 common.js 파일을 사용하기 위해서는 common.js 파일에 해당하는 타입을 만들어줘야 한다. common.d.ts*.d.ts 파일은 ambient module이라고 불린다. *.d.ts 파일은 보통 내 프로젝트가 라이브러리일 경우 사용한다.ambient: 주이(주변)의, 잔잔한, 은은한 123declare function a(){}// 대부분의 유명한 라이브러리가 아래와 같이 export = 으로 되어있다.export = a; 이제 common.js 파일을 타입스크립트 파일에서 가져다가 사용하기 위해서는 아래와 같이 import를 해줘야 한다. 12// common.d.ts 파일에서 export = 방식으로 export를 해주었기 때문에 require로 호출해서 사용해야 한다.import A = require('./common'); 혹은 위의 방식을 아래와 같이 대체해서 쓸 수 있다. 12// A에 포함되어있는 모든 모듈들(*)을 가져온다는 의미import * as A from './common'; 20210425 Update 외부 라이브러리 사용하기 in TypeScript자바스크립트는 commonJS 문법으로 사용된 역사가 길다. 따라서 타입스크립트에서 외부 라이브러리를 사용할때에는 반드시 해당 라이브러리의 저장소를 확인하도록 하자.해당 라이브러리가 어떤 언어로 비중있게 개발이 되었는지, 타입이 정의된 파일(index.d.ts)을 제공하는지에 대한 확인이 필요하다.대표적인 예시로 redux와 axios 라이브러리를 살펴보자. reduxredux 라이브러리는 TypeScript 76.6%, JavaScript 19.0%로 개발이 된 라이브러리이다.한 번 프로젝트에서 npm 명령으로 redux를 설치한 뒤에 node_modules 폴더의 하위의 redux폴더를 살펴보면 index.d.ts 파일을 확인할 수 있다. 이 파일은 redux 관련 타입들을 정의하고 있다.대부분 최신 문법(ES2015/ES6)으로 작성이 되어있기 때문에 일반적인 import 구문으로 필요한 요소를 불러와서 사용하면 된다. axiosredux와는 반대로 axios 라이브러리는 JavaScript 93.3%, TypeScript 3.9%의 비율로 개발이 된 라이브러리이다. 그럼 axios에서는 타입을 지원하지 않는 것일까? 아니다. axio도 적은 비율이지만, 3.9%의 TypeScript의 비율을 갖기 때문에 설치후 node_modules 폴더의 하위를 살펴보면 index.d.ts 파일을 제공한다.*.d.ts 파일의 내부 코드를 살펴보면 declare 구문을 확인할 수 있는데 타입이 없는 것을 새로 정의할때 사용하는 키워드이다.또 생소한 ///(Triple slash)&lt;reference /&gt;를 볼 수 있다. 이는 *.d.ts파일의 최상단에 위치하면서 특수한 기능을 한다. 이 ///&lt;reference /&gt;는 path와 type 등 다양한 속성을 갖는데, path는 경로에 대한 정보를, type은 package이름에 대한 정보를 갖는다.라이브러리를 만들때 필요한 외부 package의 타입을 참조하는 경우, reference의 type 속성에 해당 라이브러리의 이름을 속성값으로 넣어준다. (node_modules 폴더 하위에 설치된 이름) 그러면 해당 module의 index.d.ts(타입 정의 파일)을 해당 d.ts 파일에서 참조할 수 있게 되는 것이다. DefinitelyTyped만약에 사용하고자 하는 외부 라이브러리에서 타입을 지원하지 않는 경우(index.d.ts파일을 제공 안하는 경우), 이런 경우에는 DefinitelyTyped 저장소에서 해당 라이브러리를 지원하는지 살펴보자. DefinitelyTyped/DefinitelyTyped→ https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types 사용방법사용방법은 아래와 같이 npm i @types/을 작성해주면 된다.12# jquery 모듈을 DefinitelyTyped에 정의된 타입파일과 같이 설치하는 경우$ npm i jquery @types/jquery","link":"/2021/04/24/202104/210424-Typescript_TIL-2/"},{"title":"210424 Memoirs 블로그 운영 93일차(만 3개월 2일차) 회고록","text":"93일 243개 포스팅2021년 4월 24일 12시 32분 오늘은 지난 1월 22일부터 하루도 빠짐없이 자기개발을 하며 블로그를 운영한 93일 동안의 나를 되돌아보며 회고록을 작성해보려고 한다.여지까지 243개의 글을 포스팅했다.내가 현재 어떤 방향으로 블로그를 운영하고 있으며, 향후 어떻게 자기개발을 하며 블로그를 운영할지에 대해서 자기반성의 시간을 갖으려고 한다. console.log(지속적인 자기개발을 위한 나침반 === 개인 블로그);// true우선 블로그 관리를 93일동안 하루도 빠짐없이 하면서 느꼈던 좋은 점은 나의 블로그가 지속적인 학습을 위한 나의 버팀목이자 공부 방향을 위한 나침반이었다는 점이다. 내가 개발자로 취업준비를 하면서 가장 중요하게 생각했던 것은 바로 꾸준한 자기개발이었다. 주변에서는 현재 개발자로 취업준비하는 것에 대해 취업준비 기간이 늘어나는 것을 걱정한다. 물론 나 역시 취업 준비기간이 늘어나는 것에 대해 아예 걱정이 안되는 것은 아니지만, 그보다 더 걱정되었던 것은 바로 내가 다시 개발자로 일을 하게 되었을때 퇴근한 뒤에 혹은 휴일에 나 스스로 꾸준히 자기개발을 하며 자기성장을 할 수 있을까?였다. 이전에 회사에서 짧게나마 개발자로 근무를 하면서 느꼈던 점은 개발자가 성장을 하려면 일 외적으로 꾸준하게 끊임없이 자기개발을 해야된다는 점이다.일을 하다보면 자기개발을 소홀히 하는 경우가 많다. “업무가 힘들어서” “늦게 퇴근해서” “체력이 안되서” 등 여러가지 변명들로 자기합리화를 하면서 자기개발을 등한시 하는 경우가 많다. 이런 부분 때문에 혹여 지금 어느 회사에 들어간다고 하더라도 나 스스로 이런 자기개발 습관이 잡혀있지 않다면 어쩌면 이 개발 일을 업으로 삼으며 오래하지 못할 것 같다고 생각했다. 그래서 이전에도 물론 자기개발을 했지만 1월 22일부터는 하루도 빠지지 않고 자기개발을 하고 공부한 내용중에 다른 사람들과 공유하고 싶은 내용이나 스스로 다시 보면서 반복학습이 필요한 내용에 대해서 블로그 포스팅을 하고 있다.이전에는 생각보다 내가 알고 있는 내용에 대해서 남에게 설명하려고 하면 어려웠는데 지금은 블로그 글을 작성하면서 글을 정제하다보니, 이전보다는 남에게 내가 알고 있는 것에 대해 설명을 잘 할 수 있게 된 것 같다. 지금까지 자기개발한 내용과 느낀점지금까지 프론트엔드 개발 공부와 자료구조/알고리즘에 대해서 학습한 내용들에 대해서 블로그 포스팅을 했다. 꼼꼼하게 공부한 내용을 정리하다보니 내가 알고 있다고 생각했는데 잘 알지 못했던 부분에 대해서 알게 되었고 시간날때마다 포스팅했던 블로그 글을 읽으면서 자연스럽게 반복학습이 되었다.그래서 이전에는 왠지 추상적으로 알고 있다고 느껴졌던 개념들이 이제는 조금씩 형태가 잡혀서 구체화되가고 있다는 생각이 든다. 물론 아직 많은 학습이 필요한 것은 사실이다. 하지만 여지까지 꾸준하게 블로그 관리를 하면서 자기개발을 하다보니 이전에 내가 가지고 있었던 고민인 &quot;내가 개발자로 다시 일하게 되었을때 스스로 꾸준하게 자기개발을 이어가면서 자기성장을 할 수 있을까?&quot;에 대한 어느정도의 고민이 해결된 것 같다. 원래는 이렇게 글을 쓰고 정리를 하는 것을 즐기지 않았는데 어느새 블로그가 나의 일상의 일부가 되버려서 습관처럼 조금이라도 자기개발을 위한 공부를 하고, 공부한 내용중에 중요해서 반복학습이 필요하다고 느끼거나 다른 사람들과 공유하고 싶다고 생각되는 내용을 블로그 포스팅을 하고 있다. 앞으로의 계획지금까지는 토양을 비옥하게 하기 위한 일종의 기반이 되는 지식을 쌓기 위해 학습을 해왔다.내가 작성한 코드에 대해서 설명이 가능하려면 좀 더 확실히 내가 사용하려는 기술들에 대한 이해가 수반되어야 된다고 생각되었기 때문이다.그리고 그냥 어설프게 어플리케이션을 만드는 것이 아닌 견고한, 확장성 있는 어플리케이션을 만들고 싶다는 욕심에 TDD를 위한 테스트 코드 작성이나 자바스크립트의 superset인 타입스크립트에 대해서 집중해서 공부하였다.공부했던 개념들이 어느정도 학습이 된 후에는 5월달부터 본격적으로 어플리케이션을 만들어 볼 것이다. 개인 프로젝트를 진행하면서 겪었던 문제와 해결방법에 대해서 구체적으로 블로그에 포스팅하고, 내가 사용하는 기술에서 기본적인 CS지식과 연관시킬 수 있는 부분에 대해서도 추가적으로 기록을 할 것이다.단순히 제출을 위한 프로젝트가 아닌, 나의 학습에 있어 또 다른 나침반의 역할을 해줄 수 있는 프로젝트를 하나씩 만들어 갈 것이다. 회고록을 마무리하며오늘까지 93일(만 3개월 2일차)동안 블로그를 운영하면서 처음에는 공부했던 내용을 하나 하나 기록하자는 의미에서 시작했던 블로그가 이제는 나의 지속적으로 학습을 할 수 있도록 도와주고 개발자로서 좋은 습관을 갖을 수 있도록 도와준 것 같다. 개발자가 왜 개인 컨텐츠 채널을 운영해야 되는지 그 필요성에 대해서 다시 한 번 깨닫게 해주는 좋은 계기였던 것 같다.그래도 블로그를 운영함으로써 내가 이전에 가지고 있던 가장 큰 고민이었던 꾸준한 자기개발과 개발자로서 좋은 습관이 어느정도 잡혀가는 것 같아 이 부분이 가장 큰 수확이지 않았나싶다.앞으로도 졸꾸(졸도할 정도로 꾸준히) 자기개발을 하며 자기성장을 할 수 있는 개발자가 될 수 있도록 노력해야겠다. :)","link":"/2021/04/24/202104/210424-memoirs/"},{"title":"210425 DFS(Depth-First-Search):깊이 우선 탐색","text":"DFS(Depth-First-Search) 깊이 우선 탐색은 그래프의 모든 정점을 탐색하는 가장 단순한 방법이다.현재 정점과 인접한 간선들을 하나씩 검사하다가 방문하지 않은 정점으로 향하는 간선이 있다면 해당 간선을 따라가 정점을 검사하고, 더 이상 갈 곳이 없는 막힌 정점에 도달하면 포기하고 마지막에 따라왔던 간선으로 되돌아간다.거쳐왔던 모든 정점들에 대한 정보를 저장하기 위해서는 재귀호출을 이용해서 간단하게 해결할 수 있다. 재귀 호출한 함수가 종료하면 호출한 위치로 다시 돌아가기 때문이다. 12345678910111213141516171819202122232425262728293031323334// 1부터 n까지의 원소를 갖는 집합의 부분집합 구하기function solution(n) { let answer = []; // 지나간 정점을 구분하기 위해서 길이가 n인 check 배열 생성(1부터 n개까지 표시하기 위해서 n+1) let ch = Array.from({ length: n + 1 }, () =&gt; 0); // v = 1 (시작점) function DFS(v) { // n은 solution의 인자값 3이기 때문에 // v(vertex)가 4일때 D(4)만 조건문에 만족한다. if (v === n + 1) { //부분집합 완성할때마 let tmp = ''; for (let i = 1; i &lt;= n; i++) { if (ch[i] === 1) tmp += i + ' '; } if (tmp.length &gt; 0) answer.push(tmp.trim()); } else { ch[v] = 1; // 정점의 왼쪽 가지치기 DFS(v + 1); // 다시 해당 노드로 돌아왔을때에는 ch[v]의 값을 0으로 초기화하고 ch[v] = 0; // 정점의 오른쪽 가지치기 DFS(v + 1); // 오르쪽 노드에서 다시 정점으로 돌아왔을때에는 모든 코드가 실행 완료되었기 때문에 // 상위 노드로 이동해서 waiting된 시점의 코드부터 실행한다. } } // v = 1 (시작점) DFS(1); return answer;}console.log(solution(3));","link":"/2021/04/25/202104/210425-Algorithm_DFS/"},{"title":"210425 TypeScript TIL - DefinitelyTyped에 없는 package인 경우(custom package typing), internal module&#x2F; external module&#x2F; ambient module, 외부 라이브러리의 Typing 5가지 경우, 라이브러리를 만드는 경우, *.d.ts파일과 *.ts파일의 사용, Type intersection, call&#x2F;bind&#x2F;apply를 사용한 type 구체화, TS 유틸리티, 데코레이터","text":"DefinitelyTyped에 없는 package인 경우만약에 사용하고자 하는 package가 기본적으로 type을 제공하지 않고, DefinitelyTyped에도 없는 경우, 이런 경우에는 내가 직접 typing을 해줘야 한다. (전부 typing하지 않고 내가 사용할 부분만 부분 typing한다.) 예를들어, can-use-dom이라는 타입을 제공하지 않는 패키지를 설치해서 사용하는데, DefinitelyTyped에도 없는 경우, 이 경우에는 아래와 같이 프로젝트 폴더 안에 *.d.ts파일을 작성해줘야 한다.되도록이면 types와 같은 별도 폴더를 생성해서 내부에 *.d.ts파일을 작성해주도록 한다. /&lt;Project folder&gt;/types/can-use-dom.d.ts 1234declare module &quot;can-use-dom&quot; { const canUseDOM: boolean; export default canUseDOM;} /&lt;Project folder&gt;/index.ts 123import canUseDOM from 'can-use-dom';console.log(canUseDOM); 이제 작성해준 can-use-dom.d.ts파일을 *.ts 파일 내에서 인식시키기 위해서 tsconfig.json파일에서 &quot;typeRoot&quot;옵션의 속성값으로 작성해준 *.d.ts파일의 폴더 경로를 적어줘야 한다. tsconfig.json 123456789101112{ &quot;compilerOptions&quot;: { ... &quot;typeRoots&quot;: [&quot;./types&quot;, &quot;./node_modules/@types&quot;], ... }, &quot;exclude&quot;: [&quot;*.js&quot;], // &quot;include&quot;에는 구체적으로 특정 파일을 작성하지 않도록 한다. // 만약 구체적으로 파일을 명시하게 되면 해당 타입스크립트 파일만 컴파일되기 때문에 // 내가 작성한 *.d.ts 파일이 컴파일 되지 않는다. &quot;include&quot;: [],} 만약에 *.d.ts내에서 기존에 정의된 interface를 재정의해서 사용하고자 하는 경우에는 아래와 같이 작성을 해준다. index.d.ts 1234567891011// window(global)인 경우에는 ambient module을 사용할 수 없기 때문에 external module로 만들어 줘야 한다.// ambient module을 external module로 만들어주기 위해서 export {}를 최상단에 적어준다.export {}declare global { export interface Window { hello: string; } interface Error { code?: any; }} index.ts 12345window.hello = 'a';const error = new Error('');// error 객체에 code 속성을 사용하고자 하는 경우,// 위와같이 기존의 Error interface를 확장해서 타입을 정의해줘야 한다.error.code; internal module, external module, ambient module internal module내부 모듈은 namespace이다. external module외부 모듈은 import/export를 사용하는 것을 말한다. ambient module남이 만든 경우에는 declare module을 해주지 않아도 되지만, 내가 만든 package인 경우, declare module을 해줘야 되는데 이 declare module을 작성해주는 것이 ambient module이다. typing이 틀리게 되어있는 경우1$ npm i connect-flash @types/connect-flash 만약에 DefinitelyTyped에서 정의된 타입이 잘못되어 있는 경우, 기존에 설치한 @types/connect-flash를 지우고, 직접 d.ts 파일을 작성해서 typing을 해준다. 1$ npm rm @types/connect-flash types/connect-flash.d.ts 파일을 생성해서 ambient module을 만들어준다. 아래의 코드는 @types/connect-flash설치후 자동 생성된 index.d.ts 파일의 일부이다.아래의 코드를 코면 Express namespace 내부에 Request interface가 정의되어있는데, 이는 Express에 이미 정의되어있는 Request interface가 확장되어 있는 것이다. 이외에도 global을 ambient module로 감싸서 ambient module로 재정의할 수 있다. 12345678910/// &lt;reference types=&quot;express&quot; /&gt;declare namespace Express { export interface Request { flash(): { [key: string]: string[] }; flash(message: string): string[]; flash(type: string, message: string[] | string): number; flash(type: string, format: string, ...args: any[]): number; }} 외부 라이브러리의 Typing 5가지 경우 (총정리) (1)TypeScript로 만들어진 Package (2)JavaScript로 개발이 되었지만 index.d.ts파일을 제공하는 Package (3)타입을 제공되지 않은 Package이지만 DefinitelyTyped에서 index.d.ts파일이 제공되는 경우 (4)DefinitelyTyped에서도 type을 제공해주지 않는 경우이 경우에는 내가 직접 *.d.ts 파일을 생성해서 ambient module을 작성해주고, tsconfig.json 파일의 typeRoots 옵션에서 파일의 경로를 지정해서 연결시켜줘야 한다. (5)DefinitelyTyped에서 제공된 index.d.ts파일에서 정의된 타입이 잘못된 경우기존에 설치한 type package를 제거하고 직접 짠다. 라이브러리를 만드는 경우만약에 라이브러리를 만드는 경우, 아래와 같이 tsconfig.json파일에서 옵션을 작성해줘야 한다.아래와 같이 옵션을 추가해주게 되면 “declarationDir”에서 지정한 폴더의 하위에 컴파일된 파일에 대해 자동으로 *.d.ts파일을 생성해준다. 123456&quot;compilerOptions&quot;: { ... &quot;declaration&quot;: true, &quot;declarationDir&quot;: &quot;./types&quot;, ...} *.d.ts 파일과 *.ts파일의 사용일반적으로 *.d.ts 파일의 경우에는 라이브러리를 만들때와 같이 특별한 상황에서 사용하거나 남이 정의한 타입의 정의가 잘못된 경우에 만들어서 사용한다.내가 프로젝트를 만들때 작성한 타입 선언과 관련된 코드들을 별도의 파일에 일괄적으로 모아서 재사용성 가능하도록 만드는 경우에는 *.ts 파일내에서 작성을 해준다. type intersectiontype intersection은 A &amp; B 라고 했을때 A와 B 타입을 모두 만족해야 한다. 1234567891011121314151617181920interface A { hello: true;}interface B { bye: true;}const a: A = { hello: true};const b: B = { bye: true};const c: A &amp; B = { hello: true, bye: true}; 위와같이 타입 A와 타입 B를 각 각 개별적인 interface로 선언해주는 이유는 타입의 중복선언 없이 경우에 따라 | 나 &amp; 로 조합해서 타입을 정의하기 위해서 이다. interface와 type alias를 같이 조합 가능123456789101112131415interface A { hello: true;}interface B { bye: true;}type c = { hi: false};const c: A &amp; B &amp; C = { hello: true, bye: true, hi: false}; call, bind, apply를 사용한 type 구체화tsconfig.json 파일에서 &quot;strictBindCallApply&quot;: true 옵션의 활성화하면 type checking이 좀 더 확실해진다. 좀 더 엄격하게 하면 type checking이 좀 더 확실해진다. map을 사용해서 number 타입의 배열요소를 string 타입의 값으로 바꿔보도록 하자. 123// 간단하게 아래와 같이 코드를 작성할 수 있다.[1, 2, 3].map((item) =&gt; item.toFixed(1));// result: ['1.0', '2.0', '3.0'] 위와같이 간단하게 코드를 작성해 줄 수 있지만, call을 사용해서 제네릭으로 args의 타입과 반환 값의 타입을 구체적으로 명시해서 타입 추론을 정확하게 할 수 있다.마우스를 함수의 위에 올려보면 구체적으로 어떤식으로 제네릭 typing을 해야하는지 힌트를 얻을 수 있다. 얻은 힌트를 토대로 타입부분만 구체적으로 명시해주면 된다. 12345// return type: string[]// args type(in callback function): numberconst result = Array.prototype.map.call&lt;number[], [(item: number) =&gt; string], string[]&gt;([1, 2, 3], (item) =&gt; { return item.toFixed(1);}); TS 유틸리티TS Utility를 사용하면 interface에 정의한 타입을 손쉽게 조작해서 사용할 수 있다. 아래의 TypeScript 공식 홈페이지의 handbook에서 TS 유틸리티에 대한 내용을 참고하자.참고를 위해 아래에 링크를 첨부했다. https://www.typescriptlang.org/docs/handbook/utility-types.html TS 유틸리티 중에서 아래 항목들은 유용하게 사용될 수 있으니 꼭 실습을 통해 익숙해지도록 하자. Partialinterface에서 정의한 타입들의 속성 이름에 ?를 붙인 것과 같은 효과를 낸다. 12345678910111213141516interface A { a: number; b: boolean; c: string;}// 타입을 A로 지정하면 interface A에 들어있는 모든 요소를 넣어줘야 한다.const a: A = { a: 1, b: true, c: 'hello'};// 만약 부분적으로 속성을 넣어주고 싶다면, 별도의 interface 수정없이// Partial을 이용해서 간단하게 작성할 수 있다.const b: Partial&lt;A&gt; = { c: 'bye'}; Readonly만약에 interface에서 정의한 모든 타입 속성들에 readonly를 붙여주고 싶다면 일일이 붙여줘도 되지만,간편하게 Readonly TS 유틸리티를 사용해서 처리할 수 있다. Pick&lt;T,K&gt;만약에 interface에서 정의한 타입 속성들 중에 일부만을 뽑아서 새로운 type으로 정의해서 사용하고 싶다면,Pick&lt;T,K&gt; TS 유틸리티를 사용해서 구현할 수 있다. 123456interface A { a: number; b: string; c: boolean;}type B = Pick&lt;A, 'a' | 'c'&gt;; Omit&lt;T,K&gt;pick TS 유틸리티는 특정 타입 속성을 뽑아서 새로운 타입을 정의했지만 Omit TS 유틸리티는 타입속성들 중에서 특정 타입 속성만 제거해서 새로운 타입을 정의할 수 있다. Exclude&lt;T,U&gt;Exclude TS 유틸리티는 T 타입 중에서 U 타입을 제외한 타입으로 새로운 타입을 정의할때 사용한다. 12type T0 = Exclude&lt;string | number | (() =&gt; void), Function&gt;;// T0 = string | number Extract&lt;T,U&gt;Extract TS 유틸리티는 T 타입과 U 타입의 공통 타입으로 새로운 타입을 정의할때 사용한다. 12type T0 = Extract&lt;'a' | 'b' | 'c', 'a' | 'f'&gt;;// T0 = &quot;a&quot; ReturnTypeReturnType TS 유틸리티는 함수의 return값의 타입을 가져와서 새로운 타입으로 정의할때 사용된다.이 TS 유틸리티는 Redux에서 많이 사용된다. Required만약에 interface에서 작성한 타입 속성의 이름에 ?에 붙어있는 경우, 해당 타입 속성은 생략이 가능하다.하지만 모든 속성들로부터 ?를 제거하고 모두 필수요소로 만들어주려면 어떻게 해야할까?바로 Required TS 유틸리티를 사용하면 된다. 123456789interface A { a?: number; b?: string;}const a: Required&lt;A&gt; = { a: 1, b: 'hello'}; OmitThisParameterType으로부터 this parameter를 제거할때 사용되는 TS 유틸리티이다.아래의 예시에서는 toHex라는 함수에서 OmitThisParameter TS 유틸리티를 사용해서 this parameter를 제거하고 제거한 결과에서는 number 타입의 값 5를 bind함수를 사용해서 this binding을 하고 있다. 1234567function toHex(this: Number) { return this.toString(16);}const fiveToHex: OmitThisParameter&lt;typeof toHex&gt; = toHex.bind(5);console.log(fiveToHex()); 데코레이터데코레이터는 JavaScript의 개념이다.우선, 데코레이터를 사용하기 위해서 tsconfig.json에서 &quot;experimentalDecorators&quot;: true를 컴파일 옵션에 넣어줘야 한다.데코레이터의 사용은 중복없이 새롭게 기능을 추가할 수 있도록 도와준다. 클래스에서는 코드 중복의 제거가 어렵기 때문에 이는 매우 유용하다.데코레이터의 종류에는 class 데코레이터, property 데코레이터, 함수 데코레이터, parameter 데코레이터가 있다. 123456789101112131415@makeGenderclass Person { @validate name: string; age = 27; constructor() { this.name = name; } setName(name: string) { this.name = name; } @readonly sayName(): any { return this.name; }} 데코레이터 함수 만들기makeGender 데코레이터 함수의 매개변수 target의 타입을 Person으로만 적어주면 Person 객체 인스턴스의 타입이 되지만, 데코레이터 속성으로써 꾸며주고자 할때에는 이 target 매개변수의 타입은 typeof를 붙여서 정의해주도록 한다. 클래스에 gender관련 속성과 함수를 추가해주는 데코레이터 함수 123456789// target이 원본 class 객체라면 반환되는 class 객체는 새롭게 만들어낸 class이다.function makeGender(target: typeof Person) { return class extends target { gender = 'male'; sayGender() { this.gender; } };} 읽기 전용 readonly 데코레이터 함수함수 decorator 함수의 경우에는 세번째 매개변수 인자가 descriptor이지만, 매개변수를 위한 decorator 함수를 만드는 경우에는 세번째 매개변수 인자가 index: number이다. descriptor의 속성에는 아래 세 가지가 있다. writable: 수정가능 여부 configurable: 설정가능 여부 enumerable: 반복가능 여부 123function readonly(target: any, key: any, descriptor: PropertyDescriptor) { descriptor.writable = false;}","link":"/2021/04/25/202104/210425-Typescript_TIL/"},{"title":"210426 React with NextJS TIL - NextJ의 pages와 layout, _app.js, NextJS - Head, NextJS 전용 router, eslint 설정, NextJS v9의 새로운 기능들, front-end&#x2F;back-end 서버간의 CORS 설정","text":"NextJS 프로젝트 기본 설정next v9 1$ npm i next@9 package.json 1234&quot;script&quot;: { &quot;dev&quot;: &quot;next&quot;, &quot;build&quot;: &quot;next build&quot;} NextJS의 js파일에서는 import React from 'react';구문이 필요없다.그 이유는 NextJS 프레임워크가 pages 폴더내의 js파일들을 code splitting된 개별적인 page component로 만들어주기 때문이다. pages와 layoutReact에서는 react-router로 routing 경로를 설정해줘야 했는데, NextJS는 front server를 가지고 있기 때문에 pages폴더내의 파일의 이름을 url로 자동 mapping 시켜준다. pages/profile.js =&gt; /profilepages/signup.js =&gt; /signuppages/about/lee.js =&gt; /about/signup _app.js: page들의 공통처리를 작성만약에 page들에서 공통적으로 처리하는 것이 있다면 pages/_app.js 파일의 내부에서 작성해주도록 한다. 예를들어 모든 페이지에서 antd.css 파일을 사용한다면, 아래와같이 _app.js파일을 작성해준다._app.js는 전체 페이지 컴포넌트들의 공통적인 부분을 작성한다면, layout은 일부 페이지 컴포넌트들의 공통 부분을 처리하기 위해서 작성한다. 12345678910111213141516171819202122import React from 'react';import 'antd/dist/antd.css';import PropTypes from 'prop-types';import Head from 'next/head';const App = ({ Component }) =&gt; { return ( &lt;&gt; &lt;Head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;Main Title&lt;/title&gt; &lt;/Head&gt; &lt;Component /&gt; &lt;/&gt; );};App.propTypes = { Component: PropTypes.elementType.isRequired};export default App; 위와같이 작성해주면 pages의 하위에 작성된 javascript파일들의 return문에 있는 JSX 부분이 App 컴포넌트의 인자 Component로 들어가서 _app.js 파일에 적용된 내용들이 공통적으로 적용된다. NextJS에서 head 부분을 수정할 수 있는 Head를 제공한다.NextJS에서 html의 head section의 내용을 수정할 수 있는 Head를 제공한다.위에 첨부한 코드와 같이 간편하게 'import Head from next/head'를 작성해주면 Head 부분의 meta 태그나 title 부분을 손쉽게 수정할 수 있다. NextJS 자체 router가 있다.NextJS에서는 React router를 사용하지 않고, NextJS자체의 router를 사용한다.href는 내부의 anchor tag가 아닌 Link 태그의 속성에 넣어줘야 한다.처음 Link 태그를 클릭하고 화면이 전이되는 시간이 약간 더딘 것을 볼 수 있는데, 이는 현재 개발모드(dev mode)이기 때문이다.배포모드(prod mode)에서는 빠르기 때문에 이 부분은 걱정하지 않아도 된다. 12345import Link from 'next/link';&lt;Link href=&quot;/&quot;&gt; &lt;a&gt;Home&lt;/a&gt;&lt;/Link&gt;; NextJS에는 기본으로 react hot loader가 적용되어있다.SSR을 위한 서버반드시 React와 NodeJS를 사용할 필요는 없다. 단지 SSR을 위해서 서버가 필요할 뿐 Sprint, Django와의 조합으로 모두 사용이 가능하다. NextJS v9의 새로운 기능NextJS v9부터 Dynamic routing과 API routing기능이 새로 추가되었기 때문에 별도로 custom front-end 서버를 만들 필요가 없고 모두 NextJS 안에서 가능하다. front-end 서버와 back-end 서버간의 CORS 설정브라우저와 프론트 간에는 CORS 문제가 없지만, port 번호만 달라도 CORS가 달라지기 때문에 프론트엔드와 백엔드 서버간에는 CORS 설정이 필요하다. 따라서 백엔드 서버에서 별도로 CORS 설정을 해줘야 되고, 프론트엔드 서버에서는 cookie를 보내는 것에 대한 with Credentials에 대한 설정을 해줘야 한다.보통 실무에서 React - Node(Front) - Node(Back) 의 구성으로 두 개의 노드 서버를 구축해서 사용한다.CSR에서는 Front-end 서버로부터 일괄적으로 모든 페이지에 대한 파일을 다운받는데, 이 부분이 비효율적이기 때문에 별도로 code splitting이 이뤄진다. code splitting의 특성상 요청한 url과 연관된 페이지에 대한 파일을 다운받기 때문에 별도로 요청을 받아 처리하는 방식으로 동작한다. (NextJS에서 알아서 처리해준다) MongoDB의 사용은 추천하지 않는다.대부분의 데이터들은 서로 N:N(다대다) 방식으로 서로 연관성이 있다. 따라서 NoSQL인 MongoDB를 사용하는 것보다 SQL을 사용해서 데이터를 테이블에 관리하는 것이 효율적이다. Loading 페이지를 없애기 위한 목적으로 SSR 사용CSR의 경우, 초기 화면이 로딩될때 화면에 로딩중인 화면이 표시되는 경우가 있다. 이러한 화면 처리를 없애기 위한 목적으로도 SSR을 사용하기도 한다. 서버에서는 한 번 방문한 페이지와 데이터에 대해서 캐싱을 해준다. 캐싱이 적용되면, 다음에 같은 페이지를 방문했을때 캐싱되어있는 페이지를 통으로 받아오기 때문에 훨씬 빠른 화면 로딩을 보여준다.","link":"/2021/04/26/202104/210426-React_with_NextJS/"},{"title":"210426 React with TypeScript TIL - Webpack + TypeScript, react&#x2F;react-dom - DefinitelyTyped, webpack.config.json, event handler와 useRef typing, class state typing, useState의 타입추론, useCallback typing, class component의 createRef typing, react-hot-loader 사용","text":"프로젝트 기본 구성typescript, react, react-dom, webpack, webpack-cli 설치 12345$ npm i typescript# 웹 환경에서는 react-dom, 모바일은 react-native$ npm i react react-dom$ npm i -D webpack webpack-cli 이전에 React에서 webpack + babel을 사용해서 JavaScript의 최신문법과 JSX 문법을 이전의 JavaScript 문법으로 변환해주었는데, 이번 TypeScript에서는 webpack + TypeScript 조합으로 변환을 해준다. TypeScript는 자체적으로 babel처럼 최신문법이나 jsx 문법을 이전 문법(ES3 or ES5)으로 바꿔준다. 따라서 별도로 Babel을 사용할 필요는 없다. 하지만 TypeScript + Babel의 조합으로 사용하는 경우도 있기 때문에 이 부분을 알아두도록 하자. package.json 1234567&quot;script&quot;: { &quot;dev&quot;: &quot;webpack&quot;,}$ npm run dev or$ npx webpack Webpack + TypeScript 이어주기웹팩과 타입스크립트를 이어주기 위해서 loader를 사용해야 되는데, 유명한 loader에는 아래 두 가지가 있다. ts-loader awesome-typescript-loader (atl) 두 loader의 차이점은 atl의 경우에는 use babel 옵션을 사용해서 babel과 함께 사용할 수 있게 해준다. 따라서 잠재적으로 babel을 함께 사용하는 경우에는 atl을 사용하도록 한다. 또한 atl은 ts-loader 보다 속도가 빠르다고 한다. 1$ npm i -D awesome-typescript-loader 이전의 React에서는 webpack + babel을 위해서 babel-loader를 사용했다. react, react-dom - DefinitelyTypedreact와 react-dom의 경우에는 자체적으로 타입을 제공하지 않지만, 유명한 라이브러리이기 때문에 DefinitelyTyped에서 제공해주는 type을 사용하면 된다. antd design과 같은 경우는 TypeScript로 만들어져있기 때문에 별도의 설치없이 사용할 수 있다. 1$ npm i @types/react @types/react-dom webpack.config.json기존에 react에서의 webpack 설정파일과 다른점은 babel을 사용하지 않기 때문에 babel의 설정부분만 약간 차이가 있다. devtools의 경우에는 production mode일때 반드시 hidden-source-map으로 속성을 바꿔줘야 한다. 만약에 source-map으로 써주면 브라우저의 개발자 모드에서 내가 작성한 코드가 전부 노출이 된다. 123456789101112131415161718192021222324const path = require('path');const webpack = require('webpack');module.exports = { mode: 'development', // production devtool: 'eval', // hidden-source-map resolve: { extensions: ['.jsx', '.js', '.tsx', '.ts'], }, entry: { app: './client' }, module: { rules: [{ test: /\\.tsx?$/, loader: 'awesome-typescript-loader', }] }, output: { filename: '[name].js', path: path.join(__dirname, 'dist'), }} 간단하게 위의 웹팩 설정파일을 해석하면 entry에 작성해준 client라는 이름의 파일이 main으로 module에 작성해준 rules의 내용에 맞춰 .ts 혹은 .tsx 확장자의 파일이 awesome-typescript-loader를 통해 자바스크립트의 이전 문법으로 변환이 된다. 원래 자바스크립트로 리액트를 했을때 웹팩의 설정에서는 이 부분을 babel-loader로 해주었지만, TypeScript에서는 awesome-typescript-loader를 넣어준다.최종적으로 변환된 .js파일은 output에서 작성한 내용대로 file이름은 entry에서 작성해준 app 이름을 기준으로 js파일이 생성되는데 경로는 프로젝트 폴더의 하위에 dist라는 폴더를 새로 생성해서 그 하위에 생성된다. event handler, useRef typing함수를 별도로 빼주지 않고 인수로 넣어주게 되면 자동으로 타입추론이 된다. 하지만 보통은 React에서 성능 최적화를 위해서 함수를 별도로 빼서 작성을 해주게 된다.타입스크립트에서 별도로 빼준 함수는 타입 추론을 할 수 없기 때문에 함수의 매개변수에 대해서 별도의 typing이 필요하다.아래의 예시 코드에서 onSubmitForm 함수는 별도의 함수로 빼서 작성해주었기 때문에 매개변수 e에 대해서 별도의 타입 정의를 해주었다. Form에 들어가는 이벤트이면서 FormElement에 들어가기 때문에 제네릭으로 React.FormEvent&lt;HTMLFormElement&gt;타입으로 작성을 해주었다. useRef의 경우, 아래와 같이 제네릭으로 useRef로 어떤 DOM element를 지정하기 위한 것인지 구체적으로 타입을 지정해줄 수 있다. 타입을 지정하지 않는다면, .focus()를 사용하는 경우, 속성을 제대로 찾을 수 없다.inputEl 요소가 처음에 null로 초기화가 되어있고, input 태그의 내부에서 값이 초기화되기 때문에 외부 함수에서는 존재하는지 확실하지 않기 때문에 존재를 보증하는 !를 쓰거나 if 조건분기문 처리를 해서 작성해주도록 한다. 12345678910111213141516171819202122const inputEl = useRef&lt;HTMLInputElement&gt;(null);const onSubmitForm = (e: React.FormEvent&lt;HTMLFormElement&gt;) =&gt; { ... const input = inputEl.current; if(input){ input.focus(); } // or input!.focus();}...return( ... &lt;form onSubmit={onSubmitForm}&gt; &lt;input ref={inputEl} type=&quot;number&quot; value={value} onChange={(e) =&gt; setValue(e.target.value)} /&gt; ...) class state typingsetState를 할때 이전 상태를 참조하기 위해 함수형으로 업데이트를 하는 경우, 이전 상태에 대해 타입을 추론할 수 없는 경우가 생긴다.따라서 state에 대한 interface를 작성하고, 클래스에서 상속받은 React.Component에 제네릭으로 처리한다. 제네릭의 첫번째 인자는 props에 대한 타입이고, 두번째 인자는 state에 대한 타입이다. 1234567891011121314151617181920212223242526272829interface IState { first: number; second: number; value: string; result: string;}class Example extends React.Component&lt;{}, IState&gt; { ... // null로 초기화한 경우에는 타입도 null을 넣어 일치시켜줘야 한다. inputEl: HTMLInputElement | null = null; inputRef = (c: HTMLInputElement) =&gt; { this.inputEl = c; }; ... render() { return ( ... &lt;input ... ref={this.inputRef} ... /&gt; ... ) }} React 공식사이트에서는 hooks를 사용할 것을 권장한다.이전 코드를 해석하기 위해서 class 컴포넌트로 작성하는 법도 알아야 되지만, 실제로 hooks로 적었을때 코드가 간결해지고 리액트 공식사이트에서도 hooks의 사용을 권장한다. useState의 타입추론만약에 useState에서 타입추론을 할 수 없는 경우, useRef에서와 같이 useState&lt;T&gt;()과 같은 형태로 타입을 명시해줄 수 있다. useCallback typing함수 전체가 실행될때 불필요하게 함수내에 선언한 함수가 다시 생성되는 것을 방지하기 위해 함수를 useCallback으로 감싸준다. 이렇게 다른 함수로 감싸주게 되면, 타입추론이 방해된다.따라서 useCallback 자체에 함수 typing을 해주거나 콜백함수의 event 객체에 직접 typing을 해줌으로써 확실한 타입 추론이 가능하도록 해준다. 123456789// event 객체에 대한 typing을 해준다.const onSubmitForm = useCallback((e: React.FormEvent&lt;HTMLFormElement&gt;) =&gt; { ...}, [value])// 제네릭에 함수 자체를 typing해준다.const onChange = useCallback&lt;(e: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt; void&gt;((e) =&gt; { ...}, []) class component의 createRef typingclass component에서 useRef hooks와 비슷하게 사용할 수 있는 React.createRef가 있다. 1234567891011import { createRef } from 'react';// Declaration in class componentonRefInput = createRef&lt;HTMLInputElement&gt;();// Use ref objectconst input = this.onRefInput.current;input.focus();// input element&lt;input ref={this.onRefInput} /&gt; react-hot-loader 사용하기react-hot-loader는 *.d.ts를 제공한다. 설치1$ npm i react-hot-loader 사용1234567import * as React from 'react';import { hot } from 'react-hot-loader/root';import Example from './Example';const Hot = hot(Example); //HOCReactDOM.render(&lt;Hot /&gt;);","link":"/2021/04/26/202104/210426-React_with_Typescript_TIL/"},{"title":"210426 SSR(Server-Side-Rendering)과 CSR(Client-Side-Rendering), SPA(Single Page Application)와 MPA(Multiple Page Application)의 차이","text":"SSR(Server-Side-Rendering)과 CSR(Client-Side-Rendering)의 차이렌더링이란 요청받은 내용을 브라우저 화면에 표시하는 작업을 말한다. 그렇다면 SSR(Server Side Rendering)과 CSR(Client Side Rendering)은 어떤 차이가 있을까?간단하게 동작방식을 노트에 그려보았다. 간단히 말해 브라우저에서 보여주는 화면을 어느 시점에서 최종적으로 만들어서 보여주느냐에 따라 CSR, SSR로 구분한다. SSR(Server Side Rendering)서버 사이드 렌더링은 전통적인 렌더링 방식으로, 브라우저에서 프론트엔드 서버로 요청을 보내면 프론트엔드 서버에서는 백엔드 서버로 필요한 데이터에 대한 요청을 보내게 된다. 벡엔드 서버에서는 데이터베이스로부터 받은 데이터를 프론트엔드 서버로 넘겨준다.프론트엔드 서버에서는 백엔드 서버로부터 넘겨받은 데이터와 HTML 및 static 파일들을 조합해서 브라우저로 넘겨주고 최종적으로 브라우저에 요청한 페이지에 대한 정보를 렌더링하게 된다. SSR 방식은 완전하게 만들어진 HTML파일을 받아오고 화면에 렌더링한다. 따라서 웹 서버에 요청을 할때마다 브라우저에 새로고침이 발생하고 서버에 새로운 페이지에 대한 요청을 하는 방식으로 이루어진다.장점으로는 초기 로딩 속도가 상대적으로 빠르기 때문에 사용자가 컨텐츠를 빠르게 볼 수 있다. 그리고 모든 검색엔진에 대해서 검색엔진 최적화(SEO)가 가능하다.단점으로는 매번 페이지를 요청할때마다 새로고침이 발생하기 때문에 사용자 UX가 떨어지고, 서버에 매번 요청을 하기 때문에 트래픽 및 서버의 부하가 커진다. CSR(Client Side Rendering)클라이언트 사이드 렌더링은 React, Vue, Angular와 같은 SPA(Single Page Application)가 렌더링되는 방식으로, SPA는 하나의 페이지에서 화면에 표시되는 컴포넌트만 바꿔치기해서 페이지가 넘어가는 것 처럼 보이는게 하는 것으로, 전통적인 렌더링 방식인 SSR과는 동작에 있어 차이가 있다.브라우저에서 프론트엔드 서버로 요청을 보내게 되면, 프론트엔드 서버에서는 브라우저로 하나의 HTML 및 static 파일들을 응답으로 보내게 된다.(처음에는 데이터가 없는 문서를 반환한다)받은 파일들이 로드되면서 만약에 화면에 표시되는 데이터가 있다면 일단은 데이터가 없기 때문에 로딩화면을 표시하게 된다.그 다음으로 브라우저에서는 백엔드 서버로 데이터에 대한 요청을 보내고 백엔드 서버는 데이터베이스로부터 필요한 데이터를 받아 브라우저로 데이터를 넘겨주고, 브라우저에서는 기존에 표시했던 로딩화면을 없애고 받은 데이터를 화면에 표시한다. 브라우저가 초기에 프론트엔드 서버로부터 HTML과 static파일들을 요청한 뒤에 로드가 되면, 사용자의 상호작용에 따라 JavaScript를 통해 동적으로 화면이 렌더링이 된다. 장점으로는 첫 로딩에서 HTML 및 static파일들만 받아오면, 이후에는 동적으로 빠르게 화면 렌더링이 가능하기 때문에 사용자 UX가 뛰어나다. 그리고 서버에 요청하는 횟수가 훨씬 적기 때문에 서버에서의 부담이 적다.단점으로는 모든 HTML과 static 파일들이 로드될때까지 기다려야 된다는 점이다. 또한 처음에는 HTML 파일이 비어있기 때문에 크롤러가 데이터를 수집할 수 없어 SEO(검색엔진 최적화)에 문제가 발생할 수 있다. (단, 구글 검색엔진의 경우에는 javascript까지 크롤링한다) SSR과 CSR을 초기 로딩속도, 서버 부담, SEO를 기준으로 비교해보면 아래와 같은 차이가 있다. SSR CSR 초기 로딩속도 CSR에 비해 다운 받는 파일이 많지 않아서 속도가 빠르다. 모든 JavaScript 파일을 다운 받아와야 하기 때문에 초기에 오래 걸린다. 서버 부담 서버와 잦은 요청/응답(view 변경시)을 하기 때문에 서버에 부담이 크다. data의 요청이 있을때만 서버에 요청을 하기 때문에 서버에 부담이 적다. SEO(Search Engine Optimization) HTML에 대한 정보가 처음에 포함되어 있어 데이터를 수집할 수 있다. 맨 처음에는 HTML파일에 데이터가 비어있기 때문에 크롤러가 데이터를 수집할 수 없다. MPA(Multiple Page Application) vs SPA(Single Page Application) MPA SPA 화면마다 HTML파일이 존재하고, 사용자가 화면을 요청할때마다 웹 서버가 필요한 데이터와 HTML을 파싱해서 보여주는 방식으로 동작한다. 단 하나의 페이지로 이루어진 어플리케이션으로, 단 하나의 HTML을 기반으로 JavaScript를 이용해서 동적으로 화면의 컨텐츠를 바꾸는 방식으로 동작한다. 동적이지 않은 페이지를 상황에 맞게 클라이언트에 보여주기 때문에 SSR 방식을 채택 동적으로 DOM을 구성하여 Re-rendering되기 때문에 CSR 방식을 채택 정리하자면, SPA와 MPA의 차이는 용어대로 페이지를 한 개 쓰냐 여러 개 쓰냐의 차이이고, CSR과 SSR은 렌더링을 어느 시점에서 하느냐의 차이이다. 자 여지까지 SSR과 CSR의 차이에 대해서 살펴보았다. SSR은 한 번에 완전한 HTML을 렌더링하기 때문에 초기 로딩속도는 빠르지만, 새로운 페이지로 이동할 경우 중복되는 파일들을 다시 내려받아야 되기 때문에 CSR보다 속도가 느리다.반면 CSR은 초기 로딩속도는 느리지만, 첫 페이지 로딩때 모든 파일들을 내려받기 때문에 페이지를 이동할때마다 해당 페이지에서 필요한 데이터만 불러와서 사용하면 된다. React의 기본 동작방식(CSR)과 NextJS를 통한 SSR구현React의 동작방식을 살펴보면 클래스 컴포넌트 기준으로 살펴보면, 우선 render()함수가 우선 실행이 되고, ComponentDidMount() 함수를 통해 데이터를 가져와서 다시 한 번 render()함수가 실행되는 방식으로 동작을 한다.(CSR)React는 초기 렌더링에서 모든 컴포넌트를 내려받기 때문에 어플리케이션의 규모가 커지게 되면, 로딩속도가 지연된다.따라서 이제부터 공부해 볼 NextJS는 이러한 한계점을 극복하기 위해서 React를 SSR방식으로 쉽게 구현할 수 있도록 도와주는 React 프레임워크이다.NextJS는 초기 로딩때 모든 파일들을 내려받지 않고, 필요에 따라 파일들을 불러올 수 있도록 Code Splitting을 지원한다. 프레임워크이기 때문에 정해진 폴더구조가 있는데 아래와 같다. pages 폴더 : 각 page / route들이 위치 components 폴더 : React component들이 위치즉, 처음 브라우저가 실행되면 모든 파일들을 다 내려받지 않고, 필요한 첫 페이지에 필요한 page 정보만 내려받게 되고, 이후에 다른 페이지로 이동하면 이동한 페이지에서 필요한 page만 내려받는 식으로 동작을 하게 된다. 이외에 다른 구체적인 기능들은 다른 포스팅에서 다뤄보도록 하겠다.","link":"/2021/04/26/202104/210426-SSR_and_CSR/"},{"title":"210427 React with TypeScript TIL - useState에서 빈 배열([])로 초기화시키는 경우, props 타입(함수형, 클래스형 컴포넌트), webpack-dev-server 사용, setTimeout과 useRef의 사용","text":"useState에서 빈 배열([])로 초기화시키는 경우typescript에서 useState hooks를 사용해서 초기 값을 빈 배열로 할 경우, 타입스크립트에서는 빈배열([])을 never type으로 인식하기 때문에 구체적인 타입을 지정해줘야 한다. type 선언을 위해 사용한 interface는 같은 파일에서 정의를 하지 말고, 별도의 파일(types.ts)로 빼서 다른 파일에서 재사용 가능하도록 정의를 하는 것이 좋다.(코드의 가독성, 재사용성)types.ts 1234export interface IDetailInfo { id: string; contents: string;} main.ts 1234import { IDetailInfo } from './types';...const [detail, setDetail] = useState&lt;IDetailInfo[]&gt;([]);... props의 타입지정 함수형 컴포넌트에서의 props 타입아래와 같이 props 매개변수에 직접적으로 typing을 해줘도 되지만, React의 함수형 컴포넌트에서는 이렇게 작성하지 않는다. 123456import { IDetailInfo } from './types';const ChildComponent = ({ detail }: { detail: IDetailInfo }) =&gt; {};.... 원래 React에서는 propTypes를 설치해서 넘겨받은 props에 대해서 타입검사를 했는데, 타입스크립트에서는 이 역할을 대체해주기 때문에 propTypes를 사용할 필요가 없다. React에서는 제네릭 방법으로 간단하게 함수형 컴포넌트의 props에 대한 타입을 적을 수 있다.함수형 컴포넌트는 생략해서 사용을 해왔지만 FunctionComponent 타입으로 제네릭을 포함하고 있다.이 제네릭 요소에 아래와 같이 props의 타입을 정의를 해주면 된다.(함수형 컴포넌트에서는 useState hooks를 사용하기 때문에 state자리는 필요가 없다.) 123456import { IDetailInfo } from './types';const ChildComponent: FunctionComponent&lt;{ detail: IDetailInfo }&gt; = ({ deatil }) =&gt; { ...};... 클래스형 컴포넌트에서의 props 타입클래스형 컴포넌트에서는 이전에 배웠듯이 React.Component의 제네릭 첫번째 인자가 props 타입을 정의하는 자리이다. 만약에 state에 대한 정의가 필요없다면, props에 대한 타입 정의만 넣어주면 된다.(단, state에 대한 타입정의만 필요한 경우에는 props자리에 {} 빈 객체로 표기해줘야 한다.) 1234567import { IDetailInfo } from './types';...class ChildComponent extends React.Component&lt;{ detail: IDetailInfo }&gt; { ...}... webpack-dev-server 사용매번 webpack을 구동시켜주기 귀찮기 때문에 webpack-dev-server를 사용해서 알아서 변화감지를 통해 구동시킬 수 있다. 설치 1$ npm i webpack-dev-server -D package.json 123&quot;script&quot;: { &quot;dev&quot;: &quot;webpack-dev-server --hot&quot;} webpack.config.js 1234567... output: { ... // webpack-dev-server를 사용하기 위해 publicPath를 추가 publicPath: '/dist', }... setTimeout과 useRef의 사용setTimeout을 하는 경우 반드시 clearTimeout을 해줘야 한다. 따라서 화면의 렌더링에는 영향을 주지 않는 useRef hooks를 사용해서 setTimeout에 대한 정보를 변수로 저장한다. 1234567891011let timeout = useRef&lt;number | null&gt;(null);let startTime = useRef&lt;Date&gt;();...// nodejs의 setTimeout이 아닌 window의 setTimeout을 사용해야 한다.timeout.current = window.setTimeout(() =&gt; { setState('now'); startTime.current = new Date();});...clearTimeout(timeout.current);... useRef의 타입을 지정할때 주의해야될 부분이 있는데, 아래와 같이 useRef의 타입은 3가지 경우로 overloading되어서 정의되어 있다. 경우에 따라 알맞은 useRef의 타입정의를 골라서 타입정의를 해야한다. 위에서는 두번째 useRef의 타입을 사용하였다.기존의 useRef(null)의 경우 내부의 useRef의 current가 read-only 속성이기 때문에 값을 넣을 수 없다. 하지만 useRef&lt;number|null&gt;(null) 로 정의를 해주게 되면, React.refObject.current에서 React.MutableRefObject&lt;number | null&gt;.current로, current가 변경가능한 mutable한 속성이 된다. @types/react/index.d.ts 12345678910111213141516// intialValue의 타입과 useRef의 타입을 일치시켰을때 Mutable한 RefObject가 된다.function useRef&lt;T&gt;(initialValue: T): MutableRefObject&lt;T&gt;;//MutableRefObject interfacefunction useRef&lt;T = undefined&gt;(): MutableRefObject&lt;T | undefined&gt;;interface MutableRefObject&lt;T&gt; { current: T;}function useRef&lt;T&gt;(initialValue: T|null): RefObject&lt;T&gt;;// RefObject interfaceinterface RefObject&lt;T&gt; { readonly current: T | null;} Class state에서의 주의점class component에서 state를 constructor의 내부에 작성을 해주거나 constructor없이 작성을 해 줄 수 있다. 만약에 아래와 같이 React.Component에서 State 타입을 지정해주고, state를 constructor의 외부에 작성을 해주면, state에 빈 배열이 있는 경우 타입 에러가 난다.즉, Component에서 제네릭으로 적용한 state의 타입적용이 안된다. 따라서 state에 직접 state의 타입을 지정해줘야 한다. 123456789101112interface IState { state: string; message: string; result: string[];}class Example extends Component&lt;{}, IState&gt; { state: IState = { state: '', message: '', result: [] };}","link":"/2021/04/27/202104/210427-React_with_Typescript_TIL/"},{"title":"210428 React TIL - React router, Dynamic route matching, props의 history, location, match 객체, QueryString과 URLSearchParams, render props, Switch와 exact","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. React RouterReact router의 React에서의 역할과 Website 개발시에 어떤 역할을 하는지에 대한 내용을 중점으로 내용을 정리해보려고 한다. 1$ npm i react-router react-router-dom react-router는 web/app에서 react-router를 사용하기 위한 기본 뼈대라고 이해하면 된다. 그리고 react-router-dom가 우리가 웹 개발시에 실제로 사용하는 것으로 react-router는 react-router-dom의 내부적으로 사용되는 것이다. HashRouter, StaticRouter, BrowserRouterrouter의 종류에는 HashRouter, StaticRouter, BrowserRouter가 있는데, BrowserRouter가 가장 많이 사용되고, StaticRouter는 주로 서버에서 사용이 되기 때문에 주로 사용되는 것은 BrowserRouter와 HashRouter이다.이 Router들을 사용하기 위해서는 최상위 컴포넌트를 Router 태그로 감싸줘야 한다. 우리가 페이지로 만들 컴포넌트들은 Router 태그의 내부에 Route 태그로 작성을 해주게 되는데, 그 속성으로 path와 component가 있다. Route 컴포넌트는 가상의 페이지 주소를 만들어서 component를 연결시켜준 것이다.Link 컴포넌트에서는 react의 router만 가상으로 만들어서 알고 있는 페이지로 링크를 걸어서 이동시켜주는 것이다. 123456789101112131415161718import { Link } from 'react-router-dom';return ( &lt;BrowserRouter&gt; {/* 공통부분 */} &lt;div&gt; &lt;Link to=&quot;/&quot;&gt; &lt;Link to=&quot;/about&quot;&gt; &lt;Link to=&quot;/project&quot;&gt; &lt;/div&gt; {/* 바뀌는 부분 */} &lt;div&gt; &lt;Route path=&quot;/&quot; component={Home} /&gt; &lt;Route path=&quot;/about&quot; component={About} /&gt; &lt;Route path=&quot;/project&quot; component={Project} /&gt; &lt;/div&gt; &lt;/BrowserRouter&gt;) 새로고침시에 페이지 표시할 수 없는 경우페이지를 새로고침하게 되면, 페이지를 표시할 수 없다고 나오는데, 이는 Route 컴포넌트로 가상의 페이지 주소를 만들어서 각 컴포넌트를 연결시켜준 것이기 때문에 서버에서는 해당 주소를 찾을 수 없는 것이다. 따라서 이런 경우에는 webpack.config.js파일에서 아래와 같이 설정을 추가해줘야 한다. 123456.....devServer: { ..... historyApiFallback: true,}..... HashRouter와 BrowserRouter의 차이HashRouter를 사용하게 되면 URL에 /#/의 Hash가 표시되는 것을 볼 수 있다. 기존에 BrowserRouter를 사용하게 되면 페이지를 새로고침할 경우 별도의 설정을 해주지 않으면 페이지 새로고침이 되지 않았는데, HashRouter의 경우에는 별도의 설정을 해주지 않아도 페이지 새로고침이 문제없이 된다. 이러한 이유때문에 HashRouter를 사용하면 배포가 쉽다는 이야기가 나온다./#/를 포함한 이하의 주소는 react-router와 브라우저만 알고 있는 부분으로, 서버에서는 인식을 하지 못한다. 이러한 문제는 SEO(검색엔진 최적화)의 문제로 직결되는데, 검색엔진을 최적화하기 위해서 기본적으로 각 페이지에 대한 정보를 서버에 물어보기 때문이다. 위와같은 이유로 실무에서는 HashRouter를 사용하지 않고 BrowserRouter를 사용한다.하지만 SEO와 같은 부분을 고려할 필요가 없는 관리자 페이지의 경우에는 HashRouter를 사용해서 구현을 해도 문제가 없다.(물론 BrowserRouter를 사용한다고 하더라도 SEO를 위해 별도의 설정이 필요하다.) Dynamic Route Matching&lt;Route /&gt;컴포넌트의 갯수가 늘어나게 되면, 코드의 양이 방대해지기 때문에 이 Route 컴포넌트를 동적 라우팅이 가능하도록 바꿔줘야 한다. 일단 아래와 같이 Route들이 설정되어있다고 가정한다. 만약에 프로젝트가 늘어나서 10개가 더 늘어난다고 하면, Route 컴포넌트를 10개 더 추가해야 할 것이다.하지만 이러한 문제는 동적 라우팅으로 해결이 가능하다. 123456789101112131415161718import { Link } from 'react-router-dom';return ( &lt;BrowserRouter&gt; {/* 공통부분 */} &lt;div&gt; &lt;Link to=&quot;/project/a&quot;&gt; &lt;Link to=&quot;/project/b&quot;&gt; &lt;Link to=&quot;/project/c&quot;&gt; &lt;/div&gt; {/* 바뀌는 부분 */} &lt;div&gt; &lt;Route path=&quot;/project/a&quot; component={AProject} /&gt; &lt;Route path=&quot;/project/b&quot; component={BProject} /&gt; &lt;Route path=&quot;/project/c&quot; component={CProject} /&gt; &lt;/div&gt; &lt;/BrowserRouter&gt;) 우선 path는 아래와 같이 /project/:name으로 작성을 해주고 component의 속성에 작성해준 컴포넌트의 경우에도 별도로 새로 생성을 해준다. :name은 /project/a의 경우에는 a로 동적으로 바뀌는 부분이다.동적으로 라우팅되는 원리는 Route 컴포넌트가 component 속성으로 넣어준 component에 props로 history, location, match속성을 넣어주는데, 이 속성들 중에서 match 속성을 이용해서 Project라는 컴포넌트내에서 분기처리를 통해 동적으로 라우팅을 해주게 되는 것이다. 1&lt;Route path=&quot;/project/:name&quot; component={Project} /&gt; 만약에 Route와 연결이 되지 않은 컴포넌트에서 props의 history, location, match를 사용하고 싶은 경우, 해당 컴포넌트를 내보내주는 부분에서 아래와 같이 withRouter로 감싸서 내보내주면 된다. 12345import { withRouter } from 'react-router-dom';.....export default withRouter(Project); props의 history, location, match 객체 history : 앞으로가기 뒤로가기 등 페이지를 넘나든 내역을 가지고 있는 객체 (goBack, goForward, push 등 프로그래밍적으로 페이지 전이가 가능하도록 메서드를 제공) location : pathname(url주소), search, hash의 속성을 가지고 있는 객체 match : 동적 라우팅의 분기처리에 사용할 params의 name에 대한 속성을 가지고 있다. 1234567891011render(){ const { match: {params: { name }}} = this.props; if(name === 'a') { return &lt;AProject /&gt; } else if(name === 'b') { return &lt;BProject /&gt; } else if(name === 'c') { return &lt;CProject /&gt; }} Browser에서 제공하는 history API기본적으로 브라우저에서 history API를 제공한다. history.pushState(&quot;&quot;,&quot;&quot;,&quot;/about&quot;);과 같이 작성을 해주게 되면, URL을 업데이트해준다.React에서도 내부적으로 browser의 history API를 사용한다. QueryString과 URLSearchParams아래와 같이 주소에 데이터를 전달할 수 있다. 전달받은 데이터는 props의 location 객체의 search부분을 통해 참조할 수 있다. search는 아래의 주소를 예로들면, ?query=10&amp;hello=lee이다.만약에 query key에 대한 값 또는 hello key에 대한 값을 참조하려면 어떻게 해야할까?URLSearchParams객체로 search를 감싸서 객체를 참조하면 된다. 12345&lt;Link to=&quot;/project/a?query=10&amp;name=lee&quot; /&gt;;// 맨 앞의 ? 를 제외한 문자열을 URLSearchParams 객체로 감싸준다.let urlSearchParams = new URLSearchParams(this.props.location.search.slice(1));console.log(urlSearchParams.get('query')); // 10console.log(urlSearchParams.get('name')); // lee render props만약에 Route 컴포넌트에서 부모의 props를 Route에서 연결된 컴포넌트에 전달하고자 한다면 어떻게 해야할까? 1&lt;Route path=&quot;/project&quot; render={(props) =&gt; &lt;Project {...props} /&gt;} /&gt; Switch와 exactSwitch는 감싸진 Route 컴포넌트들 중에서 일치하는 것들 중에 첫 번째만 화면에 표시해준다.exact는 Route 컴포넌트의 속성으로 넣어주는데, exact가 들어간 Route 컴포넌트의 경우에는 완전히 path가 일치해야만 해당 컴포넌트를 화면에 표시한다.아래의 간단한 예시를 살펴보자. 아래의 예시에서 첫번째 Route에 exact가 없다면, /about의 path를 입력하면 첫번째 Route가 화면에 표시된다. 그 이유는 두 Route 모두 /(root)라는 공통분모를 가지고 있고, Switch는 일치하는 첫번째 Route의 component만 화면에 표시되기 때문이다.그렇기 때문에 첫번째 Route(상위 주소)에 exact 속성을 넣어서 완벽하게 path가 일치하는 경우만 해당 component가 화면에 표시될 수 있도록 하면 해결할 수 있다. 1234&lt;Switch&gt; &lt;Route exact path=&quot;/&quot; component={Home} /&gt; &lt;Route path=&quot;/about&quot; component={About} /&gt;&lt;/Switch&gt;","link":"/2021/04/28/202104/210428-React-review_study/"},{"title":"210429 Be you, only better - 과거의 나와 현재의 나에 대한 자기반성","text":"지난 과거의 나와 현재의 나 그리고 두려움을 극복하기 위한 자기반성이번 포스팅에서는 나의 내적으로 좀 의미있는 반성의 시간을 갖으며, 글을 작성해보려고 한다.반성이 없는 계획과 발전은 의미가 없기 때문에 시간이 많이 흘렀지만, 지난 나의 과거를 다시 되새기고 현재 내가 걱정하는 것과 두려워하는 것을 구체화시켜서 현재의 나를 좀 더 발전시켜 나가고자 한다. 우선 현재의 나는 프론트엔드 개발자로서 취업을 준비하고 있고, 내가 지원하고자 하는 포지션에서 필요로하는 기본적이고 중요한 기술적인 부분을 스스로 공부하며 자기개발을 해나가고 있는 중이다. 현재의 나에게 있어, 가장 큰 두려움은 제대로 된 개발자가 될 수 있을까?이다. 나에게 있어 제대로 된 개발자란, 자기개발을 꾸준히 하고, 견고한 어플리케이션 개발에 대해 끊임없이 고민을 하며, 내가 하는 일과 어플레케이션에 대해 자부심을 가지는 개발자가 되는 것이다. 나는 2018년 10월부터 2020년 4월 17일까지 짧다면 짧은 1년 반이라는 시간동안 일본에서 개발자로 일을 했었다. 그때 당시 내가 개발자로서 일을 하면서 느꼈고, 현재 내가 가지고 있는 두려움에 영향을 준 경험에 대해서 적어보며 직접 이 두려움에 직면해서 극복해보려고 한다. 현재 나에게 가장 큰 영향을 준 계기는 6개월간 담당했었던 헬스케어 관련 안드로이드 어플리케이션 개발경험과 3개월 간 일본 대기업에 파견을 가서 담당했던 개발업무 그리고 퇴사하기 전 마지막으로 담당했던 뉴스 웹 어플리케이션 유지보수 업무였던 것 같다.이 세 가지 업무에 있어서 공통점은 내가 작성한 코드에 대한 코드리뷰나 피드백이 없었거나 있었다고 하더라도 정말 시간에 쫒기듯이 지나갔던 그런 간단한 피드백이 있었던 업무들이었다.두 번째 공통점은 첫번째 공통점과 같은 맥락의 내용이지만, 업무를 함에 있어 제대로 된 조언을 받을 수 있는 시니어 엔지니어가 없었다는 점도 있다. 간단하게 공통점들만 추려서 이야기를 했지만, 정말 프로젝트 하나 하나 해나가며 느꼈던 점은 “이렇게 개발하면 안되는데” 라는 자기 고민과 내 커리어에 대한 걱정이었다. 물론 걱정만 한 것은 아니다. 회사가 끝나고 스스로 인터넷의 여러 컨텐츠들을 보며 스스로 자기개발을 한적도 있다. 하지만 실제 회사에서의 업무와 자기개발 사이에서의 괴리감이 컸기 때문에 꾸준히 자기개발을 이어가지는 못했다.하지만 이러한 경험을 통해서 내가 배웠던 것은 제대로 된 개발을 위해서는 코드리뷰와 개발문화가 잘 정착된 그런 회사에서 일을 하며, 끊임없이 자투리 시간을 활용하여 개발관련하여 자기개발을 해나가야 된다는 점과 테스트 코드 작성과 코드 리팩토링을 통해 더 견고하고 확장성 있는 어플리케이션 개발에 대해 끊임없이 고민을 하고 자기발전을 해야 좋은 개발자로 성장로 성장 할 수 있다는 것이다. 이전에 일본에서 개발 업무를 하면서 코드리뷰나 테스트 코드 작성에 대해서 업무에 도입을 하고 싶었지만, 일주일에 몇 번이고 릴리즈를 해야되는 업무환경과 최대한 단기에 많은 프로젝트를 끝내야 하는 근무환경, 돌아가면 다 된다는 식의 마인드를 가진 영업같은 개발부서 담당자와의 마찰로 인해서 그런 부분은 많이 어려웠다.그래서 지금 나의 가장 큰 목표는 사내에 제대로 된 개발문화가 잘 정착된 회사에 입사하는 것이고, 내가 하고자하는 프론트엔드포지션에 적절한 퍼포먼스를 내며 업무를 할 수 있는 회사에 들어가는 것이다. 그러기 위해서 개발관련 회사들의 기술 블로그에 대해서 분석을 하며, 내가 이상적으로 생각하는 기업에 입사하기 위해서는 어떤 것들이 준비되어야 하는지 살펴보았다. 공통점은 바로 다른 개발자들과의 협업이 가능한 개발자와 테스트 주도 개발(TDD), 사용하는 기술에 대한 기본기가 있는 개발자라는 공통 키워드가 있었다.회사에서는 혼자 일을 하는 것이 아니기 때문에 다른 개발자들과 커뮤니케이션이 매우 중요하다. 개발자들과 원할한 커뮤니케이션을 하기 위해서는 개발 업무에서 자주 사용되는 용어에 대해서 잘 알고 유연하게 남들과 소통할 수 있는 커뮤니케이션 능력이 필요하다.또한 테스트 주도 개발(TDD)은 작성한 코드들을 모듈화시키고 유지보수나 코드 리팩토링시에 테스트 자동화를 통한 빠르고 견고한 어플리케이션 개발을 하는데 필요하다.그리고 내가 작성한 코드에 대해서는 한 줄 한 줄 모두 이유를 설명할 줄 알아야 한다. 사실은 이런 내용들은 익히 알고 있었지만, 퇴사후 취업준비하는 5개월동안은 취업준비라는 명목하에 개발을 하다보니 급한 마음에 위에서 언급한 나의 마음가짐을 간과하고 무작정 제출하기 위한 프로젝트들을 만들어내기 바빴다. 속으로는 이러면 안되는데라는 생각은 있었지만, 어쩔 수 없다는 자기합리화로 퇴사후 초반 5개월 동안은 그렇게 보냈었다.물론 되돌아보면 아예 의미없었던 시간은 아니었다. 현재는 제대로 다시 마음을 고쳐먹고 4개월의 시간동안 지난번에 그냥 썼던 개발관련 개념들을 기본적인 개념들과 함께 하나 하나 이유를 공부하고 진행을 하면서, &quot;아, 이게 이래서 이렇게 쓰는구나!&quot;라는 깨달음의 순간들이 정말 많이 찾아왔고, 이러한 중간 중간의 희열감을 통해 더 앞으로 나아가는 나 자신을 발견할 수 있었다. 그런데 현재 나에게 또 다른 걱정아닌 걱정이 생겼다. 바로 사이드 프로젝트를 함에 있어 약간의 걱정과 두려움이 생긴 것이다.TDD에 대한 개념과 프로젝트에 도입하는 방법에 대해서 개인적으로 깃허브의 레포지토리를 파서 연습도 해보고, 내가 사용하고자 하는 기술스택에 대해서 좀 더 깊이있게 공부를 하며 블로그에 포스팅도 하였다. 이러한 작업을 통해 내가 이전에 알고 있다고 착각했던 개념들에 대해서도 구체화를 시킬 수 있었는데, 막상 프로젝트를 하려고 하면 나의 내면에서는 &quot;잘 할 수 있을까?&quot; 라는 의문이 생겼다. 여지까지 개발관련 리소스가 부족했었기 때문에 부족한 개발관련 리소스를 쌓기 위해 노력을 해왔고, 지금은 여지까지 쌓아온 개발 리소스들을 통해 실체를 만들어서 나 자신의 개발 실력에 대해서 증명을 해야한다.'두려움은 거짓 증거가 실제처럼 보이는 것이라고 한다.'내가 지금 이 블로그 포스팅을 하는 이유는 내가 현재, 아니 과거에서부터 걱정을 해왔던 부분들에 대해 좀 더 구체화를 시키고, 직접 대면하기 위해서이다. 오늘부터 본격적으로 내가 여지까지 공부하고 진행했던 내용들에 대한 결과물을 만들어내려고 한다. 프로젝트들은 단순히 제출용이 아닌 내가 여지까지 어떤 공부를 해왔고, 내가 개발에 있어 어떤 부분을 중요시 여기는 개발자인지 보여주기 위한 중요한 부분이라고 생각한다. 프로젝트를 진행할때에는 모든 작업에 대해 Issue를 작성하고, 작업한 내용들은 git flow model로 브랜치도 관리하며 commit 메세지도 꼼꼼하게 적도록 하자. 그리고 작업한 내용들 중에서 내가 중요하다고 생각되는 내용이나 내가 프로젝트를 하면서 깨달은 내용들에 대해서는 반드시 README와 블로그에 꼼꼼하게 글을 남기도록 하자.추가적으로 모든 프로젝트들은 간단하게 와이어 프레임을 짜고, TDD 방식으로 테스트 코드를 작성해나가며 진행을 해나가도록 한다. 두려움은 거짓 증거가 실제처럼 보이는 것이라고 했다. 뭐가 두려워서 이렇게 머뭇거리고 있는 것인지 모르겠다. 더 이상 두려워하지 말고, 나 자신에 대해 할 수 있다는 믿음을 가지고 더 성장하기 위해서 한 발 내딛어보자.","link":"/2021/04/29/202104/210429-Feedback_about_my_life/"},{"title":"210429 React TIL - 프론트엔드 개발시 dummy 데이터 활용, Container - Presenter 구조, Re-rendering과 태그내에서의 인라인 스타일링, 함수형 컴포넌트에서의 리렌더링, eslint 설정","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. 프론트엔드 개발시 dummy 데이터 활용만약 백엔드가 개발이 안된 상태에서 프론트엔드 개발을 진행해야 되는 경우라면 더미 데이터를 활용해서 우선적으로 프론트엔드 개발을 하도록 한다.예를들어 로그인 화면을 만드는데, 로그인과 로그아웃등 상황에 따른 다른 프론트 페이지를 만들어야 한다면, 이 경우에 간단하게 더미로 상태 데이터를 만들어서 화면을 만들도록 한다.실무에서는 Redux나 Mobx를 사용해서 dummy state를 만든다. Container - Presenter 구조이전에 클래스 컴포넌트로 작성을 했을때는 데이터를 받아서 로직을 처리하는 부분(Container)과 화면에 출력해주는 부분(Presenter)을 나눠서 구조를 짰는데, hooks가 도입된 이후에는 위와같이 구조를 나누는 것을 추천하지 않는다고 한다. 반복되는 작업은 처음만 수작업으로, 그 다음부터는 라이브러리 사용수작업으로 만드는 하나 하나 만들어보는 것도 의미가 있다. 하지만 매번 하나 하나 만들어보는 것보다는 라이브러리를 사용해보는 연습을 하는 것도 중요하다. 실제 실무에서 모든 것을 수작업으로 만들지는 않기 때문이다. 자식 component에 props로 넘겨주는 함수의 경우에는 반드시 useCallback으로 감싸서 처리한다.Re-rendering과 태그내에서의 인라인 스타일링React에서는 VirtualDOM을 검사해서 이전 VirtualDOM 상태와 달라지는 부분이 있다면 해당 부분만 다시 렌더링한다. 렌더링을 최적화 시키기 위해서는 태그 내에서 인라인 스타일링하는 것을 지양해야 한다. 왜냐하면 태그내에서 스타일링을 해주는 경우 스타일을 객체 형태로 정의하게 되는데, 객체는 주소참조로, 값이 같더라도 서로 비교를 하면 다른 값으로 인식된다.따라서 styled-components를 사용해서 스타일을 적용한 개별 컴포넌트로 생성을 하거나 useMemo를 사용해서 style을 별도로 정의해서 인라인 스타일로 정의해야 한다. useMemo는 값 자체를 캐싱하기 때문에 deps에서 설정한 값이 바뀌지 않는 한 재사용을 한다. 12345import styled from 'styled-components';const wrapper = styled.div` margin-top: 10px;`; 12const style = useMemo(() =&gt; ({ marginTop: 10 }), []);&lt;div style={ style }&gt; 함수형 컴포넌트에서의 리렌더링함수형 컴포넌트는 렌더링 될때 함수 안의 내용들이 다시 실행되는 것은 맞지만, useCallback, useMemo로 처리된 함수나 값은 deps의 값이 바뀌지 않는 이상, 다시 실행되지 않고, return 부분에 정의된 VirtualDOM 중에서도 이전과 비교했을때 바뀐 부분만 다시 렌더링한다. 실무에서의 components 폴더 내부 구조실무에서는 components 폴더내에 바로 컴포넌트 파일들을 넣지 않고, layout, forms 폴더로 나눠서 구성한다. component의 props type 검사1$ npm i prop-types 123456789101112import PropTypes from 'prop-types';const AppLayout = ({ children }) =&gt; { ...}AppLayout.propTypes = { // node: 화면에 그려지는 모든 것 (return 내) children: PropTypes.node.isRequired};export default AppLayout; eslint 설정여러사람들이 한 팀을 이뤄서 코드를 작성할때에는 코드의 공통 규칙을 정해두는 것이 좋다.그래야 마치 한 사람이 코드를 작성한 것처럼 코드가 깔끔해진다.아래의 .eslintrc를 추가해준 다음에 IDE에서 별도의 설정을 해줘야 한다.extends 부분이 구체적인 규칙을 설정하는 부분인데 위의 설정은 느슨하게 규제를 하였다. 필요에 따라 엄격하게 바꿔서 사용해보도록 하자. eslint 관련 package 설치1234$ npm i eslint -D$ npm i eslint-plugin-import -D$ npm i eslint-plugin-react -D$ npm i eslint-plugin-react-hooks -D eslint 설정파일.eslintrc 12345678910111213141516{ &quot;parserOptions&quot;: { &quot;ecmaVersion&quot;: 2020, &quot;sourceType&quot;: &quot;module&quot;, &quot;ecmaFeatures&quot;: { &quot;jsx&quot;: true } }, &quot;env&quot;: { &quot;browser&quot;: true, &quot;node&quot;: true, &quot;es6&quot;: true }, &quot;extends&quot;: [&quot;eslint:recommended&quot;, &quot;plugin:react/recommended&quot;], &quot;plugins&quot;: [&quot;import&quot;, &quot;react-hooks&quot;]}","link":"/2021/04/29/202104/210429-React-review_study/"},{"title":"210427 Ant Design - antd와 styled-components, emotion, antd Grid system, antd Form and Button, antd Card component","text":"antd와 styled-componentsantd 디자인을 사용해서 디자이너가 없어도 그럴듯하게 디자인을 할 수 있다. 다만 개성이 없어지는 특징이 있지만, 보통 실무에서 고객사 프로젝트를 할때에는 디자이너가 직접 커스텀 디자인을 하게 된다. antd 디자인을 사용하면 버튼이나 아이콘 등 질좋은 디자인 컴포넌트들을 사용할 수 있다. https://ant.design/ styled-components와 emotionreact에서 css를 적용할때 전처리기(sass, less)를 사용하기도 하고, css를 적용해서 component를 생성할 수 있는 styled-components를 사용하기도 한다.emotion은 styled-components와 매우 비슷하기 때문에 styled-components를 잘 익히고 나면 공식문서를 보고 바로 적용해서 쓸 수 있다. emotion은 SSR에 최적화해서 사용할 수 있다는 장점이 있기 때문에 시간날때 한 번 써보는 것을 권장한다. npm trends라는 사이트에서 현재 사용중인 패키지가 유사한 다른 패키지와 사용률에서 어떤 차이가 있는지 비교할 수 있다. https://www.npmtrends.com/ 1$ npm i antd styled-components @ant-design/icons antd와 @ant-design/icons를 별도로 설치하는 이유는 어플리케이션의 용량 대부분이 이미지파일의 용량이 차지하기 때문에 이러한 아이콘 이미지들을 최적화시키기 위해서 별도의 다른 라이브러리로 분리한다. 그래서 antd와 icons를 위한 @ant-design/icons 라이브러리가 분리된 것이다. antd와 react 연결공식 사이트에서 Ant Design of React 공식문서를 보면 antd와 react를 연결하는 방법을 찾을 수 있다.https://ant.design/docs/react/introduce NextJS에는 기본적으로 내부에 webpack이 있다. 원래 import는 자바스크립트 파일만 가능하고, CSS파일은 할 수 없다. 하지만 webpack이 CSS파일이 import되어있으면 알아서 style tag로 바꿔서 HTML에 붙여준다.webpack은 이미지도 별도의 처리를 해서 넣어주기도 하고 loader를 사용해서 여러가지 파일을 확장자와 상관없이 합쳐주는 역할을 한다. antd Grid system - Row, Colhttps://ant.design/components/grid/antd에서 제공하는 Grid system을 활용해서 간편하게 반응형 페이지를 만들 수 있다. 화면의 레이아웃은 우선 가로로 나누고 나눠진 가로에서 세로로 나눠서 화면 레이아웃을 마크업한다.xs(모바일), sm(테블릿), md(데스크탑), lg, xl(대형화면) 등의 속성을 Col 태그에 값과 함께 넣어서 손쉽게 반응형 페이지를 만들 수 있다.예시 1234567891011121314// 모바일 화면일 경우에는 각 칼럼이 100%(24)의 너비로 스택과 같은 형태로 화면에 배치된다.// 데스크탑 크기의 화면일 경우 25% 50% 25%의 너비로 칼럼이 세로로 배치된다.// gutter는 화면의 칼럼들이 너무 붙어있는 경우 사이 간격을 약간씩 떨어뜨릴 수 있도록 하는 속성이다.&lt;Row gutter={8}&gt; &lt;Col xs={24} md={6}&gt; first column &lt;/Col&gt; &lt;Col xs={24} md={12}&gt; second column &lt;/Col&gt; &lt;Col xs={24} md={6}&gt; third column &lt;/Col&gt;&lt;/Row&gt; antd Form and Buttonantd를 사용해서 간편하게 Form 태그와 Button태그를 만들 수 있다. Form 태그의 내용을 submit하기 위해서는 Button 태그의 속성에 htmlType=”submit”을 넣어줘야 한다. Form 태그에는 onFinish 속성을 넣어 submit이 된 후의 처리에 대해서 별도로 작성해줄 수 있다. onFinish 속성에 넣을 함수의 경우 기본적으로 e.preventDefault()가 적용되어 있기 때문에 적어주지 않아도 된다. antd Card component깔끔하게 하나의 section으로 분리해서 디자인이 가능하기 때문에 유용한 컴포넌트이다.","link":"/2021/04/27/202105/210427-Ant_design/"},{"title":"210430 React Redux","text":"2021/05/13 Update - createSelector shopping cart 연습예제 React는 프레임워크가 아닌 라이브러리?상태관리 라이브러리에 대해서 포스팅을 하는 글에서 갑자기 뜬금없이 React가 프레임워크인지 아닌지에 대한 이야기를 하는 이유는 React에서는 상태관리와 라우팅을 기본적으로 제공하지 않기 때문에 라이브러리로 분류한다는 이야기를 하기 위해서이다. Vue와 Angular에서는 자체적으로 상태관리를 지원하기 때문에 프레임워크라고 하지만 React는 다르다.하지만 생태계 자체로써 봤을때에는 React도 프레임워크이다. React의 상태관리 라이브러리의 선택지 Redux와 MobXReact는 Vue와 Angular와 다르게 자체적으로 상태관리를 지원하지 않기 때문에 Redux나 MobX를 선택적으로 사용해서 상태를 관리한다. ContextAPI + useReducer가 Redux와 MobX를 대체할 수 있다?JavaScript를 ContextAPI+useReducer라고 하고, jQuery를 Redux나 MobX라고 비유한다면, ContextAPI+useReducer의 조합으로 Redux나 MobX를 구현할 수 있다는 결론이 나온다. 하지만 손쉽게 사용할 수 있도록 이미 만들어져 있는 Redux나 MobX를 사용하는 것이 좋다. 양방향, 단방향 modeling화면과 데이터가 있다고 가정을 하자. 만약에 화면을 바꾸면 데이터도 바뀌고, 데이터를 바꿔도 화면이 바뀌는 양방향 Pattern이라면, 구조가 복잡해졌을때 버그가 생기기 쉽다. (Angular)이러한 문제를 해결하기 위해 Facebook에서는 단방향 Pattern을 개발했는데 이것이 바로 flux이다.https://facebook.github.io/flux/이 flux pattern을 통해 alt나 reflux, redux가 개발이 되었고, redux를 개발한 Dan Abramov가 Facebook에 합류를 하고, hooks를 만드는데 큰 기여를 했다고 한다. Redux를 쓰면 각 컴포넌트의 state를 안써도 된다?Redux를 쓰면 각 컴포넌트의 state를 안 쓸 수는 있지만, 안써야 된다는 의미는 아니다. 만약에 컴포넌트에서 독립적으로 쓰이는 state라면(단일 종속), 개별적으로 state를 써서 관리를 하는 편이 좋고, 컴포넌트간에 서로 연관성이 있는 state라면, Redux를 써서 관리를 해주는 것이 좋다. Redux의 장점/단점 상태를 단방향으로만 바꿀 수 있다. (action - dispatch - reducer - state update) action이 dispatch되면 기록이 남기 때문에 history 기능을 통해 에러가 발생했을때 디버깅하기 쉽고, 기록을 통해 action 이전의 상태로 되돌릴 수도 있다.(상태를 업데이트할때 불변성을 지켜서 작성해야 한다) reducer에서 action을 받아서 해당 action에 맞게 state의 새로운 객체를 만들어서 기존의 상태를 업데이트한다. Redux의 사용 설치 1$ npm i redux 공식문서https://redux.js.org/introduction/getting-started 사용순서reducer/initial state - store 초기화 - action 작성 - dispatch w/action 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import { createStore } from 'redux';const reducer = (prevState, action) =&gt; { switch (action.type) { case 'LOG_IN': // state의 구조가 복잡해지면 spread 문법을 사용해서 얕은 복사를 하는 것이 지저분해질 수 있기 때문에 immutable이나 immer를 사용해서 코드를 깔끔하게 만들 수 있다. return { ...prevState, user: action.data }; case 'LOG_OUT': return { ...prevState, user: null }; case 'ADD_POST': return { ...prevState, posts: [...prevState.posts, action.data] }; // 만약에 action.type에 대한 이름이 오타나는 경우를 대비해서 default는 작성해야 한다. // reducer에서는 기존의 state를 업데이트하기 위해서 새로운 state 객체를 생성하기 때문에 문제가 될 여지가 있다. default: return prevState; }};const initialState = { user: null, posts: []};const store = createStore(reducer, initialState);store.subscribe(() =&gt; { // react-redux 안에 들어있다. // 화면을 바꿔주는 코드는 여기서 알아서 처리해준다. // 디버깅하는 경우 사용});// action을 만들때에는 아래와 같이 업데이트하고자 하는 값을// 추상적으로 매개변수를 받아서 작성해주는 것이 좋다.const logIn = (data) =&gt; { return { type: 'LOG_IN', data };};const logOut = (data) =&gt; { return { type: 'LOG_OUT', data };};const addPost = (data) =&gt; { return { type: 'ADD_POST', data };};store.dispatch( logIn({ id: 1, name: 'lee', admin: true }));store.dispatch(logout());store.dispatch( addPost({ userId: 1, id: 1, content: 'sample content' }));console.log(store.getState()); // state 확인 Redux를 caching 목적으로 사용게시판의 리스트 정보의 경우에는 redux에 저장해서 게시물 상세페이지에 이동했다가 다시 게시판 리스트로 돌아왔을때 서버로 데이터를 요청하지 않고, redux에 있는 리스트 정보를 가져오게 할 수 있다. 게시물 상세페이지에 대한 정보도 redux에 담아서 caching해서 사용할 수 있다.redux를 global한 상태관리 개념으로만 생각하지 말고 caching의 관점에서도 생각해서 사용해보도록 하자. Redux의 폴더구조redux에서 action과 reducer의 코드가 길어질 수 있기 때문에 별도의 폴더와 파일로써 분리를 해주는 것이 좋다.(action과 reducer는 매개변수와 함수 내부의 변수만 참조하는 순수함수이기 때문에 분리하기 용이하다)action 객체를 나누는 기준은 initialState의 항목을 기준으로 한다. (공통화 시킬 수 있는 부분은 최대한 공통화 처리) userAction.js 1234567891011121314151617const login = (data) =&gt; { return { type: 'LOG_IN', data };};const logOut = () =&gt; { return { type: 'LOG_OUT' };};export default { login, logOut}; postAction.js 12345678910const addPost = (data) =&gt; { return { type: 'ADD_POST', data };};export default { addPost}; reducer는 redux에서 제공해주는 combineReducers를 사용해서 객체로 묶어준다. userReducer.js 12345678910111213141516171819202122// Reducer를 분리했기 때문에 초기값에 대한 범위도 줄어들었다.const INITIAL_STATE = { isLoggingIn: false, data: null};export const userReducer = (prevState = INITIAL_STATE, action) =&gt; { switch (action.type) { case 'LOG_IN': // state의 구조가 복잡해지면 spread 문법을 사용해서 얕은 복사를 하는 것이 지저분해질 수 있기 때문에 immutable이나 immer를 사용해서 코드를 깔끔하게 만들 수 있다. return { ...prevState, data: action.data }; case 'LOG_OUT': return { ...prevState, data: null }; default: return prevState; }}; postReducer.js 1234567891011// Reducer를 분리했기 때문에 초기값에 대한 범위도 줄어들었다.const INITIAL_STATE = [];export const postReducer = (prevState = INITIAL_STATE, action) =&gt; { switch (action.type) { case 'ADD_POST': return [...prevState.posts, action.data]; default: return prevState; }}; root-reducer.js 12345678import { userReducer } from './userReducer.js';import { postReducer } from './postReducer.js';// initialState의 객체 속성이름 사용export default combineReducers { user: userReducer, posts: postReducer,} action의 타입의 경우에는 reducer와 action 객체에서 공통으로 참조되는 속성이기 때문에 별도의 파일로 빼서 사용하는 것이 좋다. user.types.js 1234export const UserActionTypes = { LOG_IN: 'LOG_IN', LOG_OUT: 'LOG_OUT'}; post.types.js 123export const PostActionTypes = { ADD_POST: 'ADD_POST'}; Redux - middlewareaction은 객체로, 기본적으로 동기방식으로 처리되고 dispatch 함수는 action을 받아서 처리하기 때문에 중간단계에서 비동기 처리할 틈이 없다.하지만 dispatch와 reducer 사이에서 동작하는 middleware를 사용하면 사이에서 비동기 처리를 해 줄 수 있다. 이때 사용되는 비동기 처리 middleware에는 redux-thunk와 redux-saga가 있다.(middleware는 반드시 비동기 처리를 하기 위해 사용되는 것이 아닌, dispatch와 reducer 사이에서 특정 처리를 하기 위해서 사용된다는 것을 알아두자) 1234567const enhancer = applyMiddleware();// 아래와 같이 compose를 생략가능// compose는 합성해주는 역할로, redux devtool 플러그인을 추가적으로 붙일때 compose함수를 합성할 수 있다.const enhancer = compose(applyMiddleware(), devtool);// 세 번째 인수가 enhancer인데, 기능 증강의 의미로 이해한다.const store = createStore(reducer, initialState, enhancer); applyMiddleware 함수의 인수로는 아래와 같이 3단 함수가 들어간다. 123456789101112131415// 3단 함수가 들어간다.// 특정 시점을 구분해서 처리할 필요가 없는 경우에 아래와 같이 작성해준다.const firstMiddleware = (store) =&gt; (dispatch) =&gt; (action) =&gt; { console.log(action); // 부가적인 동작 추가 (1) // 기능추가를 전 단계에서 추가할 수 있다. dispatch(action); // 기본 기능 (dispatch(action)을 reducer에 전달) // 기능추가를 후 단계에서 추가할 수 있다.};// 중첩을 해둔 이유는 store - next 사이, next - action 사이, action에서만 실행되도록 처리하고 싶을때 구분할 수 있다.function firstMiddleware(store) { return function (next) { return function (action) {}; };} redux-thunk위의 firstMiddleware 함수에서 기본 기능의 전/후로 코드를 추가해 줄 수 있다고 했는데, 바로 이 부분에서 비동기 처리를 할 수 있다.원래 redux에서는 비동기 action이라는 것은 없었는데, middleware를 사용해서 redux기능을 확장하여 구현할 수 있다.action이 객체인 경우에는 동기방식(sync action creator)으로 처리가 되는데, 함수인 경우에는 비동기 처리(async action creator)가 되도록 조건문으로 처리해준다.일반 객체(동기 방식)의 경우에는 일반적인 처리로 action 객체(sync action creator)가 reducer 함수로 바로 전달이 되고, 비동기 방식의 경우에는 store.dispatch와 store.getState가 async action creator 함수의 인자로 전달이 되어 실행이 된다.(비동기 처리 함수인 login은 우선 서버로 로그인을 요청하고, try 블럭에 있는 비동기 처리 부분을 실행한 다음에 실행에 성공하면 비동기 함수 내부의 함수를 한 번 더 실행한다.) userAction.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 로그인 요청 sync action creatorconst logInRequest = (data) =&gt; { return { type: 'LOG_IN_REQUEST', data };};const logInSuccess = (data) =&gt; { return { type: 'LOG_IN_SUCCESS', data };};const loginFailure = (error) =&gt; { return { type: 'LOG_IN_FAILLURE', error };};const login = (data) =&gt; { // async action creator return (dispatch, getState) =&gt; { // 서버로 로그인 요청 보내기 dispatch(logInRequest(data)); try { // 서버로의 로그인 요청이 성공한 경우, // setTimeout으로 비동기 처리 테스트 (2초뒤에 loginSuccess) // setTimeout 부분에 axios.post().then().catch()로 하기 setTimeout(() =&gt; { dispatch( logInSuccess({ userId: 1, name: 'hyungi' }) ); }, 2000); } catch (e) { // 서버로의 로그인이 실패하는 경우, dispatch(loginFailure()); } };}; index.js 1234567891011121314151617181920212223242526272829303132333435const firstMiddleware = (store) =&gt; (dispatch) =&gt; (action) =&gt; { console.log(action); // 부가적인 동작 추가 (1) // 기능추가를 전 단계에서 추가할 수 있다. dispatch(action); // 기본 기능 (dispatch(action)을 reducer에 전달) // subscribe 실행 // 기능추가를 후 단계에서 추가할 수 있다.};// thunk middleware는 매우 간단하기 때문에 직접 구현해서 사용해도 된다.// STORE: redux store// dispatch와 reducer 구간 사이에서 동기와 비동기 처리를 구분한다.const thunkMiddleware = (store) =&gt; (dispatch) =&gt; (action) =&gt; { // 원래 action은 객체(동기)인데, // redux와 약속을 한다. 비동기인 경우에는 action을 함수로 두겠다. if (typeof action === 'function') { // 전달되는 곳 - // dispatch와 store.dispatch는 같다. return action(store.dispatch, store.getState); } return dispatch(action);};//middleware는 인수 제한없이 추가가 가능하다.const enhancer = applyMiddleware(firstMiddleware, thunkMiddleware);// createStore의 세 번째 인자로 enhancer를 넣어준다.const store = createStore(reducer, initialState, enhancer);// 로그인 시도// dispatch에서 반환되는 결과값의 타입이 함수인 경우,store.dispatch( login({ id: 1, name: 'lee', admin: true })); thunk로는 간단한 비동기 처리를 하고, 만약에 thunk로 비동기 처리를 하는데 있어 한계가 있을때(복잡한 비동기 처리)에는 redux-saga를 사용하도록 하자. react-reduxReact와 Redux를 연겨시키기 위해서는 react-redux를 사용해야 한다. 1$ npm i react-redux 123456789import { Provider } from 'react-redux';import storage from 'storage';...ReactDOM.render( &lt;Provider storage={storage}&gt; &lt;App /&gt; &lt;/Provider&gt;)... 최상위 component를 react-redux에서 제공해주는 Provider로 감싸서 storage의 속성으로 앞서 작성해준 storage 객체를 값으로 넣어준다.이제 감싸진 하위 컴포넌트들에서 dispatch의 접근이 가능하다. 123456789101112131415161718192021222324252627import { useDispatch, useSelector } from 'react-redux';// state : initial stateconst user = useSelector((state) =&gt; { state.user.data;});const posts = useSelector((state) =&gt; { state.posts;});const dispatch = useDispatch();const onClick = useCallback(() =&gt; { // useDispatch를 통해서 action함수를 인수와 함께 실행해준다. dispatch( logIn({ id: 'lee', password: '123' }) );}, []);return ( &lt;div&gt; {user ? &lt;div&gt;{user.name}&lt;/div&gt; : 'Please login'} &lt;button onClick={onClick}&gt; LOGIN &lt;/button&gt; &lt;/div&gt;); react-redux devtools와 연결하기redux-devtools https://github.com/zalmoxisus/redux-devtools-extension 1$ npm i redux-devtools-extension -D 12345678910111213import { composeWithDevTools } from 'redux-devtools-extension';...//middleware는 인수 제한없이 추가가 가능하다.const enhancer = composeWithDevTools( applyMiddleware( firstMiddleware, thunkMiddleware, ), );// createStore의 세 번째 인자로 enhancer를 넣어준다.const store = createStore(reducer, initialState, enhancer);... 배포환경인 경우 구분해서 enhancer 설정하기 (주의)배포환경에서 devtools를 사용하게되면 redux의 데이터 구조가 개발자 도구에서 노출이 된다. 1234const enhancer = process.env.NODE_ENV === 'production' ? compose(applyMiddleware(firstMiddleware, thunkMiddleware)) : composeWithDevTools(applyMiddleware(firstMiddelware, thunkMiddleware)); Redux dev tools를 사용하면 각 action이 dispatch될때마다 업데이트되는 상태의 변화를 확인할 수 있다. 이는 reducer에서 불변성을 지켜서 기존의 상태값을 업데이트해주고, history로 쌓아주기 때문에 가능한 것이다. class component에서의 redux 사용 - connectfunctional component에서와 동일하게 state와 dispatch를 가져온다. functional component에서의 react-redux 사용 12345678910import { useDispatch, useSelector } from 'react-redux';// state : initial stateconst user = useSelector((state) =&gt; { state.user.data;});const posts = useSelector((state) =&gt; { state.posts;});const dispatch = useDispatch(); class component에서의 react-redux 사용아래와 같이 export 구문에서 connect로 123456789101112131415161718192021import { connect } from 'react-redux';// 컴포넌트가 렌더링될때마다 매번 아래 함수가 실행이 된다. (성능상 문제)// 위의 문제를 해결하기 위해서 reselect를 써서 해결한다.// hooks를 쓰면 reselect를 사용하지 않아도 된다.// 하나의 객체이기 때문에 uesr나 posts 중에 하나만 바뀌어도 전부 다 계산을 다시한다.// hooks는 분리가 되어있다.const mapStateToProps = (state) =&gt; ({ user: state.user, posts: state.posts});// this.props.dispatchLogIn 로 불러와서 사용// this.props.dispatchLogOut 로 불러와서 사용const mapDispatchToProps = (dispatch) =&gt; ({ dispatchLogIn: (data) =&gt; dispatch(logIn(data)), dispatchLogOut: () =&gt; dispatch(logOut())});// 확장하는 HOC를 붙여줘서 class 내부에서 dispatch와 state를 불러서 사용할 수 있다.export default connect(mapStateToProps, mapDispatchToProps)(App); container componentReact에서 데이터를 가져오는 컴포넌트를 컨테이너 컴포넌트라고 하는데, redux를 사용해서 데이터를 가져오는 컴포넌트를 컨테이너 컴포넌트라고 한다. immerspread문법을 사용해서 불변성을 지키는 방법은 직관적인 방법이 아니다. 만약 객체의 구조가 깊어지면 불변성을 지키기 위해 spread 문법을 많이 사용하게 되는데 코드가 복잡해지고 가독성이 안좋아진다.따라서 immer라는 라이브러리를 사용해서 직관적으로 불변성을 지키면서 상태 데이터를 업데이트할 수 있다. immer도 다음 state 객체를 만들어내는 역할을 한다. 123// immer의 기본 형태// draft 부분을 prevState의 복사본으로 보면 된다.으로 보면 된다.nextState = produce(prevState, (draft) =&gt; {}); 아래의 기존 reducer 함수를 immer를 사용해서 간략하게 작성해본다.기존 reducer 123456789101112131415161718192021222324const reducer = (prevState, action) =&gt; { switch (action.type) { case 'LOG_IN': // state의 구조가 복잡해지면 spread 문법을 사용해서 얕은 복사를 하는 것이 지저분해질 수 있기 때문에 immutable이나 immer를 사용해서 코드를 깔끔하게 만들 수 있다. return { ...prevState, user: action.data }; case 'LOG_OUT': return { ...prevState, user: null }; case 'ADD_POST': return { ...prevState, posts: [...prevState.posts, action.data] }; // 만약에 action.type에 대한 이름이 오타나는 경우를 대비해서 default는 작성해야 한다. // reducer에서는 기존의 state를 업데이트하기 위해서 새로운 state 객체를 생성하기 때문에 문제가 될 여지가 있다. default: return prevState; }}; immer 적용 reducerreducer 내부의 switch case문을 produce 함수로 감싸준다. 123456789101112131415161718192021222324import produce from 'immer';const reducer = (prevState, action) =&gt; { // draft는 복사본 // 복잡한 return produce(prevState, (draft) =&gt; { switch (action.type) { case 'LOG_IN': draft.data = null; draft.isLoggingIn = true; break; case 'LOG_OUT': draft.data = action.data; draft.isLoggingIn = false; break; case 'ADD_POST': // 사본 데이터에 action에서 받은 데이터를 push해준다. draft.push(action.data); break; default: break; } });}; redux toolkitredux toolkit은 redux에서 자주 쓰이는 기능을 리덕스 팀에서 라이브러리로 만들었는데, redux toolkit을 사용하면, 기존의 redux, devtools, redux-thunk, redux-saga, immer를 사용안하고, redux toolkit의 내장된 기능으로 사용할 수 있다. 중요한 메인 기능은 createSlice와 createAsyncThunk이다. 1$ npm i @reduxjs/toolkit https://redux-toolkit.js.org/ react hotloader에서 react-refresh로 변경기존에 react에서 지원하던 hotloader가 react-refresh로 업데이트가 되었고,@pmmmwh/react-refresh-webpack-plugin가 추가되었다. (둘 다 devDependency로 설치) webpack.config.js 1234567const reactRefreshWebpackPlugin = require('@pmmmwh/react-refresh-webpack-plugin');// 기존 webpack 설정 파일에서 일부 변경plugins: ['react-refresh/babel'],....plugins: [ new ReactRefreshWebpackPlugin()], 기존에 최상위 컴포넌트를 hot으로 감싸서 처리했던 것을 이제는 감싸서 처리를 안해줘도 된다. webpack 버전 업데이트에 따른 변경사항12345// webpack v5, webpack-cli v4// 기존&quot;dev&quot;: &quot;webpack-dev-server --hot&quot;// 변경&quot;dev&quot;: &quot;webpack serve --env development --hot&quot; 기존에 redux store에 연결하던 부분을 redux toolkit을 사용해서 업데이트123456789// 기존의 redux와 devtools 제거import { configureStore } from '@reduxjs/toolkit';// initial state, thunk middleware는 내장되어있기 때문에 제거, devtools을 달아줬던 enhancer도 사라진다.// 간단하게 store는 아래와 같이 초기화시켜준다.// thunk, immer, devtools// initial state는 preloadedState 속성인데,// 나중에 SSR할때 서버로 부터 초기 데이터를 받아올때 해당 속성에 초기 데이터를 넣어주면 된다.const store = configureStore({ reducer }); 만약에 custom middleware를 사용하고 싶은 경우, 1234567891011121314//기존의 thunk middleware도 포함시키기 위해서 getDefaultMiddleware 가져오기const {configureStore, getDefaultMiddleware} from '@reduxjs/toolkit';const firstMiddleware = (store) =&gt; (next) =&gt; (action) =&gt; { console.log('logging', action); next(action);};const store = configureStore({ reducer, middleware: [firstMiddleware, ...getDefaultMiddleware()], // production 모드가 아닐때만 devTools 사용 devTools: process.env.NODE_ENV !== 'production',}); 기존에 hook으로 구현했던 부분을 toolkit으로 대체기존의 reducer를 대체 12345678import userSlice from './user';import postSlice from './post';export default combineReducers({ // slice 자체가 아닌 slice안의 reducer를 넣어줘야 한다. user: userSlice.reducer, posts: postSlice.reducer}); reducer와 action이 분리되어 있었는데, 합친 개념이 slice이다. 특정 action은 특정 reducer에 종속되어있다. 따라서 나누지 말고 합치게 되었다. 1234567891011121314151617181920212223242526272829303132333435363738394041// immer 내장되서 별도로 가져올 필요 없음import { createSlice } from '@reactjs/toolkit';import { logIn } from './actions/user';const initialState = { data: []};const userSlice = createSlice({ // reducer이름 name: 'user', initialStae, // 동기적인 액션 or 내부적인 액션 reducers: { // 동기적인 액션 // action creator를 따로 만들어서 사용할 필요가 없다. logOut(state, action) { state.data = null; } }, // 비동기적인 액션 or 외부적인 액션) // toolkit에서는 action으로 받는 데이터를 action.payload로 고정해서 받는다. // pending, fullfilled, rejected는 하나의 set로써 작성해주면 된다. extraReducers: { [logIn.pending](state, action) { // user/logIn/pending = [logIn.pending] state.isLoggingIn = true; }, [logIn,fullfilled](state, action){ // user/logIn/fullfilled = [logIn.fullfilled] // 성공했을때의 반환값 payload state.data = action.payload; state.isLoggingIn = false; }, [logIn.rejected](state, action){ // user/logIn/rejected = [logIn.rejected] // 실패했을때에는 action.error state.data = null; state.isLoggingIn = false; } }});export default userSlice; app.js 12345678910111213141516171819import userSlice from './reducers/user';// 비동기 action creator의 경우에는 따로 외부로 빠져나가 있기 때문에// 별도로 호출해서 dispatch로 호출을 한다.const onClick = useCallback(() =&gt; { dispatch( // logIn에 넣어준 객체값은 pending의 meta.arg에 들어있다. logIn({ id: 'lee', password: '1q2w3e4r' }) );}, []);// 동기 action creator의 경우에는 slice.actions. 로 참조햇// dispatch로 호출한다.const onLogout = useCallback(() =&gt; { dispatch(userSlice.actions.logOut());}, []); 이전에는 동기/비동기 action creator를 한 곳에서 모두 관리를 했는데, toolkit을 사용할때는 reducer에서는 직접적으로 동기적인 action creator를 생성해서 관리를 해주고, 비동기적인 action creator는 기존에 별도의 action 파일에서 관리를 해주게 된다. actions/user.js 123456789101112131415161718192021import { createAsyncThunk } from '@reduxjs/toolkit';// action의 이름, action을 호출할때 받는 데이터 (data),const logIn = createAsyncThunk('user/logIn', async (data, thunkAPI) =&gt; { // ex) const response = await axios.post('/login', {email, password}) // reducer의 state를 가져올 수 있다. // const state = thunkAPI.getState(); // state.user.data; // return 한 데이터는 성공했을때의 데이터 // throw error exception은 실패했을때의 데이터 // loading - success - failure // pending - fullfilled, rejected 로 thunk에서는 구분 // createAsyncThunk에서는 try-catch 분기 처리안해줘도 된다.});// dispatch 부분에서 넘겨준 객체 데이터가 위의 action에서 data로 넘겨지고,// 해당 data를 사용해서 서버로 요청을 보내서 받게되는 response를 처리하는 것이라고 생각하면 된다.// dispatch(logIn({// userId: 1,// name: 'lee',//})) 123456import { createAsyncThunk } from '@reduxjs/toolkit';// action의 이름, action을 호출할때 받는 데이터 (data),const logIn = createAsyncThunk('user/logIn', async (data, thunkAPI) =&gt; { // 서버로부터 비동기 요청을 해서 받은 데이터를 return해주게 되는데, // 이 데이터가 바로 userSlice에서 비동기 처리 데이터를 받는 부분에서의 action.payload가 된다.}); toolkit의 특징이 request마다 고유의 requestId를 부여해주는데, 같은 요청을 여러번 했을때 구분을 하기 위한 목적으로 사용할 수 있다. extraReducers를 builder로 작성하는 방법builder로 chaining된 형태로 작성을 하게 되면, 나중에 타입스크립트를 적용했을때 타입추론이 잘된다.그리고 switch 문의 default 처리도 작성해 줄 수 있다. 1234567891011121314151617181920212223... extraReducers: (builder) =&gt; builder .addCase(logIn.pending, (state, action) =&gt; { state.isLoggingIn = true; }) .addCase(logIn.fullfilled, (state, action) =&gt; { state.data = action.payload; state.isLoggingIn = false; }) .addCase(logIn.rejected, (state, action) =&gt; { state.data = null; state.isLoggingIn = false; }) // 공통 처리를 해주고 싶을때 addMatcher를 사용할 수 있다. // switch case문에서 조건을 연달아서 처리하는 경우와 동일 처리 .addMatcher((action) =&gt; { return action.type.includes('/pending'); }) // switch case문일때 default의 처리와 동일 처리 .addDefaultCase((state, action) =&gt; { // default })}); redux를 쓰지 말아야 할 때 (input/async) input input은 redux를 사용하지 않는 것이 좋다. 굳이 redux에서 써야한다면, input 태그의 blur 속성의 함수에서 처리를 해주거나 마지막 한 번만 form 태그의 onSubmit에서 hooks로 정의된 값을 일괄적으로 dispatch해주는 편이 좋다. useSelector를 사용해서 state값을 가져올때에도 문제점이 있는데, 아래와 같이 useSelector를 사용해서 값을 참조하게 되면, 참조하고 있는 user state의 값이 업데이트 될 때마다 컴포넌트가 re-rendering된다. 1const { email, password } = useSelector((state) =&gt; state.user); 물론 useState를 사용해도 input 태그에 한 글자 쓸때마다 해당 state에 대해서 re-rendering이 된다.하지만, useSelector를 사용해서 user state정보를 객체로 참조한다면, 다른 컴포넌트들에서도 user state의 정보중에 하나만 참조하게 되었을때도 참조하고 있는 모든 컴포넌트들이 모두 re-rendering되어 성능상 문제가 생긴다. 따라서 useSelector를 사용해서 state의 값을 참조할때에는 최대한 작은 단위로 나눠서 참조하도록 한다. 12const eamil = useSelector((state) =&gt; state.user.email);const password = useSelector((state) =&gt; state.user.password); 다 풀어서 사용하기 힘들기 때문에 성능상 문제가 없다면 객체로 묶어서 참조해도 된다. 너무 빠른 최적화는 변화에 대처하기 힘들어진다. 필요에 따라 적절하게 판단해서 최적화를 시켜나가도록 하자. reselector를 사용해서 문제해결reselector로 유명한 라이브러리를 toolkit에 포함시켰다. redux를 사용하는 경우에는 import해서 사용하도록 하자.만약에 JSX에서 함수 연산이 되고, input 태그에 값이 입력될때마다 함수가 다시 렌더링이 된다면, 성능문제가 될 수 있다.따라서 별도로 useMemo로 JSX의 함수 연산부분을 빼서 처리를 한다면, 값을 기억할 수 있다. 하지만 useMemo로 문제가 해결된 것은 아니다.useMemo를 사용함으로써 deps에 넣은 값이 바뀌었는지 아닌지 별도로 검사를 해줘야 되기 때문에 실제 캐싱한 연산보다 deps에서 비교검사해야 되는 부분의 처리가 크다면 효율적이지 않은 문제해결 방법이다. 따라서 component의 윗 단(밖)에서 createSelector 함수를 사용해서 해결할 수 있다. 1234567891011121314151617import { createSelector } from '@reduxjs/toolkit';// selector는 useSelector 내에서 작성해준 함수이다.const priceSelector = (state) =&gt; state.user.prices;//createSelector가 memoization 역할을 해준다.const sumPriceSelector = createSelector( priceSelector, // state.user.prices (prices) =&gt; prices.reduce(a, c) =&gt; a + c, 0),);const App = () =&gt; { ... const totalPrice = useSelector(sumPriceSelector); ...} 2021/05/13 UpdatecreateSelector 연습예제쇼핑카트 예제로 createSelector를 연습해보았다. 1234567891011121314151617181920212223242526272829303132333435363738394041// 다음과 같이 hidden과 cartItems를 속성으로 갖는 상태값이 있다고 가정한다.const state = { hidden: true, cartItems: []};// createSelector가 caching의 역할을 해주기 때문에 selector에서 가져오고자 하는 값이 변하지 않으면 두번째 인자로 넣어준 콜백함수를 처리하지 않고 기존의 값을 재사용하도록 한다.const selectCart = (state) =&gt; state.cart;// 상태값에서 cartItem만을 추출해서 가져오는 함수 (state에서 \b값이 변하지 않으면 캐싱된 값을 가져다 쓴다.)export const selectCartItems = createSelector( [selectCart], (cart) =&gt; cart.cartItems);// 상태값에서 cartItem만을 추출해서 가져오는 함수export const selectCartHidden = createSelector( [selectCart], (cart) =&gt; cart.hidden);// cartItem의 총 갯수를 체크하는 함수export const selectCartItemsCount = createSelector( [selectCartItems], (cartItems) =&gt; cartItem.reduce( (accumulatedQuantity, cartItem) =&gt; accumulatedQuantity + cartItem.quantity, 0 ));// cart에 담긴 상품의 총 가격을 체크하는 함수export const selectCartTotal = createSelector( [selectCartItems], (cartItems) =&gt; cartItems.reduce( (accumulatedPrice, cartItem) =&gt; accumulatedPrice + cartItem.quantity * cartItem.price, 0 )); 비동기 처리(async) 만약에 비동기 서버요청이 특정 컴포넌트 하나에서만 실행이 되고, 다른 컴포넌트들에게는 영향을 주지 않는다면, 해당 컴포넌트에서만 axios를 사용해서 비동기 처리를 해주면 된다. (별도로 redux에서 비동기처리로 작성해 줄 필요가 없다) 1234567891011121314151617const [isLoading, setLoading] = useState(false);const [error, setError] = useState(false);const [done, setDone] = useState(false);const onClick = useCallback(async () =&gt; { setLoading(true); setDone(false); setError(false); try { const response = await axios.post('/login'); setDone(true); } catch (e) { setError(e); } finally { setLoading(false); }}, []); 만약에 비동기 액션이 더 생겨난다면 hooks로 관리해야 되는 상태들이 많이 늘어난다. 그리고 같은 비동기 요청을 여러번 연달아서 한다면, 여러개의 요청에 대한 정보를 전부 담을 수 없다.","link":"/2021/04/30/202104/210430-Redux_TIL/"},{"title":"210501 babel &#x2F; webpack &#x2F; react-scripts","text":"본 포스팅 내용은 과거에 개인적으로 공부할때 정리했던 ReactJS의 내용을 복습의 목적으로 다시 정리하는 포스팅입니다. react-scriptsreact-based project를 사용할때 직접 빌드 환경을 셋팅하는 것은 많은 시간이 소요된다. 그래서 React 개발팀이 react-scripts라는 npm 패키지를 만들었는데, 이 패키지에는 평균적인 React app에서 많은 사람들이 필요로하는 것들에 대한 기본적인 setup을 포함하고 있다.위에서 설명한 Babel과 Webpack 또한 react-scripts의 dependency로써 포함되어있다. babelbabel은 높은 버전의 ECMAScript(unsupported or cutting-edge)를 ES5로 변환해주는 역할을 하는 transpiler이다. (ES5는 범용적인 브라우저에서 지원을 하기 때문) webpackwebpack은 dependency를 분석기이자 module bundler이다. 예를들어 module A가 B를 dependency로 요청을하고, module B가 C를 dependency로 요청을 한다면 webpack은 C-B-A 와 같이 dependency map을 생성한다. 실제로는 매우 복잡하지만, 기본적인 컨셉은 webpack이 모듈들을 복잡한 dependency 관계들과 함께 번들들로 통합한다. webpack과 babel의 관계: 웹팩이 종속성을 처리할 때, 웹팩은 자바스크립트 위에서 작동하기 때문에 모든 것을 자바스크립트로 변환해야 한다. 그 결과, 다른 로더를 사용하여 다른 유형의 리소스/코드를 javascript로 변환한다. ES6 또는 ES7에 대한 변환이 필요할 때에는 babel-loader를 사용해서 webpack과 babel을 연결시켜 사용한다.","link":"/2021/05/01/202105/210501-React-review_study/"},{"title":"210501 Setting up my own webpack with react and babel","text":"CRA을 사용하지 말고, 나만의 React 프로젝트를 위한 웹팩을 만들자React프로젝트를 만들때 CRA를 사용해서 손쉽게 React 프로젝트를 생성 할 수 있다. 하지만 이렇게 프로젝트를 생성하다보면, 막상 Webpack을 직접 커스텀해서 해야할때 마냥 어렵게만 느껴지고, 프로젝트에 불필요한 요소들이 자동으로 생성되기 때문에 좋지 않다. 기본적으로 설치해야 될 패키지들은 아래를 참고하자.package.json 123456789101112131415&quot;dependencies&quot;: { &quot;react&quot;: &quot;^17.0.1&quot;, &quot;react-dom&quot;: &quot;^17.0.1&quot;, }, &quot;devDependencies&quot;: { &quot;@babel/core&quot;: &quot;^7.11.6&quot;, &quot;@babel/preset-env&quot;: &quot;^7.11.5&quot;, &quot;@babel/preset-react&quot;: &quot;^7.10.4&quot;, &quot;@pmmmwh/react-refresh-webpack-plugin&quot;: &quot;^0.4.3&quot;, &quot;babel-loader&quot;: &quot;^8.1.0&quot;, &quot;react-refresh&quot;: &quot;^0.9.0&quot;, &quot;webpack&quot;: &quot;^5.3.2&quot;, &quot;webpack-cli&quot;: &quot;^4.1.0&quot;, &quot;webpack-dev-server&quot;: &quot;^3.11.0&quot; } webpack5 부터 새롭게 react-refresh를 사용해서 Hot reloading을 구현한다. (아래 webpack 설정파일에 적용)webpack.config.js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546const path = require('path');const ReactRefreshWebpackPlugin = require('@pmmmwh/react-refresh-webpack-plugin');module.exports = { name: 'webpack_with_hot_loader', mode: 'development', devtool: 'inline-source-map', resolve: { extensions: ['.js', '.jsx'] }, entry: { app: './index' }, module: { rules: [ { test: /\\.jsx?$/, loader: 'babel-loader', options: { presets: [ [ '@babel/preset-env', { targets: { browsers: ['last 2 chrome versions'] }, debug: true } ], '@babel/preset-react' ], plugins: ['react-refresh/babel'] }, exclude: path.join(__dirname, 'node_modules') } ] }, plugins: [new ReactRefreshWebpackPlugin()], output: { path: path.join(__dirname, 'dist'), filename: '[name].js', publicPath: '/dist' }, devServer: { publicPath: '/dist', hot: true }}; package.json 123&quot;scripts&quot;: { &quot;dev&quot;: &quot;webpack serve --env development&quot;},","link":"/2021/05/01/202105/210501-setup_my_own_webpack/"},{"title":"210502 React with TypeScript TIL - keyof와 typeof의 혼합 사용, JavaScript의 Curring 방식의 함수 작성","text":"keyof와 typeof의 혼합 사용특정 변수의 타입을 정의할때 만약에 해당 타입이 다른 타입과 종속적인 관계에 있다면, 종속관계에 있는 해당 타입의 정의를 참조해서 타입을 정의해야 한다. 아래는 스프라이트 기법으로 이미지를 잘라내서 사용하기 위한 이미지 좌표의 타입 정의이다. 12345678const coords = { img1: '0', img2: '-142px', img3: '-284px'} as const;type a = keyof typeof coords; // coords 타입(typeof)의 key 값(img1 | img2 | img3)type imgCoords = typeof coords[keyof typeof coords] // '0' | '-142px' | '-284px' JavaScript의 Curring 방식의 함수 작성일반적으로 버튼의 클릭이벤트 속성으로 함수를 넣어줄때에는 아래와 같이 해당 함수명만 작성을 해준다. 1234const onClickBtn = () =&gt; { // 이벤트 처리};&lt;button onClick={onClickBtn}&gt; Button &lt;/button&gt;; 만약에 버튼이 클릭되었을때 실행되는 함수에 특정 인수를 넣어 실행되게 하는 경우에는 curring 방식으로 함수를 작성해줘야 한다. 12345// event 객체는 두 번째 인자로 넣어준다. (클릭 이벤트의 경우에는 타입이 React.MouseEvent)const onClickBtn = (name) =&gt; (e: React.MouseEvent&lt;HTMLButtonElement&gt;) =&gt; { //이벤트 처리};&lt;button onClick={onClickBtn('lee')}&gt; Button &lt;/button&gt;; useEffect는 별도로 typing 해줄 것이 없다.const로 컴포넌트를 가져오면 interface를 가져올 수 없다.import를 사용해서 컴포넌트를 가져오게 되면, interface도 같이 가져올 수 있지만, const를 사용해서 컴포넌트를 가져오게 되면, interface를 가져올 수 없다.","link":"/2021/05/02/202105/210502-React_with_Typescript_TIL/"},{"title":"210504 React TDD Practice with RTL","text":"RTL(React-Testing-Library)는 모든 테스트를 DOM 위주로 한다. 별도로 props나 state를 조회하는 테스트는 하지 않는다. 그 이유는 컴포넌트를 리팩토릴할때 내부 구조와 네이밍은 많이 바뀔 수 있어도 실제 동작방식은 크게 바뀌지 않기 때문이다.따라서 RTL에서는 컴포넌트의 기능이 똑같이 동작한다면, 컴포넌트의 내부 구현 방식이 바뀌어도 실패하지 않도록 테스트를 지원한다. DOM 시뮬레이션은 JSDOM이라는 도구를 사용하여 document.body에 React 컴포넌트를 렌더링한다. @testing-library/jest-dom 은 DOM관련 matcher를 사용할 수 있게 지원해주는 라이브러리이다. snapshot 테스트12345678910import { render } from '@testing-library/react';import Profile from './Profile';describe('&lt;Profile /&gt; 컴포넌트 테스트', () =&gt; { test('snapshot 테스트',() =&gt; { const snapshot = render(&lt;Profile surname=&quot;lee&quot;, givenname=&quot;hyungi&quot; /&gt;); // container는 검사하는 컴포넌트의 최상위 DOM을 가르킨다. expect(snapshot.container).toMatchSnapshot(); });}); getByText텍스트를 사용해서 원하는 DOM을 선택할 수 있다.App.js 123456789101112131415161718192021222324252627282930313233const CustomInput = ({ children, value, onChange }) =&gt; { return ( &lt;div&gt; &lt;label htmlFor=&quot;search&quot;&gt;{children}&lt;/label&gt; &lt;input id=&quot;search&quot; type=&quot;text&quot; placeholder=&quot;PlaceholderText&quot; value={value} onChange={onChange} /&gt; &lt;/div&gt; );};const App = () =&gt; { const [text, setText] = useState(''); const handleChange = (e) =&gt; { setText(e.target.value); }; return ( &lt;div&gt; &lt;CustomInput value={text} onChange={handleChange}&gt; Input: &lt;/CustomInput&gt; &lt;p&gt;You typed: {text ?? '...'}&lt;/p&gt; &lt;/div&gt; );};export default App; App.test.js 123456789101112131415161718import { render, screen } from '@testing-library/react';describe('When everything is OK', () =&gt; { test('should render the App component without crashing', () =&gt; { render(&lt;PosterSection /&gt;); screen.debug(); }); test('should select the children that is being passed to the CustomInput component', () =&gt; { render(&lt;App /&gt;); screen.getByText('Input:'); // getByText의 인수로 정규표현식을 넣어줄 수 있다. screen.getByText('/Input/'); // screen.getByText(&quot;Input&quot;)에서 에러가 발생하기 때문에 not.toBeInTheDocument에 대한 체크를 할 수 없다. expect(screen.getByText('Input')).not.toBeInTheDocument(); });}); screen에 없는 텍스트를 체크하면 에러가 발생하기 때문에 .not.toBeInTheDocument matcher를 이용해서 테스트를 할 수 없다.이 경우에는 아래와 같이 발생한 에러를 변수에 담아서 해당 변수에 값이 정의되어있는지 확인을 통해 테스트를 진행할 수 있다. 123456789101112let error;try { // screen에서 Input은 존재하지 않기 때문에 // error 발생 screen.getByText('Input');} catch (err) { // 발생한 에러에 대한 정보를 error 변수에 담기 error = err;}// error 변수에 에러에 대한 정보가 정의되어 있는지 확인expect(error).toBeDefined(); getByRolegetByText 대신에 태그의 ARIA role 속성을 이용(getByRole)해서 screen에 표시된 텍스트를 참조/테스트할 수 있다. 123456test('should select the input element by its role', () =&gt; { render(&lt;PosterSection /&gt;); screen.getByRole('textbox'); // input tag에 대한 role(ARIA property)로부터 expect(screen.getByRole('textbox')).toBeInTheDocument();}); label 태그의 text가 children으로 정의된 텍스트로 되어있기 때문에 아래와 같이 getByLabelText를 사용해서 label 요소를 선택할 수 있다. 1234test('should select a label element by is text', () =&gt; { render(&lt;PosterSection /&gt;); screen.getByLabelText('Input:');}); input 태그의 placeholder 텍스트를 이용해서 DOM 1234test('should select input element by placeholder text', () =&gt; { render(&lt;PosterSection /&gt;); screen.getByPlaceholderText('PlaceholderText');}); queryBy vs getBygetBy*의 경우에는 존재하지 않는 요소에 대한 검사를 하는 경우, 에러를 반환하지만, queryBy*의 경우에는 에러를 반환하지 않는다.따라서 null이나 존재하지 않는 DOM요소를 검사할때에는 getBy말고 queryBy를 사용해서 테스트를 하도록 하자. queryByRold 123456// 존재하지 않는 요소를 검사const result = screen.queryByRole('textbox');console.log(result); // nulltest('should not find the role 'whatever' in our component',() =&gt; { expect(screen.queryByRole('whatever')).toBeNull();}); jest –watch로 테스트하면 여러 옵션들이 있는데, t 옵션을 이용해서 사용한 test 이름을 regex 패턴을 사용해서 검색할 수 있다.(Press t to filter by a test name regex pattern.)위와같이 console.log를 사용해서 결과값을 확인하고자 할때 사용하면 유용한 옵션이다. findBy search variant비동기 이벤트에 대한 테스트 코드 작성 jest-environment-jsdom-sixteen 패키지 설치1$ npm i -D jest-environment-jsdom-sixteen test command script 작성1&quot;test&quot;: &quot;jest --watch --env=jest-environment-jsdom-sixteen&quot; jest.config.js 123module.exports = { testEnvironment: 'jest-environment-jsdom-sixteen'}; App.js 123456789101112131415function getUser() { return Promise.resolve({ id: '1', name: 'lee' });}const App = () =&gt; { const [user, setUser] = useState(null); useEffect(() =&gt; { const fetchUser = async () =&gt; { const user = await getUser(); setUser(user); }; fetchUser(); }, []);};","link":"/2021/05/04/202105/210504-React_testing_mini_project/"},{"title":"210505 screen.getBy*와 getBy의 차이, findBy*&#x2F;getBy*&#x2F;queryBy* method의 차이","text":"테스트 코드를 작성하던 도중에 헷갈렸던 개념들에 대해서 반복학습을 위해 포스팅을 해두려고 한다. screen.getBy vs getByscreen을 사용하는 이점은 필요한 query를 추가할 때 더 이상 render 함수로 컴포넌트를 렌더링해서 최신 상태를 유지 하지 않아도 된다는 점이다.beforeEach()함수 내에서 공통적으로 컴포넌트를 렌더링했다면, 이하의 test 블럭에서는 screen을 통해 렌더링된 컴포넌트를 통해 테스트 코드를 작성할 수 있다. findBy* / getBy* / queryBy* method의 차이 getBy*조건에 맞는 요소를 찾았을때 query에 일치하는 노드를 반환조건에 맞는 요소를 못 찾았을때 에러를 반환→ 존재하지 않는 DOM element를 assert할때에는 try-catch문으로 발생한 에러를 별도의 함수에 담아 tobedefined() matcher로 확인을 해야 한다. 이 경우에는 queryBy*를 사용해서 assert하는 것이 좋다. queryBy*조건에 맞는 요소를 찾았을때 query에 일치하는 노드를 반환조건에 맞는 요소를 못 찾았을때 null을 반환→ 존재하지 않는 DOM element를 assert할때 유용하다. findBy*조건에 맞는 요소를 찾았을때 resolved Promise 반환조건에 맞는 요소를 못 찾았을때 rejected Promise 반환 기존의 waitForElement()함수는 이제 deprecated되었기 때문에 findBy* 함수를 사용해야 한다. 123456test('nowPlaying API method 테스트 - 데이터가 문제없이 화면에 로드되는지 확인', async () =&gt; { const { findByTestId } = render(&lt;Movie /&gt;); await findByTestId('loading-text'); await findByTestId('contents-list');});","link":"/2021/05/05/202105/210505-React-unit-test-questions/"},{"title":"210504 Error) You should not use Link outside a &lt;Router&gt;","text":"&lt;StaticRouter&gt;와 &lt;MemoryRouter&gt;를 사용이번에 컴포넌트 단위로 단위테스트를 하던 중에 You should not use XXX and &lt;withRouter&gt; outside a &lt;Router&gt;에러를 접했다.Router의 내부에서만 사용할 수 있는 &lt;withRouter&gt;와 &lt;Link&gt;를 사용한 컴포넌트를 테스트를 할때에는 개별 컴포넌트만 호출해서 렌더링한 후에 테스트를 진행하기 때문에 실제로는 Router 내에 구현이 되어있다고 하더라도 위와같은 에러가 발생하게 되는 것이다. 위의 에러에 대한 해결책은 테스트 코드에서 &lt;MemoryRouter&gt; 또는 &lt;StaticRouter&gt;를 사용해서 테스트하려는 컴포넌트를 감싸서 렌더링해주면 된다.Header.test.js 123456789101112131415161718192021import React from 'react';import { fireEvent, render, screen } from '@testing-library/react';import Header from './Header';import { MemoryRouter } from 'react-router';describe('header 컴포넌트 내부의 메인 메뉴관련 테스트 코드', () =&gt; { let header; beforeEach(() =&gt; { header = render(&lt;MemoryRouter&gt;&lt;Header/&gt;&lt;/MemoryRouter&gt;) }); describe('메뉴 클릭시 제대로 연결된 페이지가 렌더링되는지 확인하는 테스트 코드', () =&gt; { test('Movie 메뉴 클릭시 Movie page component가 렌더링된다.', () =&gt; { const { getByTestId } = header; expect(getByTestId('movie-menu')).toBeTruthy(); fireEvent.click(getByTestId('movie-menu')); expect(screen.findByTestId('movie-page-container')).toBeInTheDocument; }); ......","link":"/2021/05/04/202105/210504-unit_testing_error/"},{"title":"210505 RTL(React Testing Library)를 활용한 테스트 (+Axios Mockup 테스트)","text":"게시물 업데이트2021/02/09 : RTL을 활용한 테스트 (+ Axios Mockup 테스트)2021/05/05: Axios Mockup 테스트 추가 포스팅 실습 Repository :https://github.com/LeeHyungi0622/react_testing_library_mock_axios_request_practice_repo React Testing Library 설치1$ npm install --save-dev @testing-library/react (1) react-testing-library에서 지원하는 render()function을 이용해서 테스트하고자 하는 component의 rendering 테스트하기1234import { render } from &quot;@testing-library/react&quot;;import Fetch from &quot;./Fetch&quot;;....const { getByTestId } = render(&lt;Fetch url={url} /&gt;); (2) rendering한 component의 요소에 data-testid를 지정해서 해당요소를 검사할 수 있다. 123456789import '@testing-library/jest-dom/extend-expect';it('fetches and displays data', async () =&gt; { const url = '/greeting'; // Fetch component를 rendering하게 되면 rendering된 객체를 확인할 수 있다. // rendering된 객체 중에 속성으로 data-testid로 id를 지정해준 부분을 체크할 수 있다. const { getByTestId } = render(&lt;Fetch url={url} /&gt;); expect(getByTestId('loading')).toHaveTextContent('Loading data...');}); (3) 각 테스트 단계가 끝나면 테스트 상태를 초기화 시키기 위해서 afterEach(cleanup)를 사용한다. (cleanup은 react-testing-library에서 지원한다)123import { cleanup } from '@testing-library/react';afterEach(cleanup); (4) component에 전달할 props를 직접 작성한 뒤에 주입해서 테스트할 수 있다.123456it('fetches and displays data', async () =&gt; { // Fetch component에 전달할 url props const url = '/greeting'; const { getByTestId } = render(&lt;Fetch url={url} /&gt;); expect(getByTestId('loading')).toHaveTextContent('Loading data...');}); (5) 방법1) nodemodules를 mocking - Axios를 통한 데이터 처리를 테스트할때 실제 HTTP를 통한 데이터 취득은 이루어지지 않는다. 그 이유는 실제 서버로부터 데이터를 취득하면 시간도 상대적으로 오래 걸리는 경우도 있기 때문이다.따라서 Axios를 통한 데이터 취득을 할때, 실제 서버로부터 취득하는 데이터를 mockup해서 테스트를 진행한다. Step1) src 폴더 아래에 __mocks__ 폴더를 만들고, axios module의 mockup을 위해 axios.js파일을 생성한다. axios.js 1234export default { // axios get() mockup function get: jest.fn().mockResolvedValue({ data: {} })}; 이렇게 mocks 폴더 내부에서 mockup axios 파일을 만들어주면, 테스트 파일내에서 axios module을 호출할때, 실제 axios module이 아닌, __mocks__ 폴더내에서 mockup한 axios module을 호출하게 된다. 테스트 파일에서 이 mockup한 axios module을 override해서 값을 새롭게 정의한 다음에 새롭게 customize된 값을 가진 axios module을 이용해서 테스트를 진행할 수 있다. 123import axiosMock from 'axios';axiosMock.get.mockResolvedValueOnce({ data: { greeting: 'hello world' } }); sample code출처 : https://www.leighhalliday.com/mocking-axios-in-jest-testing-async-functions 123456789101112131415161718192021222324252627import mockAxios from 'axios';import unsplash from '../unsplash';it('fetches data from unsplash', async () =&gt; { // setup mockAxios.get.mockImplementationOnce(() =&gt; Promise.resolve({ data: { results: ['cat.jpg'] } }) ); // work const images = await unsplash('cats'); // expect expect(images).toEqual(['cat.jpg']); expect(mockAxios.get).toHaveBeenCalledTimes(1); expect(mockAxios.get).toHaveBeenCalledWith( 'https://api.unsplash.com/search/photos', { params: { client_id: process.env.REACT_APP_UNSPLASH_TOKEN, query: 'cats' } } );}); (6) 비동기 처리부분을 테스트하는 경우, 해당 부분이 화면에 rendering되는 것을 기다려줘야 한다. (waitForElement()은 deprecated되었기 때문에 waitFor()로 대체 사용한다)12345// 실제 데이터가 넘어갔을때 화면에 보여지는 tag 부분에 &quot;resolved&quot; id를 삽입하였다.const resolvedSpan = await waitFor(() =&gt; getByTestId('resolved'));// 이제 mockup으로 넣어준 데이터가 테스트 component의 span tag에 제대로 출력이 되는지 요소검사를 한다.expect(resolvedSpan).toHaveTextContent('hello world'); (7) mockup axios function이 제대로 호출되고 있는지 확인한다.12345// axiosMock.get mockup function이 한 번 호출되었는지 체크expect(axiosMock.get).toHaveBeenCalledTimes(1);// axiosMock.get이 호출될때 같이 넘겨준 argument를 체크// axios.get() function의 argument로 지정 url을 넘겨주었는지 체크expect(axiosMock.get).toHaveBeenCalledWith(url); (8) TypeError: _axios.default.create is not a function 에러가 발생axios.create로 axios 인스턴스를 생성해서 사용하는 경우에는 axios-mock-adapter라이브러리를 사용해서 mocking을 하도록 하자. (9) 방법2) axios-mock-adpater를 이용한 axios mocking앞서 실습해 본 방법말고 이번에는 axios-mock-adapter를 이용해서 axios mocking을 해보려고 한다. mocking을 하게 되면 실제로 요청은 발생하지 않지만 마치 발생한 것처럼 작동하게 https://github.com/ctimmerm/axios-mock-adapter 실제 프로젝트를 진행하면서 작성한 코드를 첨부한다.MockAdapter 인스턴스를 생성할때의 첫 번째 인자로는 axios 라이브러리를 넣어주고, 두번째 인자로는 가짜로 200ms delay 시간을 설정해준다. Api.test.js 12345678910111213141516171819202122232425262728293031323334353637383940414243import React from 'react';import '@testing-library/jest-dom';import { render, waitFor, screen } from '@testing-library/react';import 'regenerator-runtime/runtime';import MockAdapter from 'axios-mock-adapter';import axios from 'axios';import Movie from './Pages/Movie';import { MemoryRouter } from 'react-router';describe('Movie 컨텐츠 관련 API 테스트', () =&gt; { const mock = new MockAdapter(axios, { delayResponse: 200 }); const data = { adult: false, backdrop_path: '/9yBVqNruk6Ykrwc32qrK2TIE5xw.jpg', genre_ids: [14, 28, 12, 878, 53], id: 460465, original_language: 'en', original_title: 'Mortal Kombat', overview: 'Washed-up MMA fighter Cole Young,', popularity: 6382.461, poster_path: '/xGuOF1T3WmPsAcQEQJfnG7Ud9f8.jpg', release_date: '2021-04-07', title: 'Mortal Kombat', video: false, vote_average: 7.8, vote_count: 2004 }; mock.onGet('https://api.themoviedb.org/3/movie/now_playing').reply(200, { data }); test('nowPlaying API method 테스트 - 데이터가 문제없이 화면에 로드되는지 확인', async () =&gt; { render( &lt;MemoryRouter&gt; &lt;Movie /&gt; &lt;/MemoryRouter&gt; ); // callback 함수 안의 함수가 에러를 발생시키지 않을 때까지 대기 await waitFor(() =&gt; screen.findByTestId('loading-text')); // callback 함수 안의 함수가 에러를 발생시키지 않을 때까지 대기 await waitFor(() =&gt; screen.findByTestId('section-title')); });}); reset과 restore 사용mock instance에는 reset과 restore이라는 함수를 제공한다. reset은 mock handler를 제거하는 목적으로 사용되며, 테스트 케이스별로 다른 mock 설정을 하고 싶은 경우 이 함수가 사용된다.restore은 axios에서 완전히 mocking 기능을 제거를 하며, 테스트 도중에 요청을 실제로 날릴때 사용된다.","link":"/2021/05/05/202105/210505-React_testing_library_start/"},{"title":"210509 JavaScript engine과 Web API 그리고 Callback queue","text":"자바스크립트의 비동기 코드가 runtime에 의존적이라고 하는 이유에 대한 의문을 시작으로 이번 포스팅을 시작하려고 한다. 이전에 비동기 함수를 테스팅하는 과정에서 에러가 발생을 했었는데 이때 비동기 코드가 runtime에 의존적인 결과코드이기 때문에 별도의 설정을 해줘야 했고, 이 에러를 겪고 해결해나가기 전에는 그 이유에 대해서 잘 몰랐었기 때문에 혹시 과거의 나와 같이 의문을 갖는 사람들을 위해 포스팅을 해보려고 한다. 이 이유에 대해서 이해하려면 자바스크립트 코드의 실행과정에 대해서 우선적으로 이해를 해야한다. 자바스크립트 코드의 실행과정 자바스크립트에서는 작성된 코드들은 call stack에 쌓인 후에 실행이 된다. 하지만 JS엔진에서는 비동기 작업을 지원하지 않기 때문에 코드들 중에서 비동기 작업을 Web API(browser에 의해 제공)에 위임을 한다. JS runtime engine의 역할JS runtime engine은 standard JS 코드를 실행하는 역할을 하는 JS engine(V8 for chrome)로 아래와 같은 프로그래밍 언어로써 독립적으로 요구되는 특징들을 지원한다. variable, functions, scoping, scope chaining, execution context, execution scope Web API의 역할 Web API는 Browser로부터 제공되는 wrapper로, 표준 JS 프로그래밍 언어의 일부가 아니며, 아래의 기능들을 지원한다. ajax, events(onkeyPress, onBlur와 같은 이벤트들) console.log, window object, DOM을 읽고 쓰기와 같은 기능 비동기 작업(Asynchronous task)지원 표준 JS 언어는 위와같은 기능들을 지원하지 않기 때문에 브라우저는 JS 엔진을 브라우저의 custom wrapper들로 감싸서 웹에서의 여러 조작들이 지원 가능하도록 한다.(위와 같은 이유로 browser가 JS의 runtime으로 불린다) cf) Node는 JS에게 위와 같은 기능들을 제공하는 server side에서 동작하는 runtime이다. JS 엔진은 비동기 작업들을 Web API에 위임한 후에 call stack으로 다시 돌아가서 동기 작업에 대한 작업을 지속한다.그리고 Web API는 넘겨받은 비동기 작업들을 수행한 뒤에 콜백함수를 callback queue로 넘긴다. 이벤트 루프(event loop)는 callback queue에서 대기 상태로 보관이 되었던 task(비동기 작업)를 call stack에 쌓여져있는 실행 컨텍스트들이 모두 실행이 되는 시점에 call stack으로 넘기게 된다. call stack으로 넘긴 비동기 작업의 실행이 끝나고 나면, 이벤트 루프는 task queue로부터 대기중인 또 다른 비동기 작업이 있는지 검사를 하고 있다면 call stack으로 다시 넘기게 된다. (이벤트 루프(Event loop)는 call stack에 현재 실행 중인 실행 컨텍스트(execution context)가 없는지와 task queue에 실행할 task가 있는지 반복적으로 확인을 한다.) 자바스크립트의 비동기 코드가 runtime에 의존적이라고 하는 이유는 자바스크립트 엔진은 자바스크립트 언어로써의 특징만을 지원하며, 비동기와 같은 기능들은 지원을 하지 않기 때문에 browser와 같은 별도의 runtime에 의존할 수 밖에 없기 때문이다. 자바스크립트는 싱글 스레드 기반이지만, 비동기 처리가 가능하다.자바스크립트는 싱글 스레드 기반이지만, 비동기 작업들을 runtime 환경에 위임함으로써 작업들이 완료될 때까지 다른 코드들을 실행할 수 있는 것이다. 2021/05/10 Update Synchronous와 Asynchronous 그리고 Blocking과 Non-BlockingSynchronous는 요청을 보낸 뒤에 해당 요청에 대한 응답을 받아야 다음 동작을 실행하는 방식을 말하며, Asynchronous는 요청을 보낸 후의 응답과 관계없이 다음 동작을 순차적으로 실행하는 것을 말한다.Blocking은 자신의 수행결과가 끝날 때까지 제어권을 갖고 있는 것을 말하며, Non-Blocking은 요청이 되었을때 제어권을 자신을 호출한 쪽으로 넘기고 자신을 호출한 쪽에서 다른 일을 할 수 있도록 하는 것을 말한다.","link":"/2021/05/09/202105/210509-JavaScript_Engine_and_Browser/"},{"title":"210509 ReferenceError regeneratorRuntime is not defined","text":"NodeJS 프로젝트를 진행하면서 비동기로 작성한 함수를 위한 테스트 코드를 작성하고 실행을 했는데 ReferenceError: regeneratorRuntime is not defined 에러가 발생했다.이전에 ReactJS 프로젝트를 진행하면서 테스트 코드를 작성했을때도 위와 똑같은 에러가 발생했었는데 이러한 경우에는 아래의 두 방법으로 해결을 할 수 있다. Solution 1) regenerator-runtime를 설치해서 비동기 함수를 작성한 파일의 최상단에 import 'regenerator-runtime/runtime';를 선언해주면 간단하게 해결할 수 있다. Solution 2) .babelrc 의 설정을 아래와 같이 업데이트한다. Using @babel/preset-env without that target set will result in the tests using async failing with ReferenceError: regeneratorRuntime is not defined. 123456789101112{ &quot;presets&quot;: [ [ &quot;@babel/preset-env&quot;, { &quot;targets&quot;: { &quot;node&quot;: &quot;current&quot; } } ] ]} 이 regenerator-runtime이 무엇인가 한 번 구글링을 해보니,regenerator-runtime은 compiled/transpiled 비동기 함수들을 위한 runtime을 지원해주는 녀석이라고 한다.Babel과 같은 컴파일러는 자바스크립트 syntax에 대해서 최신 문법을 이전 문법으로 변환을 해주지만, runtime에 의존적인 결과 코드(비동기 코드)들은 regenerator-runtime으로부터 지원을 받는다고 한다.","link":"/2021/05/09/202105/210509-Runtime_error/"},{"title":"210510 TTV, TTI, SSG의 개념","text":"이번 포스팅에서는 개발자라면 상식으로 알고 있어야 하는 TTV(Time To View), TTI(Time To Interact), SSG(Static Site Generation)의 개념에 대해서 정리해보려고 한다. 우선 앞의 세 가지 개념에 대해서 정리를 하기 전에 간단하게 웹의 변천사에 대해서 살펴보자. 1990년 중반까지의 웹 사이트는 static sites로, 서버에서 이미 만들어진 HTML 파일들을 받아오는 형태로 동작한다. 이 static sites의 단점은 주소를 요청할때마다 다시 서버로부터 새로운 html 파일을 받아서 re-rendering을 하기 때문에 화면이 blinking되어 사용감에 불편함을 준다. 이러한 전체 화면이 re-rendering이 되는 것을 개선하고자 1996년부터 html 문서 내에 또 다른 문서를 포함시킬 수 있는 &lt;iframe&gt;태그가 도입이 되었고, 이때부터는 페이지 내에서 부분적으로 문서를 받아와서 업데이트를 할 수 있게 되었다. 1998년이후부터는 현재 비동기 요청시에 사용되는 fetch API의 원조격인 XMLHttpRequest API가 등장을 하게 되는데, 이때부터 JSON 포멧으로 서버로부터 필요한 데이터만 받아 올 수 있게 되었다. 받아 온 데이터는 자바스크립트로 동적으로 HTML element를 생성해서 페이지에 로딩할 수 있게 되었다. 위의 방식이 2005년부터 공식적으로 ajax라는 이름으로 불려지면서, SPA의 방식이 본격적으로 도입이 되기 시작했다.하나의 웹 페이지에서 필요한 부분의 일부만 렌더링하기 때문에 사용자에게 앱을 사용하는 것과 같은 사용감을 주기 때문에 웹 앱이라고 불리기 시작했다. SSR과 CSR의 개념에 대해서는 이전에 별도로 블로그 포스팅을 해뒀기 때문에 아래 링크의 포스팅을 참고하도록 하자. https://leehyungi0622.github.io/2021/04/26/202104/210426-SSR_and_CSR/ CSR의 문제를 보안하기 위한 SSRCSR방식은 처음에는 서버로부터 빈 화면만 받아오게 되고, 이후에 서버로부터 필요한 자바스크립트 파일과 필요한 각종 라이브러리 및 소스코드를 받아오게 되는데 사이즈가 크기 때문에 초기의 긴 로딩 시간과 검색엔진 최적화(SEO) 문제라는 두 가지 문제점을 가지고 있다.이러한 이유 때문에 1990년 중반부터 사용된 static sites에서 영감을 받은 SSR방식이 도입되었다.하지만 SSR 방식의 도입이 앞의 두 가지 문제점은 어느정도 개선을 할 수 있었지만, 완벽한 솔루션이 되지는 못했다. 그 이유는 첫 페이지 로딩시에 blinking issue와 서버 과부화라는 두 가지 문제점 때문이었다. 또한 초기에 화면이 빠르게 로딩되더라도 사용자와 interact하기 위한 대기시간이 필요(화면이 로딩되어 클릭을 했는데 아무런 반응이 일어나지 않는 경우)했고, 이 부분을 이해하기 위해서는 이번 포스팅에서 다룰 TTV(Time To View)와 TTI(Time To Interact)의 개념에 대해서 알고 있어야 한다. TTV(Time To View)와 TTI(Time To Interact)TTV란 사용자에게 페이지가 보여지기 위한 시간을 의미하며, TTI란 사용자와 상호작용이 가능(Interactable)하기까지의 시간을 의미한다.CSR의 경우에는 처음에 서버로부터 빈 html파일을 받아오고, 이후에 별도의 서버 요청에 의해 웹 페이지에서 필요한 모든 로직들이 담긴 자바스크립트 파일들(동적으로 html을 생성할 수 있는 웹 어플리케이션 로직이 담겨져 있는 자바스크립트 파일들)을 받아오게 된다. 바로 이 시점에서 사용자는 페이지를 볼 수 있게 되고 동시에 상호작용이 가능하게 된다.이처럼 CSR은 TTV와 TTI의 시점이 일치한다. 반면에 SSR의 경우에는 초반에 서버 사이드에서 완성된 html 파일을 받아와서 화면에 렌더링을 해주기 때문에 사용자는 바로 페이지를 확인할 수 있다.(TTV) 하지만 아직 동적으로 제어할 수 있는 자바스크립트 파일은 받아오지 않았기 때문에 렌더링된 페이지를 클릭해도 사용자와 아무런 상호작용이 일어나지 않게 된다. 따라서 이후에 동적으로 처리하기 위한 별도의 자바스크립트 파일을 서버에 요청을 해서 받아온 이후에 비로소 사용자와 상호작용이 가능한 웹 페이지가 구성이 된다.(TTI)이처럼 SSR은 TTV와 TTI의 시점이 다르다.(공백시간이 존재) 웹 사이트 성능분석을 위한 TTI와 TTVCSR 방식으로 웹 어플리케이션을 개발을 할때에는 사용자에게 최종적으로 번들링해서 보내주는 자바스크립트 파일들을 효율적으로 분할(code splitting)해서 첫 페이지 로딩시에 필요한 자바스크립트 파일들만 보내도록 구성하는 작업이 필요하다.반면에 SSR은 TTV와 TTI 간에 존재하는 시간 공백을 어떻게 하면 효율적으로 줄일 수 있는지 고민해야 한다. SSG (Static Site Generation)요즘에는 SSR, CSR만을 고집하지 않고 SSG 방식으로 개발을 한다. 예를들어 React를 SSR에 최적화된 Gatsby 라이브러리와 함께 개발을 하게 되면, 정적으로 웹 페이지를 생성을 해서 서버에 미리 배포해 둘 수 있다.하지만 모든 페이지가 정적인 것이 아닌, 추가적으로 데이터를 서버로부터 받아오거나 동적으로 처리해야 되는 부분이 있다면, 페이지와 함께 자바스크립트 파일을 별도로 받아올 수 있기 때문에 동적이 페이지의 처리도 가능하다.React와 Gatsby 라이브러리의 조합 이외에도 React와 NextJS의 조합으로도 개발이 많이 되고 있다. NextJS는 SSR이외에도 static generation, no pre-rendering, pre-rendering을 지원을 하기 때문에 SSR과 CSR을 적절히 조합을 해서 웹 어플리케이션을 개발을 할 수 있다. 웹 어플리케이션을 개발을 할 때, TTV와 TTI를 고려해서 적절하게 SSR, CSR, SSG의 방식을 조합해서 개발을 한다면 사용자에게 좋은 사용감을 주는 웹 어플리케이션을 개발할 수 있다.앞으로 개인적으로 하는 사이드 프로젝트도 이러한 사용자의 편의성을 염두해두고 진행을 해봐야겠다. 참고 :https://huspi.com/blog-open/definitive-guide-to-spa-why-do-we-need-single-page-applications","link":"/2021/05/10/202105/210510-TTV_TTI_SSG/"},{"title":"210510 Node.js TDD Practice 네번째 이야기 - 비동기 요청 에러에 대한 테스트 코드 작성","text":"Error handling이번 포스팅에서는 req.body로부터 넘겨받은 데이터를 MongoDB에 저장할때 발생할 수 있는 에러 케이스에 대해 에러 처리를 하기 위한 테스트 코드 작성에 대한 내용을 작성해보려고 한다. 현재 테스트에 있어서 프론트단이 별도로 개발되어있지 않기 때문에 Postman을 이용해서 임의로 만든 API를 테스트 할 것이다. Postman을 사용하면 API 개발의 생산성을 높여 줄 수 있으며, Request를 임의로 전달해서 테스트를 할 수 있다. Postman의 사용은 위의 캡처와 같이 우선 테스트할 Request의 method를 선택을 하고, Endpoints를 작성해준다. 요청시에 Body에 별도로 데이터를 담아서 전달해야되는 경우에는 Body 옵션을 선택해서 전송하고자하는 데이터의 타입을 지정해서 데이터를 작성해주면 된다. 모든 설정이 끝난뒤에 Send버튼을 클릭해주면, 앞서 endpoint에 작성해준 URL과 mapping되는 router의 callback 함수(controller)가 앞서 Postman에서 설정한 내용을 기반으로 실행이 된다.(Body에 별도의 데이터를 담아서 전달한 경우에는 callback 함수(controller)의 req.body를 통해 전달이 된다.) 1234567891011121314app.use('/api/products', productRoutes);router.post('/', productController.createProduct);// 비동기 요청에서 발생되는 에러에 대한 예외처리exports.createProduct = async (req, res, next) =&gt; { try { const newProduct = await productModel.create(req.body); console.log('createProduct', newProduct); res.status(201).json(newProduct); } catch (error) { next(error); }}; 동기함수의 경우에는 Request(요청)에 대한 Response(응답)가 와야 그 다음 Request를 받아서 처리할 수 있다. 비동기 함수의 경우에는 Request에 대한 Response와 상관없이 다음 요청을 연달아 받고 받은 비동기 요청의 순서에 맞게 순차적으로 Response를 받아 처리하게 된다. 따라서 별도로 비동기 요청(Async Request)에 대한 결과를 출력할때에는 await-async 또는 .then()을 통해서 비동기 요청에 대한 결과 값을 출력해야 한다. 실제 단위 테스트 코드에서도 비동기 처리 코드에 대한 테스트 코드의 경우에 await-async로 비동기 처리를 해줘야 한다. 임의로 에러 발생시키기Postman에서 Request의 조건을 만족하지 않는 데이터를 body를 통해 전달하게 되면, 처리가 종료되지 않고 hang에 걸리게 된다.그 이유는 API 부분에서 별도로 에러를 처리하는 부분을 작성해주지 않았기 때문이다. 1234567test('should handle errors', async () =&gt; { const errorMessage = { message: 'description property missing' }; const rejectedPromise = Promise.reject(errorMessage); productModel.create.mockReturnValue(rejectedPromise); await productController.createProduct(req, res, next); expect(next).toBeCalledWith(errorMessage);}); 위의 코드에서 작성한 테스트 코드의 전제 조건은 MongoDB에서 처리하는 부분은 문제가 없다는 것을 가정으로 하기 때문에 MongoDB에서 처리하는 에러 메시지 부분은 Mock 함수를 이용해서 작성을 해준다. 비동기 요청(Async Request)에 대해서 성공을 하는 경우에는 Promise.resolve(value)를 값으로 반환하고, 에러인 경우에는 Promise.reject(reason)이 반환된다. resolve 메소드의 인자 값은 then 메소드를 통해 처리가 가능하다. 따라서 위의 테스트 코드의 경우에도 임의로 만들어준 에러 메시지 객체를 Promise.reject()로 감싸서 create 함수의 반환값으로 임의 지정하고 있다. 동기요청에 대한 에러의 경우에는 Express가 알아서 처리를 해주지만, 비동기 요청에 대한 에러는 Express에서 별도의 처리를 해주지 않기 때문에 아래와 같이 next 인자를 통해서 비동기 에러 부분을 넘겨줘야 한다. 123456789exports.createProduct = async (req, res, next) =&gt; { try { const newProduct = await productModel.create(req.body); console.log('createProduct', newProduct); res.status(201).json(newProduct); } catch (error) { next(error); }};","link":"/2021/05/10/202105/210510-nodejs-tdd-practice/"},{"title":"210510 anchor 태그에서 target&#x3D;&quot;_blank&quot; 속성의 보안적 문제","text":"anchor 태그에서 target=”_blank” 속성의 보안적 문제anchor 태그에 연결되어있는 페이지를 새로운 창에 열고자 할때 속성에 target=”_blank”를 넣어주는데, 보안적으로 문제가 있다. 바로 Tabnabbing이라는 공격의 위험성이 있다는 것이다.Tabnabbing이란 HTML 문서 내에서 링크를 클릭했을때 새롭게 열린 탭이나 페이지에서 기존 문서의 location을 피싱 사이트로 변경해서 정보를 탈취하는 공격기술을 말한다.이 공격은 이메일이나 커뮤니티에서 쉽게 사용될 수 있기 때문에 주의해야 한다.따라서 anchor 태그에 target=”_blank” 속성을 넣어 줄 때에는 반드시 rel=&quot;noreferrer noopener&quot;라는 속성을 같이 넣어서 취약점을 보완해야 한다.","link":"/2021/05/10/202105/210510_anchor_tag_security_issue/"},{"title":"210517 React with NextJS TIL - Next Redux Wrapper, HYDRATE, redux-thunk, 제너레이터 함수에 대한 이해, redux-saga","text":"2021/05/17 - redux-thunk 내용추가 Next Redux Wrapper일반적으로 React에 Redux를 붙일 때에는 하나의 Redux store만 존재하기 때문에 어렵지 않다.하지만 Next.js에서 Redux를 사용하게 되면 여러 개의 Redux store가 생성된다.그 이유는 Next.js에서는 User가 Request를 보낼때마다 Redux store를 새로 생성하기 때문이다. 그리고 Next.js에서 제공하는 getInitialProps와 getServerSideProps에서도 Redux store에 접근이 가능하도록 해야되기 때문에 NextJS에서 Redux를 붙일때 꽤나 복잡하다. 하지만 이를 간편하게 해주는 라이브러리가 있는데 바로 Next Redux Wrapper이다. next redux wrapper 설치/store/configureStore.js 1234567891011121314import { createWrapper } from 'next-redux-wrapper';import { createStore } from 'redux';import reducer from '../reducers';const configureStore = () =&gt; { const store = createStore(reducer); return store;};const wrapper = createWrapper(configureStore, { debug: process.env.NODE_ENV === 'development'});export default wrapper; _app.js 1234567891011121314151617181920...import wrapper from '../store/configureStore';const App = ({ Component }) =&gt; { return ( &lt;&gt; &lt;Head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;Next.js with Redux sample&lt;/title&gt; &lt;/Head&gt; &lt;Component /&gt; &lt;/&gt; )}App.propTypes = { Component: PropTypes.elementType.isRequired,}export default wrapper.withRedux(App); Next에서는 버전 6 이후부터는 일반적으로 Redux를 사용했을때 Provider로 감싸주는 부분이 생략되었다.(별도로 Provider로 감싸주지 않아도 알아서 처리해준다) 이제 각 각의 컴포넌트에서 만들어준 store 객체를 이용해서 action을 dispatch해주게 되면 store에 있는 상태값이 바뀌게 된다. 1234store.dispatch({ type: 'CHANGE_NICKNAME', data: 'kim'}); HYDRATE만들어 준 reducer에서 action.type으로 사용할 HYDRATE를 next-redux-wrapper로부터 import해준다.HYDRATE의 등장은 SSR을 위한 것으로, getInitialProps와 getServerSideProps에서도 Redux store에 접근이 가능하도록 하기 위한 처리이다. 123456789101112131415import { HYDRATE } from 'next-redux-wrapper';const rootReducer = (state=INITIAL_STATE, action) =&gt; { switch(action.type) { case HYDRATE: return { ...state, ...action.payload } ... // reducer 초기화될때를 한 번 실행이 되기 때문에 default를 넣어줘야 한다. default: return state; }} 분리된 reducer는 아래와 같이 combineReducers를 사용해서 합쳐준다.(기존의 방식과 다른 부분은 SSR을 위해 HYDRATE 상태를 포함하는 reducer가 포함시키는 부분이다) 1234567891011121314151617181920212223import { HYDRATE } from 'next-redux-wrapper';import user from './user';import post from './post';import { combineReducers } from 'redux';const rootReducer = combineReducers({ index: (state = {}, action) =&gt; { switch (action.type) { case HYDRATE: return { ...state, ...action.payload }; default: return state; } }, user, post});export default rootReducer; styled-components와 SSRSSR에서 styled-components를 적용하기 위해서는 별도의 설정이 필요하다.SSR일때는 Front 서버에서 HTML을 데이터와 합쳐서 화면에 그려주게 된다. styled-component는 SSR에 대한 별도의 설정이 없다면, styled-components가 적용이 안된 상태로 화면에 렌더링되는 것이다. 상태 데이터 속성 이름과 시퀄라이즈DB쪽에서는 시퀄라이즈를 통해 특정 정보가 또 다른 정보과 관계가 있다면, 해당 데이터들을 함쳐서 첫 문자를 대문자로 바꿔준다.따라서 상태값 속성 중에 상태 자체로써만 사용되는 데이터 속성의 경우에는 전부 소문자 표기로 속성이름을 넣어주고, 다른 정보와 관련이 있는 속성의 경우에는 첫 문자를 대문자로 해서 넣어주도록 한다. front/reducers/post.jsUser, Images, Comment는 복수의 정보들을 합쳐서 주기 때문에 아래와 같이 첫 문자를 대문자 표기로 하였다. (프론트 개발시에는 서버쪽에서 어떻게 데이터를 보낼 것인지에 대해 서버 개발자에게 사전에 협의하는 것이 중요하다) 1234567891011121314151617181920212223242526272829303132333435363738394041const initialState = { mainPosts: [ { id: 1, User: { id: 1, nickname: 'lee' }, content: 'first content', Images: [ { src: 'https://images.pexels.com/photos/3998365/pexels-photo-3998365.png' }, { src: 'https://images.pexels.com/photos/12064/pexels-photo-12064.jpeg' }, { src: 'https://images.pexels.com/photos/159775/library-la-trobe-study-students-159775.jpeg' } ], Comments: [ { User: { nickname: 'lee' }, content: 'comment1' }, { User: { nickname: 'kim' }, content: 'comment2' } ] } ], // 업로드된 게시글의 이미지 경로 imagePaths: [], // 게시글 업로드가 완료되었을때 postAdded: false}; redux-thunkredux의 기능을 향상시켜주는 middleware인 redux-thunk는 redux가 비동기 액션을 dispatch할 수 있도록 도와준다. 하나의 비동기 액션에서 dispatch를 여러번 할 수 있다.(비동기 액션에 여러 개의 동기 액션 처리 가능) 공식 사이트 : https://github.com/reduxjs/redux-thunk middleware는 3단 고차함수의 형태를 가진다.본래 동기 액션의 경우에는 객체형태이지만, 비동기 액션의 경우에는 함수이기 때문에 지연함수로써 나중에 실행하도록 처리한다. 123456789// 동기 액션 처리하기 이전에 각 action의 속성을 logging해주는 middleware// redux dev tools 대체const loggerMiddleware = ({ dispatch, getState }) =&gt; (next) =&gt; (action) =&gt; { console.log(action); return next(action); }; 로그인, 로그아웃 처리의 경우, 서버로 한 번 들렸다가 처리되기 때문에 단순 요청 액션 객체가 있는 것이 아닌, 각 각의 처리에 대해서 Request, Success, Failure에 해당하는 action이 분리해서 존재한다. 로그인의 경우 redux-thunk 사용 예시thunk는 아래와 같이 dispatch를 여러번 처리할 수 있다. 123456789101112131415export const loginAction = (data) =&gt; { return (dispatch, getState) =&gt; { // getState는 initial state를 가져온다. const state = getState(); dispatch(loginRequestAction()); axios .post('/api/login') .then(() =&gt; { dispatch(loginSuccessAction()); }) .catch((err) =&gt; { dispatch(loginFailureAction(err)); }); };}; 제너레이터 함수의 이해redux-saga에서는 제너레이터 함수를 사용하기 때문에 우선적으로 제너레이터 함수에 대해 이해가 필요하다. 12345678910111213141516171819const gen = function* () {};// generator function은 .next()로 함수 내부를 실행시켜 줄 수 있다.gen().next();const gen = function* () { console.log(1); yield; console.log(2); yield; console.log(3); yield 4;};const generator = gen();generator; // get {&lt;suspended&gt;}generator.next(); // 1 {value: undefined, done: false}generator.next(); // 2 {value: undefined, done: false}generator.next(); // 3 {value: undefined, done: false}generator.next(); // 4 {value: 4, done: false}generator.next(); // {value: undefined, done: true} 제너레이터의 yield가 있는 부분은 함수의 중단점이다.자바스크립트 함수의 특징은 몸체 부분이 전부 실행되지만, 제너레이터 함수의 경우에는 yield 지점에서 멈추고, yield 와 함께 숫자나 문자를 넣어주면 value로 반환이 된다. 함수 실행 도중에 중간에 멈추게 하기 위한 목적에서 제너레이터 함수가 사용되기도 하며, 제너레이터 함수는 테스트 시에 매우 유용하다. 12345678910// 절대 멈추지 않는 함수const gen = function* () { while (true) { yield 'infinite'; }};// saga에서는 무한 반복이 아니라 매번 .next()호출될때마다 중단된다.const g = gen();g.next(); // {value: 'infinite', done: false} 위와같은 특성을 이용해서 이벤트 리스너와 같은 처리를 할 수 있다. redux-saga실습 Repository :https://github.com/LeeHyungi0622/react-nextjs-twitter 처리에 대한 delay를 함으로써 비동기 처리와 같은 효과를 줄 수 있다. 더블 클릭에 대한 요청에 대해서 take latest처리(thunk에서는 클릭한 요청이 모두 요청된다) 스크롤 이벤트 리스너에 비동기 요청을 처리하는 경우, DDos 공격과 같이 무한으로 연속된 형태로 서버에 요청을 보낼 수 있다.이러한 문제를 redux-saga에서는 throttle과 debounce를 적용해서 1초에 3번이상 액션이 발생하면 차단하는 것과 같은 조건처리 기능을 추가할 수 있다. throttle은 \b스크롤링 이벤트 구현시에 많이 사용되고, debounce의 경우에는 검색창에 검색어를 입력할때 매번 결과창이 업데이트되면 정신없을때, 입력한 검색어가 완성이 되었을때 결과창이 없데이트 될 수 있도록 처리할때 사용한다. redux-sage 설치123$ npm i redux-saga# Next.js에서 redux-saga를 적용하기 위해$ npm i next-redux-saga thunk와는 다르게 sagaMiddleware에는 추가적인 기능이 있다. configureStore.js 12345678910111213141516import createSagaMiddleware from 'redux-saga';const configureStore = () =&gt; { const sagaMiddleware = createSagaMiddleware(); // 개발용일때에만 devTools를 붙인다. (보안적 요소 고려) // history가 쌓이게 되면 메모리도 많이 잡아먹는다. const middlewares = [sagaMiddleware, loggerMiddleware]; const enhancer = process.env.NODE_ENV === 'production' ? // 개발할때에는 redux-saga, thunk만 추가해준다. compose(applyMiddleware(...middlewares)) : composeWithDevTools(applyMiddleware()); const store = createStore(reducer, enhancer); store.sagaTask = sagaMiddleware.run(rootSaga); return store;}; _app.js 123import withReduxSaga from 'next-redux-saga';...export default wrapper.withRedux(withReduxSaga(App)); /front/sagas/index.js saga에는 다양한 effects (all, fork, call, put)가 존재한다. (1) all : 배열 내에 작성해 준 제너레이터 함수들을 일괄 실행시켜준다.(2) fork : 제너레이터 함수를 실행시켜주는 역할로 비동기 함수를 실행해준다. (response와 상관없이 요청을 보내고 바로 다음 요청을 실행한다) (3) call : fork와 같이 제너레이터 함수를 실행하는 역할을 하지만 fork와는 다르다.call은 동기 함수 실행을 담당한다.(response를 기다렸다가 다음을 실행) (4) take : 첫번째 인수의 액션이 실행되면, 두번째 인수로 넣어준 제너레이터 함수가 실행된다.take의 단점은 일회성이다. 따라서 제너레이터 함수 내에서 while(true){ }로 감싸주거나, takeEvery를 사용한다.takeLatest를 사용해서 실수로 두 번 연달아서 클릭 이벤트가 발생했을때 마지막 이벤트만 실행될 수 있도록 처리해준다.(응답을 하나 취소하는 것이지 서버로의 요청은 취소가 되지 않는다) 123function* watchLogIn() { yield takeLatest('LOG_IN_REQUEST', logIn);} throttle을 이용해서는 1회 요청의 제한시간을 지정해줄 수 있다. 123function* watchAddPost() { yield throttle('ADD_POST_REQUEST', addPost, 10000);} (4) put : dispatch 객체와 같은 기능 root saga를 만들어서 내부에 비동기 처리들을 일괄 작성해둔다. thunk에서는 비동기 액션 creator를 직접 실행하지만, saga에서는 비동기 액션 creator가 event listener와 같은 느낌을 준다. redux-saga 예시코드 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import { all, fork, call, put, take } from 'redux-saga/effects';import axios from 'a';// generator(*)// 패턴대로 코딩을 하기 때문에 익숙해지면 된다.// 제너레이터 함수가 아닌 일반함수이다.function logInAPI(data) { return axios.post('/api/login', data);}// 로그인시 사용자 정보를 넘겨 줄 때에는 아래와 같이 action 매개변수를 통해 data를 받아서 넘겨준다.function* login(action) { try { // result에 결과값이 담긴 상태로 다음 액션이 실행되야 하기 때문에 반드시 call로 작성해준다. // logInAPI에 action.data를 넣어준다. const result = yield call(logInAPI, action.data); yield put({ type: 'LOG_IN_SUCCESS', // 성공 결과는 result.data에 담겨있다. data: result.data }); } catch (err) { yield put({ type: 'LOG_IN_FAILURE', // 실패 결과는 err.response.data에 담겨있다. data: err.response.data }); }}function logOutAPI() { return axios.post('/api/logOut');}function* logout() { try { const result = yield call(logOutAPI); yield put({ type: 'LOG_OUT_SUCCESS', data: result.data }); } catch (err) { yield put({ type: 'LOG_OUT_FAILURE', data: err.response.data }); }}function addPostAPI() { return axios.post('/api/logOut');}function* addPost() { try { const result = yield call(addPostAPI); yield put({ type: 'ADD_POST_SUCCESS', data: result.data }); } catch (err) { yield put({ type: 'ADD_POST_FAILURE', data: err.response.data }); }}function* watchLogIn() { yield take('LOG_IN_REQUEST', login);}function* watchLogOut() { yield take('LOG_OUT_REQUEST', logout);}function* watchAddPost() { yield take('ADD_POST_REQUEST', addPost);}export default function* rootSaga() { yield all([fork(watchLogIn), fork(watchLogOut), fork(watchAddPost)]);} redux-saga를 쪼개서 reducer와 연결하기위와같이 일괄적으로 index.js에 redux-saga에 대한 코드를 작성하게 되면, 코드가 길어지게 된다.reducer를 합칠때와 같이 combineReducer와 같은 것을 사용해서 합칠 필요가 없다. 실습 Repository :https://github.com/LeeHyungi0622/react-nextjs-twitter reducer와 연결 sagas/ 폴더 이하의 파일에는 비동기 처리에 대한 코드를 작성해주고, reducer에서는 비동기 처리에 대한 각 각의 action.type에 대한 조건 처리를 추가해주면 된다. redux-saga 관련 코드가 실행되는 흐름에 대해서 노트 필기를 해보았다. 복습할때 참고하도록 하자.","link":"/2021/05/17/202105/210514-React_with_NextJS/"},{"title":"210511 Node.js TDD Practice 다섯번째 이야기 - 통합 테스트 (Integration Test) 코드 작성","text":"통합 테스트(Integration Test)모듈을 통합하는 단계에서 수행하는 테스트로, 단위 테스트를 통해서 모듈들이 모두 잘 작동하는 것을 확인한 뒤에 모듈들을 서로 연동하여 테스트를 수행하는 것을 말한다. Supertest단위 테스트에서 jest 모듈을 사용해서 테스트를 했다면, 통합 테스트는 supertest라는 모듈을 이용해서 구현을 한다. 단위 테스트에서는 MongoDB 부분은 문제가 없다는 가정하에 mock 함수로 처리를 했는데 통합 테스트에서는 supertest를 사용해서 실제로 요청을 보내서 테스트를 진행한다. 통합 테스트 코드 작성통합 테스트 코드는 실제 작성한 코드의 로직을 기반으로 작성한다. 예를들어 아래와 같이 MongoDB에 req.body를 통해 넘겨받은 데이터를 새로 추가하고 추가된 데이터를 별도의 변수에 담아 response의 status code가 201인 경우에 json 데이터로 함께 전달하고 있다. 123456789exports.createProduct = async (req, res, next) =&gt; { try { const newProduct = await productModel.create(req.body); console.log('createProduct', newProduct); res.status(201).json(newProduct); } catch (error) { next(error); }}; 위의 코드를 통합 테스트 코드로 작성을 하면, 아래와 같이 통합테스트 코드를 구현할 수 있다. 12345678910111213const request = require('supertest');const app = require('../../server');const newProduct = require('../data/new-data.json');test('POST /api/products', async () =&gt; { // 실제 코드와 같이 요청을 보내고 함께 보낸 데이터를 response 변수에 담는다. const response = await request(app).post('/api/products').send(newProduct); // 위의 response 객체의 상태코드를 확인 expect(response.statusCode).toBe(201); // response의 body(json) 데이터를 확인을 한다. expect(response.body.name).toBe(newProduct.name); expect(response.body.description).toBe(newProduct.description);}); 에러에 대응하는 통합 테스트 코드 작성아래와 같이 인위적으로 에러를 발생시키기 위해 MongoDB의 schema에 위반되는 데이터를 넣어주었다.넣어준 다음에 response를 통해 에러코드가 500번인 것을 확인하고, response.body를 통해서 에러 메시지를 확인하도록 한다.하지만 이 시점에서 에러 메시지를 확인하면 빈 객체 (Object {})가 반환되는 것을 알 수 있다. 123456789test('should return 500 on POST /api/products', async () =&gt; { const response = await request(app) .post('/api/products') // 에러를 인위적으로 발생시키기 위해 아래와 같은 데이터 전달 .send({ name: 'phone' }); expect(response.statusCode).toBe(500); console.log('response.body', response.body); expect(response.body).toStrictEqual({ message: '' });}); 이 문제를 해결하기 위해서는 express가 에러를 어떻게 핸들링하는지에 대한 이해가 필요하다. express에서는 middleware에서 에러가 발생하면 이 에러를 에러 처리기(handler)로 보내준다.즉시 에러를 보내주기 때문에 다음으로 실행되어야 할 middleware 중에 에러처리기가 아닌 middleware는 처리를 생략하게 된다. 에러처리기는 아래와 같이 인자로 총 4개의 인자가 들어간다. 123app.use(function (error, req, res, next) { res.json({ message: error.message });}); 참고 :http://thecodebarbarian.com/80-20-guide-to-express-error-handling.html 하지만 비동기 요청으로 인한 에러는 controller에서 작성한 콜백함수의 세번째 인자인 next를 통해서 전달을 해줘야 한다. 그렇지 않으면, 에러 처리기에서 위의 에러 메시지를 받지 못하기 때문에 서버가 crash된다. 따라서 아래와 같이 발생한 에러에 대해서는 middleware의 세번째 인자인 next를 통해서 error를 넘겨줘야 한다.(이 부분은 이미 단위 테스트 학습과정에서 학습을 하였다) 123456789101112const app = require('express');app.get('*', function (req, res, next) { setImmediate(() =&gt; { next(new Error('woops')); });});app.use(function (error, req, res, next) { res.json({ message: error.message });});app.listen(3000); 에러 핸들러 작성123app.use((error, req, res, next) =&gt; { res.status(500).json({ message: error.message });});","link":"/2021/05/11/202105/210511-nodejs-tdd-practice/"},{"title":"210514 Redux vs Mobx vs ContextAPI","text":"Redux Mobx ContextAPI 원리가 간단하다. 어플리케이션이 별로 크지 않은 경우에 사용하도록 한다. 에러에 대한 추적이 가능하다. 에러에 대한 추적이 불가능하다. 코드의 양이 많다. 코드의 양이 적다.(생산성이 높다) 비동기 요청을 했을때에는 요청에 대한 실패에 대비를 해야 되기 때문에 이런 처리부분에 대해 해당 상태관리 라이브러리가 제공하는지에 대해 고려해서 사용해야 한다.(대기, 성공, 실패) 이 부분에 대해 Redux와 Mobx는 제공이 되지만, ContextAPI의 경우에는 직접구현을 해야한다.","link":"/2021/05/14/202105/210514-Redux_Mobx_ContextAPI/"},{"title":"210515 React with NextJS TIL - Next.js에서 styled-components를 사용하기 위한 설정","text":"참고 : https://github.com/vercel/next.js/tree/master/examples/with-styled-components Next.js에서 styled-components 사용하기Next.js에서 styled-components를 사용하려면 몇 가지 설정이 필요하다. (1) babel-plugin-styled-components 설치하기 이 패키지를 설치하는 이유는 Next에서 최초로 SSR이 된 이후에 내부 라우팅을 통해 페이지가 이동하면서 CSR을 하게 되는데, className의 해시값이 서버에서 생성되는 값과 브라우저에서 생성되는 값이 불일치하게 되면서 에러가 발생하기 때문에 설치해줘야 한다. 1npm i -D babel-plugin-styled-components (2) .babelrc 파일 생성하기 1234{ &quot;presets&quot;: [&quot;next/babel&quot;], &quot;plugins&quot;: [[&quot;styled-components&quot;, { &quot;ssr&quot;: true }]]} (3) pages 폴더 아래에 _document.js 파일 생성하기 참고: https://nextjs.org/docs/advanced-features/custom-document 1234567891011121314151617181920212223242526272829303132333435import Document from 'next/document';// Import styled-components ServerStyleSheetimport { ServerStyleSheet } from 'styled-components';export default class MyDocument extends Document { static async getInitialProps(ctx) { // ServerStyleSheet instance를 생성한다. const sheet = new ServerStyleSheet(); // Page에 있는 컴포넌트에서 style을 검색한다. const originalRenderPage = ctx.renderPage; try { ctx.renderPage = () =&gt; // page의 컴포넌트에 적용된 스타일을 검색한다. originalRenderPage({ enhanceApp: (App) =&gt; (props) =&gt; sheet.collectStyles(&lt;App {...props} /&gt;) }); const initialProps = await Document.getInitialProps(ctx); return { ...initialProps, styles: ( &lt;&gt; {initialProps.styles} {sheet.getStyleElement()} &lt;/&gt; ) }; } finally { sheet.seal(); } }}","link":"/2021/05/15/202105/210515-React_with_NextJS/"},{"title":"210516 ReactJS TIL - 컴포넌트 폴더 구조, 해쉬태그 구현을 위한 정규표현식","text":"컴포넌트 폴더 구조컴포넌트를 작성할때 각각의 개별 컴포넌트를 하나의 개별 폴더로 분리해서 작성해주는 것이 좋다.그 이유는 styled-components로 만든 컴포넌트들을 하나의 파일에 같이 작성을 해주게 되면 가독성에 좋지 않기 때문에 하나의 컴포넌트 폴더 내부에 index.js와 styles.js로 분리해서 styles.js 파일 내에는 styled-components로 작성해준 컴포넌트들만 따로 분리해서 작성해주도록 한다. ex)/components/ImagesZoom/index.js/components/ImagesZoom/styles.js 해쉬 태그를 위한 정규 표현식참고 사이트 : https://regexr.com/ (1) //g : g는 여러개 요소를 찾는 옵션(2) /#./g : .은 # 뒤에 한 개 문자까지 선택(3) /#../g : ..은 # 뒤에 두 개 문자까지 선택(4) /#.+/g : .+는 # 뒤의 모든 문자들을 선택(5) /#[^\\s#]+/g : 공백(\\s)을 제거하고 다음 해쉬 태그 기준으로 끊기 split 메서드와 함께 사용하는 경우에는 아래와 같이 정규표현의 조건부를 괄호로 감싸줘야 한다. 123/(#[^\\s#]+)/g;'첫 번째 게시글 #해시태그 #익스프레스'.split(/(#[^\\s#]+)/g);","link":"/2021/05/16/202105/210516-React-review_study/"},{"title":"210525 React eslint 설정, shortid","text":"React ESLintESLint는 자바스크립트 문법 중에 에러가 있는 곳에 표시를 해주는 도구이다. 개발자가 직접 정의한 내용대로 코드를 점검하고 에러가 있으면 코드에 직접 표시를 해주기 때문에 매우 편리하며, 문법 에러뿐만 아니라 다른 개발자들과 협업을 할때 코딩 스타일을 정할 수 있어 매우 유용하다. 12$ npm i -D babel-eslint eslint-config-airbnb eslint-plugin-import eslint-plugin-react-hookseslint-plugin-jsx-a11y .eslintrc 1234567891011121314151617181920212223242526272829303132{ // 기본 eslint가 코드를 해석하지 않고, // babel-eslint가 코드를 해석하기 때문에 // 최신 문법도 에러를 발생시키지 않는다. &quot;parser&quot;: &quot;babel-eslint&quot;, &quot;parserOptions&quot;: { &quot;ecmaVersion&quot;: 2020, &quot;sourceType&quot;: &quot;module&quot;, &quot;ecmaFeatures&quot;: { &quot;jsx&quot;: true } }, &quot;env&quot;: { &quot;browser&quot;: true, &quot;node&quot;: true, &quot;es6&quot;: true }, // recommend는 코드에 대한 규제가 약하기 때문에 airbnb &quot;extends&quot;: [&quot;airbnb&quot;], &quot;plugins&quot;: [&quot;import&quot;, &quot;react-hooks&quot;], &quot;rules&quot;: { &quot;jsx-a11y/label-has-associated-control&quot;: &quot;off&quot;, &quot;jsx-a11y/anchor-is-valid&quot;: &quot;off&quot;, &quot;no-console&quot;: &quot;off&quot;, &quot;no-underscore-dangle&quot;: &quot;off&quot;, &quot;react/forbid-prop-types&quot;: &quot;off&quot;, &quot;react/jsx-filename-extension&quot;: &quot;off&quot;, &quot;react/jsx-one-expression-per-line&quot;: &quot;off&quot;, &quot;object-curly-newline&quot;: &quot;off&quot;, &quot;linebreak-style&quot;: &quot;off&quot; }} shortid로 랜덤한 id 만들어주기(dummy data)설치 1$ npm i shortid 사용 123import shortId from 'shortid';shortId.generate(); faker개발할때 dummy로 넣어서 사용할 데이터 제공해준다. 분리된 reducer에서 서로 종속관계에 있는 데이터를 업데이트하는 방법상태 데이터를 변경하기 위해서는 action을 통해서 변경할 수 있다. 따라서 reducer로 분리는 되어있지만 각 상태 데이터 내에 서로 종속관계에 있는 데이터를 포함하고 있다면, 이런 경우에는 이에 맞는 새로운 action을 추가해주면 된다.","link":"/2021/05/25/202105/210525-React-review_study/"},{"title":"210524 기록과 지속적인 학습의 관계","text":"요즘 공부하면서 느끼지만, 개발에 쓰이는 기술들은 트렌드가 정말 빠르게 바뀌는 것 같다. 기존에 사용했던 기술의 단점이 보완되서 더 나은 새로운 기술들이 등장하고, 그렇게 되면서 자연스럽게 학습해야 되는 양도 자연스럽게 늘어가는 것 같다.물론 그 근간이 되는 기본 지식을 탄탄하게 쌓는다면, 그 변화의 물살을 타고 서핑하는 기분으로 배움의 즐거움을 느낄 수 있을 것이다. 이번 포스팅에서는 내가 작업한 것들을 종합해서 정리하기 위한 목적의 웹 페이지를 만들면서 자연스럽게 내가 여지까지 만들었던 깃허브 레포지토리와 블로그의 글들을 전체적으로 보았는데, 그때 느낀점들을 위주로 글을 남겨보려고 한다. 지식의 생명주기와 기록개발관련 공부를 하면서 항상 느끼는 점은 공부해야 되는 양과 범위가 방대하다는 것이다. 저번주에 학습했던 내용이 몇 일이 지나면 새롭게 느껴지고, 몇 달 전에 학습했던 내용에 대해서는 더욱 더 새롭게 느껴진다. 그래서 공부했던 내용은 주기적으로 반복학습이 필요하다. 이번에 블로그와 깃허브 저장소들을 전체적으로 살펴보면서 내가 어떤 것들을 학습하고 기록했는지 살펴보았다. 다행히 깃허브 저장소의 README에 꼼꼼하게 작업한 내용들과 목적, 그리고 내가 포스팅한 블로그 글의 링크도 첨부해두어서 마치 이전 학습을 진행했을때로 잠시 돌아가서 금방 학습했던 내용들을 복습할 수 있었다. 만약 내가 이런 기록들을 남기지 않았더라면, 아마도 또 다시 그 때와 비슷한 시간을 투자해서 다시 학습을 해야하는 상황이 벌어졌을지도 모른다. 인터넷에 검색을 하면 정말 내가 작성한 블로그 글과는 비교가 안 될 정도로 잘 정리된 블로그 글들이 많다. 하지만 아무리 잘 작성되어있는 글이라고 하더라도 내가 작성했던 글만큼 잘 읽히지 않는다. 그리고 내가 직접 글을 작성해보면 덤으로 다른 사람에게 설명하는 연습이 되기도 한다. 그래서 나만의 언어로 정제를 해서 글을 쓰는 작업은 정말 중요한 것 같다. 뭔가 나만의 노트에 필기를 하게 되면 나만 보는 것이기 때문에 막 적게 되는데, public한 블로그와 같은 곳에 블로그 글을 포스팅하게 되면 여러 번 글을 다듬게 되는 것 같다. 그 과정을 통해 자연스럽게 작성한 글을 여러번 읽고 복습하게 되는 좋은 부수효과도 얻게 되는 것 같다. 내가 블로그 포스팅을 하는데 있어 가장 좋은 동기부여는 Google Search Console이었던 것 같다.내 블로그의 검색엔진을 최적화(SEO)하고, 네이버와 구글의 Search console에 블로그를 등록해서 내 블로그가 어떤 검색어로 노출이 많이 되고 사람들이 방문을 하는지 알 수 있도록 하였다. 가끔 Google search console에 방문해서 요즘에는 어떤 검색어를 통해서 사람들이 블로그에 방문을 하는지 살펴보고, 관련해서 내가 작성한 글을 잘 작성했었는지 다시 한 번씩 읽어보고 있다. 앞으로도 꾸준하게 다양한 개발관련 블로그 글을 작성해서 다른 사람들과 같이 공유하고 싶다. (최근(2021/05/16)에 Google adsense 광고를 블로그에 달았다 :-))","link":"/2021/05/24/202105/210524-Memoirs/"},{"title":"210526 immer","text":"immerstate의 데이터 구조가 복잡해지면 불변성을 지키기 위해 변하지 않는 값에 대해 spread 문법을 사용해서 얕은 복사를 하는 것이 지저분해질 수 있는데 immer를 사용하면 이 문제를 해결할 수 있다. 1$ npm i immer Redux에서만 아니라 React의 useState, setState에서도 immer를 사용할 수 있으며,hooks를 위한 useImmer도 있다.(useState 대체 가능) 우선 Redux의 reducer 함수에서의 immer의 사용에 대해서 알아보자. reducer함수는 이전 상태를 action을 통해 다음 상태로 만들어내는 역할을 한다.이때 새로운 상태는 반드시 불변성을 지켜서 구현을 해줘야 하는데, 구조가 복잡한 객체의 불변성을 지켜서 상태값을 업데이트하는 것이 복잡한 경우에는 immer를 사용한다. immer의 기본 사용 형태기존 reducer함수의 switch-case 문을 produce의 내부에 작성해준다. 123const reducer = (state = initialState, action) =&gt; { return produce(state, (draft) =&gt; {});}; immer를 사용하게 되면, 기존에 state로 작성했던 부분이 draft로 대체가 된다. draft는 별도의 불변성과 상관없이 작성을 해줘도 immer가 알아서 새로운 state를 불변성 지켜서 만들어준다. (state 인자는 건들이지 말고 draft만 조작하도록 한다 - return produce()부분이 불변성을 지킨 새로운 state 값을 반환해준다) 기존 reducer 1234567891011const reducer = (state = initialState, action) =&gt; { switch (action.type) { case ADD_POST_REQUEST: return { ...state, addPostLoading: true, addPostDone: false, addPostError: null }; }}; immer 적용후기존의 reducer에서는 state를 spread 문법으로 복사하고 변화된 항목만을 작성해주었지만, immer를 사용하게 되면, 업데이트해야하는 항목의 값만 작성해주면 된다. 1234567891011const reducer = (state = initialState, action) =&gt; { return produce(state, (draft) =&gt; { switch (action.type) { case ADD_POST_REQUEST: draft.addPostLoading = true; draft.addPostDone = false; draft.addPostError = null; break; } }} immer 적용 전/후 예시아래의 예시는 개별 Post에 대한 Comment를 업데이트해주는 부분이다.기존의 reducer에서는 특정 Post의 Comment를 업데이트 해주기 위해 해당 Post를 찾아서 새로운 Post객체로 생성한 뒤에 새로 생성된 Post 객체의 Comments 객체 속성에서 추가된 새로운 Comment를 추가해주고, mainPosts 객체도 새로운 mainPosts 객체를 생성해서 Comment가 새로 추가된 Post 객체를 업데이트 해주었다. 이렇게 업데이트해야하는 부분과 유지해야 되는 부분을 고려해서 매번 새로운 객체로 생성하게 되면 코드의 가독성이 좋지 않다. 하지만, immer를 사용하게 되면, 아래와 같이 간단하게 Comment를 업데이트해 줄 Post를 찾고, 해당 포스트의 Comments 속성에 새로운 Comment만 추가해주고, 이외에 업데이트 해야되는 속성도 개별 지정해서 업데이트를 해주면 끝난다. 123456789101112131415161718192021case ADD_COMMENT_SUCCESS: { const post = draft.mainPosts.find((v) =&gt; v.id === action.data.postId); post.Comments.unshift(dummyComment(action.data.content)); draft.addCommentLoading = false; draft.addCommentDone = true; break; // action.data.content, postId, userId // 우선 넘겨받은 게시물의 id를 통해서 어떤 게시물인지 index를 통해 찾는다. // 바뀌는 것만 새로운 객체로 만들고 나머지 객체는 참조를 유지해줘야 한다. (불변성) // const postIndex = state.mainPosts.findIndex((v) =&gt; v.id === action.data.postId); // const post = { ...state.mainPosts[postIndex] }; // post.Comments = [dummyComment(action.data.content), ...post.Comments]; // const mainPosts = [...state.mainPosts]; // mainPosts[postIndex] = post; // return { // ...state, // mainPosts, // addCommentLoading: false, // addCommentDone: true, // };} SWRRedux ToolkitRedux 사용시에 코드양이 많아지는데, 코드양을 줄여주기 위한 방법 중 하나로 Redux Toolkit이 있다.","link":"/2021/05/26/202105/210526-Redux_with_immer/"},{"title":"210821 Hotfix branch - local master branch와 remote master branch의 차이","text":"이번 포스팅에서는 실제 업무에서 master branch를 base로 hotfix branch를 끊어서 작업하면서 겪었던 문제에 대해서 작성해보려고 한다. Hotfix branch에서의 작업이번에 프로모션 페이지 컴포넌트 작업을 stage를 base branch로 두고 작업을 했었다. 이렇게 작업을 하게 되면, 개별 프로모션 페이지 컴포넌트 작업내용 뿐만 아니라 stage에서 여지까지 merge되었던 작업내용들까지 같이 master branch로 merge가 되기 때문에 master branch에서 개별 hotfix branch를 끊어서 작업을 해야했다. 그런데 local master branch에서 remote master branch를 pull해서 최신 상태로 업데이트를 한 뒤에 local master branch를 base로 새로운 hotfix branch를 끊어서 작업을 했는데, 여기서 문제가 발생했다. local master branch와 remote master branch의 차이에 대한 이해앞서 언급한 문제 상황에 대한 결론을 우선적으로 말하자면, local master branch는 현재 stage와 merged back된 상태이고, remote master branch는 stage에 merged back되지 않는 개별 branch 상태이다.따라서 local master branch를 기반으로 hotfix branch를 끊게 되면, 작업 후에 remote master branch로 push를 하게 되면, merged back된 stage의 작업 내용들까지 같이 넘어가게 된다. 이러한 문제로 인해 hotfix branch는 반드시 origin/master branch를 base로 끊어서 작업을 해야한다. origin/master를 base로 hotfix branch 작업12$ git checkout -b hotfix/promotion-page master (X)$ git checkout -b hotfix/promotion-page origin/master (O) local master branch와 remote master branch에서 각 각 hotfix branch를 끊었을 때의 차이를 확인해보기 위해 개인 Github Repository에서 시뮬레이션을 해보았다. https://github.com/LeeHyungi0622/hotfix-practice-repo 우선, 실제 업무의 상황과 동일하게 master branch와 stage branch를 끊고, stage branch의 상태를 아래와 같이 master branch의 뒤쪽에 배치되도록 branch를 구성하였다. 다음으로 local master branch와 remote master branch를 각 각 base branch로 hotfix branch를 끊어 보았다. 따라서 hotfix branch를 생성할 때에는 remote master branch(origin/master)를 base로 생성해야 한다.","link":"/2021/08/21/202108/210821-hotfix-branch/"},{"title":"210821 &quot;Zone.js has detected that ZoneAwarePromise (window|global).Promise has been overwritten","text":"이번 포스팅에서는 현재 서비스 중인 Angular web app이 iOS 모바일 단말기에서 정상적으로 동작하지 않는 문제에 대해서 작성해보려고 한다. [Issue] 브라우저에서 디버깅해본 결과, 위의 ERROR를 확인할 수 있었다.현재 Promise polyfill과 zone.js가 로드되는 과정에서 문제가 되고 있다. 프로젝트 내에서는 zone.js가 로드된 다음에 Promise polyfill가 로드되고 있지만, 이 문제를 해결하기 위해서는 Promise polyfill이 로드된 다음에 zone.js가 로드되어야 한다.그 이유는 Zone.js의 ZoneAwarePromise가 다음에 로드된 Promise polyfill에 의해 Overwritten되기 때문이다. [Solution] 현재 프론트엔드 프로젝트의 src/polyfills.ts의 코드를 보면, import 'zone.js/dist/zone'; statement를 확인 할 수 있다. Promise polyfill 다음으로 zone.js를 로드하기 위해서는 앞서 언급한 statement를 main.js로 이전해서 작성해줘야 한다. (위의 Solution을 적용한 뒤에 정상적으로 동작을 하는지 확인이 필요) [Promise polyfills과 zone.js의 역할]Promise polyfills비동기 코드의 장점을 살리면서 콜백지옥(callback hell)을 피하기 위해 ES6 표준 spec에 포함되어있는 Promise 패턴을 사용한다. 이 Promise polyfill을 사용해서 Promise가 지원되지 않는 브라우저에서 사용될 수 있도록 해야한다. https://developer.mozilla.org/ko/docs/Web/JavaScript/Reference/Global_Objects/Promise zone.js(Zones)Zones는 Angular의 Change Detection을 위해 중요한 역할을 한다. Angular에서는 click, change, input, submit과 같은 사용자에 의한 이벤트와 XMLHttpRequest를 통한 원격 서버로부터의 데이터 취득, setTimeout과 setInterval을 통해 앱의 상태를 변화시키고 있다.Angular에서는 화면을 업데이트하는데 위와 같은 비동기 처리를 사용하고, 로직이 작성되어있는 ts 파일의 변수가 HTML Template에 바인딩되어 ts파일내의 변수의 값이 업데이트되었을때 Template내의 값도 같이 업데이트된다. 이처럼 별도로 프레임워크에 변화가 일어났다는 작업을 하지 않아도 Angular는 알아서 처리를 해주게 되는데, 이때 중요한 역할을 해주는 것이 바로 Zones이다.Angular의 Zones의 onTurnDone 이벤트가 발생할때마다 전체 어플리케이션에 대해 Change Detection을 수행하게 되고, onTurnDone 이벤트는 Angular의 NgZone이 발생을 시킨다. ref. Zones를 이용하면, 해당 Zones(JavaScript Execution context)가 실행되는 시점에서 여러 이벤트들을 hooking할 수 있다. (자바스크립트의 실행 영역을 직접 나누는 것을 도와주기 때문에 여러가지 이벤트들을 hooking하는 것이 가능) 이러한 변화감지는 Zones 덕분에 가능한 것이다. 이처럼 Zones는 Angular가 DOM을 언제 업데이트해야 하는지 쉽게 발견할 수 있게 도와주는 역할을 한다.","link":"/2021/08/21/202108/210821-zone.js-error/"},{"title":"210601 faker를 사용해서 dummy data 만들기, placeholder.com, Redux toolkit","text":"faker개발을 하다보면 임의로 데이터를 넣어 실제 페이지가 어떻게 출력이 되는지 확인해야될 때가 있다.소수의 데이터는 괜찮지만 10개 이상의 데이터를 넣어서 확인을 해야 될 때에는 여간 번거로운 일이 아니다.이런 경우에 facker라는 패키지를 사용해서 임의의 dummy 데이터를 넣어 줄 수 있다.(무한 스크롤링 테스트) [참고]: https://www.npmjs.com/package/faker 1$ npm i faker Placeholder.comdummy 이미지 대체하여 사용할 수 있다. [참고] : https://placeholder.com/ Redux toolkitcreateReducer, createAction을 사용해서 Redux를 사용할때 코드가 길어지는 것을 보완할 수 있다. 이전에 공부할때 정리한 내용을 참고하자. http://localhost:4000/2021/04/30/202104/210430-Redux_TIL/","link":"/2021/06/01/202106/210601-faker_dummy_data/"},{"title":"210602 Infinite scrolling과 Virtualized list","text":"Infinite scrolling 구현Infinite scrolling을 구현하기 위해 스크롤이 가장 최 하단으로 이동했을때 이벤트를 발생시켜야 한다.실제 실무에서는 이벤트 발생시점이 최하단이 아닌 하단에서 일정 px 떨어진 이전 시점에 미리 로드할 데이터를 로딩시켜 사용자가 스크롤링을 했을때 끊기는 느낌이 들지 않도록 한다. 123456789101112131415161718192021222324252627282930313233343536373839404142useEffect(() =&gt; { // 현재 스크롤 위치 function onScroll() { // 스크롤된 높이를 구하는데 아래 세가지 함수가 많이 쓰인다. console.log( // (1) 얼마나 내렸는지 // 가장 최하단으로 스크롤을 내리면, 최 하단으로 내려간 페이지의 최 상단이 마지막 내려간 지점이 된다. window.scrollY, // (2) 화면에 보이는 길이(최상단에서 하단 스크롤 위까지 길이) document.documentElement.clientHeight, // (3) 총 길이 (스크롤의 제일 위부터 아래까지 총 길이) // (=) window.scrollY + document.documentElement.clientHeight document.documentElement.scrollHeight // (1)과 (2)의 합이 (3)과 같을때 스크롤이 가장 최 하단으로 내려갔다는 것을 의미한다. // 이 시점에 새로 로딩 ); // 끝에서 300px 위보다 더 많이 내렸을때 if ( window.scrollY + document.documentElement.clientHeight &gt; document.documentElement.scrollHeight - 300 ) { // loadPostLoading이 false인 경우에만 새로운 요청을 한다. if (hasMorePosts &amp;&amp; !loadPostLoading) { dispatch({ type: LOAD_POST_REQUEST, data: mainPosts[mainPosts.length - 1].id }); } } } window.addEventListener('scroll', onScroll); // 반드시 스크롤했떤 이벤트를 해제시켜줘야 한다. // 안그러면 메모리상에 계속 남아있다. return () =&gt; { window.removeEventListener('scroll', onScroll); };}, [hasMorePosts, mainPosts, loadPostLoading]); throttle과 takelatestinfinite scrolling을 구현하면서 scroll이벤트가 발생할때마다 매번 비동기 요청이 발생을 하였는데, 이 문제를 해결하기 위해 throttle과 takelatest를 사용해서 해결해보려고 했다.하지만 throttle과 takelatest 모두 요청(Async request)은 차단되지 않고, 응답(Response)만을 차단하기 때문에 만약에 5s 간격으로 throttle 처리를 하게 되면, 받은 요청에 대해서는 취소되지 않고, 5s 간격으로 지연되서 처리가 되는 것을 확인할 수 있었다. 본질적으로 해결하고자 하는 것은 scroll이벤트에서 발생되는 요청 자체를 차단하는 것이기 때문에 이 문제 해결을 위해 redux의 상태 데이터에서 해당 비동기 처리 요청에 대한 loading과 관련된 상태값을 활용하여 해결하였다. Virtualized list / React virtualizedVirtualized list은 인스타그램에서 사용되고 있는 기술이다. 실제로 서버로부터 가져오는 수천개의 데이터 중에 실제 화면에 보이는 데이터만 유지를 하고, 보이지 않는 데이터들은 메모리에서만 보관하여 실제 DOM에서는 보이지 않고 사라지도록 한다.실제 개발자 도구를 통해 확인을 해보면 스크롤할때 화면에 보이는 정보만 DOM에 표시되고, 화면에서 사라진 정보에 대해서는 DOM에서 사라지는 것을 확인할 수 있다. 그럼 왜 위와같은 기술이 생겨난 것일까? 그 이유는 실제 화면에 표시되는 게시글들 하나 하나가 모두 메모리이기 때문에 모바일에서 대량의 게시글들을 확인하게 되면, 메모리가 터지는 경우가 생긴다.그래서 위와같은 문제를 해결하기 위해 Virtualized list 기술이 등장하였다. React에서 Virtualized list를 구현하기 위해서 아래 두 개의 링크를 참고하도록 하자.[참고] : https://www.npmjs.com/package/react-virtualized [참고] : https://web.dev/virtualize-long-lists-react-window/","link":"/2021/06/02/202106/210602-infinite_scroll_and_virtualized_list/"},{"title":"210605 CORS 문제해결","text":"CORS(Cross-Origin Resource Sharing) ?브라우저상에서 정보를 받는 곳과 보내는 곳이 신뢰할 수 있는 곳인지 확인하는 절차가 필요하다.발신자와 수신자를 신뢰할 때 정보의 신뢰성과 보안이 보장될 수 있기 때문이다. CORS issue는 브라우저와 서버 간의 요청에서만 발생되며, 서버와 서버간에는 발생하지 않는다.요청을 보내는 쪽과 받는 쪽의 도메인이 서로 다른 경우, browser가 해당 요청을 차단한다. 동일 출처 정책(same origin policy)하나의 근원지로부터 문서 또는 스크립트를 로드함으로써 다른 근원지로부터 오는 잠재적인 악성 문서를 격리하는 정책이다. CORS 해결방법직접적으로 사용자들의 browser를 변조해서 CORS 문제를 해결할 수 없기 때문에 아래의 두 가지 방법으로 CORS 문제를 해결할 수 있다. 해결방법1: Proxy 방식예를들어 브라우저에서 다른 도메인인 백엔드 서버로 요청을 보내는 것은 문제가 되기 때문에 브라우저와 같은 도메인인 프론트엔드 서버를 거쳐서 백엔드 서버로의 요청과 응답을 처리하는 방법이다. 해결방법2: Access-Control-Allow-Origin header 허용header의 Access-Control-Allow-Origin의 통해 요청이 오는 특정 도메인을 허용할 수 있다. 예를들어 Express server에서 router의 callback 함수에서 response를 통해 header의 Access-Control-Allow-Origin 속성을 통해 특정 도메인을 허용해주거나, cors middleware를 사용해서 해결할 수 있다. 12345678910res.setHeader('Access-Control-Allow-Origin', '*');// cors middlewareapp.use( cors({ // 요청을 보낸 도메인이 자동으로 허용된다. origin: true, credentials: false }));","link":"/2021/06/05/202106/210605-cors/"},{"title":"210603 Nodejs + Sequelize + MySQL","text":"Express.js 사용의 이유아래와 같이 Node에서 제공하는 http 모듈 패키지를 사용해서 서버를 구현할 수 있다.하지만 체계적으로 파일을 분리해서 관리하기에는 다소 어려움이 있다.POST 방식의 처리와 GET 방식의 처리를 하기 위해서 req.method로 조건분기 처리를 하고, 그 안에서도 req.url을 통해서 요청받은 url을 구분해야 되기 때문에 조건처리하는 부분의 코드가 길어지게 되고, 코드의 가독성도 좋지 않다. 12345678910const http = require('http');const server = http.createServer((req, res) =&gt; { console.log(req.url, req.method); res.write('&lt;h1&gt;SERVER TEST&lt;/h1&gt;'); res.end('&lt;h1&gt;TEST END&lt;/h1&gt;');});server.listen(3065, () =&gt; { console.log('Server is running');}); 이러한 이유로 인해 Node.js의 프레임워크인 Express.js를 사용해서 서버를 구현하는 것이다. head와 bodybody는 res.json 또는 res.send로 보내지는 부분이다. head는 이 body(요청에 대한 응답)에 대한 부가적인 정보를 담고 있다.(예를들어, 응답의 날짜, 데이터 타입, 용량 등의 정보를 담고 있다.) Swagger API DocumentationSeagger 툴을 사용해서 API를 문서화할 수 있다. [참고] : https://swagger.io/ REST API 방식100% REST 방식을 지켜서 작성하기 어렵다. 어느정도 협상을 통해 REST 방식으로 API를 작성하도록 한다. get : 취득 post : 생성 put : 전체 수정(통째로 덮어쓰기) delete : 제거 patch : 부분 수정 options : 서버에 요청에 대한 확인 head : header 정보 취득 MySQL8(1) MySQL Community server 설치(2) MySQL workbench 설치 (terminal 말고 시각화해서 내부 데이터베이스 확인) sequelize와 mysql2 driver 설치 및 설정1$ npm i sequelize sequelize-cli mysql2 mysql2 : node.js와 mysql을 연결해주는 드라이버 sequelize : 직접 query를 작성해주지 않아도 javascript를 sql로 변환해주는 라이브러리 12# 기본 sequelize 설정$ npx sequelize init 설정 순서npx sequelize init으로 초기화를 시켜주면, 프로젝트 내에 config, migrations, models, seeders폴더가 자동생성된다. (1) config/config.json개발, 테스트, 배포 모드에 따라 사용되는 database를 다르게 설정할 수 있다. 1234567891011121314151617181920212223{ &quot;development&quot;: { &quot;username&quot;: &quot;root&quot;, &quot;password&quot;: &quot;1q2w3e4r&quot;, &quot;database&quot;: &quot;development-db&quot;, &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;dialect&quot;: &quot;mysql&quot; }, &quot;test&quot;: { &quot;username&quot;: &quot;root&quot;, &quot;password&quot;: null, &quot;database&quot;: &quot;test-db&quot;, &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;dialect&quot;: &quot;mysql&quot; }, &quot;production&quot;: { &quot;username&quot;: &quot;root&quot;, &quot;password&quot;: null, &quot;database&quot;: &quot;production-db&quot;, &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;dialect&quot;: &quot;mysql&quot; }} (2) models/index.js1234567891011121314151617181920212223242526272829const Sequelize = require('sequelize');// 별도로 env.NODE_ENV가 선언되어 있지 않으면 'development'로 초기화한다.const env = process.env.NODE_ENV || 'development';// config.json을 호출해서 [development]에 해당하는 설정부분 config 변수에 초기화한다.const config = require('../config/config')[env];const db = {};// Sequelize는 Node.js와 mysql을 연결해주는 역할을 한다.// Sequelize가 내부적으로 mysql2 driver를 사용하기 때문에 아래와 같이 연결에 필요한 정보를 인수로 넘겨서 Node.js와 mysql을 연결시켜주게 되는 것이다.const sequelize = new Sequelize( config.database, config.username, config.password, config);// 이하 설정부분은 기존에 자동으로 생성된 설정부분Object.keys(db).forEach((modelName) =&gt; { if (db[modelName].associate) { db[modelName].associate(db); }});db.sequelize = sequelize;db.Sequelize = Sequelize;module.exports = db; (3) DB 테이블 생성 / Sequelize model 생성 (models/)sequelize에서는 table을 model이라고 한다.model이 다른 model과 관련이 되어 있는 경우, model의 associate 부분에 정의를 해준다.우선적으로 독립적으로 분리할 수 있는 테이블들을 model로 정의를 해주고 나중에 관계를 엮을 부분을 associate에 정의하면 된다. 독립적으로 분리된 상태의 sequelize model 작성 123456789101112131415161718192021222324252627282930313233module.exports = (sequelize, DataTypes) =&gt; { // User : model name (mysql에서는 users로 자동변환되어 테이블이 생성된다. - 소문자로 변환되고 복수 이름으로 변환) const User = sequelize.define( 'User', // 첫 번째 객체의 정보는 테이블의 column 정보 { // 기본적으로 id가 column이 생성된다. email: { // 데이터 타입에는 STRING, TEXT, BOOLEAN, INTEGER, FLOAT, DATETIME가 있다. type: DataTypes.STRING(30), allowNull: false, // 필수 unique: true }, nickname: { type: DataTypes.STRING(30), allowNull: false }, password: { // PASSWORD는 암호화를 하게 되면 길이가 늘어나기 때문에 100 type: DataTypes.STRING(100), allowNull: false } }, // 두번째 객체는 User model에 대한 setting 정보를 넣어준다. { charset: 'utf8', collate: 'utf8_general_ci' //한글 저장 } ); // model 간의 관계를 정의하는 부분 User.associate = (db) =&gt; {}; return User;}; 위와같이 작성을 해주면, sequelize가 자동으로 MySQL에 테이블을 만들어준다. Sequelize model 간의 관계 설정하기 1:N : hasMany() / belongsTo() - 설정한 부분에 model의 id column이 생성된다. 한 명의 User가 여러 개의 게시글을 작성 N:N : belongsToMany()다대다(N:N)로 정의를 하게 되면, 두 개의 model을 mapping 시키는 mapping table이 새롭게 생성이 된다. mapping table의 이름은 자동으로 부여되지만, through 속성을 통해 지정을 할 수 있으며, as \u001b속성을 통해서 어떤 정보를 담고 있는지 구분할 수 있다. 하나의 Hash tag에는 여러 개의 게시글이 연관, 하나의 게시글에 여러 개의 Hash tag가 포함 사용자와 사용자 (팔로우) 사용자와 게시물의 좋아요 관 1:1 : hasOne() / belongsTo() 사용자와 사용자 정보 관계 123db.User.hasOne(db.UserInfo);db.UserInfo.belongsTo(db.User); models/user.js 12345678User.associate = (db) =&gt; { // User: Post = 1: N db.User.hasMany(db.Post); db.User.hasMany(db.Comment); // 사용자 - 좋아요 (through를 통해 중간 테이블 이름 지정) // post.getLikers 로 게시글 좋아요 누른 사람에 대한 정보를 가져올 수 있다. db.User.belongsToMany(db.Post, { through: 'Like', as: 'Likers' });}; models/post.js 1234567891011Post.associate = (db) =&gt; { db.Post.belongsTo(db.User); // 다대다 관계 (N:N) db.Post.belongsToMany(db.Hashtag); db.Post.hasMany(db.Comment); db.Post.hasMany(db.Image); // 사용자 - 좋아요 (through를 통해 중간 테이블 이름 지정) // 아래와같이 as로 지정해주면, // post.getLikers 로 게시글 좋아요 누른 사람에 대한 정보를 가져올 수 있다. db.Post.belongsToMany(db.User, { through: 'Like', as: 'Likers' });}; belongsTo와 hasMany의 역할belongsTo로 associate 내에 선언을 해주게 되면, 해당 model의 column에 belongsTo로 선언된 model의 id column이 생성된다. 1234Comment.associate = (db) =&gt; { db.Comment.belongsTo(db.User); // UserId: {} db.Comment.belongsTo(db.Post); // PostId: {}}; database 생성하기 1$ npx sequelize db:create","link":"/2021/06/03/202106/210603-nodejs/"},{"title":"210603 Server scaling","text":"프론트엔드와 백엔드 서버의 구성프론트엔드와 백엔드 서버를 각 각의 서버 PC로 구성하는 이유에 대해서 공부를 하던 중에 서버 스케일링(Server scaling)의 개념에 대해서 알게 되었다. 대규모 트래픽이 발생하는 웹 어플리케이션의 개발을 한다고 가정했을때, 프론트엔드와 백엔드 서버를 하나의 PC에 같이 관리하는 것과 각 각의 두 PC로 분리해서 관리하는 두 가지 경우로 나눠서 생각해 볼 수 있다. 우선 하나의 서버 PC에서 프론트엔드와 백엔드 서버를 모두 관리한다고 가정해보자.만약에 프론트엔드 서버와 백엔드 서버가 8:2비율로트래픽이 발생한다면, 프론트엔드 서버의 확장이 필요하다. 이때 서버의 스케일링을 하게 되면, 기존 서버 PC와 똑같은 서버 PC가 복제가 된다. 하지만 실제로 트래픽이 발생하는 서버는 프론트엔드 서버이기 때문에 백엔드 서버의 복제는 불필요하다. 이러한 이유로 프론트엔드와 백엔드 서버를 각 각의 서버 PC로 분리해서 관리하는 것이 자원을 아끼며 효율적으로 관리할 수 있는 것이다. 주문/배달 어플리케이션을 예로들면, 주문과 결제에 대한 트래픽이 많이 발생하기 때문에 주문과 결제를 개별 서버로 분리해서 서버의 확장이 필요한 경우에 해당 서버만 스케일링을 통해 확장하면 된다. 수직적 확장(Vertical Scaling)과 수평적 확장(Horizontal Scaling)수평적 확장의 경우에는 로드 밸런싱이라는 것이 있기 때문에 서버 중에 하나가 실패하면 로드 밸런서가 요청을 다른 서버로 redirect하고, 새로운 서버가 추가되면 로드 밸런서가 해당 서버로 요청을 보내기 시작한다.이처럼 수평적 확장의 경우에는 여러 서버와 효율적으로 작동하며 서버의 수를 늘리거나 줄일때 유연성을 제공한다. 하지만 수직적 확장의 경우에는 단 하나의 거대한 서버로 관리가 되기 때문에 위와 같은 수평적 확장이 가지는 유연함이 없다.이외에도 수직적 확장은 하드웨어의 제한이라는 단점이 있다. CPU/RAM/DISK와 같은 리소스의 추가는 가능하지만, 사용자가 많아지면 충분하지 않은 경우가 생기게 되고 이러한 이유로 대기업에서는 좀 더 유연성을 제공하는 수평적인 확장이 이상적이다. 그렇다면 수직적 확장의 장점에는 어떤 점이 있을까?수직적 확장의 장점은 데이터의 일관성이 있다는 것이다. 이는 모든 데이터가 보관된 세스템이 하나이기 때문이다. transaction이 여러 서버에 거쳐 데이터를 전송하는 수평적 확장 방식의 서버관리는 복잡성과 유지관리의 어려움이 있지만, 수직적 확장 방식의 서버 관리는 이러한 어려움이 없다. 소규모 회사를 운영하고 사용자 및 데이터가 증가하는 폭이 작다면 수직적 확장이 이상적이며, 사용자 및 데이터가 증가하는 폭이 높다면 수평적 확장이 이상적이다.","link":"/2021/06/03/202106/210603-server_scaling/"},{"title":"210605 프론트엔드 - 백엔드 - 데이터 베이스 전체 흐름도","text":"프론트/백엔드 서버와 DB 전체 흐름도이번에 프로젝트를 새로 시작하면서 프론트/백엔드 서버와 DB의 전체적인 흐름에 대해서 다시 되새겨보았다. 이미지를 머릿속으로 그려가며 직접 손으로 노트에 그려보았다. (아래에 노트 첨부)","link":"/2021/06/05/202106/210605-data-flow/"},{"title":"210605 cookie, session, JWT(JSON Web Token)","text":"cookie와 session이 필요한 이유cookie와 session이 필요한 이유를 이해하기 위해서는 HTTP 프로토콜의 connectionless, stateless한 특성에 대한 이해가 필요하다.connectionless는 클라이언트에서 요청을 한 뒤에 서버로부터 응답을 받으면 해당 연결을 끊어버리는 HTTP 프로토콜의 특징이다. HTTP가 TCP를 기반으로 구현되었기 때문에 네트워크 관점에서 keep-alive는 옵션으로, 연결비용을 줄이는 것을 장점으로 비연결지향이라고 한다. stateless는 클라이언트와 서버의 통신이 끝나면 상태를 유지하지 않는 HTTP 프로토콜의 특징이다. 연결이 끊기는 순간 클라이언트와 서버간의 통신이 끝나고, 상태 정보는 유지하지 않는 것이 특징이다. cookie와 session은 앞서 설명한 HTTP 프로토콜의 두 가지 특징(connectionless, stateless)이 가지는 단점을 해결하기 위해 사용된다.우리가 특정 웹 페이지를 이용할때 한 번의 로그인을 한 뒤에 로그인한 사용자에 대한 인증을 유지하는 것이 바로 쿠키와 세션을 사용했기 때문이다. 쿠키와 세션을 사용하지 않는다면, 새로운 페이지로 이동할때마다 다시 로그인을 해야한다. cookie, session, JWT(JSON Web Token)cookie와 session의 차이점에 있어, 정보가 저장되는 위치와 보안에 대한 부분도 있지만 이보다 더 중요한 것은 lifecycle에 대해 이해하는 것이 중요하다. cookie와 session 모두 만료시간이 있지만, cookie의 경우에는 클라이언트의 로컬에 파일로 저장되기 때문에 브라우저가 종료되어도 계속 정보가 남아있지만, 세션의 경우에는 만료시간과 상관없이 브라우저가 종료되면 삭제된다는 특징을 가지고 있다. cookie 쿠키는 클라이언트(브라우저) 로컬에 저장되는 키와 값이 들어있는 데이터 파일이다. 사용자 인증 유효시간을 지정할 수 있으며, 유효기간이 정해지면 브라우저가 종료되어도 지정된 유효기간까지 인증이 유지된다는 특징을 가진다. (라이프 사이클) 쿠키는 클라이언트의 상태정보를 로컬에 저장했다가 참조한다. 클라이언트에 최대 300개까지 쿠키를 저장할 수 있으며, 하나의 도메인당 20개의 값을 가질 수 있다. 하나의 쿠키값은 4KB까지 저장하게 된다. 서버 사이드에서 클라이언트의 요청에 대한 응답을 보낼때 Response header에 Set-Cookie 속성을 사용하면 클라이언트에 쿠키를 생성할 수 있다. 클라이언트가 요청을 할때, 별도의 요청을 하지 않아도, 브라우저가 자동으로 Request header에 쿠키를 넣어서 전송하게 된다. 쿠키는 로컬에 저장되기 때문에 서버로의 요청시에 스니핑 당할 우려가 있어 보안에 취약하다. cookie의 구성cookie는 이름, 값, 유효시간, 도메인, 경로, 총 5가지 요소로 구성이 되어있으며, 아래와 같은 특징을 지닌다. 이름 : 쿠키를 구별하는데 사용된다. 값 : 쿠키의 이름과 mapping된 값이다. 유효시간 : 쿠키의 유지시간이다. 도메인 : 쿠키를 전송할 도메인 정보이다. 경로 : 쿠키를 전송할 요청 경로이다. cookie의 동작 방식클라이언트에서 서버로 페이지를 요청하면, 서버에서 쿠키를 생성해서 response header에 포함시켜 응답을 보내준다. 여기서 중요한 특징은 브라우저가 종료되어도 쿠키 만료 기간이 남아있다면, 클라이언트에서 보관을 한다.클라이언트에서 같은 요청을 하는 경우, request header에 쿠키를 담아서 함께 보낸다.서버에서 쿠키를 읽어서 이전 상태 정보를 변경할 필요가 있을때, 쿠키를 업데이트하고 response header에 업데이트된 쿠키를 담아서 응답한다. 우리가 흔히 사이트를 방문할때 아이디와 비밀번호를 변경 혹은 저장하겠느냐?는 팝업창이 뜨는데, 이것이 바로 서버 사이드에서 업데이트된 쿠키를 전달했을 경우에, 클라이언트의 기존 쿠키 정보와 다른 경우에 팝업이 되는 것이다. 쇼핑몰의 장바구니 기능과 자동 로그인 기능, 팝업창에 있는 오늘 더 이상 이창을 보지 않음 과 같은 기능이 대표적인 쿠키의 사용 예이다. session session 또한 cookie를 기반으로 하지만, 사용자 정보 파일을 브라우저가 아닌 서버 측에 저장한다. session은 cookie를 사용해서 session ID 만을 저장한다. 서버에서는 클라이언트를 구분하기 위해 session ID를 부여하고, 브라우저가 서버에 접속한 순간부터 종료되는 시점까지 인증상태를 유지한다. (접속시간을 제한하여 일정 시간 응답이 없으면 정보가 유지되지 않게 설정 가능) 쿠키와는 다르게 사용자에 대한 정보를 서버에 두기 때문에 쿠키보다는 보안적 측면에서 좋지만, 접속 사용자 정보가 많아질수록 서버 메모리를 많이 차지하게 된다. (동시간 접속자 수가 많은 웹 사이트의 경우, 서버에 과부화를 주게 되므로 성능저하의 요인) 클라이언트에서 요청을 보내면 서버가 클라이언트에게 unique한 ID를 부여하게 되는데, 이것이 바로 session ID이다. session은 서버의 자원을 사용하며, 서버의 처리가 필요하기 때문에 요청 속도가 쿠키보다 느리다. session의 동작 방식클라이언트가 서버에 접속하게 되면, session ID를 발급받는다. 이후 클라이언트는 발급받은 session ID를 쿠키를 사용해서 저장하고 가지고 있는다.클라이언트는 서버에 특정 요청을 할 때, 이 쿠키의 session ID를 서버에 전달해서 사용한다.서버는 session ID를 전달받고, 해당 session ID로 session에 있는 사용자 정보를 가져온다.사용자 정보를 가지고 서버 요청을 처리하고 클라이언트에 응답한다. 세션은 각 각의 클라이언트에게 고유의 ID를 부여하고, session ID를 통해 클라이언트를 구분해서 요청을 처리하게 된다. 대표적으로 로그인과 같은 보안상 중요한 작업을 수행할 때 사용된다. cache캐시는 이미지나 CSS, JS 파일등을 브라우저나 서버의 앞 단에 저장해놓고 사용하는 것을 말한다.종종 서버에서 변경이 일어나도 이미 브라우저에 저장된 캐시를 참조해서 변경된 내용이 반영되지 않는 경우가 생기는데, 이 경우에는 cache를 지워주거나, 서버에서 클라이언트로 응답을 보낼때 header에 cache의 만료시간 명시하는 방법을 이용하면 해결할 수 있다. JWT(JSON Web Token)세션은 사용자의 수 만큼 서버 메모리를 차지하기 때문에 이러한 서버의 메모리상의 문제를 보안한 token 기반의 인증방식인 JWT(Json Web Token)을 사용하는 추세이다. 쿠키와 세션을 로그인의 흐름로그인을 하게 되면, 브라우져와 백엔드 서버는 서로 같은 로그인 사용자 정보를 갖고 있어야 한다.그러므로 백엔드에서 로그인이 성공했다면, 프론트엔드로 로그인된 사용자 정보를 보내줘야 한다. 하지만, 백엔드에서 프론트엔드로 로그인한 사용자 정보를 그대로 보내준다면 어떻게 될까? 비밀번호와 같은 정보가 그대로 노출되어 계정이 보안에 취약해진다. 이러한 문제로 인해 백엔드에서 프론트엔드로 사용자 정보를 보낼때 데이터를 그대로 보내지 않고, 랜덤한 문자열을 보내주게 되는데 이것이 바로 쿠키(Cookie)이다.실제 정보를 대신해서 랜덤한 문자열 토큰을 보내주게 되는 것이다. 보냄과 동시에 백엔드 서버에서는 로그인한 사용자 정보를 해당 쿠키와 연결되어있다고 정의를 하게 되는데, 이 부분이 바로 세션(Session)이다. 이렇게 쿠키와 세션이 정의가 된 이후부터는 브라우져를 통해 게시글이나 댓글을 작성하게 되면, 쿠키를 함께 담아서 백엔드로 보내게 된다.백엔드 서버에서는 전달받은 쿠키를 읽어서 어떤 사용자인지 파악한 뒤에 해당 요청을 처리하게 된다. 백엔드 서버에서의 사용자 정보백엔드 서버에서 로그인한 사용자의 모든 정보를 보관하게 되면, 사용자가 많은 경우, 서버가 과부화 걸린다. 이러한 이유로 사용자의 id만을 저장하고, 전체 사용자 정보를 복구할 때에는 해당 사용자 id와 mapping되는 전체 사용자 정보를 DB에서 참조를 한다. Passport.js를 활용한 로그인 흐름아래는 passport-local을 사용한 일반 로그인 과정의 예시이다. Front-End(1) 로그인 폼에 이메일과 비밀번호를 입력(2) saga의 로그인 함수 실행 Back-End(3) login POST router 실행(4) passport.authenticate(‘local’, …) 부분이 실행(5) passport/local.js 로그인 전략 부분이 실행(6) (5)의 결과가 passport.authenticate(‘local’, …) 부분의 callback 함수로 전달되고, 로그인에 문제가 없는 경우, passport 로그인 시도(req.login)(7) passport/index.js의 serializeUser부분이 실행된다.serializeUser부분에서는 cookie에 묶어 줄 로그인 사용자 id를 전달한다.deserializeUser부분에서는 로그인이 성공한 이후부터의 요청부터 done(null, user)를 통해 req.user로 사용자 정보를 전달한다.(8) 내부적으로 header를 통해 Front-End로 전달할 cookie 정보를 전달한다.(9) Cookie 정보와 함께 Front-End로 사용자 정보를 전달한다. 노트필기 참고 로그인 기능 구현(passport)passport, passport-local 설치 1$ npm i passport passport-local passport-local은 일반적인 id, password를 통한 로그인을 위한 라이브러리 back/passport/index.js작성된 index.js 파일은 app.js에서 사용된다. 12345678910const passport = require('passport');const local = require('./local');module.exports = () =&gt; { passport.serializeUser(() =&gt; {}); passport.deserializeUser(() =&gt; {}); // 작성한 login 전략이 포함된 local.js 파일 local();}; passport-local 로그인 전략 작성 123456789101112131415161718192021222324252627282930313233343536373839404142const passport = require('passport');const { Strategy: LocalStrategy } = require('passport-local');const { User } = require('../models');const bcrypt = require('bcrypt');module.exports = () =&gt; { //(객체, 함수) passport.use( new LocalStrategy( { //req.body.email usernameField: 'email', // req.body.password passwordField: 'password' }, async (email, password, done) =&gt; { // 로그인 전략 try { const user = await User.findOne({ where: { email } }); // user가 존재하지 않는 경우 if (!user) { // 서버에러, 성공, 클라이언트 에러 return done(null, false, { reason: '존재하지 않는 사용자입니다.' }); } // user가 존재하는 경우, 비밀번호 체크 const result = await bcrypt.compare(password, user.password); if (result) { // 비밀번호가 일치하는 경우, 서버에 user 정보를 보내준다. return done(null, user); } // password가 일치하지 않는 경우 return done(null, false, { reason: '비밀번호가 틀렸습니다.' }); } catch (error) { console.error(error); return done(error); } } ) );}; back/passport/local.js 1234567// login 전략const passport = require('passport');const { Strategy: LocalStrategy } = require('passport-local');module.exports = () =&gt; { passport.use(new LocalStrategy());}; back/routes/user.js 12345678910111213141516171819202122232425262728293031...const passport = require('passport');// 일반 로그인인 경우 작성한 passport-local login 전략을 사용한다.// 앞서 전략에서 작성한 done([서버에러], [성공], [클라이언트 에러])가// 아래의 callback 함수의 err, user, info로 넘어간다.// 아래와 같이 middleware를 확장해서 작성할 수 있다.(next를 통해 에러를 전달하기 위해서)router.post('/login', (req, res, next) =&gt; { passport.authenticate('local', (err, user, info) =&gt; { if (err) { console.error(err); return next(err); } if (info) { // client error // 401: 허가되지 않음. return res.status(401).send(info.reason); } // passport login을 사용해서 로그인을 실행한다. return req.login(user, async(loginErr) =&gt; { //passport 로그인 에러발생 처리 if (loginErr) { console.error(loginErr); return next(loginErr); } // 최종 로그인 성공시, 사용자 정보를 프론트로 넘겨준다. return res.json(user); }); })(req, res, next);});","link":"/2021/06/05/202106/210605-session_cookie_login/"},{"title":"210611 Angular - Angular 프로젝트의 실행흐름 이해하기, Module, Component, Template, 단방향 바인딩과 양방향 바인딩(React와 Angular 비교), Component의 스타일링, Pipe 사용","text":"2021/06/12 단방향 바인딩과 양방향 바인딩 ~ Update Angular 프로젝트의 전체적인 실행흐름 이해하기Angular는 정해진 틀 안에서 구조화되어있기 때문에 사용하기 위해서는 어떤 흐름으로 컴포넌트가 정의가 되고 실행되는지 이해가 필요하다.ReactJS는 라이브러리로 개발자가 폴더를 만들어서 직접 구조화시켜줘야 되지만, Angular는 플랫폼으로 이미 구조화된 틀이 있다는 차이가 있다. 하지만 React와 Angular 모두 컴포넌트 기반의 개발을 한다는 점에서 같다. src/index.htmlReact와 같이 Angular의 src 폴더에 정의된 index.html 파일을 보면, 별다른 script와 style을 로드하는 부분이 없고 webpack을 통해 번들링해서 index.html에 정의된 부분에 주입해서 보여준다. src/main.ts프로젝트를 실행하게 되면 가장 먼저 호출되는 script 파일이 main.ts 이다.이 파일에서 중요한 부분은 AppModule(root module)을 실행하는 부분인데, platformBrowserDynamic 메서드와 bootstrapModule 메서드가 호출되고 있음을 알 수 있다. paltformBrowserDynamic : Angular가 작성한 코드를 컴파일해서 실행할 수 있는 JS 코드로 만들어주는 메서드이다. (브라우저에서 동적으로 컴파일한다)Angular는 서버나 브라우저 등 다양한 환경에서 실행될 수 있도록 rendering과 compile 프로세스가 분리되어있다. –aot(ahead of time compilation) 옵션을 사용해서 실행을 하면, 메서드를 다르게 해서 실행할 수 있다. src/app/app.module.ts app.module.ts 파일은 앞서 살펴 본 main.ts에서 실행된 root module인 AppModule을 정의한 파일이다.class에 @NgModule 데코레이터로만 정의가 되어있는데, 이는 Angular module로 정의하는 데코레이터이다. 데코레이터 내에 정의된 속성 중에 bootstrap을 살펴보면 root component인 AppComponent가 정의되어있음을 알 수 있다. src/app/app.component.tsapp.module.ts에서 bootstrap에 정의된 컴포넌트는 index.html에 주입해주는 컴포넌트이다.컴포넌트 데코레이터를 보면 selector 속성에 app-root값으로 설정이 되어있다. 이는 index.html에 정의된 &lt;app-root&gt;에 주입되는 컴포넌트라는 것을 의미한다. AppComponent 클래스의 내부에 정의된 title은 templateURL 속성에 정의된 ./app.component.html 내에 binding된다. ModuleModule이란 세부 구현이 숨겨지고 공개 API(method)를 이용해서 다른 코드에서 재사용 가능한 코드를 말한다.ex) 리모콘이라는 모듈이 있다고 가정했을때 각 각의 버튼이 공개된 API이고, 볼륨이나 채널 변경을 할 수 있다.ES6 module은 앞서 살펴 본 모듈의 정의 + 변수 scope이 모듈내로 제한되는 개념이다.이전에 JS에서는 즉각호출 패턴으로 모듈화를 했지만, ES6에서는 자체적으로 모듈화를 지원하여 각각의 파일들이 하나의 모듈이 되고, 각 각의 모듈로써 정의된 파일내에서 사용된 변수들의 scope는 파일 내에서 보장된다. Angular에서는 class를 export하는 방식으로 작성이 되며, module간의 종속관계를 읽고 하나의 파일로 번들링해주는 하는 식으로 Angular CLI에서 webpack을 사용해서 처리해준다. (번들링된 하나의 파일만 호스팅하는 식으로 실행)main.ts 파일을 통해서 import되는 module들을 번들링해준다. Angular module ?컴포넌트, 파이프, 서비스 등과 같은 앵귤러 어플리케이션의 주요 부분을 기능단위로 그룹핑하게 해준다. Angular 어플리케이션은 하나의 Root module을 가지며, 여러 Feature module을 가질 수 있다. 그리고 재사용할 수 있는 기능을 외부의 다른 파일에서 사용하기 위해서 사용되기도 한다. 12345678# app/todo/todo.module.ts 생성$ ng generate module todo# app/todo/# g: generate, c: component# todo내에 todos라는 컴포넌트 생성# --module 옵션으로 특정 module을 지정# --export 옵션으로 외부 다른 모듈에서 사용 가능하도록 설정$ ng g c todo/todos --module todo/todo.module.ts --export todo.module.ts 파일의 @NgModule 데코레이터의 declaration 속성에 정의된 컴포넌트들은 template에서 사용할 수 있고, export에 정의된 컴포넌트는 외부 다른 파일에서 사용될 수 있는 컴포넌트들이다.providers 속성은 작성한 서비스 관련한 파일을 정의한다. 1234567@NgModule({ declarations: [TodosComponent], imports: [CommonModule], providers: [], exports: [TodosComponent]})export class TodoModule {} 새롭게 정의한 모듈은 root module인 app.modules.ts 파일의 @NgMudule 데코레이터의 imports 속성에 정의해줘야 사용할 수 있다. \u0010이처럼 Angular에서는 하나의 root module이 필요하고, 이 root module에 template에서 사용할 module들을 정의해서 사용한다.module들간에 import해서 사용할 수 있다. ComponentAngular에서는 여러 컴포넌트들로 구성이 된다.최상위 &lt;app-root&gt;를 기준으로 하위로 가지치기 하며 여러 하위 컴포넌트로 구성된다.HTML 요소들의 그룹이며, 뷰와 로직으로 구성이 된다. 기본적으로 클래스에 로직을 작성하며, @Component 데코레이터를 여러 메타 데이터 객체로 정의한다. 각 메타 데이터 속성은selector : 선택자 (prefix로 #을 붙이면 id, 아무것도 안붙이면 element 이름)templateUrl : View에 대한 정의를 하는 부분이다. 바인딩할 데이터나 사용할 컴포넌트들을 정의할 수 있다.styleUrls : 배열의 형태로 여러 스타일과 관련된 파일을 import할 수 있다. Component를 생성할때 옵션으로 --inline-template과 --inline-style을 넣어주게 되면, 컴포넌트 파일 내에서 style과 template 관련 정의를 해준다는 의미이다.따라서 생성된 파일을 보면 별도로 html과 css파일이 생성되지 않는다. html에 대해서는 template 속성에 template literal로 마크업을 해준다. TemplateAngular template은 HTML 코드로서 표현을 하며, template 표현식과 문장을 가진다.바인딩되는 대상은 속성, 이벤트, ngModel, class, style이 있다. template에 마크업한 HTML의 이벤트 로직에 대한 작성은 class 내에 작성해준다. 작성한 로직의 데이터를 template에 반영하는 방법은 바인딩의 대상인 속성, 이벤트, ngModel, class, style 통해서 한다. 바인딩: 데이터와 로직을 담고 있는 컴포넌트와 DOM과의 interaction, communication 단방향 바인딩과 양방향 바인딩데이터 바인딩이란 비즈니스 로직인 Model과 View 간에 데이터를 동기화하는 프로세스이다. Angular에서는 Model과 View가 동기화되므로 데이터를 변경되면 뷰가 업데이트되고, 뷰를 업데이트하면 데이터가 업데이트 되는 양방향 데이터 바인딩 구조를 가지고 있다. 반면, React에서는 단방향 또는 하향식 데이터 바인딩을 사용한다.단방향 데이터 바인딩 구조는 코드를 보다 안정적이게 만들지만, Model과 View를 동기하기 위해서는 추가적인 작업이 필요하다. 단방향 바인딩모든 컴포넌트와 DOM 속성에 데이터를 바인딩 할 수 있다.Component(class) instance라는 Context내에서 템플릿 문장에 정의한 속성과 이벤트(메서드)를 찾는다. [속성]=”템플릿 문장” (이벤트)=”템플릿 문장”123&lt;div *ngFor=&quot;let todo of todos&quot; (click)=&quot;toggleTodo(todo)&quot;&gt; &lt;input type=&quot;checkbox&quot; [checked]=&quot;todo.done&quot;&gt; {{ todo.text }}&lt;/div&gt; *ngFor를 사용해서 객체에 대한 반복처리를 할 수 있다. 양방향 바인딩Angular에서는 Engine model이라는 지시자를 이용하면 양방향 데이터 바인딩을 사용할 수 있다. [(ngModel)]=”템플릿 문장”속성바인딩 할 때 사용한 [] 대괄호와 이벤트 바인딩을 할 때 사용한 () 중괄호를 사용하면 양방향 바인딩이 가능하다.(ngModel을 사용하기 위해서는 module의 @NgModule의 import 속성에 FormsModule을 넣어줘야 한다)1234&lt;div&gt; &lt;input type=&quot;text&quot; placeholder=&quot;할 일 추가&quot; [(ngModel)]=&quot;newText&quot;&gt; &lt;button (click)=&quot;addTodo(newText)&quot;&gt;Add&lt;/button&gt;&lt;/div&gt; 1234567891011121314151617181920212223242526272829303132333435363738// todos.component.tsimport { Component, OnInit } from '@angular/core';// 메타데이터는 객체로써 정의한다.@Component({ selector: 'app-todos', templateUrl: './todos.component.html', styleUrls: ['./todos.component.scss']})export class TodosComponent implements OnInit { newText = ''; todos: { done: boolean, text: string }[]; constructor() { this.todos = [ { done: false, text: '운동하기' }, { done: true, text: '공부하기' } ]; } ngOnInit(): void {} toggleTodo(todo: { done: boolean, text: string }) { todo.done = !todo.done; } addTodo(newText: string) { this.todos.push({ done: false, text: newText }); this.newText = ''; }} Component 간의 커뮤니케이션 자식 컴포넌트에서 부모 컴포넌트로 @Output() 데코레이터와 EventEmitter를 사용해서 부모에게 이벤트를 전달 할 수 있다. 부모 컴포넌트는 $event를 통해 이벤트의 데이터를 전달받는다. 자식이 부모 컴포넌트를 직접 주입받을 수 있지만, 이는 강력한 의존관계가 성립되기 때문에 필요한 경우에만 사용한다. 123456789101112131415161718192021// 부모 컴포넌트//(template)//이벤트를 전달하는 자식 컴포넌트의 속성에서 정의(onTodoAdded) = &quot;addTodo($event)&quot;//(component)addTodo(text: string){ this.todos.push({ done: false, text, });}// 자식 컴포넌트// event emitter 정의@Output() onTodoAdded = new EventEmitter();addTodo(newText: string){ this.onTodoAdded.emit(newText); this.newText='';} 부모 컴포넌트에서 자식 컴포넌트로부모 컴포넌트에서 자식 컴포넌트로 데이터를 전달하기 위해서는 자식 컴포넌트의 공개된 속성에 @Input 데코레이터를 사용하면 된다. 그러면 부모 컴포넌트 템플릿 상에서 자식 컴포넌트에 속성을 전달해서 사용 할 수 있다.그리고 ES6의 setter를 사용해서 데이터를 가공한 뒤에 비공개 속성을 업데이트 할 수 있다.(getter)자식 컴포넌트 클래스의 instance reference를 부모 컴포넌트로부터 @ViewChild 데코레이터를 사용해서 넘겨받을 수 있다. Model 정의복수의 컴포넌트에서 공통적으로 사용되는 데이터 객체가 있다면 Model로써 정의를 해서 재사용 할 수 있다. Component의 스타일링Angular에서는 View encapsulation(뷰의 캡슐화)으로 컴포넌트 안에서 정의한 CSS 스타일이 컴포넌트 밖에 영향을 주지 않는다.(Angular 프로젝트를 컴파일하면 임의의 랜덤한 속성을 DOM요소에 넣어주기 때문에 특정 요소만 선택해서 스타일링하기 때문에 가능한 것이다) 컴포넌트를 스타일링하는 방법에는 아래 세 가지 방법이 있다. 별도의 CSS 파일에서 스타일 적용하기 inline-style방식으로 스타일 적용하기 src/styles.css 에서 스타일링하면 전역적인 스타일링이 가능하다. PipePipe는 템플릿에서 보이는 데이터를 변환하는데 사용된다.이전의 AngularJS v1.x에서는 필터로써 제공이 되었던 기능이다. 사용법 {{ express | pipeName:paramValue }} 예시 1) {{ today | date }} 예시 2) {{ today | date:\"yy/dd/dd\" }} : pip를 사용해서 formatting한 데이터로 변환할 수 있다. 예시 3) {{ today | date | uppercase }} : pip를 사용해서 여러번 체이닝해서 데이터를 변환할 수 있다. 1234&lt;h2&gt;{{ today | date:&quot;M월 d일&quot; }}&lt;/h2&gt;// todos object를 json형태로 변환이 가능하다.// Angular documentation 확인하기{{ todos | json }}","link":"/2021/06/11/202106/210611-angular_study-1/"},{"title":"210607 passport.js 로그인 기능구현","text":"Passport.js를 활용한 로그인 흐름아래는 passport-local을 사용한 일반 로그인 과정의 예시이다. Front-End(1) 로그인 폼에 이메일과 비밀번호를 입력(2) saga의 로그인 함수 실행 Back-End(3) login POST router 실행(4) passport.authenticate(‘local’, …) 부분이 실행(5) passport/local.js 로그인 전략 부분이 실행(6) (5)의 결과가 passport.authenticate(‘local’, …) 부분의 callback 함수로 전달되고, 로그인에 문제가 없는 경우, passport 로그인 시도(req.login)(7) passport/index.js의 serializeUser부분이 실행된다.serializeUser부분에서는 cookie에 묶어 줄 로그인 사용자 id를 전달한다. deserializeUser부분은 로그인이 성공한 이후부터의 요청부터 router가 실행되기 이전에 실행된다.deserializeUser부분이 실행되면, 저장되었던 사용자의 id를 토대로 사용자 정보를 복구해서 done(null, user)를 통해 req.user로 사용자 정보를 전달한다. 이를통해 router에서는 req.user로 사용자 정보의 참조가 가능해진다. (8) 내부적으로 header를 통해 Front-End로 전달할 cookie 정보를 전달한다.(9) Cookie 정보와 함께 Front-End로 사용자 정보를 전달한다. 노트필기 참고 로그인 기능 구현(passport)passport, passport-local 설치 1$ npm i passport passport-local passport-local은 일반적인 id, password를 통한 로그인을 위한 라이브러리 back/passport/index.js작성된 index.js 파일은 app.js에서 사용된다. 12345678910const passport = require('passport');const local = require('./local');module.exports = () =&gt; { passport.serializeUser(() =&gt; {}); passport.deserializeUser(() =&gt; {}); // 작성한 login 전략이 포함된 local.js 파일 local();}; passport-local 로그인 전략 작성 123456789101112131415161718192021222324252627282930313233343536373839404142const passport = require('passport');const { Strategy: LocalStrategy } = require('passport-local');const { User } = require('../models');const bcrypt = require('bcrypt');module.exports = () =&gt; { //(객체, 함수) passport.use( new LocalStrategy( { //req.body.email usernameField: 'email', // req.body.password passwordField: 'password' }, async (email, password, done) =&gt; { // 로그인 전략 try { const user = await User.findOne({ where: { email } }); // user가 존재하지 않는 경우 if (!user) { // 서버에러, 성공, 클라이언트 에러 return done(null, false, { reason: '존재하지 않는 사용자입니다.' }); } // user가 존재하는 경우, 비밀번호 체크 const result = await bcrypt.compare(password, user.password); if (result) { // 비밀번호가 일치하는 경우, 서버에 user 정보를 보내준다. return done(null, user); } // password가 일치하지 않는 경우 return done(null, false, { reason: '비밀번호가 틀렸습니다.' }); } catch (error) { console.error(error); return done(error); } } ) );}; back/passport/local.js 1234567// login 전략const passport = require('passport');const { Strategy: LocalStrategy } = require('passport-local');module.exports = () =&gt; { passport.use(new LocalStrategy());}; back/routes/user.js 12345678910111213141516171819202122232425262728293031...const passport = require('passport');// 일반 로그인인 경우 작성한 passport-local login 전략을 사용한다.// 앞서 전략에서 작성한 done([서버에러], [성공], [클라이언트 에러])가// 아래의 callback 함수의 err, user, info로 넘어간다.// 아래와 같이 middleware를 확장해서 작성할 수 있다.(next를 통해 에러를 전달하기 위해서)router.post('/login', (req, res, next) =&gt; { passport.authenticate('local', (err, user, info) =&gt; { if (err) { console.error(err); return next(err); } if (info) { // client error // 401: 허가되지 않음. return res.status(401).send(info.reason); } // passport login을 사용해서 로그인을 실행한다. return req.login(user, async(loginErr) =&gt; { //passport 로그인 에러발생 처리 if (loginErr) { console.error(loginErr); return next(loginErr); } // 최종 로그인 성공시, 사용자 정보를 프론트로 넘겨준다. return res.json(user); }); })(req, res, next);});","link":"/2021/06/07/202106/210607-passport_login/"},{"title":"210611 Angular - Angular 등장배경과 Angular 프로젝트 생성","text":"Angular가 등장하게 된 배경Angular가 등장하게 된 배경을 이해하기 위해서는 우선 웹 개발의 변화에 대한 이해가 필요하다.변화는 Ajax - SPA(MVC) - Web Component 순으로 변화가 되었다. Ajax를 사용하지 않는 웹 페이지의 경우, 브라우저에서 URL을 서버에 요청하게 되면 서버가 data(서버의 사용자 세션에 저장)를 사용해서 HTML 웹 페이지를 만들어서 응답으로 보내준다. Ajax의 등장이후에는 한 번의 페이지를 받아온 이후에 ajax를 통해서 DOM element를 조작해서 페이지를 업데이트한다. 버튼 클릭과 같은 사용자 입력이 있는 경우, 데이터가 필요한 경우 ajax를 통해 서버로 요청을 보내고 서버로부터 json, xml 형태의 응답을 이용해서 DOM element를 업데이트하게 된다. ajax를 이용하면, 화면이 바뀔때마다 서버로 요청을 보내는 것이 아니라, JavaScript를 사용해서 동적으로 브라우저에 올라가있는 데이터를 이용해서 DOM을 조작하게 된다. 장점으로는 서버에 요청이 줄어들기 때문에 좀 더 빠르고 서버쪽 부담이 줄어든다. 하지만 규모가 큰 웹 어플리케이션에서 코드가 복잡해지고, 재사용이 어려웠기 때문에 코드에 대한 구조화가 필요해졌고, 매번 새로운 페이지를 서버로부터 읽어오게 되면, 리소스를 서버로부터 다시 읽어오고, 브라우저 렌더링 다시 하기 때문에 성능저하가 문제되었다. 위와같은 문제를 해결하기 위해 등장하게 된 것이 SPA라는 개발패턴이다. Ajax의 확장판이라고 이해할 수 있다. SPA 개발패턴은 URL 경로에 따라 자바스크립트로 화면을 동적으로 그리는 것을 말한다. 이때 웹 어플리케이션의 구조를 잡기 위한 프레임워크가 필요하게 되었는데, 이때 등장하게 된 프레임워크 중에 하나가 AngularJS이다. * AngularJS : 단일 페이지 웹 어플리케이션 개발을 위한 자바스크립트 프레임워크 하지만, AngularJS는 규모가 큰 어플리케이션의 개발에 있어, 성능이슈를 보이고 복잡한 지시자 문법이 개발시에 큰 어려움을 줬다.이러한 어려움을 해결하기 위해 등장하게 된 개념이 Web Component 기반의 개발이다.이 Web Component는 Custom element, Shadow DOM, HTML Import, HTML template의 종합적인 개념으로, 웹 플랫폼 자체에서 재사용이 가능한 컴포넌트 기반으로 개발하는 것이 가능해졌다. 이를통해 기존의 성능이슈와 개발 생산성 문제를 해결할 수 있었다. 이때부터 기존의 프레임워크였던 AngularJS가 플랫폼인 Angular2 혹은 Angular4로 발전하게 되었다. (1) 회색 부분 : Angular의 Core한 부분(2) Universal : SSR(Server Side Rendering)을 위한 부분(3) Service Worker : PWA(Progressive Web App)을 지원하는 부분(4) Material : Mobile/Desktop에 사용가능한 UI 컴포넌트 지원(5) Angury : 디버깅 툴 Angular v1.x Angular(v2, v4, ...) 중첩 Scope(\"$scope\", watchers) O X 지시자와 컨트롤러 O(v1.5에서 component api추가) X 비즈니스 로직 구현 Controller를 구현한 함수 타입 스크립트(ES6) 클래스에 비즈니스 로직구현 자바스크립트 모듈 시스템 require.js를 통한 구현 타입 스크립트(ES6) 모듈 시스템 지원 Angular CLIAngular CLI를 활용하여 터미널이나 콘솔을 통해 프로젝트 생성나 컴포넌트, 라우터, 서비스를 생성할 수 있다. 123456# angular CLI 설치$ sudo npm i -g @angular/cli# Angular CLI 버전 확인$ ng --version# 프로젝트 생성$ ng new [project name] Angular 프로젝트 생성Angular는 기본적으로 TypeScript를 기반으로 작성을 하고, 프로젝트를 만들면 src 폴더 하위에 아래와 같은 구성으로 생성이된다. src 폴더구성(1) app : 실제 어플리케이션 관련 파일들 위치(2) assets : 이미지, 리소스(3) environment : Prod Mode로 실행을하면, *.prod.ts에 있는 내용이 *.ts파일로 덮어써지게 된다. (서버 URL과 같은 배포에 필요한 정보들이 덮어써진다) 프로젝트 실행 1$ ng serve 자주 사용되는 명령어 확인 1$ ng help (1) –aot(ahead of time compilation) (2) –watch (3) ng completion : command line을 자동완성 (4) ng doc : Angular 공식 페이지 열기 (5) ng e2e : E2E 테스트 실행 (6) ng eject : Angular CLI가 webpack 기반인데 webpack configuration을 빼서 customize하고 싶을 때 사용한다. (7) ng generate : 새로운 컴포넌트, 라우터, 모듈, 서비스, 클래스를 Angular CLI를 통해서 만들때 사용된다. (8) ng lint : 코드의 스타일을 확인할때 사용된다. (9) ng serve : 빌드후에 프로젝트를 서버로 띄어줄때 사용한다.","link":"/2021/06/11/202106/210611-angular_study/"},{"title":"210617 Angular (작성중...)","text":"이미 MERN(MongoDB,Express, React (+ Redux), NodeJS)의 구성으로 프로젝트를 진행해본 경험이 있기 때문에 Angular의 사용에 익숙해지면 좀 수월해질 것 같다. 그럼 Angular에 중점을 두고 MEAN의 구성으로 프로젝트를 만들어가면서 Angular의 사용에 익숙해지도록 해보자. MEANM - MongoDBE - ExpressJSA - AngularN - NodeJS Angular에서 반복적으로 자주 사용되는 부분에 대해 복습을 하자. 컴포넌트의 selector는 snake case로 작성template에서 커스텀 컴포넌트를 사용하는 경우, 각 컴포넌트에 정의되어있는 selector를 사용한다.이 selector는 snake case로 작성하도록 한다. @NgModule - declarations상위 컴포넌트의 template 내에서 외부에서 작성한 커스텀 컴포넌트를 사용하기 위해서는 상위 컴포넌트가 정의되어있는 module decorator의 declarations에 정의해줘야 한다.component가 아닌 moudle을 포함시키는 경우에는 imports에 module을 추가해준다.exports는 외부에서 특정 컴포넌트를 사용하도록 해줄때 컴포넌트를 정의하는 부분이다. (확인필요) @input() 데코레이터상위 컴포넌트 템플릿에서 하위 커스텀 컴포넌트의 속성으로 상위 컴포넌트의 속성값을 넣어주는 경우,(React에서 부모 컴포넌트가 자식 컴포넌트에 props를 넘겨주는 것과 같은 경우) 하위 컴포넌트의 로직이 정의된 컴포넌트 클래스 내에 해당 속성의 변수를 정의하고 앞에 @input() 데코레이터를 사용해서 정의해준다. @output() 데코레이터자식 컴포넌트에서의 이벤트를 통해 부모 컴포넌트의 클래스에 정의된 속성값을 업데이트하는 경우,(React에서 자식 컴포넌트가 부모 컴포넌트로부터 이벤트 props를 전달받아 이벤트를 통해 부모 컴포넌트의 상태 데이터를 업데이트 하는 것과 같은 경우) React와 같이 부모 컴포넌트에서 속성에 대한 업데이트를 위한 함수를 자식 컴포넌트에 emitter를 통해 전달을 하고 있다. 자식 컴포넌트 클래스에서는 EventEmitter 인스턴스 변수를 @Output 데코레이터와 함께 선언하고 EventEmitter 인스턴스가 사용되는 함수를 정의한다. 자식 컴포넌트 클래스 123456789101112131415161718export class AddTodoComponent implements OnInit { // 부모 컴포넌트에 있는 todos 리스트에 event emitter를 통해 추가시키기 위해서 // 아래와 같이 이벤트를 정의한다.(@Output) // EventEmitter의 타입은 string 제네릭 타입이다. (전달되는 값이 string newText) @Output() onTodoAdded = new EventEmitter&lt;string&gt;(); newText: string; constructor() { this.newText = ''; } ngOnInit() {} addTodo(newText: string) { this.onTodoAdded.emit(newText); this.newText = ''; }} 부모 컴포넌트 클래스부모 컴포넌트 클래스에서는 부모 컴포넌트의 템플릿의 자식 컴포넌트에 전달된 emitter 속성값인 함수를 부모 컴포넌트 클래스 내부에서 같은 이름의 함수로 정의하여 클래스 내부에서 이벤트가 발생하도록 한다. 12345678910111213141516171819202122232425export class TodosComponent implements OnInit { newText = ''; todos: Todo[]; today: Date = new Date(); constructor() { this.todos = [ { done: false, text: '운동하기' }, { done: true, text: '공부하기' } ]; } ngOnInit(): void {} toggleTodo(todo: Todo) { todo.done = !todo.done; } addTodo(text: string) { this.todos.push({ done: false, text: text }); }} 부모 컴포넌트의 템플릿자식 컴포넌트의 속성으로 (자식 컴포넌트에서 정의된 @Output() emitter variable name) = “부모 컴포넌트 클래스에서 이벤트 메서드”의 형태로 정의한다.자식 컴포넌트 클래스와 부모 컴포넌트 클래스의 이벤트 메서드의 이름이 같게 작성을 해서 헷갈리지만, 부모 컴포넌트의 템플릿에 정의된 자식 컴포넌트의 속성으로 들어간 emitter에 대한 이벤트 메서드는 부모 컴포넌트 클래스에 정의된 이벤트 메서드이다. 1234567891011121314&lt;div class=&quot;title&quot;&gt; &lt;h1&gt;나의 하루&lt;/h1&gt; &lt;h2&gt;{{ today | date:&quot;M월 d일&quot; }}&lt;/h2&gt;&lt;/div&gt;&lt;div&gt; &lt;div *ngFor=&quot;let todo of todos&quot; (click)=&quot;toggleTodo(todo)&quot;&gt; &lt;app-todo [todo]=&quot;todo&quot;&gt;&lt;/app-todo&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt; &lt;!-- $event는 자식 컴포넌트로부터 방출된 데이터로, newText 데이터이다. --&gt; &lt;app-add-todo (onTodoAdded)=&quot;addTodo($event)&quot;&gt;&lt;/app-add-todo&gt;&lt;/div&gt;{{ todos | json }} 재사용되는 객체의 타입정의 interface재사용되는 객체의 타입정의 interface 파일의 경우에는 별도의 폴더를 만들어서 관리하도록 한다. todo.model.ts 1234export interface Todo { done: boolean; text: string;} 단방향 바인딩과 양방향 바인딩 활용단방향 바인딩 방식으로 textarea에서 입력한 값을 버튼 클릭 이벤트 메서드에 전달해서 처리하기 위해서는 HTML 태그의 속성에 value와 #[id]에 대한 정의가 필요하다. post-create.component.html 1234&lt;textarea rows=&quot;6&quot; [value]=&quot;newPost&quot; #postInput&gt;&lt;/textarea&gt;&lt;hr /&gt;&lt;button (click)=&quot;onAddPost(postInput)&quot;&gt;Save Post&lt;/button&gt;&lt;p&gt;{{ newPost }}&lt;/p&gt; post-create.component.ts #[id]로 전달된 textarea HTML 요소는 아래와 같이 버튼의 클릭 이벤트 메서드를 통해 newPost의 값을 재정의한다. 1234567export class PostCreateComponent { newPost = 'NO CONTENT'; onAddPost(postInput: HTMLTextAreaElement) { this.newPost = postInput.value; }} 위의 예시에서는 단방향으로 value와 hashtag+id를 통해 사용자로부터 입력받은 값을 초기화했다면, value와 hashtag+id를 정의하지 않고양방향 바인딩 방식으로 사용자 입력 데이터를 바인딩해줄 수 있다. 양방향 데이터 바인딩을 하기 위해서는 @angular/forms로부터 제공되는 FormsModule을 @NgModule의 메타 데이터 객체의 imports에 정의를 해줘야 된다. 1&lt;textarea rows=&quot;6&quot; [(ngModel)]=&quot;enteredValue&quot;&gt;&lt;/textarea&gt; 이처럼 Angular는 template binding 특징을 가지고 있으며, 이는 event binding, property binding, string interpolation, one-way/two-way binding을 통해 구현할 수 있다. Angular Materialhttps://material.angular.io/ 1$ ng add @angular/material 설치된 dependency를 확인해보면, @angular/material @angular/cdk가 설치된 것을 알 수 있다.material package는 이 두 가지 패키지로 구성이 되어있다.(1) @angular/material : component logic + styling(2) @angular/cdk : component logic Angular Material의 prebuilt style 적용하기angular.json 파일의 styles 속성에 설치한 material의 prebuilt style을 적용할 수 있다. 123&quot;styles&quot;: [ &quot;@node_modules/@angular/material/prebuilt-themes/indigo-pink.css&quot;], 템플릿에 스타일링을 적용하기 위해서 최상위 모듈의 @NgModule 메타 객체의 imports에 사용하고자하는 material 스타일링 모듈을 포함시켜야 한다.(자세한 내용은 공식문서를 참고한다)→ https://material.angular.io/components/input/overview 구조 디렉티브 ngIf와 ngFor의 사용템플릿의 HTML 태그의 속성에 ngIf와 ngFor을 넣어 조건과 반복구문 처리를 할 수 있다. 1234567&lt;mat-accordion multi=&quot;true&quot; *ngIf=&quot;posts.length &gt; 0&quot;&gt; &lt;mat-expansion-panel *ngFor=&quot;let post of posts&quot;&gt; &lt;mat-expansion-panel-header&gt; {{ post.title }} &lt;/mat-expansion-panel-header&gt; &lt;p&gt;{{ post.content }}&lt;/p&gt; &lt;/mat-expansion-panel&gt;&lt;/mat-accordion&gt;&lt;p *ngIf=&quot;posts.length &lt;= 0&quot;&gt;No posts&lt;/p&gt; 위와 같이 중첩의 형태로 ngIf와 ngFor를 사용할 수 있지만, 경우에 따라 구조 디렉티브를 사용하기 위해 불필요한 HTML 요소를 추가하는 경우가 있기 때문에이러한 경우에는 ng-container와 ng-template을 사용한다. Angular에서의 ng-container는 React에서의 Fragment (&lt;&gt;&lt;/&gt;)와 같다. 실제 DOM에는 추가되지 않고 복수의 HTML 요소를 묶어서 사용할 수 있다. 123456&lt;div&gt; &lt;ng-container * ngIf=&quot;true&quot;&gt; &lt;h1&gt;타이틀&lt;/h1&gt; &lt;div&gt;컨텐츠&lt;/div&gt; &lt;/ng-container&gt;&lt;/div&gt; ng-template 또한 실제 DOM에는 추가되지 않고 복수의 HTML 요소를 묶을 수 있지만, ng-template으로 묶게 되면 항상 HTML element가 보이는 것이 아니기 때문에 ngIf로 조건처리를 반드시 해줘야 한다.반면에 ng-container는 그룹화한 내부 HTML 요소를 항상 표현하기 때문에 이 점에서 ng-template과 차이가 있다. Angular에서 제공하는 form 요소기존에 input 컨트롤러에서는 양방향 데이터 바인딩을 통해 컨트롤러의 값을 정의했다.하지만 form 태그로 각 각의 컨트롤러를 감싸주게 되면, 양방향 바인딩으로 컨트롤러의 값을 정의하지 않고, 아래와 같이 간단하게 처리할 수 있다.(form 요소가 감지되면, module에서 정의한 FormsModule이 자동으로 자바스크립트 객체를 내부적으로 생성해준다. 따라서 양방향 바인딩으로 컨트롤러 값을 정의하지 않아도 아래와 같이 간단하게 input 태그를 컨트롤러로써 등록해서 사용할 수 있다) 컨트롤러의 속성에 ngModel과 name 속성을 정의해주면 된다. 그리고 form 태그가 기본 HTML 태그이기 때문에 내부에서 사용되는 버튼에 별도의 이벤트 바인딩을 하지 않고, submit 타입으로 정의해주면 된다. 1234567891011&lt;mat-card&gt; &lt;form (submit)=&quot;onAddPost()&quot;&gt; &lt;mat-form-field&gt; &lt;input matInput type=&quot;text&quot; name=&quot;title&quot; ngModel&gt; &lt;/mat-form-field&gt; &lt;mat-form-field&gt; &lt;textarea matInput rows=&quot;4&quot; name=&quot;content&quot; ngModel&gt;&lt;/textarea&gt; &lt;/mat-form-field&gt; &lt;button mat-raised-button color=&quot;accent&quot; type=&quot;submit&quot;&gt;Save Post&lt;/button&gt; &lt;/form&gt;&lt;/mat-card&gt; form 태그 내에서 정의된 값들에 접근하기 위해서 태그의 속성에 local reference를 정의한다. local reference만 정의하면 이는 html 요소 객체에 접근하는 것이기 때문에 form 태그 내에 정의된 값에 대한 객체에 접근하기 위해서 ngForm을 local reference의 값으로 정의해야 한다. 1234567891011&lt;mat-card&gt; &lt;form (submit)=&quot;onAddPost(postForm)&quot; #postForm=&quot;ngForm&quot;&gt; &lt;mat-form-field&gt; &lt;input matInput type=&quot;text&quot; name=&quot;title&quot; ngModel&gt; &lt;/mat-form-field&gt; &lt;mat-form-field&gt; &lt;textarea matInput rows=&quot;4&quot; name=&quot;content&quot; ngModel&gt;&lt;/textarea&gt; &lt;/mat-form-field&gt; &lt;button mat-raised-button color=&quot;accent&quot; type=&quot;submit&quot;&gt;Save Post&lt;/button&gt; &lt;/form&gt;&lt;/mat-card&gt; submit의 이벤트 메서드의 인자로 local reference를 전달해주게 되면, 아래와 같이 컨트롤러에 정의한 name 속성을 통해 값을 참조할 수 있다. 1234567891011export class PostCreateComponent { enteredContent = ''; enteredTitle = ''; // 외부 참조 app.component.html @Output() postCreated = new EventEmitter&lt;Post&gt;(); onAddPost(form: NgForm) { const post: Post = { title: form.value.title, content: form.value.content}; this.postCreated.emit(post); }} input 태그 입력에러 처리 12345678910111213141516171819&lt;form (submit)=&quot;onAddPost(postForm)&quot; #postForm=&quot;ngForm&quot;&gt; &lt;mat-form-field&gt; &lt;input matInput type=&quot;text&quot; name=&quot;title&quot; ngModel required minlength=&quot;3&quot; #title=&quot;ngModel&quot; /&gt; &lt;!-- form 태그의 local reference를 통해 title input 컨트롤러에 접근 --&gt; &lt;mat-error *ngIf=&quot;postForm.getControl('title').invalid&quot; &gt;{{getErrorMessage()}}&lt;/mat-error &gt; &lt;!-- input 태그에 title local reference=&quot;ngModel&quot;을 넣어주는 경우 --&gt; &lt;mat-error *ngIf=&quot;title.invalid&quot;&gt;{{getErrorMessage()}}&lt;/mat-error&gt; &lt;/mat-form-field&gt;&lt;/form&gt; long chains of property and event binding 만약에 특정 요소를 얻기위한 속성/이벤트 바인딩이 A라는 컴포넌트에서 B 컴포넌트로, B 컴포넌트에서 C 컴포넌트로, C 컴포넌트에서 D 컴포넌트로 연쇄적으로 체이닝된다면, 이는 큰 어플리케이션을 개발할때 문제가 될 소지가 있다. 좀 더 효율적으로 데이터를 전달하기 위해서는 service class를 정의해서 사용해야 한다.새롭게 생성한 service class는 components에 주입해서 사용한다. 이는 속성과 이벤트 바인딩 없이 컴포넌트에 데이터를 전달하는데 용이하다.(react의 redux와 같이 전역 storage에서 상태 데이터를 관리하는 것과 비슷하다) posts.service.ts 123456789101112131415import { Post } from &quot;./post.model&quot;;export class PostsService { private posts: Post[] = []; getPosts() { // copy posts object return [...this.posts]; } addPost(title: string, content: string) { const post: Post = { title, content }; this.posts.push(post); }} Angular는 복잡한 DI(Dependency Injection) 시스템을 가지고 있기 때문에 component class의 생성자의 인자로 앞서 작성한 service를 넣어서 Angular에게 알려주면 Angular는 service instance를 제공해준다.제공된 service instance는 클래스 내부의 변수에 초기화를 시켜주면 된다. 123456789101112131415161718import { Component, OnInit, Input } from '@angular/core';import { Post } from 'src/app/share/post.model';import { PostsService } from 'src/app/share/posts.service';@Component({ selector: 'app-post-list', templateUrl: './post-list.component.html', styleUrls: ['./post-list.component.css']})export class PostListComponent implements OnInit { @Input() posts: Post[]; postsService: PostsService; constructor(postsService: PostsService) { this.posts = []; this.postsService = postsService; }} 하지만 타입스크립트에서 shortcut을 제공하기 때문에 생성자의 인자로 넣은 service 인자를 public 접근제어자로 선언을 해주면, 자동으로 클래스 내부에 속성을 생성해준다. 12345678910111213141516import { Component, OnInit, Input } from '@angular/core';import { Post } from 'src/app/share/post.model';import { PostsService } from 'src/app/share/posts.service';@Component({ selector: 'app-post-list', templateUrl: './post-list.component.html', styleUrls: ['./post-list.component.css']})export class PostListComponent implements OnInit { @Input() posts: Post[]; constructor(public postsService: PostsService) { this.posts = []; }} 추가적으로 생성한 service 파일을 Angular가 스캔할 수 있도록 아래의 두 가지 방법 중 한 가지로 처리해줘야 한다. method1) 최상위 module 데코레이터의 providers 메타객체에 작성한 서비스를 넣어준다. method2) 작성한 service 클래스에 @Injectable() 데코레이터를 작성해준다. 1234567891011121314151617import { Injectable } from &quot;@angular/core&quot;;import { Post } from &quot;./post.model&quot;;@Injectable({providedIn: 'root'})export class PostsService { private posts: Post[] = []; getPosts() { // copy posts object return [...this.posts]; } addPost(title: string, content: string) { const post: Post = { title, content }; this.posts.push(post); }} 위의 두 가지 방법은 전체 앱에 하나의 service instance를 생성해준다. constructor와 ngOnInit() constructor 메소드는 자바스크립트 엔진이 호출하는 주체이다. 따라서 Angular 컴포넌트의 생애주기를 다루기에는 적합하지 않다. constructor를 사용하는 경우앞서 살펴보았듯이 작성한 service 클래스를 컴포넌트에 전달하는 의존성 주입을 하기 위해서 생성자를 사용하였다. ngOnInit을 사용하는 경우ngOnInit은 Angular 컴포넌트 초기화가 완료된 시점을 알기 위해 사용된다. 만약에 부모 컴포넌트에서 자식 컴포넌트로 값을 전달하고, 자식 컴포넌트의 클래스에서 @Input() 데코레이터로 값을 바인딩한다고 가정하자.이 @Input() 데코레이터로 값을 바인딩한 속성으로의 접근은 ngOnInit()내에서는 가능하지만, 생성자 내에서는 undefined이다. 따라서 바인딩된 데이터의 값을 읽기 위해서는 ngOnInit 생애주기 훅 내에서 처리한다.","link":"/2021/06/17/202106/210617-angular_study/"},{"title":"210620 RxJS (작성중...)","text":"본 블로그 포스팅은 RxJS와 관련하여 네이버의 손창욱님이 올려주신 유튜브 영상을 보고 정리해보았다. RxJS를 처음 접하는 나와 같은 사람도 정말 쉽게 이해할 수 있도록 설명을 해주셔서 너무 유익했다. 모든 어플리케이션은 궁극적으로 상태머신들의 집합 Input(입력값)이 들어오면 logic을 통해 state가 업데이트되고, 업데이트된 state는 logic을 통해 Output(결과값)이 나온다. RxJSRxJS는 비동기 처리를 원할하게 할 수 있도록 도와주는 데이터 스트림으로 정의할 수 있다. RxJS를 학습하기 위해서는 여러 새로운 용어들이 등장한다.(개별적으로 정리해서 공부) 동기/비동기 방식의 사용동기 방식은 흐름이 그대로 가기 때문에 흐름을 쉽게 파악할 수 있지만, 비동기 처리는 굉장히 느린 처리나 사용자로부터 빠르게 interaction을 받아와야 하는 경우 그리고 높은 퍼포먼스가 요구되는 상황에서 사용이 된다. RxJS의 역할RxJS이외에도 Callback, Promise, Generator, Async/Await과 같은 표준을 사용하여 비동기 처리를 할 수 있는데 왜 굳이 RxJS를 사용해야 될까? Callback함수를 사용해서 비동기 처리를 하는 경우, 에러에 대한 처리가 어렵고, 콜백 헬등의 분제가 발생한다.Promise를 사용하는 것이 좀 더 나은 방법이긴 하지만, 한 번에 하나의 데이터를 처리하기 때문에 연속성을 갖는 데이터를 처리할 수 없다는 점과 서버로 보낸 요청을 취소할 수 없다는 단점이 있기 때문에 Observable을 사용하여 위의 단점을 보안해서 비동기 처리를 할 수 있다. 그리고 RxJS는비동기 처리 영역, 데이터 전파, 데이터 처리를 담당하며, 일관된 방식으로 안전하게 데이터 흐름을 처리하는 라이브러리이다.여기서 일관된 방식이란 입력된 값의 타입에 따라 각 각 따로 처리를 하지 않고 하나의 방식으로 처리를 할 수 있다는 말이다. [참고] 입력값에 따른 데이터 처리개발자가 처리하는 입력값에는 어떤 것들이 있을까?(1) 배열 데이터, 함수 반환값 : 동기 데이터를 처리하는 방식(2) 키보드/마우스 입력, 원격지 데이터, DB 데이터 : 비동기 데이터를 처리하는 방식 위와 같이 RxJS이외의 Callback, Promise, Generator, Async/Await과 같은 표준을 사용하여 비동기 처리를 하게 되면, 데이터 처리 방식이 제각각이기 때문에 각기 다른 방식으로 처리를 해줘야 한다. 하지만 RxJS는 동기/비동기와 관계없이 데이터를 시간축을 기준으로 연속적인 데이터 스트림으로 처리한다. 위와같이 모든 데이터를 시간의 축을 기준으로 배열의 컬랙션이 오는 형태로 생각하면, 시간축 관점에서 결국 동기와 비동기는 같다는 결론이 나온다. → RxJS는 하나의 방식으로 처리한다. (인터페이스의 단일화) rxjs는 이러한 입력 데이터들을 각 각 다른 형태로 처리하지 않고, 하나의 시간 축에 따른 배열 컬랙션으로 생각하고 모두 일관된 하나의 방식으로 처리하게 된다. (모두 Observable로 처리) 이 결과 개발자는 좀 더 로직에 집중을 해서 작업을 할 수 있다. 의존관계가 있는 상태머신의존관계가 있는 상태머신에 변경된 상태 정보를 다른 상태머신에 어떻게 전달해야되는지에 대한 고민이 있을 수 있다. [Reactive Programming 위키피디아 참고] 데이터 흐름과 상태 변화 전파에 중점을 둔 프로그램 패러다임으로, 프로그래밍 언어에서 데이터 흐름을 쉽게 표현할 수 있어야 하며 기본 실행 모델이 변경 사항을 데이터 흐름을 통해 자동으로 전파한다는 것을 의미. 개발자가 변경된 데이터에 대한 처리를 하지 않아도 데이터 흐름을 통해 자동으로 빠르게 전파하게 된다는 의미이다. Observer pattern자 그럼 Observer 패턴에 대해서 살펴보자. Observer 패턴은 위와같이 상태변화가 발생하는 Subject(Observable)와 Subject의 상태 변화에 대한 대응을 하는 Observer로 구성이 되어있다. Subject의 변경사항이 생기면 자동으로 Observer의 update를 호출한다.이는 Reactive 프로그래밍의 Push-scenario(외부 환경이 어플리케이션에 데이터를 밀어넣는다)방식으로 동작하는 어플리케이션을 말한다. 필요한 데이터를 획득하기 위해서 어플리케이션이 외부 환경에 요청을 하여 데이터를 획득하는 것이 아닌, 어플리케이션은 외부 환경을 관찰하고 있다가 외부 환경에서 데이터 스트림을 방출하면 그것에 반응하여 데이터를 획득하는 것이다. Observable외부환경에서 어플리케이션의 내부로 연속적으로 흐르는 데이터(데이터 스트림)를 생성하고 방출하는 객체를 말한다. ObserverObservable이 방출(omit)한 Notification(Observable이 방출할 수 있는 푸시 기반의 이벤트 또는 값)을 획득하여 사용하는 객체를 말한다. Observer는 데이터 소비자로 데이터를 생산하는 Observable을 구독해서 Observable의 상태를 관찰한다. 그리고 Observable이 방출한 notification은 Observer에게 자동으로 전파된다. 예시)TV 방송국(Observable) / TV(Observer) / 영상정보 프레임(Observable notification) 구현의 관점에서의 구독(Subscribe) 오퍼레이터Observable의 subscribe operator를 호출할 때 인자로 Observer를 전달하는 것을 말한다. 예시posts.service.ts 123456789101112131415161718192021222324252627import { Injectable } from &quot;@angular/core&quot;;import { Post } from &quot;./post.model&quot;;import { Subject } from 'rxjs';@Injectable({providedIn: 'root'})export class PostsService { private posts: Post[] = []; private postsUpdated = new Subject&lt;Post[]&gt;(); // service를 fetching하고 있는 컴포넌트들로부터 직접적으로 posts 데이터를 조작하는 것을 피하기 위해 // 직접 Posts 정보를 가져오지 않고, 복사해서 가져오는 것이 좋다. // event-driven approach @Output emitter를 통해서 처리할 수도 있지만, // rxjs를 사용해서 처리할 수 있다. (Angular의 일부는 아니지만 core dependency) getPosts() { return [...this.posts]; } getPostUpdateListener() { return this.postsUpdated.asObservable(); } addPost(title: string, content: string) { const post: Post = { title, content }; this.posts.push(post); this.postsUpdated.next([...this.posts]); }} Observable 생성자를 new 연산자로 호출해서 Observable을 생성하고 인수로 subscriber 함수를 전달한다.여기서 전달한 subscriber 함수는 Observable의 역할인 데이터 스트림을 생성하고 방출하는 처리를 정의한 함수이다. subscriber 함수는 생성자의 인자로 전달되고 실행되지 않고, subscribe operator에 의해 Observable이 구독될 때 호출되는 callback\u0010함수이다. observable은 subscribe되기 전까지 동작하지 않는다. subscriber 함수의 구성subscriber 함수는 next, error, complete 메소드를 사용해서 notification을 방출한다.방출된 notification은 subscribe operator의 인자로 전달되서 Observable을 구독하고 있는 모든 Observer의 next, error, complete 메소드가 방출된 notification에 반응하여 동작한다. 예시post-list.component.ts 1234567891011121314151617181920212223242526272829303132333435363738394041424344import { Component, OnInit, Input, OnDestroy } from '@angular/core';import { Post } from 'src/app/share/post.model';import { PostsService } from 'src/app/share/posts.service';import { Subscription } from 'rxjs';@Component({ selector: 'app-post-list', templateUrl: './post-list.component.html', styleUrls: ['./post-list.component.css']})export class PostListComponent implements OnInit, OnDestroy { // service로 대체 // @Input() posts: Post[]; posts: Post[]; private postsSub: Subscription; constructor(public postsService: PostsService) { // this.posts = [ // {title: 'First Post', content: 'This is the first post\\'s content'}, // {title: 'Second Post', content: 'This is the second post\\'s content'}, // {title: 'Third Post', content: 'This is the third post\\'s content'}, // ]; this.posts = []; } ngOnInit(){ // initial tasks this.posts = this.postsService.getPosts(); // subscribe에는 3개의 인수를 넣어줄 수 있다. // (1 arg) 새로운 데이터가 방출되면, 실행되는 함수 // (2 arg) 에러가 발생하는 경우에 호출되는 함수 // (3 arg) Obserable이 완료되었을때 (더이상 기대되는 값이 존재하지 않을때) this.postsSub = this.postsService.getPostUpdateListener() // NEW DATA EMITTED, ERROR, COMPLETE .subscribe((posts: Post[]) =&gt; { this.posts = posts; }); } // 메모리 누수를 방지하기 위해서 unsubscribe ngOnDestroy() { this.postsSub.unsubscribe(); }} Observable과 Subject의 차이일반적인 Observable은 “passive”한 특성을 가지고 있다.(e.g.wraps callback, event, …) 반면에 Subject는 “active”한 특징(can be triggered from code)을 가지고 있다. Subject는 next()를 통해 방출되는 데이터 스트림을 Observer가 구독함으로써 새로운 데이터가 방출될때 자동으로 업데이트된 새로운 데이터 스트림을 감지할 수 있다. Angular HTTP Client데이터베이스에서 데이터를 fetch하기 위해서 Angular에서 기본으로 제공해주는 HTTP Client module을 사용하면 간단하게 처리할 수 있다. 12import { HttpClientModule } from '@angular/common/http';&lt;!-- 최상위 module의 import에 추가 --&gt; 12345678910111213141516171819202122232425262728293031323334import { Injectable } from &quot;@angular/core&quot;;import { Post } from &quot;./post.model&quot;;import { Subject } from 'rxjs';import {HttpClient} from '@angular/common/http';@Injectable({providedIn: 'root'})export class PostsService { private posts: Post[] = []; private postsUpdated = new Subject&lt;Post[]&gt;(); // service를 fetching하고 있는 컴포넌트들로부터 직접적으로 posts 데이터를 조작하는 것을 피하기 위해 // 직접 Posts 정보를 가져오지 않고, 복사해서 가져오는 것이 좋다. // event-driven approach @Output emitter를 통해서 처리할 수도 있지만, // rxjs를 사용해서 처리할 수 있다. (Angular의 일부는 아니지만 core dependency) constructor(private http: HttpClient){} getPosts() { // return [...this.posts]; this.http.get&lt;{message: string, posts: Post[]}&gt;('http://localhost:3000/api/posts') // new data, error, complete .subscribe((postData) =&gt; { this.posts = postData.posts this.postsUpdated.next([...this.posts]); }); } getPostUpdateListener() { return this.postsUpdated.asObservable(); } addPost(title: string, content: string) { const post: Post = { id: null, title, content }; this.posts.push(post); this.postsUpdated.next([...this.posts]); }} [참고] 모던 자바스크립트 Deep Dive 13.16 Angular RxJS","link":"/2021/06/20/202106/210620-rxjs/"},{"title":"210627 회사업무 적응 1주차 리뷰","text":"새로운 환경과 새로운 습관뭐 예상했던 부분이지만, 일을 시작하고 나서 매일매일 자기개발을 하던 습관이 시들해지고 있다.아직 새로운 환경에 적응하는 시기라는 이유로 합리화를 하고는 있지만, 그래도 경험상으로 비춰보면 이러한 자기 합리화가 또 다른 나 자신의 안일함 조장한다고 생각한다. 그래서 고정적으로 한 가지 습관을 만들고자 한다.바로 매주 업무를 통해서 배웠고 느꼈던 점에 대해서 일요일마다 나의 블로그에 포스팅하는 것이다. 업무에 대한 구체적인 내용은 적을 수 없지만, 개괄적으로 어떤 업무를 맡게 되었고, 그 업무를 진행하면서 어떤 점을 배우게 되었는지, 그리고 향후에 그 업무를 처리하기 위해서 어떤 부분에 대한 추가 학습이 필요한지에 대해서 내용을 정리해보고자 한다. 위의 내용들을 정리해두면 평일에 업무가 종료된 후에 새롭게 학습한 내용에 대해서 정리를 할 수 있게 되고, 그 다음 피드백을 할 수 있게 되어 선 순환 구조가 될 수 있을 것이라고 생각한다. 업무 1주차 : 프로모션 페이지 새롭게 담당하게 된 업무입사 후에 처음으로 담당하게 된 업무는 프로모션 페이지를 만드는 작업이다.모바일이 아닌 데스크탑 기준의 레이아웃 작업만 하면 되기 때문에 별도의 반응형 작업을 할 필요가 없다.이후에 모바일에 대해서는 별도의 프로젝트가 진행될 예정이라고 하셨다. 이번 프로모션 페이지에는 정적인 페이지에서 사용자의 외부입력을 통해 페이지에 동영상이나 음성을 재생하는 기능을 포함하고 있다. 업무를 통해 배운점과 느낀점 업무에 대한 배경지식의 중요성처음 위의 업무를 배당받았을때 구체적으로 기존 서비스에서 어떤 부분을 홍보하기 위한 목적에서의 프로모션 페이지인지 생각하지 않고, 우선 디자이너로부터 받은 디자인 시안에 맞게 페이지를 작성하기 시작했다.아마 이 부분이 좀 실수였지 않았나 싶다. 왜냐하면 새롭게 추가되는 기능이 어떤 것인지에 대한 배경지식을 갖은 상태에서 작업을 했더라면, 페이지를 만들면서 디자이너나 다른 개발자 분에게 여러차례 확인을 하면서 작업을 이어나갔을텐데 내가 담당하게 된 업무의 배경지식이 많이 부족했다. 영상과 음성 리소스프로모션 페이지의 작업을 하면서 페이지에 이벤트 처리에 대한 부분이 두 부분 포함되어있다는 것을 확인할 수 있었다. 그리고 디자이너로부터 받았던 Zeplin 디자인 시안 이외에 동영상과 오디오 관련 파일을 확인했다.단순히 확인에만 그치지 않고 용량은 어떻게 되는지, 해당 리소스 파일들을 어떤식으로 호출해서 사용해야 되는지에 대한 고민을 했었어야 했다.영상 및 이미지 파일은 *.mov, *.webm, *.png확장자의 파일들이었고, 용량은 로컬에 보관해서 호출하기에는 무리가 있는 사이즈였다. 왜 다양한 확장자의 영상 파일이 주어졌는가?다양한 확장자의 영상 파일이 주어진 이유는 여러 브라우저에서 재생할 수 있는 영상의 확장자가 재각각이기 때문이다. 브라우저 비디오 오디오 mp4 webm ogv mp3 ogg 인터넷 익스플로러 O (9+) X X O (+9) X 크롬 O (all) O (25+) O (all) O (all) O (all) 파이어폭스 O (35+) O (28+) O (3.5+) O (22+) O (3.5+) 사파리 O (3.2+) X X O (4+) X 오페라 O (25+) O (16+) O (11.5+) O (15+) O (11.5+) iOS 사파리 O (all) X X O (4.1+) X 안드로이드 브라우저 O (4.4+) X X O (2.3+) O (2.3)","link":"/2021/06/27/202106/210627-1-week-review/"},{"title":"210704 회사업무 적응 2주차 리뷰","text":"프로모션 페이지 작업 회고처음에는 금방 마무리될 줄 알았던 프로모션 페이지 작업이 생각보다 오래 걸렸다. 그래서 이번 포스팅에서는 오래걸린 이유와 코드리뷰를 통해서 배운점들을 위주로 정리를 해보려고 한다.이번에 정리하는 포스팅은 정기적으로 코드를 작성할 때 참고하여 다음에 작성하게 될 코드에서는 같은 지적을 받지 않도록 해야겠다. 생각보다 작업이 오래걸린 이유 레이아웃 잡기이번에 작업이 오래걸린 이유 중에 하나는 레이아웃을 잡는데 시간이 많이 소요된 것이다.그렇게 복잡하지 않은 구조의 레이아웃이라 크게 고민을 하지 않고 마크업을 하고 스타일링을 했었는데, 예기치 못한 부분에서 레이아웃이 약간 어긋나버리는 경우가 생겼었다.이번을 계기로 레이아웃을 잡는 연습을 좀 해야겠다는 생각이 든다. 해상도에 따른 이미지 태그와 border 사이의 갭이전에 이미지 태그를 사용해서 작업을 했을 때 이미지 태그의 하단에 갭이 생겨서 vertical-align 속성을 활용해서 해결을 한 적이 있었는데, 이번에는 이미지 클릭시에 이미지 태그 주변에 border-line을 표시하는 이벤트를 처리하면서 또 다른 문제에 직면했다. 그 문제는 border와 이미지 태그 사이에 약간의 갭이 표시되는 문제였다. 해상도에 따라 갭의 위치와 두께가 지속적으로 변해서 어떻게 해결을 해야하나 고민을 했었다.처음에 시도한 방법은 이미지 태그를 사용하지 않고 div 태그를 사용하는 것이었다. div태그의 배경으로 이미지를 처리하고 div에 border-line을 넣어주었는데 약간의 갭이 생기는 문제는 해결되었지만, 클릭하고 border가 생겼을때, 내부의 이미지가 좌측하단으로 살짝 밀리는 느낌이 있었다. 지금 생각해보면 배경을 설정할때 별도의 위치 속성에 center를 주었다면 해결 할 수 있었을 것이라고 생각한다. 두 번째로 시도한 방법은 이미지 태그에 배경을 주는 것이었다. 기존의 코드에서 CSS로 이미지 태그에 배경을 넣어주었는데, 정말 거짓말처럼 이전에 생겼던 문제가 발생하지 않았다.정말 사소하지만 혹시나 다른 분이 작업을 하다가 나와 같은 문제가 생긴다면, 같이 공유해도 좋을 것 같다는 생각이 들었다. Angular 라이프 사이클 훅 이번에 작업을 하면서 제플린 디자인 시안을 받아서 작업을 하였는데, 디자이너 분이 적어주신 개괄적인 디자인 시안에 대한 내용을 제대로 확인하지 않아서 금요일날 업무를 마무리하고 최종적으로 디자이너분에게 확인을 할 때 수정해야 되는 부분이 갑자기 생겨서 부랴부랴 수정을 했었다. (다음부터는 작업 전에 제대로 좀 확인해야 될 것 같다) 수정해야되는 부분 중에 한 가지가 페이지가 초기에 로드되었을 때의 이벤트 처리를 하는 부분이었는데, Angular의 라이프사이클 훅 메서드에 대한 이해가 부족해서 해결을 하지 못했었다. 페이지가 처음에 로드되었을 때 정적 페이지에 있는 비디오가 클릭이벤트를 통해서 자동으로 재생이 되도록 처리해야 되는데, 나는 처음에 ngOnInit 메서드 내에서 클릭해야 되는 DOM 요소를 클릭하도록 처리를 하였었다.하지만 클릭시에 호출되는 이벤트 함수 내에서 비디오 태그 DOM 객체가 사용되었는데, ngOnInit메서드 내에서는 아직 컴포넌트 뷰가 초기화되지 않은 상태여서 정상적으로 이벤트 함수가 실행되지 않았었다. (정돈님이 피드백 주신 내용)이외에도 muted가 적용되면, 별도의 interaction 없이 자동재생이 가능하도록 처리를 할 수 있다. muted가 속성으로 적용되지 않는 경우에는 직접 ts파일 내에서 muted 속성을 지정해줘야 하는 경우도 있다. 코드리뷰이번 코드리뷰를 통해서 남들과 협업 할 때 어떻게 하면 코드를 좀 더 가독성있게 작성할 수 있을지에 대해서 배울 수 있었던 것 같다. index를 통해서 값을 표현하지 않기소제목만 보면 무슨 말인지 알기 힘들지만, 말 그대로 특정 배열에서 값을 가져올 때 단순히 index값으로 접근하지 않고, 기본 메서드나 정규표현식으로 필터를 해서 값을 추출하는 방식으로 코드를 작성하는 것이 가독성 측면에서 좋다는 피드백을 받았었다. 정돈님이 업무할 때 말씀하신 내용 중에 하나가 주석을 달아야 할 정도의 코드라면 좋지 못한 코드라는 내용이 있었다. 생각해보면 가장 좋은 코드는 주석없이도 코드만 보더라도 깔끔하게 이해되는 코드인 것 같다. index를 통해서 값을 읽어오게 되면 처음 코드를 보는 개발자의 입장에서는 왜 이렇게 접근을 했는지 한 번 더 생각해야 되는 경우가 생긴다.하지만, 기본적으로 제공되는 메서드나 정규표현식을 사용한다면, 아마 다른 개발자 분들이 볼 때 직관적으로 이 코드가 왜 이렇게 작성이 되었는지 한 눈에 파악하기 쉬울 것이기 때문에 이 피드백은 다음에 코드작성시에도 유의하면 좋을 것 같다.","link":"/2021/07/04/202106/210704-2-week-review/"},{"title":"220402 학습 16일차 자기반성","text":"2022년 4월 2일, 퇴사한지는 어느덧 42일차가 되었고, 새롭게 학습을 시작한지는 16일차가 되었다. 그리고 추가적으로 건강관리의 일관으로 시작한 식단조절 및 운동을 한지는 27일차가 되었다. 이번 포스팅에서는 나의 내적으로 좀 의미있는 반성의 시간을 갖으며, 글을 작성해보려고 한다.반성이 없는 계획과 발전은 의미가 없기 때문에 지난 나의 과거를 다시 되새기고 현재 내가 걱정하는 것을 구체화시켜서 현재의 나를 좀 더 발전시켜 나가고자 한다. 현실적으로 해결할 수 있는 문제와 그렇지 않은 문제최근 오랜만에 공부를 시작하면서 “내가 의욕이 많이 없구나”라는 것을 느꼈다. 이전에는 새로운 것을 배우면 적용시켜보기 위해 따로 만들어보고 만들어보는 과정을 통해 또 추가적으로 공부를 했었다.그런데 요즘은 처음 배우는 내용임에도 불구하고, 복습과 시뮬레이션해보는 과정을 따로 하지 않았다.왜 그랬을까?이 문제부터 생각에 접근해봐야겠다. 우선 의욕이 없는 이유는 내가 아직도 현실적으로 해결할 수 없는 문제에 대해 신경을 쓰고 있다는 것과 아직도 나 자신에 대한 확신이 부족하기 때문이다. 아무리 주변 경력있으신 분께 조언을 얻으면서 좋은 말씀을 들어도 결국은 현실적으로 해결할 수 있는 부분에 집중해서 현실에 최선을 다하는 것은 내 몫이며 어쩌면 그것이 전부이지 않을까 생각이 된다. 사람마다 각기 무언가를 배우거나 적용하는 방법이 다르듯이, 내가 앞으로 나아가고자 하는 방향에 대해 나 자신이 스스로 찾아보고 부딪히고 체득하면서 나만의 루트를 만들어가는 것이 맞는 것 같다. 내가 알고 있는 것과 모르는 것, 메타인지내가 알고 있는 것과 모르고 있는 것, 메타인지를 높이기 위한 가장 좋은 방법은 시뮬레이션이라고 한다.무언가 새로 배웠다면, 내가 진짜 이해를 하고, 스스로 응용을 할 수 있는지 확인하는 과정(시뮬레이션)을 거쳐서 메타인지를 높이는 것이 중요하다.당장 듣고 바로 적용해보면, 마치 내가 다 이해한 것 마냥 착각을 하고 그 다음 그다음 세션으로 넘어가고, 결국 나중에는 이게 누적이 되서 다시 이전으로 돌아가서 다시 학습을 해야되는 최악의 상황이 온다.이제부터는 무언가를 학습했다면, 그것을 새롭게 적용시켜보는 과정을 거쳐서 완전한 내것으로 만들어야 한다. 단기 성취를 느낄 수 있는 목표 설정이번에 퇴사를 하고, 새로운 도메인에 대한 학습을 시작하였다. 혼자서 스스로 학습해야되는 부분이 많고, 그 기간도 상대적으로 길기 때문에 단기적으로 성취감을 느끼면서 계속 동기부여를 얻으면서 앞으로 나아갈 수 있는 목표설정이 필요하다.그래서 자격증 취득이나 짧게는 하루, 일주일, 한달 단위로 해서 계속 자기 피드백을 받을 수 있도록 목표설정을 하고 나 스스로에 대해 피드백을 해야한다.지금 이 시점에서 16일이라는 시간이 흘렀지만, 지금부터 다시 제대로 교정하고 다잡아야겠다.지금 이 습관이 나중에 새로운 회사에 입사해서도 새로운 기술을 습득하고 업무를 함에 있어 좋은 원동력이 될 수 있도록 차근차근 준비해야겠다.","link":"/2022/04/02/202204/220402-Feedback/"},{"title":"220402 Hadoop과 친해지기 그 세 번째 이야기","text":"이번 포스팅에서는 하둡의 생태계에 대한 전반적인 구성에 대해서 정리해보려고 한다. 하둡은 다양한 기술들의 집합체로, 총 3개의 세션으로 구분할 수 있다.우선 첫 번째 Query Engine 5개의 기술스택, 두 번째 Core Hadoop Ecosystem으로 16개의 기술스택, 세 번째 External Data Storage로 3개의 기술스택, 도 합 24개의 기술들로 구성이 되어있다. 아직 막 하둡의 개념정도만 공부하고 있는 이 시점에 이렇게 방대한 하둡의 생태계에 대해서 정리하는 이유는 하둡의 전체적인 생태계를 이해하고 숲을 그려놓은 상태에서 부분 기술들을 학습하게 되면, 나중에 해당 기술스택이 왜 쓰이는지, 어떤 기술스택이 어떤 기술 스택의 대체로 사용이 될 수 있고 어떤 상황에서 어떤 기술스택 간의 조합이 적절한지에 대한 판단을 할 수 있기 때문이다. 아직은 다 알지도 알 수도 없지만, 지금은 하둡의 생김새에 대해서 알아보자. 우선, 하둡의 가장 핵심적인 부분부터 살펴볼겠다. Core Hadoop Ecosystem [STEP 1]하둡의 가장 핵심적인 생태계의 구성을 살펴보면, 가장 바닥에 HDFS(Hadoop Distribution File System)이 기반을 잡고 있다. 이 부분은 이전에 하둡의 역사에 대해서 알아봤듯이, 구글이 만든 GFS(Google File System)이라는 기술을 기반으로 만든 하둡의 버전의 기술이다.HDFS는 빅데이터를 클러스터의 컴퓨터들에 분산 저장하는 시스템이다. 클러스터의 하드 드라이브들이 하나의 거대한 파일 시스템을 사용하고, 그 데이터의 여분 복사본까지 만들어서 혹여나 컴퓨터가 불에 타서 녹아버리면, 백업 복사본을 사용해서 자동으로 손실을 회복하도록 한다.(HDFS = 분산 데이터 저장소 역할) [STEP 2]HDFS의 위에는 YARN(Yet Another Resource Negotiator)이 있는데, 이 친구는 '또 다른 리소스 교섭자'의 의미로, 데이터의 처리 부분을 담당한다.HDFS에서 데이터의 저장 부분을 담당했다면, YARN은 HDFS에 저장된 데이터를 처리하기 위한 일종의 데이터 관리자의 역할을 해준다고 보면된다.다시말해, YARN은 컴퓨터 클러스터의 리소스를 관리하는 시스템이다. 자세히 말하면 YARN은 누가 작업을 언제 실행하고 어떤 노드가 추가 작업을 할 수 있고, 없는지에 대한 결정을 한다. 클러스터를 작동하는 심장박동 같은 곳이 YARN이다. [STEP 3]이러한 YARN의 리소스 교섭역할을 통해 그 위에 새로운 어플리케이션 계층이 만들어진다. 그 중 하나는 MapReduce로, 데이터를 클러스터 전체에 걸쳐 처리하도록 하는 프로그래밍 메타포 혹은 프로그래밍 모델이다.이 MapReduce는 Mapper와 Reducer로 구성이 되어있는데, 이 프로그램을 사용할 때 사용하는 두 개의 구분된 스크립트 혹은 함수라고 볼 수 있다.Mapper는 클러스터에 분산되어있는 데이터를 효율적으로 변형시킬 수 있고, Reducer는 그 데이터를 집계하는 역할을 한다. 간단하게 역할 정의를 했지만 실제로는 아주 다양한 역할을 수행한다.본래 MapReducer와 YARN은 거의 같은 역할을 했는데, 최근 분리되었다. 그로 인해 YARN 위에 MapReducer와 같은 역할을 더 효율적으로 수행하는 또 다른 어플리케이션이 개발이 되었다. [STEP 4]MapReducer 위에는 Pig라는 기술이 구축이 되는데, 만약 Java나 Python으로 MapReduce를 코딩하기보다 SQL과 같은 스크립팅 언어에 더 익숙하다면 이 Pig라는 친구가 적격이다. Pig는 고수준의 API로, 많은 경우 SQL과 비슷한 간단한 스크립트를 작성해서 쿼리를 연결하고, 복잡한 답을 구할 수 있다.다시말해, Pig는 작성된 스크립트를 MapReducer가 읽을 수 있도록 번역하고, MapReducer는 다시 YARN과 HDFS에게 데이터를 처리하고 원하는 답을 가져오게 한다. [STEP 5]Hive는 Pig와 같은 레벨의 계층에 있는 어플리케이션으로, 실제 SQL쿼리를 받고 파일 시스템에 분산된 데이터를 SQL 데이터베이스처럼 취급한다.shell client나 ODBC등을 통해 Open Database Connectivity 데이터베이스에 접근을 할 수 있고, Hadoop 클러스터에 저장이 되어있는 데이터가 내부적으로는 관계형 데이터베이스가 아님에도 불구하고, SQL로 쿼리할 수 있다.SQL에 익숙하다면, Hive API를 유용하게 사용할 수 있다. [STEP 6]최 상단에는 Ambari라는 애플리케이션이 위치하는데, 이 친구는 클러스터와 그 위에 동작하는 어플리케이션들의 상태를 볼 수 있게 한다. 이와 비슷한 기능을 하는 다른 기술이 있는데, Cloudera와 MapR이 있으며, 내가 지금 학습용으로 설치한 Hortonworks는 Ambari를 사용한다. [STEP 7]Mesos는 Hadoop의 일부는 아니지만, YARN과 동일한 계층으로 YARN의 대안 정도로 생각될 수 있다. Mesos 또한 YARN과 같이 리소스 교섭자 역할을 하며, YARN과 Mesos는 서로 다른 방식으로 처리를 한다.즉, Mesos도 클러스터의 리소스를 관리하는 또 하나의 방법이다. Mesos는 YARN과 같이 협업하게 할 수도 있다. [STEP 8]Mesos와 연결되는 기술이 바로 하둡의 생태계에서 가장 흥미로운 기술인 Spark로 이어진다. Spark는 MapReducer와 동등 레벨의 어플리케이션인데, Spark는 YARN과 Mesos중 어느 쪽을 기반으로 하든 데이터에 쿼리를 실행할 수 있다.MapReducer와 동등 레벨의 어플리케이션이기 때문에 MapReduce와 같이 Python이나 Java 혹은 Scala를 사용해서 Spark의 스크립트를 작성해야한다.그런데 그 중에서 Scala가 추천된다.Spark는 무지막지하게 빠르고, 현재 전성기를 누리고 있으며, 이 기술에 대한 다양한 개발활동이 진행 중에 있다. 매우 흥미롭고 강력한 기술이며, 클러스터의 데이터를 신속하고 효율적이며 안정적으로 처리할 수 있다는 강점을 가지고 있다.이 외에도 굉장한 다양성을 가지고 있어서, 클러스터에 걸친 정보로 머신 러닝을 수행하는 SQL 쿼리도 처리가 가능하고, 실시간으로 스트리밍 되는 데이터를 처리하는 등 다양하고 멋진 작업을 할 수 있다. [STEP 9]TEZ는 Spark와 비슷한 기술을 사용한다. 특히 방향성 비사이클 그래프를 사용하고, 그로인해 TEZ는 MapReduce의 일을 할 때 더 유리해진다. 그 이유는 쿼리 실행에 더 효율적인 계획을 세우기 때문이다. TEZ는 Hive와 함께 사용이 되어 성능을 가속한다.위의 구조도에서는 Hive가 MapReducer위에 올라가있는데, TEZ위에서도 동작을 할 수 있다. Hive가 MapReduce를 통과할 때 보다 TEZ를 통과할 때 더 빠를 수 있다.둘이 서로 다른 방식으로 쿼리를 최적화하고, 클러스터에서 효율적인 답을 구하기 때문이다. [STEP 10]HBASE는 YARN과 MapReduce 계층을 아우르고 있는데, HBASE는 클러스터의 데이터를 트랜잭션 플랫폼으로 노출하는 역할을 하며, NoSQL 데이터베이스라고 불린다.HBASE는 기둥형 데이터 스토어이고, 기둥형 데이터 스토어란 단위 시간당 실행되는 트랜잭션의 수가 큰 아주 빠른 데이터베이스를 말한다.그러므로 데이터를 웹 어플리케이션이나 웹 사이트에 노출시켜서 OLTP(Online Transaction Processing)트랜젝션을 하는데 적합하다.요약하자면, HBASE는 클러스터에 저장된 데이터를 노출시키고, 데이터는 Spark나 MapReduce등에 의해 전환되었을 수도 있고, 그 후에 결과를 다른 시스템에 노출시킬 방법을 제공한다. [STEP 11]Apache STORM은 스트리밍 데이터를 처리하는 방식이다. 만약 센서나 웹 로그로부터 데이터를 스트리밍한다면, Apache STORM이나 'Spark Streaming'을 통해 실시간으로 처리할 수 있다.Apache STORM은 스트리밍 데이터를 실시간으로 처리하기 위해서 개발이 되었으며, 이로인해 더 이상 일괄로 처리할 필요가 없어졌다. 데이터가 실시간으로 입력됨에 따라 실시간으로 기계학습을 업데이트하거나 데이터를 데이터베이스에 저장할 수 있다. 엄청 쿨한 기능이다. [STEP 12]OOZIE는 클러스터의 작업을 스케쥴링한다. Hadoop 클러스터에 여러 단계나 시스템이 필요한 작업을 해야 할 때가 있다. 이러한 경우에 OOZIE는 일정에 따라 이런 작업을 순차적으로 진행할 수 있도록 스케줄링한다.예를들어, 데이터를 Hive에 호출 -&gt; Pig를 통해 통합 -&gt; Spark를 통해 쿼리 -&gt; 결과를 HBASE로 변환시킨다고 하면, OOZIE가 이 모든 것을 관리해서 안정적이고 일관성 있게 실행 할 수 있다. [STEP 13]Zookeeper도 OOZIE와 같이 옆으로 따로 빠져있는데, Zookeeper는 클러스터의 모든 것을 조직화하는 기술이다. 이 기술을 사용해서 어떤 노드가 살아있는지 추적할 수 있고, 여러 어플리케이션이 사용하는 클러스터의 공유 상태를 안정적으로 확인한다.많은 어플리케이션이 실제로 Zookeeper에 의존을 하며, 어떤 노드가 다운되더라도 일관성 있고 안정적인 성능을 전체 클러스터에 걸쳐 유지할 수 있다.예를들면, Zookeeper는 어떤 것이 마스터 노드이며 어떤 노드가 살아있고 다운되어있는지 추적하는데 사용될 수 있으며, 이외에도 더 다양하게 사용될 수 있지만, 많기 때문에 차차 알아가는 것이 좋다. [STEP 14]그 다음으로 살펴볼 내용은 데이터 수집에 특화된 시스템이다.이 데이터 수집 특화 시스템의 3대장으로는 Sqoop/FLUME/Kafka가 있다. 어떻게 외부 데이터를 클러스터와 HDFS로 가져올 수 있을까?바로 Sqoop은 Hadoop의 데이터베이스를 관계형 데이터베이스로 엮어낸다. ODBC나 JDBC(Java Database Connectivity)로 소통 가능한 데이터는 Sqoop을 통해 HDFS의 파일로 변형할 수 있다.다시말하면, Sqoop은 레거시 데이터베이스와 Hadoop을 연결하는 연결 장치라고 볼 수 있다. [STEP 15]그 다음 데이터 수집 특화 시스템에는 FLUME이 있다. FLUME을 통해 대규모 웹로그를 안정적으로 클러스터에 불러올 수 있다. 그 예로, 웹 서버 여러 개를 가지고 있다고 가정하자. FLUME은 실시간으로 웹 서버의 웹로그를 감시하고, 클러스터에 게시해서 STORM이나 Spark Streaming을 사용해서 실시간으로 처리한다. [STEP 16]Kafka도 데이터 수집을 하지만 좀 더 포괄적으로 사용된다. PC 혹은 웹 서버 클러스터에서 모든 종류의 데이터를 수집해서 Hadoop 클러스터로 내보낸다.","link":"/2022/04/02/202204/220402-hadoop_bigdata_class/"},{"title":"220402 Mini project 회고 및 정리 - 타이타닉 생존자 예측 데이터 분석","text":"이번 포스팅에서는 학습 16일차에 Kaggle에 있는 타이타닉 생존자예측 dataset을 분석하고, 분석한 dataset을 시각화하는 연습한 내용을 정리하려고 한다.이번 미니 프로젝트를 통해 얻었던 나 자신에 대한 피드백은 우선 첫 번째 dataset에 대한 사전 분석이 부족했다는 것이다. 그리고 두 번째, DataFrame과 시각화 작업에 대해 연습이 부족하여 작업함에 있어 미숙한 부분이 많았다. 이 피드백을 통해 알게된 개선해야 될 부분에 대해서는 앞으로 차근차근 채워가도록 해야겠다. 타이타닉 생존자 예측 dataset주어진 dataset은 총 12개의 칼럼과 891개의 행으로 구성되어있다. 즉, 한 사람당 총 12 종류의 정보를 포함하고 있다.특정 승객의 새존 여부를 알아보려면 survived 항목의 값을 살펴보면 된다. (1:생존)주어진 test.csv 파일의 구조는 train.csv 파일과 거의 동일하지만, survived 항목이 없다. 12345678import pandas as pdimport numpy as np # 데이터 분석을 위한 NumPyimport matplotlib.pyplot as plt # 데이터 시각화 라이브러리import seaborn as sns # 데이터 시각화 라이브러리train_df = pd.read_csv('train.csv')print(train_df) 타이타닉 dataset 분석데이터셋을 시각화하고 결론을 도출해내기 전에는 우선적으로 주어진 dataset에 대한 분석이 필요하다. 이번 미니 프로젝트에서 간과했던 부분이었는데, dataset이 주어졌다면, 우선적으로 주어진 dataset의 columns 구성(df.columns.values),데이터 자료 구성 정보(df.info())를 확인해야한다. dataset의 각 칼럼 항목 확인123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# dataset의 column 항목을 확인print(train_df.columns.values)# ['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch' 'Ticket' 'Fare' 'Cabin' 'Embarked']# 'PassengerId' : 승객 번호# 'Survived' : 생존여부 (0:사망, 1:생존)# 'Pclass' : 객실등급 - (1:1등급, 2:2등급, 3:3등급)# 'Name' : 이름# 'Sex' : 성별# 'Age' : 나이# 'SibSp' : 동반한 형제자매와 배우자의 수# 'Parch' : 동반한 부모와 자식의 수# 'Ticket' : 티켓 번호# 'Fare' : 티켓 요금# 'Cabin' : 객실 번호# 'Embarked' : 승선한 항구 (C:Cherbourg, Q:Queenstown, S:Southampton)print(train_df.info())# output :&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns): # Column Non-Null Count Dtype--- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.7+ KBprint(train_df.describe())# output : PassengerId Survived Pclass Age SibSp \\count 891.000000 891.000000 891.000000 714.000000 891.000000mean 446.000000 0.383838 2.308642 29.699118 0.523008std 257.353842 0.486592 0.836071 14.526497 1.102743min 1.000000 0.000000 1.000000 0.420000 0.00000025% 223.500000 0.000000 2.000000 20.125000 0.00000050% 446.000000 0.000000 3.000000 28.000000 0.00000075% 668.500000 1.000000 3.000000 38.000000 1.000000max 891.000000 1.000000 3.000000 80.000000 8.000000 Parch Farecount 891.000000 891.000000mean 0.381594 32.204208std 0.806057 49.693429min 0.000000 0.00000025% 0.000000 7.91040050% 0.000000 14.45420075% 0.000000 31.000000max 6.000000 512.329200 탑승석 등급에 따른 생존률12345678910# Pclass를 기준으로 그룹정렬해주고, Survived의 평균값을 Survived의 값을 기준으로 내림차순 정렬합니다.pclass_survived = train_df[['Pclass','Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)sns.barplot(data = pclass_survived, x='Pclass', y='Survived')# output :Pclass Survived0 1 0.6296301 2 0.4728262 3 0.242363 comment : 탑승석의 등급이 높을 수록 생존률이 높음을 확인할 수 있었다. 성별에 따른 생존률12345678sex_survived = train_df[['Sex','Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)sns.barplot(data=sex_survived, x='Sex', y='Survived')# output : Sex Survived0 female 0.7420381 male 0.188908 comment : 성별이 여성일수록 생존률이 높음을 확인할 수 있었다. 승선한 형제자매와 배우자 수에 따른 생존률1234567891011121314sibsp_survived = train_df[['SibSp','Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='SibSp',ascending=True)print(sibsp_survived)sns.barplot(data=sibsp_survived, x='SibSp', y='Survived')# output: SibSp Survived0 0 0.3453951 1 0.5358852 2 0.4642863 3 0.2500004 4 0.1666675 5 0.0000006 8 0.000000 comment : 승선한 형제자매와 배우자의 수가 1명 -&gt; 2명 -&gt; 0명 -&gt; 3명 -&gt; 4명 순으로 생존률이 높음을 확인할 수 있었다. 즉, 승선한 형제자매와 배우자의 수가 0~2명인 경우에 생존률이 상대적으로 높았다.","link":"/2022/04/02/202204/220402-mini-project-feedback/"},{"title":"220402 SQL Basic-2","text":"이번 포스팅에서는 SQL의 기초에 대한 내용을 정리하겠다. SELECT 문의 주요 기능 Selection : 질의에 대해 반환하고자하는 테이블의 행을 선택하기 위한 기능 projection : 질의에 대해 반환하고자 하는 테이블의 열을 선택하기 위한 기능 Join : 공유 테이블 양쪽의 열에 대해 링크를 생성하여 다른 테이블에 저장되어있는 데이터를 결합하여 가져오기 위해 SQL의 조인 기능을 사용 SELECT 절로 할 수 있는 것들 특정 열과 모든 열을 선택 연산자를 사용 +, -, *, / 그룹화 Null 값의 처리 별칭 (Alias) 중복 행의 제거 (Distinct) 등 SELECT 문의 기본 문법SELECT [DISTINCT] 열 이름[as Alias]FROM 테이블 이름[WHERE 조건식][ORDER BY 열 이름 [ASC OR DESC]]; [] : 선택사항 SELECT 문장 작성 규칙명령어는 대문자로 작성하고, 나머지는 소문자로 작성하면 가독성이 좋아진다. SELECT 문장 작성 규칙12SELECT *FROM hr.employees; –는 주석처리할 때 사용출력 결과를 파일로 내보내기Oracle SQL Developer에서 테이블의 데이터를 선택해서 마우스 우측 클릭하고 &quot;export&quot;를 선택하면 파일을 추출할 수 있는 창이 팝업되고, 원하는 확장자를 선택해서 추출하면 된다. 특정 열 선택하기SELECT 열 이름[as Alias]FROM 테이블 이름; 1234-- 예시SELECT employee_id, first_name, last_nameFROM hr.employees; 조건식과 연산자 적용하기SELECT *FROM 테이블 이름WHERE 조건식; 12345678SELECT *FROM hr.employeesWHERE email LIKE 'V'||'%';-- ||는 concat 명령어이다.-- 위의 WHERE 절과 같은 의미이다.WHERE email LIKE 'V%'-- 이메일이 대문자 V로 시작되는 모든 값(조건에 만족하는 모든 열의 값)을 출력하라는 의미 연산자의 종류에는 비교연산, SQL연산, 논리연산, 부정비교, 부정SQL이 있고, 마지막 두 개를 제외하고 나머지가 가장 많이 쓰인다. 비교연산 실습12345678910-- 비교연산 실습SELECT *FROM employeesWHERE salary &gt;= 6000;-- ORDER BY 절 (default: ASC)SELECT *FROM hr.employeesWHERE salary &gt;= 6000ORDER BY salary; SQL연산 실습123456789101112131415161718192021222324252627282930313233-- SQL연산-- BETWEEN A AND BSELECT *FROM hr.employeesWHERE salary BETWEEN 6000 AND 8000ORDER BY salary;-- IN (LIST)SELECT *FROM hr.employeesWHERE salary IN (6000, 8000)ORDER BY salary;-- LIKE '비교문자열' : 비교문자열과 형태가 일치한다.(%(모든 문자열), _ (자릿수) 사용)SELECT *FROM hr.employeesWHERE email LIKE 'B'||'%'ORDER BY salary;-- 총 대문자 B를 포함한 6자 이메일 값만 출력SELECT *FROM hr.employeesWHERE email LIKE 'B'||'_____'ORDER BY salary;-- IS NULL : NULL값만 출력SELECT *FROM hr.employeesWHERE manager_id IS NULLORDER BY salary; 논리연산 실습1234567891011121314151617-- AND : 앞 뒤 조건의 동시만족SELECT *FROM hr.employeesWHERE salary = 8000 AND JOB_ID = 'SA_REP'ORDER BY salary;-- OR : 한쪽만 TRUE라도SELECT *FROM hr.employeesWHERE salary = 8000 OR salary = 6000ORDER BY salary;-- NOT : 뒤의 조건과 같지 않으면-- 이후의 연산자는 직접 실습해보기 주어진 데이터로 boxplot 그래프 그리기전체 매출과 스테이크의 매출을 비교하기 위해서 전처리된 dataset을 만들어주고, 해당 데이터를 boxplot 그래프로 그려서 실제 판매량이 어떻게 되는지 비교를 한다. 123456789import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snstable_df = pd.read_csv('table.csv')table_dfsns.boxplot(x='TYPE', y='SALES_', data=table_df) boxplot을 x=TYPE, y=SALES_를 기준으로 그려보았다.결과 그래프를 분석해보면, boxplot은 최대값, 최소값, 중앙값, 사분위수 등 이상치를 표시한다. 박스 아랫면에 있는 면을 일사분이라고 하며, 전체 데이터값의 25%값을 알려주며, 박스의 중앙라인은 중앙값(전체 데이터의 50%), 박스 윗면은 삼분위수(전체 데이터의 75%)전체 데이터가 어떻게 되는지 나타낸다. 위에 점으로 표시된 것은 이상치를 표시한다. 박스의 크기가 각 각의 전체 판매(overallsales)로 왼쪽의 박스는 전체 매출, 두번째 박스는 스테이크의 판매 매출(steak_sales)을 나타낸다. 세일즈 값이 박스 크기이기 때문에, 스테이크 값이 전체 매출값에 비해서 높다는 판단을 내릴 수 있다. 엑셀에서 했던 데이터 전처리 작업을 SQL로 하기123456SELECT order_no, item_id, reserv_no, quantity, ROUND(sales/1000, 0) AS SALES_, 'overall_sales' AS TYPEFROM order_infoUNION ALLSELECT order_no, item_id, reserv_no, quantity, ROUND(sales/1000, 0) AS SALES_, 'Stake' AS TYPEFROM order_infoWHERE item_id = 'M0005' 위 아래 있는 SELECT문의 사이에 UNION ALL이 있기 때문에 두 테이블을 붙여주게 된다.","link":"/2022/04/04/202204/220402-sql-basic-2/"},{"title":"220404 SQL Basic-1","text":"이번 포스팅에서는 SQL의 기본적인 부분에 대해서 정리해보려고 한다. SQL?SQL(Structured Query Language)의 약어로, RDBMS(관계형 데이터베이스 관리 시스템)을 접근하여 조작할 수 있으며, ANSI 표준을 따른다. SQL로 할 수 있는 일은? 데이터베이스를 질의(=조회 실행, Query)할 수 있다. 행을 삽입(Insert), 갱신(Update), 삭제(Delete) 할 수 있다. 데이터베이스, 테이블, 프로시져, 뷰 등을 생성(Create)할 수 있으며, 권한을 줄 수 있다. 관계형 데이터 베이스 관리 시스템(RDBMS/Relational Database Management System)? RDBMS는 SQL의 기본이 되는 데이터베이스이다. MY-SQL, ORACLE, DB2, MySQL 등이 대표적인 RDBMS이다. RDBMS는 테이블(Table)이라고 불리는 객체를 갖는다. 테이블(Table)은 행과 열로 구성이 되어있다. 관계형 데이터베이스의 구조관계형 데이터베이스는 2차원의 테이블과 관계를 이용한다. 테이블(Table)테이블은 행과 열로 구성이 되어있으며, 행은 로우, 레코드, 튜플, Observation이라고 불린다.테이블에는 하나의 기본키(Primary Key)가 있으며, 속성(열, 컬럼, 필드, 도메인, variable)로 구성된다. 그리고 행과 열의 교차점에 있는 것이 데이터 값(컬럼 값)이라고 한다.행을 유일하게 식별할 수 있는 데이터 값을 기본 키(Primary Key)라고 한다. 그리고 기본 키 값을 가지고 있는 열을 기본키 열이라고 한다. 다른 프로그래밍 언어와의 비교공통 요소에는 데이터 타입, 변수, 함수, 명령어, 연산자가 있는데, 다른 프로그래밍 언어와 공통적으로 있는 요소이다. 문자 숫자 날짜 CHAR(n) NUMBER(p,s) DATE VARCHAR2(n) ‘ABC’,’가나다’,’1’,’1234’ 1, 1200, 1,235 17/10/06/11:00, 2022-01-01, 20200101 (1) 변수 : @v ex) v = ‘Steve’ or v = 100 (2) 함수 : 함수는 SQL에서 빈번하게 사용된다. (SUM(), AVG(), MAX()) (3) 명령어 : 문법을 만들고 실행하기 위한 키워드, 예약어 (SQL을 만드는 근간)예) SELECT, FROM, WHERE, GROUP BY, ORDER BY (4) 연산자 : 연산을 위해 정의된 기호 (), * / + - = &lt; &gt; 차이나는 요소(5) (중요) 테이블 / 관계 : 데이터를 담아 놓은 데이터 덩어리 / 데이터끼리 연결하는 개념 (6) SQL, 데이터베이스 이론(프로그래밍 랭귀지 이론) : 해당 랭귀지를 위한 문법과 이론들 실행과정USER -&gt; SQL -&gt; DBMS -&gt; DBUSER &lt;- &lt;- &lt;- DBMS &lt;- DB SQL 학습 Retrieve DML(데이터 조작어) : 주로 많이 사용 DDL(데이터 정의어) : DML 다음으로 많이 사용 DCL(데이터 제어어) TCL(트랜잭션 제어어) 구분 (중요도 순) 문장 설명 Retrieve(DML) SELECT 데이터베이스로부터 데이터 검색 DML(Data Manipulation Language) INSERT, UPDATE, DELETE 개별적으로 데이터베이스 테이블에서 새로운 행을 입력하고 기존의 행을 변경하고, 원치 않는 행을 제거 TCL(Transaction Control Language) COMMIT, ROLLBACK, SAVEPOINT DML명령문으로 만든 변경을 관리 DDL(Data Definition Language) CREATE, ALTER, DROP(구조삭제), REMANE, TRUNCATE(auto-commit) 데이터 구조를 생성, 변경, 제거 DCL(Data Control Language) GRANT, REVOKE ORACLE 데이터베이스와 그 구조에서 엑세스 권한을 제공하거나 제거 SQL 도메인(기본)ERD -&gt; Table, Key, Relation, Join -&gt; DML, DDL, TCL 순으로 확장하면서 학습 (심화)스키마, 관계대수, 개념/논리/물리/각종 이론 -&gt; 정규화, 반정규화, 함수적 종속성, 연결함정, 이상현상 -&gt; … -&gt; Index, Optimizer, Tunning -&gt; OLAP, MDM, DB보안순으로 확장","link":"/2022/04/02/202204/220402-sql-basic-1/"},{"title":"220403 SQL Basic","text":"이번 포스팅에서는 SQL의 기초에 대한 내용을 정리하겠다. 비교 분석하기SUM, COUNT, GROUP_BY를 사용하여 총 판매 금액과 총 판매 수량을 비교한다. 판매금액을 구하는 방법,판매 수량을 구하는 방법,메뉴 아이템 별로 나누는 방법 (그룹화) 단일행 함수와 그룹 함수함수? 함수란 어떤 결과를 위해 미리 만들어 놓은 명령어(도구)이다. 함수들은 기본적인 SQL문을 더욱 강력하게 사용할 수 있게 해주며, 데이터 값을 조작하는데 도움을 준다. SQL함수의 특징 데이터 값을 계산하거나 조작한다. 데이터 값을 조작, 날짜와 숫자 등 데이터 타입을 상호 변환, 행에 대해 조작한다.(단일 행 함수) 행의 그룹에 대해 계산하거나 요약 (그룹함수) (1) 단일 행 함수 : 문자 함수, 숫자 함수, 날짜 함수, 변환 함수, 일반 함수(1:1로 값이 mapping되어 수정된다) 데이터 타입의 종류 문자 : CHAR(n) : n크기만큼 고정길이의 문자 타입을 저장 (최대 2000바이트) 문자 : VARCHAR2(n) : n크기만큼 가변 길이의 문자 타입을 저장한다.-&gt; 데이터를 관리하고 저장하는데 효율성을 위해서 가변/불가변 데이터 타입을 구분해서 사용한다. 고정 데이터 분석(CHAR)/유연한 데이터 분석(VARCHAR2) 숫자 : NUMBER(p, s) : 수자타입을 저장 (p: 정수, s: 소수 자릿수) 날짜 : DATE : 날짜 타입 : 날짜 타입을 저장 9999년 12월 31일까지 저장 단일 행 함수의 종류 문자타입 함수 : 문자를 입력받아 문자와 숫자를 반환 숫자 타입 함수 : 숫자를 입력받아 숫자를 반환 날짜 타입 함수 : 날짜에 대한 연산을 한다. 숫자를 반환하는 MONTHS_BETWEEN 함수를 제외하고 모든 날짜 타입 함수는 날짜 값을 반환 변환 타입 함수 : 임의의 데이터 타입의 값을 다른 데이터 타입으로 변환 일반 함수 : 그 외 NVL, DECODE, CASE, WHEN, 순위 함수 등 단일 행 함수의 특징 행별로 하나의 결과를 반환 SELECT, WHERE, ORDER BY 절 등에서 사용 중첩 사용가능 (안쪽(하위) -&gt; 바깥쪽(상위) 단계순으로 진행) 주요 단일 행 문자타입 함수 요약 LOWER : 문자열을 소문자로 UPPER : 문자열을 대문자로 INITCAP : 문자열의 첫 번째 문자를 대문자로 SUBSTR : 문자열 중 일부분을 선택 / 인덱스는 1부터 시작, 마지막 인덱스 번호는 끝지점 가리킴 REPLACE : REPLACE(‘[TARGETR 문자열]’, ‘[TARGET 문자]’, ‘[수정 문자]’) CONCAT : 두 문자열을 연결 (|| 연산자와 같다) LENGTH : 문자열의 길이 INSTR : 명명된 문자의 위치를 구한다. INSTR(‘ABCD’, ‘D’) -&gt; 4 LPAD : 왼쪽부터 특정 문자로 자리를 채운다. LPAD(‘ABCD’, 6, *) -&gt; **ABCD RPAD : 오른쪽부터 특정 문자로 자리를 채운다. LTRIM : 주어진 문자열의 왼쪽 문자를 지운다. LTRIM(‘ABCD’, ‘AB’) RTRIM : 주어진 문자열의 오른쪽 문자를 지운다. RTRIM(‘ABCD’, ‘CD’) 주요 단일 행 숫자타입 함수 요약 *ROUND : 숫자를 반올림한다.(0이 소숫점 첫째자리) *TRUNC : 숫자를 절삭한다. TRUNC(15.451, 1) -&gt; 15.4 (소수점 첫째 자리에서 절삭) MOD : 나누기 후 나머지를 구한다. CEIL : 숫자를 정수로 올림한다. (무조건 반올림 정수+1) FLOOR : 숫자를 정수로 내림한다. (무조건 소수점 이하 버리기) SIGN : 양수(1), 음수(-1), 0인지 구분하여 출력 POWER : 거듭제곱 출력 SQRT : 제곱근 출력 날짜 타입 함수 MONTHS_BETWEEN : 두 날짜 사이의 개월수를 계산 ADD_MONTHS : 월을 날짜에 더한다. ADD_MONTHS(HIRE_DATE, 5) NEXT_DAY : 명시된 날짜부터 돌아오는 요일에 대한 날짜를 출력(SUNDAY:1, MONDAY:2,) LAST_DAY : 월의 마지막 날을 계산 ROUND : 날짜를 가장 가까운 연도 또는 월로 반올림 (YEAR or MONTH) ROUND(HIRE_DATE, ‘MONTH’) TRUNC : 날짜를 가장 가까운 연도 또는 월로 절삭 (YEAR or MONTH) TRUNC(HIRE_DATE, ‘YEAR’) 1234- DUAL : Oracle에서 제공하는 더미 테이블- SYSDATE : 시스템 날짜(오늘)SELECT SYSDATEFROM DUAL; 연산 결과에 따른 타입 변환123456SELECT 1,2,3FROM DUAL;- 암묵적으로 문자타입의 숫자와 숫자와의 연산을 숫자로 변환해서 연산해준다.SELECT 1 + '2'FROM DUAL; 타입 변환함수123456789101112131415SELECT TO_CHAR(SYSDATE, 'YY'),SELECT TO_CHAR(SYSDATE, 'YYYY'), TO_CHAR(SYSDATE, 'MM'), TO_CHAR(SYSDATE, 'MON'), TO_CHAR(SYSDATE, 'YYYYMMDD'), TO_CHAR(TO_DATE('20171008'), 'YYYYMMDD'),FROM dual;SELECT TO_NUMBER('123')FROM dual;SELECT TO_DATE(20171007, 'YYMMDD')FROM dual;SELECT TO_DATE('20171007', 'YYMMDD')FROM dual; 일반 함수일반 함수 중에서 가장 많이 사용되는 함수들을 정리 순위함수(RANK)1234567891011SELECT employee_id, salary, RANK() OVER(ORDER BY salary DESC)RANK_급여, DENSE_RANK() OVER(ORDER BY salary DESC)DENSE_RANK_급여, ROW_NUMBER() OVER(ORDER BY salary DESC)ROW_NUMBER_급여FROM hr.employeesWHERE employee_id BETWEEN 100 AND 106;- RANK() : salary를 기준으로 내림차순 정렬을 하고, 순위를 매긴다. (중복 순위가 있는 경우, 그 다음 순위는 한 단계 건너뛴다. 공동순위 있음)- DENSE_RANK() : salary를 기준으로 내림차순 정렬을 하고, 순위를 매긴다. (중복 순위와 상관없이 순위를 순차적으로 출력한다. 공동순위 있음)- ROW_NUMBER() : salary를 기준으로 내림차순 정렬을 하고, 순위를 매긴다.(중복 순위가 있는 경우, 순차적으로 순위를 할당한다. 공동순위 없음) NULL 값을 특정 값으로 치환하는 NVL함수123456-- commission_pct의 값이 NULL인 경우, 1로 치환해서 수식을 계산한다.SELECT salary * NVL(commission_pct, 1)FROM hr.employeesWHERE employee_id BETWEEN 100 AND 106ORDER BY commission_pct; DECODE 문열을 선택하고, 특정 조건값이면 치환값으로 업데이트하고, 특정 조건값이면 인상여부에 대한 칼럼에 ‘10%인상’, ‘미인상’ 값을 넣어 출력한다. 12345678910DECODE(열 이름, 조건값, 치환값(조건 값에 해당할 경우, 출력값), 기본값(조건 값에 해당하지 않을 경우 출력값))SELECT first_name, last_name, department_id, salary, DECODE(department_id, 60, salary*1.1, salary) 조정된급여, DECODE(department_id, 60, '10%인상', '미인상') 인상여부FROM hr.employeesWHERE employee_id BETWEEN 100 AND 106; CASE 문CASE문은 DECODE와 달리 복수의 조건을 줄 수 있다. 123456789101112131415CASE WHEN 조건1 THEN 출력값1 WHEN 조건2 THEN 출력값2END AS [칼럼이름]-- job_id가 'IT_PROG'인 직원중에 급여가 9000이상이면 '상위급여', 6000과 8999 사이이면 '중위급여', 그 외에는 '하위급여'로 출력한다.SELECT employee_id, first_name, last_name, salary, CASE WHEN salary &gt;= 9000 THEN '상위급여' WHEN salary BETWEEN 6000 AND 8999 THEN '중위급여' ELSE '하위급여' END AS 급여등급FROM hr.employeesWHERE job_id = 'IT_PROG'; (2) 그룹함수 : 함수, GROUP BY, HAVING(기준 열의 행에 대해 그룹으로 묶어서 함수를 적용한다) 여러 행 또는 테이블 전체에 대해 함수가 적용되어 하나의 결과를 가져오는 함수를 말한다.그룹 당 하나의 결과가 주어지도록 하기 위해 '행의 집합'에 대해 연산할 경우 GROUP BY 절을 이용하고, HAVING절을 이용하여 그룹에 대한 조건을 제한 할 수 있다. 그룹함수의 종류 COUNT (행의 갯수를 센다) : *(whildcard)의 경우에는 null도 개수에 포함해서 센다.(그 외 나머지는 null 값을 제외하고 연산을 한다) SUM (합계) AVG (평균) MAX (최댓값) MIN (최솟값) STDDEV (표준편차) VARIANCE (분산) 12345SELECT COUNT(*)행갯수, AVG(salary)급여FROM hr.employees;SELECT SUM(salary) AS 총월급여, SUM(salary)*0.1 AS 인상분, SUM(salary)*1.1 AS 변경후_총월급여FROM hr.employees; GROUP BY 절그룹 함수는 여러 행 또는 테이블 전체에 대해 함수가 적용되어 하나의 결과를 가져온다.그룹 당 하나의 결과가 주어지도록 행의 집합에 대해 그룹화 연산할 경우, GROUP BY절을 사용하도록 한다.그리고 GROUP BY절은 그룹함수와 같이 자주 사용된다. 12345678910111213SELECT A, SUM(B)FROM table_nameGROUP BY A--A열의 값을 기준으로 그룹화를 하고, B열은 그룹화된 그룹값들의 합을 구한다.SELECT job_id AS 직무, SUM(salary) AS 직무별_총급여, AVG(salary) AS 직무별_평균급여FROM hr.employeesWHERE employee_id &gt;= 10GROUP BY job_idORDER BY 직무별_총급여 DESC, 직무별_평균급여;-- hr.employees 테이블에서 job_id, salary의 합계, salary의 평균급여 정보를 출력한다. 조건은 employee_id가 10이상인 직원들을 job_id로 그룹화하고, 직무별 총급여를 내림차순으로, 직무별 평균급여를 오름차순으로 정렬을 해서 출력한다. HAVING 절HAVING절은 그룹으로 묶여진 결과값에 대해서 다시 한 번 조건문을 주는 것이다. HAVING절의 위치는 GROUP BY 절 다음에 위치를 하며, 그 다음으로 ORDER BY 절이 위치를 한다.GROUP BY [열이름] -&gt; HAVING [조건식] -&gt; ORDER BY [열 이름] 12345678SELECT job_id AS 직무, SUM(salary) AS 직무별_총급여, AVG(salary) AS 직무별_평균급여FROM hr.employeesWHERE employee_id &gt;= 10GROUP BY job_idHAVING SUM(salary) &gt; 30000ORDER BY 직무별_총급여 DESC, 직무별_평균급여;-- hr.employees 테이블에서 직무, 직무별 총급여, 직무별 평균급여 칼럼을 출력하는데, employee_id가 10이상이고 job_id를 기준으로 그룹화하며, salary가 30000초과하는 데이터를 직무별 총급여를 기준으로 내림차순, 직무별 평균급여를기준으로 오름차순하여 출력한다. 총 판매 금액과 총 판매 수량을 조회하여 비교123456789SELECT item_id, SUM(sales) AS SALE, SUM(QUANTITY) AS QTY, COUNT(order_no) AS ORDER_CNTFROM order_infoGROUP BY item_idORDER BY SUM(sales) DESC-- order_info 테이블에서 item_id, 판매수익의 합계, 양의 합계, 주문번호 칼럼을 출력을 하는데, item_id를 기준으로 그룹화를 하고, 판매 총 수익을 기준으로 내림차순하여 출력한다.","link":"/2022/04/03/202204/220403-sql-basic-3/"},{"title":"220405 Hadoop과 친해지기 그 네 번째 이야기","text":"이번 포스팅에서는 하둡의 생태계에 대한 전반적인 구성에 대해서 정리해보는 그 네번째 시간으로, Hadoop Ecosystem의 Core 영역을 지나 외부 데이터 스토리지 영역(External Data Storage)에 대해서 알아보도록 하겠다. 기존에 하고 있던 개발관련 공부의 분량이 많아서 아주 가끔씩 시간을 내서 하둡에 대해서 공부를 하고 있는데, 처음에는 너무 막막해서 어려웠지만, 지금은 그래도 어느정도 이 친구가 어떤 기술스택들의 집합체이고, 각자가 어떤 역할을 하는지에 대해서 정도만 개괄적으로 알게 된 것 같다.그냥 새로운 사람을 만났을 때 생김새와 이름, 사는 곳 정도만 알고 있는 수준일테지만, 좀 더 깊이 있게 알아가다보면 어느 순간 정말 이 기술에 대해 어느정도 잘 알고 있다고 말 할 수 있을 날이 오지 않을까 싶다. 자 그럼 시작해보자. [STEP 1] 사실 이전에 Core Hadoop Ecosystem 영역에 있던 HBase도 이 External Data Storage의 범주에 속하지만 하둡 스택의 일부이기 때문에 빠져있다.위의 외부 데이터 스토리지 영역에서 볼 수 있듯이, MySQL은 물론이고 어떤 SQL 데이터베이스라도 클러스터와 통합해서 사용할 수 있다. 이전에 배웠던 Sqoop을 사용해서 우리 클러스터로 데이터를 가져올 수 있다는 것을 배웠고, 반대로 MySQL로 내보낼 수도 있다.Spark와 같은 여러 기술은 JDBC나 ODBC 데이터베이스에 기록을 할 수 있고, 중앙 데이터베이스에 직접 저장하거나 필요하다면 그곳에서 결과를 검색할 수도 있다. Cassandra와 MongoDB는 HBase처럼 ‘기둥형 데이터 스토어’이며, 웹 애플리케이션 등에 데이터를 실시간으로 노출하는데 활용할 수 있다.고로 실시간 어플리케이션과 클러스터 사이에 Cassandra나 MongoDB 같은 층을 만들어두는 것을 추천한다고 한다.Cassandra와 MongoDB 둘 다 많은 처리량을 가진 간단한 ‘키-값 데이터 스토어’를 사용함에 있어 인기있는 선택으로, MySQL, Cassandra, MongoDB는 모두 클러스터와 통합할 수 있는 외부 데이터베이스이다. [STEP 2] 클러스터 위에서 작동하는 쿼리엔진에는 몇 가지가 있는데, 이 기술을 사용해서 대화형으로 SQL 쿼리를 입력할 수 있다. 위의 원 안의 기술들이 다른 원 안의 기술들과 꼭 들어맞지는 않는다.Hive도 비슷한 일을 할 수 있는데, Hive는 Hadoop과 더 견고하게 엮인 기술이라 위의 외부 데이터 스토리지 범주에 포함되어있지 않지만, Hive도 데이터 쿼리하는데 사용된다. Apache DRILL은 또 하나의 멋진 기능으로, 다양한 NoSQL, 데이터베이스에 SQL쿼리를 작성해서 사용할 수 있도록 도와준다.예로 DRILL은 Hbase, Cassandra, MongoDB의 데이터베이스와 소통을 할 수 있으며, 소통한 내용을 엮어서 이질적인 데이터 스토어들에 걸쳐 쿼리를 작성하고, 그 결과를 한데 모아줄 수 있다. [STEP 3] HUE는 Hive, HBase에 잘 작동하는 쿼리를 대화형으로 생성할 수 있도록 도와준다. 실제로 Cloudera에서는 HUE가 Ambari의 역할을 담당하여, 모든 것을 내려다 볼 수 있도록 시각화하고 전체 Hadoop 클러스터에 쿼리를 실행한다. [STEP 4] Apache PHOENIX는 Apache DRILL과 비슷한데, 전체 데이터 스토리지 기술에 걸쳐 SQL 스타일의 쿼리를 할 수 있게 한다. 한 단계 더 나아가서 ACID(Atomicity, Consistency, Isolation, Duration) OLTP를 제공한다.이를 통해 NoSQL, Hadoop 데이터 스토어에 ACID가 보장되면, 관계형 데이터베이스의 관계형 데이터 스토어와 매우 비슷해진다.presto 또한 전체 클러스터에 쿼리를 실행할 수 있는 또 다른 방법이며, Apache Zeppelin은 클러스터와의 상호작용과 사용자 인터페이스를 노트북 유형으로 접근하는 새로운 관점을 채택했다.앞서 설명한 모든 기술은 쿼리를 실행하고 별도의 코드를 작성할 필요 없이 클러스터에서 어떤 의미를 추출하는 작업을 수행한다. [하둡이 어려운 이유?]여지까지 봐도 어렵게만 느껴지지만, 모든 기술을 사용하고 이해하기는 상대적으로 쉽다고 한다.다만, 하둡이 어려운 이유는 이렇게 많은 기술들이 있고, 서로 중복되는 기능들이 많기 때문이다.하지만, 그 기술간의 차이점은 무엇인지, 무엇을 선택할 것인지,그리고 어떻게 종합해서 현실의 문제를 해결할 것인지를 알고 있다면, 그것이 바로 기술자를 고용해서 쓰는 이유이다.","link":"/2022/04/05/202204/220405-hadoop_bigdata_class/"},{"title":"220403 SQL Basic-4","text":"이번 포스팅에서는 SQL의 기초에 대한 내용을 정리하겠다. 전체 상품의 월별 매출 추이와 스테이크의 월 별 매출 추이 확인(동등 조인, SUBSTR, DECODE 응용)문제를 해결하기 위해서는 매 달 매출정보와 전체 매출과 스테이크 매출을 구분할 수 있는 별도의 조건이 필요하다. 데이터 분석을 할때에는 핵심이 되는 부분을 선정해서 그 부분부터 점점 펼쳐나가는 분석기법이 필요하다. 시계열 (Time Series)?시간의 흐름에 따라 변화를 나타낸 그래프를 시계열 그래프라고 하는데, 시계열 그래프는 시간의 흐름을 보여주기 때문에 시계열이라고 한다. 선 그래프의 사용선 그래프는 흐름/추이를 확인할 때 매우 유용하게 사용되는 그래프이다.선 그래프를 그리기 위해서는 JOIN이 필요하다. 가장 중요한 개념이기 때문에 잘 알아둬야 한다. ERD(Entity Relationship Diagram)?ERD란 개체(테이블) 간의 관계를 이해하기 쉽게 그림으로 표현 한 것으로 데이터를 조작하고 분석하는 기초자료로 활용한다. 개체 간에는 관계(relation)이 있는데, 이러한 관계는 점선이나 실선으로 표현된 화살표 모양의 선으로 나타낸다. (1) P (Primary Key) : 기본 키 (=주 키)라고 하며, 기본 키는 데이터를 식별하는 ‘식별자’ 역할을 한다.(2) U (Unique Key) : 고유 키라고 하며, 행에서 유일한 값을 갖는 데이터 값으로 구성된 열 (유일하게 식별할 수 있는 열이지만, 기본 키가 아닌 값)(3) F (Foreign Key) : 외래 키(=보조 키)라고 하며, 참조 테이블의 기본 키 또는 고유 키를 참조한다.(다른 테이블과 연결을 위한 열로, 주로 다른 테이블 간의 연결을 위해 주로 다른 테이블의 기본 키를 참조한다.)(4) 키값이 아닌 열 : P, U, F를 제외한 나머지 열을 말한다. ERD는 설계도로, ERD를 기반으로 SQL의 논리로직을 작성하게 된다. 관계 차수와 관계 선택 사양 ERD를 통해 관계 차수(cardinality)와 관계 선택 사양(optionality)을 표현한다. 관계 차수는 1:1, 1:N, M:N 등 하나의 개체(Primary key)에 몇 개의 개체가 대응되는지를 표현한다. 관계 선택 사양은 관계가 필수인지 아닌지(없을 수도 있는지)를 표현한다. 관계 차수(끝 모양) 관계 선택 사양(선 종류) 실선은 필수 관계를 나타내며, A와 B가 필수 관계임을 나타낸다. (B가 존재하려면 A가 반드시 존재)점선은 선택적인 관계로, A와 B가 선택적 관계 (B는 A가 없어도 존재할 수 있음) ERD 읽기 연습 JOIN한 개 이상의 테이블과 테이블을 서로 연결하여 사용하는 기법 조인(JOIN)의 종류이너 조인(INNER JOIN)(=동등 조인(equi)))조인 조건이 정확히 일치하는 경우에 결과를 출력한다.A와 B테이블이 있다고 가정했을 때, A열에 있는 특정 열과 B열에 A의 특정 열과 동일한 값이 있는 열을 서로 연결 시키는 것을 말한다. 아우터 조인(OUTER JOIN)조인 조건이 정확히 일치하지 않아도 모든 결과를 출력한다. 이너 조인보다 좀 더 복잡하다. 이너조인과 아우터 조인만 알아도 80-90% 테이블 간의 연결에 대해 80-90% 커버 가능 곱집합(CARTESIAN PRODUCT)가능한 모든 행을 조인한다. 비동등 조인(non equi join)조인 조건이 정확히 일치하지 않는 경우에 결과를 출력한다. 자체 조인(self join)자체 테이블에서 조인하고자 할 때 사용된다. 이너 조인(INNER JOIN)(=동등 조인(equi))) 자세히 알아보기이너 조인을 할 때에는 WHERE절에서 두 테이블의 열이 갖고 있는 데이터 값을 논리적으로 연결하며, 연결 기호로는 등호(=)를 사용한다..으로 지정한 테이블 이름으로부터 열의 이름을 참조한다. 양쪽 테이블에서 조인 조건이 일치하는 행만 가져오는 가장 일반적이고 자주 쓰이는 조인이다. 123SELECT *FROM employees A, departments B -- 각 테이블을 별칭으로 지정WHERE A.department_id = B.department_id; 이너 조인(inner join)의 조인 규칙 SELECT 절에는 출력할 열 이름을 기술하며, FROM 절에는 접근하려는 테이블 이름을 기술한다. WHERE 절에는 조인 조건을 기술한다. FROM 절 외의 절에는 조회의 명확성을위해 열 이름 앞에 테이블 이름을 붙인다. 1234SELECT A.employee_id, A.first_name, A.last_name, B.department_id, B.department_nameFROM hr.employees A,hr.departments BWHERE A.department_id = B.department_idORDER BY A.employee_id; 아웃터 조인(OUTER JOIN) 자세히 알아보기이너 조인의 경우에는 조인하는 기준 열의 데이터가 일치해야 조인이 가능하다.하지만, 아웃터 조인의 경우에는 조인하는 기준 열의 데이터가 일치하지 않아도 조인을 할 수 있다.리포트나 데이터 분석을 할 때 항상 같은 값만으로 구성되지 않기 때문에 이런 경우에는 아웃터 조인을 사용한다. JOIN은 WHERE절이 매우 중요하다. 1234-- 예시SELECT 테이블 이름 1.열 이름 1, 테이블 이름 2.열 이름 2, ...FROM 테이블 이름 1, 테이블 이름 2WHERE 테이블 이름 1.열 이름 1 = 테이블 이름 2.열 이름 2(+); 아웃터 조인은 이너 조인 조건을 만족하지 못해 누락되는 행을 출력하기 위해 사용된다. 아웃터 조인은 이너 조인과 다르게 WHERE절의 끝에 (+) 기호를 붙여서 사용한다. (+) 기호는 조인할 행이 없는, 즉 데이터 값이 부족한 테이블의 열 이름 뒤에 기술해준다. (+) 기호는 아웃터 조인하려는 한쪽에만 기술할 수도 있다. 테이블 양쪽에는 기술할 수 없다.A.department_id(+) = B.department_id(+) (X) (+) 기호를 붙이면 데이터 값이 부족한 테이블에 null 값을 갖는 행이 생성되어 데이터 값이 충분한 테이블의 행들이 null 행에 조인하게 된다. (이너 조인)A 테이블 (+누락된 항목) + B 항목 (matching되는 데이터가 부족한 쪽(조인을 하고자 하는 쪽)) (아웃터 조인)위의 이너 조인에서는 null값과 matching되는 B의 열 항목이 없기 때문에 해당 항목이 누락이 되지만, 아웃터 조인에서는 null값인 경우에도 null값을 그대로 유지하면서 나머지 열 데이터를 null로 초기화시켜서 출력한다. 1234567891011SELECT A.employee_id,A.first_name,A.last_name,A.department_id,B.department_id,B.department_nameFROM hr.employees A,hr.departments BWHERE A.department_id = B.department_id(+)ORDER BY A.employee_id;-- hr.employees 테이블과 hr.departments 테이블을 각 각 A, B로 Alias해주고,-- department_id를 기준으로 테이블 A와 B를 JOIN해준다. 조인해주려는 쪽이 B 테이블이기 때문에-- WHERE 절에서 B.department_id(+)과 같이 B테이블의 칼럼명 뒤에 (+)를 붙여준다.-- 출력순서는 A테이블의 employee_id를 기준으로 오름차순으로 한다.-- 출력되는 칼럼은 A.employee_id, A.first_name, A.last_name, A.department_id, B.department_id, B.department_name으로 한다.-- 만약에, 반대로 (+)기호를 주게되면, INNER JOIN과 같은 형태로 테이블이 출력되게 된다. 내가 출력하고자하는 기준열이 오른쪽에 있으면, RIGHT OUTER JOIN[(+)가 왼쪽],내가 출력하고자하는 기준열이 왼쪽에 있으면, LEFT OUTER JOIN[(+)가 오른쪽] 이라고 말한다. 그 외의 다른 SQL에서는 표준 SQL문법인 아래의 형태로 OUTER JOIN을 한다. 1234SELECT A.employee_id,A.first_name,A.last_name,A.department_id,B.department_id,B.department_nameFROM hr.employees A LEFT OUTER JOIN hr.departments BON A.department_id = B.department_idORDER BY A.employee_id; 집합 연산자 (SET OPERATOR) SELECT 문을 여러 개 연결하여 작성하며, 각 SELECT 문의 조회 결과를 하나로 합치거나 분리한다. 집합 연산자는 합집합, 교집합, 차집합의 논리와 같다. 집합 연산자의 종류 UNION : SELECT 문의 조회 결과의 합집합. 중복되는 행은 한 번만 출력한다. (합집합) UNION ALL : SELECT 문의 조회결과의 합집합. 중복되는 행도 그대로 출력한다. (합집합) INTERSET : SELECT 문의 조회 결과의 교집합. 중복되는 행만 출력한다. (교집합) MINUS : 첫 번째 SELECT 문의 조회결과에서 두 번째 조회 결과를 뺀다. (차집합) 123456SELECT 열 이름1, 열 이름2, 열 이름3, ...FROM 테이블 이름집합 연산자 -- UNION, UNION ALL, INTERSETSELECT 열 이름1, 열 이름2, 열 이름3, ...FROM 테이블 이름[ORDER BY 열 이름 [ASC or DESC]]","link":"/2022/04/03/202204/220403-sql-basic-4/"},{"title":"220408 데이터 파이프라인 학습 용어 정리","text":"이번 포스팅에서는 데이터 파이프라인을 공부하면서 나왔던 용어에 대해서 정리를 하고자 한다. 이 포스팅에 학습을 하면서 몰랐던 용어에 대해서 정리를 해두려고 한다. 온프레미스(On-premise)?온프레미스는 기업의 서버를 클라우드 같은 원격 환경에서 운영하지 않고, 자체적으로 보유한 전산실 서버에 직접 설치해 운영하는 방식을 의미한다.온프레미스는 클라우드 컴퓨팅 기술이 나오기 전까지 기업 인프라 구축의 일반적인 방식이었다. 장점으로는 기업의 비즈니스 정보를 보안성 높게 관리할 수 있다는 것이 있으며, 단점으로는 시스템 구축에 있어 많은 시간이 걸리고, 비용이 많이 들어간다는 것이다. 따라서 기업에서 보안성이 높은 데이터는 온프레미스 환경에서 관리하고, 보안성이 낮은 데이터는 클라우드 환경을 사용하는, 복합적인 하이브리드 IT인프라 형태로 관리가 되기도 한다. cf) 클라우드 방식의 서비스를 오프 프레미스(Off-premise)라 한다. Adhoc하게 데이터를 분석한다고?Ad-Hoc하다는 의미는 &quot;좀 여유롭게&quot; 혹은 &quot;특정한 형식 없이 사용할 수 있는&quot;이라는 의미를 갖고 있다. 예를들어 아래와 같은 쿼리문이 있다고 하면, myId는 늘 변하게 되는 변수이기 때문에 미리 정의된 쿼리문이 아닌 즉석에서 바꿔서 사용되는 쿼리인 것이다. 이러한 쿼리를 Ad-Hoc하다고 할 수 있다. ex. var newSqlQuery = &quot;SELECT * FROM table WHERE id =&quot; + myId; Scale up서버의 메모리나 CPU, 디스크의 메모리가 부족해서 특정 JOB을 돌릴 때 죽거나 CPU가 70-80%올라갔을 때 메모리, 디스크, CPU를 올려주는 것을 말한다. Scale outDistribute system의 일종으로, 여러 서버 노드를 두고 필요시에 노드의 갯수를 늘려주는 것을 의미한다.유연한 아키텍처 구성에 중요하다. Object storage?오브젝트 스토리지는 클라우드에서 일반적으로 사용되는, 계층이 없는 데이터 저장 방법이라고 한다.다른 데이터 스토리지 방법과는 달리 오브젝트 기반의 스토리지는 디렉토리 트리를 사용하지 않으며, 개별 데이터 단위(object)가 스토리지 풀의 동일한 레벨에 있다.각 오브젝트에는 애플리케이션에서 검색하는데 사용되는 고유 식별자가 있으며, 또한 각 오브젝트는 함께 검색되는 메타데이터를 포함할 수 있다. CSP(Cloud Service Provider)?데이터 파이프라인 강의를 듣는데, 데이터 파이프라인을 클라우드 기반의 환경에서 구축하는 수업이다보니, CSP라는 용어가 많이 나왔다.CSP란 Cloud Service Provider의 약어로, 클라우드 서비스를 제공하는 업체, 벤더사를 의미한다.최근 기업의 비즈니스 플랫폼이 클라우드로 옮겨가고 있고, 클라우드 서비스에는 설치 없이 웹에서 필요한 소프트웨어를 빌려쓰는 형태인 SaaS(Software as a Service), 원하는 만큼 컴퓨팅 인프라를 쓰는 IaaS(Infrastructure as a Service), 소프트웨어 서비스를 개발할 때 필요한 플랫폼을 제공하는 서비스 PaaS(Platform as a Service가 있다. 클라우드 서비스를 제공하는 대표적인 업체로 아마존(AWS), 마이크로소프트(MS Azure), IBM 그리고 Oracle이 있다. 이기종 데이터(Heterogeneous data)?우리가 살고 있는 현재, 데이터들의 가치는 무한히 높아지고 대용량 데이터를 저장하기 위한 데이터베이스의 필요로 인해서 서로 다른 특징들을 가지는 다양한 유형의 대용량 데이터베이스가 많은 분야에 적용되고 있다.그로 인해 다양한 종류의 데이터베이스에 대한 접근이 쉬워져서 이기종 데이터베이스간의 데이터 동기화의 필요성이 대두되고 있다.여기서 이기종 데이터이란 각 기 다른 두 종의 데이터라고 하면 될 것 같다. OLTP(Online Transaction Processing)/OLAP(Online Analytical Processing)?우선 OLTP는 한 온라인 트랜젝션을 처리하는 것을 말한다. 네트워크상에서 온라인 사용자들의 Database에 대해 일괄 Transaction을 처리하는 것을 의미한다.반면 OLAP는 Database가 자체적으로 운용되는 시스템이라기 보다는 DW 등의 시스템과 관련되어 Data를 분석하고 의미있느 정보로 치환하거나 복잡한 모델링을 가능하게 하는 분석 방법을 말한다. 데이터 백업 vs 아카이브백업은 데이터가 손상되거나 손실될 경우를 대비해서 저장하는 데이터의 사본이며, 원본 데이터는 백업을 생선한 후에도 지우지 않고 유지한다.그리고 데이터 복원(Restore)를 목적으로 한다.아카이브(Archive)는 참고용으로 생성한 데이터 사본이다. 종종 아카이브를 생성한 후에 원본 데이터를 지우기도 하며, 아카이브는 여러 가지 목적이 있는데 보편적으로 이전 데이터에서 일부 데이터를 찾기 위한 목적으로 사용된다. 그리고 아카이브는 인덱스를 제공하기 때문에 사용자는 오래된 콘텐츠에서 과거의 데이터를 되찾을 수 있다.일부 서버에서 아카이브 시스템의 크기나 접근 기간으로 아카이브를 삭제하여 시스템을 최적화하고 저장공간을 최적화한다. TPS(Transactions Per Second)초당 트랜젝션 수로, 일반적인 관점에서 초당 특정 엔티티가 수행한 원자 동작의 수를 가르킨다. 제한된 관점에서는 DBMS 벤터와 사용자 공동체가 초당 데이터베이스 트랜젝션 수를 가리키기 위해 사용되는 것이 보통이다.[출처] : https://ko.wikipedia.org/wiki/%EC%B4%88%EB%8B%B9_%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%98_%EC%88%98 20220412 업데이트 프로비저닝(Provisioning)요즘 AWS를 학습하면서 프로비저닝이라는 용어가 자주 등장한다. 이프로비저닝의 의미는 직영하면 “제공하는 것”이란 뜻인데, 어떤 종류의 서비스이든 사용자의 요구에 맞게 시스템 자체를 제공하는 것을 말한다. 제공해줄 수 있는 것은 인프라 자원이나 서비스, 장비를 포함한다. 2020/04/15 업데이트 메타데이터?데이터에 대한 데이터로, 데이터에 관한 구조화된 데이터이다.주로 데이터를 설명해주는 역할을 해주는 데이터로, 대량의 정보 가운데에서 찾고 있는 정보를 효율적으로 찾아내서 이용하기 위해 일정한 규칙에 따라 콘텐츠에 대하여 부여되는 데이터이다.어떤 데이터 즉 구조화된 정보를 분석, 분류하고 부가적 정보를 추가하기 위해 그 데이터 뒤에 함께 따라가는 정보를 말한다.ex. 코드나 테이블에 대한 설명, 칼럼 정보를 설명, 머신러닝은 feature에 대한 설명출처: 위키백과 ETL?추출, 변환, 적재(Extract, Transform, Load)는 컴퓨팅에서 데이터베이스 이용의 한 과정으로 특히 데이터 웨어하우스에서 다음의 내용을 아우른다. 동일 기종 혹은 타 기종의 데이터 소스로부터 데이터를 추출 조회 또는 분석을 목적으로 적절한 포멧이나 구조로 데이터를 저장하기 위해 데이터를 변환 최종 대상(Database, DataStore, DM, DW)으로 변환 데이터를 적재한다. 출처 : 위키백과데이터 파이프라인을 구축한다 = ETL 구축한다 컴퓨터 클러스터컴퓨터 클러스터는 여러대의 컴퓨터들이 연결되어 하나의 시스템처럼 동작하는 컴퓨터들의 집합을 말한다. RegionAmazon S3에서 사용자가 만드는 Bucket을 저장할 Region을 선택할 수 있다. 리전의 선택 기준은 지연 시간 최적화, 비용 최소화, 규정 요구 사항 준수 등 다양한 기준들이 있다.각 국가별 Region내에서도 Zone에 iDC(Internet Data Center)센터가 위치해있는데, 우리나라에는 4개의 iDC 센터가 위치해있다. 데이터 스트림에서 원하는 데이터를 캡쳐할 때 캡쳐링되는 크기를 윈도우라고 한다?내용 추가 작성하기","link":"/2022/04/08/202204/220408-data-pipeline-term/"},{"title":"220408 데이터 파이프라인 스터디 1일차","text":"이번 포스팅에서는 1일차에 학습했던 데이터 파이프라인과 관련된 학습내용을 정리하면서 간단하게 회고 내용을 작성해보려고 한다. 1일차 학습했던 학습내용을 개괄적으로 정리하자면, 우선 데이터 파이프라인의 흐름을 각 STEP별로 나눠서 이해하고, 데이터 파이프라인에 필요한 AWS서비스에 대해서 이해 그리고 데이터 파이프라인 구성시에 고려해야될 고려사항에 대해서 학습했다. 학습을 하면서, 모르는 용어가 많이 나왔기 때문에 실제 실무에서 일을 할때 같은 엔지니어들과 소통을 하기 위해서는 이 용어가 매우 중요하기 때문에 용어 공부를 위해 별도의 포스팅에 모르는 용어가 나올때마다 추가를 해가면서 학습을 이어가고 있다. 1일차 학습하면서 느낀점은 생각보다 어렵지만, 이전에 파이프라인이라는 말만 듣고 “뭐지?” 했을때의 막연함은 많이 사라진 것 같아서 기분은 좋다. 무언가 새로운 기술을 배울때 마냥 쉽기만 하면, 다른 사람들에게도 진입장벽이 낮다는 것을 의미하는 것이기 때문에 경쟁력을 갖추기 위해서는 어려운 기술적인 부분도 커버할 수 있는 엔지니어가 되려면 열심히 배워야겠다고 느꼈다. 아무튼 각설하고, 이전에 데이터 엔지니어 채용공고를 많이 찾아보았는데, 공고에서 AWS 클라우드 환경에서의 개발/운영 경험에 대한 요구사항이 많았다. 왜 인가 생각을 하면서 학습을 해보았는데 이 궁금증이 해결이 되었다.간단히 말하면, 온프레미스(On-premise)방식과 클라우드 환경(Off-premise)에서의 데이터 처리가 차이가 있는데, 요구사항이 많다는 것은 온프레미스 방식보다는 클라우드 환경에서의 데이터 처리가 훨씬 더 효율적이고 좋다는 것을 의미한다. 어떤 부분이 좋은지에 대해서는 자세히 포스팅의 내용에서 다뤄보도록 하겠다. (1) 데이터 파이프라인의 흐름의 이해파이프라인은 하나의 데이터 처리 단계의 출력이 다음 단계의 입력으로 이어지는 형태로, 서로 파이프가 연결된 것과 같은 연결구조로 생각하면 된다. [1단계] 데이터 수집 : 이 데이터가 왜 중요한지? 요구사항 수집 및 데이터 선정 [2단계] 데이터 전처리(Transformation) 저장 : 데이터 수집 단계에서 취득한 데이터를 전처리하고 저장하는 과정 [3단계] 데이터 시각화 분석 : 전처리 후 저장된 데이터를 SQL이나 데이터 시각화 지식이 없는 다른 부서에 제공을 해주기 위한 데이터 시각화 과정 (2) 데이터 파이프라인에 필요한 AWS 서비스에 대한 이해앞서 이미 정리를 했지만 온프레미스 방식의 서버 운영보다는 AWS와 같은 클라우드 기반의 서버 운영방식(off-premise)이 훨씬 장점이 많다고 했다. 그렇다면, AWS 서비스를 기반으로 데이터 파이프라인을 구축했을 때, 많은 AWS 서비스 중에서 어떤 서비스를 선택해서 사용해야되고, 해당 서비스가 어떤 기능을 제공하는지에 대한 기반 지식이 필요하다.이번 파트에서는 AWS 기반으로 데이터 파이프라인을 구축했을 때, 데이터 파이프라인의 각 세션(수집/전처리 및 저장/분석 및 시각화)에서 AWS의 어떤 서비스를 적용해야되는지에 대해 정리를 해보려고 한다. 데이터의 원천우선 데이터를 수집하기 위해서는 데이터가 나오는 곳이 있어야한다. 각 기업마다 사용되는 외부 분석 서비스도 있고, 기업에서 운영되는 서비스의 기반 플랫폼이 되는 앱 또는 웹 사이트에 의해서 발생하는 이벤트를 통해서 데이터가 생성되어 이를 통해 데이터를 취득할 수 있다. [1단계] 데이터 수집 단계 :우선 첫 번째, 데이터 수집 단계에는 아래의 네 가지 AWS 서비스가 있다. Amazon Kinesis Streams Amazon Kinesis Firehose Amazon API Gate wway Lambda function 우선 첫 번째 Amazon Kinesis Streams은 일종의 큐 서비스이며, 레디스 큐(=스트림 큐))라고도 한다. 스트림은 흐름의 의미로, 강물에 비유하기도 하며 앞단부터 차근차근 쌓아놓고 흘러가는 데이터의 형태라고 이해하면 된다.강물에서 물고기를 그물을 사용해서 잡는다고 가정했을 때, 강물이 바로 분석하고자 하는 데이터이고, 물고기가 바로 내가 찾고자하는 데이터이며, 던진 그물의 크기를 윈도우라고 한다. 데이터 전체를 일괄적으로 저장할 수 없기 때문에 폭포가 있는 강처럼 일정 사이즈 용량으로 한정하고, 일정 시간이 되면, 데이터가 소멸하게 되는 형태로 구성된다. 데이터를 스트림 형태로 사용하는 이유는 대량의 데이터를 가지고 있기 때문에 여러 서비스에서 분석 가능하며, 만약 앱상에서 발생한 데이터를 클라우드에 저장하거나 한다면, 사용자가 앱 내에서 네이게이션을 하고 있을 때 앱의 퍼포먼스에 영향을 줘서 버벅거리거나 하는 사용감에 불편함을 줄 수 있다.앞서 언급한 단점으로 인해서 스트림 형태로 데이터를 사용한다. 수집된 데이터 -&gt; RDBMS에 직접 저장하게 되면, RDBMS는 리소스가 한정적이기 때문에, 트래픽이 일괄적으로 몰리면 앱이 느려지는 경향이 있다.따라서 이러한 경우에는 수집된 데이터 -&gt; (큐 서비스) -&gt; RDBMS의 형태로 RDBMS에 데이터를 적재하기 전에 큐 서비스가 사이에서 모든 데이터를 받고, 사용자가 네비게이션을 할 때 불편함이 없도록 원활하게 앱이 동작할 수 있도록 도와준다. 두 번째로 Amazon Kinesis Firehose라는 큐에 있는 데이터를 별도의 코딩없이 S3에 저장할 수 있도록 지원하는 서비스가 있다. (요즘에는 S3 뿐 아니라 ElasticSearch와 같은 third party에 실시간으로 데이터를 넣을 수 있는 스토리지가 있다) 세 번째로 Amazon API Gate way라는 외부에서 웹/앱에서 AWS와 연결시켜주는 역할을 해주는 프록시 서비스가 있다. 각 서비스간에 서로 영향을 주지 않는 마이크로 서비스라고도 하며, 10개의 서비스 중에 1개의 서비스가 fail out이 되더라도 나머지 9개가 정상적으로 동작할 수 있도록 도와주는 서비스이다. 마지막으로 네 번째, Lambda function이있다. Lambda function은 Event-driven 데이터 처리를 할때 이벤트에서 발생하는 여러가지 서비스를 핸들링하기 위해서 사용된다. 초기에는 Amazon kinesis Stream -&gt; (Lambda function) -&gt; S3 / RDBMS의 형태로 해서 데이터를 저장하였다. [1.5단계] 데이터 수집과 전처리의 교차점 : AWS Glue이 과정에는 AWS Glue라는 서비스가 있다.우선 AWS Glue의 등장배경은 AWS의 Pipeline이라는 서비스이다. 데이터 파이프라인을 통해서 기본적인 ETL(Extract/Transformation/Load)라는 처리를 하게 되는데, Amazon의 Pipeline이라는 녀석이 관리되는 파이프라인이 많아졌을 때 관리 및 운영하는데 어려움이 있었기 때문에 이러한 문제를 개선하기 위해 등장한 친구가 바로 AWS Glue이다. AWS Glue에는 파이프라인의 기본적인 서비스들이 추가가 되어있으며, 가장 비용 효율적으로 잘 활용되고 있는 부분은 바로 메타 스토어 정보가 포함되어있는 부분이다. 이 메타 스토어 정보에는 데이터 위치/포멧/버전의 변경사항등에 대한 정보를 포함하고 있다. [2단계] 전처리 및 저장 : Amazon EMR, Amazon S3Amazon EMR는 AWS에 있는 Hadoop Eco System을 가지고 있는 관리형 서비스이다.과거에는 하둡을 설정하려면 서버에서 리눅스 설치, 하둡에 필요한 라이브러리 및 서비스를 설치하고 각 노드들을 연결하기 위해 네이밍을 하는 작업들을 해야만 했는데, 대략 2주정도 소요가 된다고 한다.그런데 이 Amazon EMR을 이용하면, 기본적으로 구축된 환경을 제공해주기 때문에 최대 5-10분 사이에 구축된 서비스를 이용할 수 있다.Amazon EMR을 통해 전처리하고 전처리된 데이터를 저장하고자하는 타겟 요소에 저장할 수 있도록 도와준다. [3단계] 분석 및 시각화 : AWS Athena, Tableau, Periscope Data, SupersetAWS Athena는 adhoc하게 데이터를 분석할 수 있도록 도와준다.그외의 시각화 툴인 녀석들을 이용해서 SQL관련 지식이 없는 타 부서 업무 담당자에게 시각화를 해서 정보를 제공해줄 수 있다. 앞서 정리한 [1단계] ~ [3단계]에서 정리한 내용은 틈틈이 실습을 하면서 내가 지금 하고있는 실습이 몇 단계에 해당되는 내용인지 확인을 하고 실습 및 활용을 하는 것이 좋다. (3) 데이터 파이프라인을 구축할 때 고려해야되는 사항요즘 기업에서 데이터를 활용해서 현재 운영중인 서비스를 개선하는 경우가 많다. 그렇기때문에 데이터 파이프라인 구축에 있어 절대적으로 우선 지속적이고 에러없는 상태여야하며, 요구사항에 맞게 가능한한 빠르게 대응해야한다. 데이터가 갑작스럽게 많아지는 경우가 생겨서 과부하가 생기는 경우도 있기 때문에 시스템적으로 이러한 문제에 대해 유연하게 Scability가 있어야 한다. 즉, Scale up과 Scale Out이 자유로워야 한다는 의미이다.이러한 부하는 이벤트성 데이터에서 많이 발생하게 되는데, 이벤트 관련 푸시 알림 발송에서는 일괄적으로 발송되기 때문에 복합적으로 다양한 이벤트들이 발생하는 경우를 대비하여야 한다.수집되는 데이터 또한 분석에 유연하도록 분석하기 쉬운 format으로 관리되어야하며, JSON 포멧이 많이 활용되고 있다.","link":"/2022/04/08/202204/220408-data-pipeline-study/"},{"title":"220409 데이터 파이프라인 스터디 2일차","text":"이번 포스팅에서는 2일차에 학습했던 데이터 파이프라인과 관련된 학습내용을 정리하면서 간단하게 회고 내용을 작성해보려고 한다. 2일차 학습했던 학습내용을 개괄적으로 정리하자면, 우선 Data &amp; AI LANDSCAPE를 통해서 현재 빅데이터 관련 서비스에 어떠한 변화가 있는지 살펴보았고, Data Pipeline 구축에 접근하는 방법에 대해서 배워보았다. Data Pipeline 구축을 할 때 Lambda Architecture라는 Architecture를 활용해서 배워보았는데, 실제 실무에서는 어떤식으로 접근해서 Data Pipeline을 구축하는지 궁금했는데 그 궁금증이 어느정도 해소되었던 소중한 시간이었다. (1) DATA와 AI관련 서비스의 트렌드 변화 2021년 기준으로(상단의 이미지 참조), 여러 데이터 기반의 회사가 등장하고, 대표적으로 넷플릭스가 등장하면서 분석 및 추천에 많은 공헌을 하였다. 이러한 서비스들은 오픈소스화하는 경우가 많았고, 오픈소스화된 서비스들은 클라우드 진영에서 자체 서비스화하기 위해 더욱 다양한 서비스들이 등장하기 시작했다. 이전에는 페이스북, 트위터, 리프트의 기업들이 데이터 기반으로 비즈니스를 확대하면서, 여러 서비스들을 운영하기 시작했다. [참고]Airflow는 Airbnb에서 만든 전체 플로우를 컨트롤할 수 있는 툴(workflow tool)인데 오픈소스화하면서, 이 것을 기반으로 클라우드 진영에서는 자체 서비스화하기 위해서 다양한 서비스들이 등장을 하였다.그 외에도 데이터 예측을 위해 AI 진영에도 다양한 서비스들이 등장하였다. (2) 데이터 파이프라인 구축에 접근하는 방법 우선 많은 Architecture중에서 Lambda Architecture로 접근하는 이유는, 데이터 파이프라인 구축할때 cost측면에서 적고, 다양한 Architecture를 구성할 수 있다는 면에서 장점이 있기 때문이다. IoT기기나 SNS, 스마트 기기로부터 모인 데이터는 Raw Data의 형태로 Raw Data Store에 저장된다. 데이터에 대한 비즈니스적 요청에 의해 Batch-Processing-Engine을 통해서 처리를 하기도 하고, Real-Time-Processing-Engine을 통해서 처리를 하기도 한다. 만약 Batch-Processing에 대한 비즈니스적 요구가 있다면 Batch-Processing-Engine을 통해서 Serving Data Store에 Raw Data Store의 데이터를 저장하게 된다. (보통 이 일련의 과정을 ETL(Extract/Transformation/Load)작업이라고 한다) 만약 Real-Time에 비즈니스적 요구가 있다면 Real-Time Processing Engine을 활용해서 Serving Data Store에 저장을 하게 된다. 저장된 Serving Data Store의 데이터는 기증을 해서 Batch나 Real-Time Processing Engine을 통해 어느 수준의 데이터를 저장할지 결정을 하게 된다. [저장된 데이터의 사용]그렇게 저장된 데이터를 기반으로 분석이나 DW(Data Warehouse)(여러 dataset들을 관련성있는 것끼리 묶어서 저장해놓은 format)을 기반으로 DM(Data Mart)를 구성한다.DM는 시각화 할 수 있는 장표단위로 구성을 하거나, AI와 관련된 Feature store의 개념으로 AI를 돌릴 수 있는 dataset들을 구성할 수 있다. 데이터 분석가들이 그래프를 통해서 분석을 할 수 있기 때문에, 최종적으로 데이터 시각화를 하게 된다. [Raw Data Store -&gt; AWS S3]AWS에서의 Object storage인 S3 Data Lake에 저장을 하기 위해서 Batch-Processing Engine과 Real-Time Processing에 mapping되는 각 각의 AWS의 서비스들을 선정한다. Amazon API Gateway / Amazon Kinesis Streams / Amazon kinesis Firehose / Amazon Pinpoint 위에 정리한 구성으로 접점을 잘 연결해서 파이프라인을 구축한다. 위의 AWS 서비스들을 구체적으로 알아보자. Amazon Pinpoint는 일종의 CRM 서비스로, 특정 웹이나 앱에서 발생하는 이벤트를 통해 마케팅을 할 수 있는 툴킷이다. 실제로 Amazon API Gateway + Amazon Kinesis Streams 구성을 통해서 데이터를 수집하는 것이 가장 좋지만, Amazon API Gateway의 비용적 부담이 될 수 있다. 따라서 Pinpoin의 경우에는 SDK를 가지고 웹이나 앱을 배포하게 되면, 우선 저장하는데까지의 비용은 크지 않기 때문에, 다음과 같은 구성으로 Amazon Pinpoint -&gt; Amazon Kinesis Stream -&gt; Amazon Kinesis Firehose -&gt; S3 로 구성하는 것이 상대적 비용적 절감을 가져올 수 있다.이처럼 비용적 측면을 고려하여 수집에 대한 또 다른 아키텍처를 만들 수 있다.(AWS 자격증을 이제 막 공부하는 입장이지만, 왜 비용적 부분에 대한 공부도 필요한지에 대해서 이제 알게 되었다. 서비스 운영에 있어 cost측면에서의 고려는 중요하다.) S3 Data Lake에 데이터 저장된 이후S3에 데이터가 저장이 되면, 텍스트나 JSON 형태로 데이터가 저장이 된다.그 기반으로 데이터적 요구사항에 따라서 데이터를 처리하게 되는데, 가장 일반적으로 Batch Processing Engine을 통해 데이터를 처리한다.통상적으로 현재시간-1일 기준의 데이터를 가지고 처리한다.저장된 데이터를 가지고 이제 ETL과정을 진행한다. 데이터를 트랜스폼해서 저장을 하는데, AWS에서 저장할때는 T1,T2이라는 용어를 사용한다. (회사마다 사용되는 용어 상이) 저장된 데이터를 DM 데이터로 저장하는 이 3가지 스탭이 보통 일반적인 과정이다. S3 Data Lake에 저장된 데이터 전처리S3 Data Lake에 저장된 데이터는 일련의 데이터 전처리 과정을 거쳐야한다. 전처리를 위해서 사용되는 AWS 서비스에는 Batch-Processing시에 Spark/AWS DMS이 필요하며, Real-time Processing시에는 Spark streaming/Amazon Kenesis Analytics가 있다. 앞선 포스팅에서도 이미 정리를 했지만, AWS에는 EMR이라는 하둡 Eco System과 관련된 전반적인 서비스가 설치되어있는 서비스가 있다. 배포하는데 10분 내외로 매우 짧기 때문에 손쉽게 Batch-Processing을 할 수 있다.AWS의 DMS는 이기종 데이터를 트랜스폼하는데 사용한다. 웹이나 앱을 OLTP하기 위해 현재 서비스에서 활용되고 있는 DB를 가져와야되는 경우가 있다. 예를들어, 고객 및 상품 데이터의 마스터분석을 위해서 현재 운영중인 서비스의 데이터베이스에서 그대로 데이터를 가져와야한다. 이 경우에 DMS를 사용한다. SPARK에서 현재 운영중인 서비스의 디비를 가져올 수도 있는데, 이 경우에는 서비스에 부하를 줄 수 있다. 이러한 이유때문에 AWS DMS를 사용하도록 한다. AWS의 서비스를 선택할때에는 서버부하와 코스트면을 고려해서 선택해야한다. Real-Time Process선택시에도 AWS의 EMR을 사용해서 간단하게 Spark streaming가 설치된 환경을 구축해서 사용할 수 있으며, Spark streaming을 사용하는데 있어, 단점은 항상 켜놔야되는 점이 있고, Amazon Kinesis Analytics를 통해 실시간으로 데이터를 분석할 수 있다. 전처리 이후에 Serving Data Store와 관련된 AWS 서비스전처리 이후에 Serving Data Store와 관련된 AWS 서비스에는 Amazon ES(Elastic Search)/Amazon DynamoDB/Amazon RDS가 있다. Amazon ES(Elastic Search)Amazon ES는 데이터량이 지속적으로 많아지는 서비스에는 부적합하다. 계속 컴퓨팅 및 스토리지 영역을 확장해야되는 필요성이 있기 때문이다. 이로인해 지속적인 운영으로인해 비용이 발생한다.그럼 어떤 경우에 Amazon ES가 적합할까? 신규 서비스를 런칭했을때 들어오는 서비스 태깅, 가입, 페이징에 대한 분석을 빠르게 하기 위해서 활용할때 유용하다.데이터 유지는 보통 2주정도하고 여유가 있다면 한 달정도 잡고 분석하기 좋은 서비스이며,Amazon DynamoDB, Amazon RDS(MySQL과 같은 DBM포함)가 있다. Amazon RedshiftAmazon의 Redshift는 칼럼 베이스의 분석툴로, 온프레이스 오라클의 대표적인 서비스들을 포함한다. 데이터 분석에 많이 사용된다. 그외에도 AWS 내에 포함되어있는 presto 서비스를 사용해서 SQL을 활용해서 데이터베이스를 분석할 수 있으며, 어플리케이션 중에서 제일 괜찮아서 활용도가 높고, 쉽게 시각화도 가능하다. 이런 일련의 데이터 파이프라인적 접근을 이해하고, 람다 아키텍처의 구성을 이해하고 있다면, 나중에 CSP(Cloud Service Provider)가 바뀌었을 때에도 이런 서비스들을 맵핑하고 데이터의 인풋과 아웃풋 포멧을 정의한다면 쉽게 파이프라인을 구축할 수 있다. ML/DL에서 사용되는 데이터 구성Serving Data Store가 구축이 되고, 자연스럽게 데이터를 보게 되면, Data Discovery/Predictive Modeling 즉, ML/DL에 대한 니즈 예측값에 대한 데이터를 구성할 수 있다.(우선 서빙 데이터 스토어를 제대로 구성하는 것이 중요하다!)Serving Data를 제대로 구성해야만 예측값을 구하기 위한 코스트를 줄이기 위한 방법중 하나이다. Serving Data Store가 구성이 안되었는데, Raw data 스토어를 통해서 Amazon SageMaker나 툴들을 사용해서 predict ML 코딩을 위해서 들이는 코스트 보다는 데이터 전처리에 대한 코스트가 크기 때문에 이 방법은 비추한다.(코스트가 많이드는 파이프라인) S3 Data Lake에 쌓인 데이터 관리S3에 데이터가 많이 쌓인다. 과거에는 DBMS를 많이 사용했고, DBMS의 데이터를 관리하고 Archiving해서 테입 드라이브에 저장을 했다.현재는 AWS 서비스 중에서 Glacier 서비스가 있는데, S3중에서 최 하위 제일 느린 서비스인데, Archiving 하기 좋다. 이 서비스에 Archiving 할 수 있는 생명주기를 가져가는 것이 좋고, Data Cataloging은 하둡같은 곳에서 메타스토어라고 하는데, 메타 스토어를 통해서 관리해줘야 분석할 수 있는 사람이 메타데이터를 통해서 분석을 빠르게 해줄 수 있다. S3 DataSecurity &amp; Governance생명주기는 Governance에 해당되는데, 이 부분을 잘 고려해서 파이프라인 구축을 시작해야한다. 처음부터 완벽할 수 없고, 비어있는 부분이 많고, 발전시킬 부분에 대해서 고려를 해가면서 작업을 해나가야한다. 이와 관련된 많은 CSP 서비스들이 등장하고 있다. 이제 앞서 배운 파이프라인 구축에 접근하는 방법을 통해 나만의 파이프라인, 플랫폼을 구성하도록 개념과 각종 서비스들에 대해 반복학습을 해야겠다.","link":"/2022/04/09/202204/220409-data-pipeline-study/"},{"title":"220410 데이터 파이프라인 스터디 3일차","text":"이번 포스팅에서는 Data 분석 대상에 대한 부분과 이전 1, 2일차때 공부했던 데이터 파이프라인 구성시에 필요한 AWS 서비스 이외에 기본적으로 필요한 AWS 서비스에 대해서 학습한 내용에 대해서 정리해보려고 한다. (1) Data 분석 대상우리가 분석할 데이터는 기업의 플랫폼이 되는 웹 또는 앱에서 발생한다.이전 포스팅에서도 이 부분에 대해서 언급을 했었는데, 이러한 데이터를 이용해서 실제 사용자가 해당 서비스에서 어떤 것을 필요로 하는지, 니즈를 파악해서 기존 서비스를 개선할 수 있다. 데이터는 웹과 앱에서 발생하는 이벤트를 통해서 취득하게 되는데, 사용자가 페이지(화면)에서 특정 카테고리를 클릭하고 어떤 경로로 또는 어떤 depth로 메뉴를 타고 들어가서 정보를 확인하는지, 이러한 일련의 네비게이션 과정에 대해서 파악을 한다.이러한 일련의 사이클들에 대해서 파악을 하게 되면, 사용자들이 서비스에서 어떻게 메뉴를 구성했을때 주문이 많아졌는지, 그리고 어떤 부분에 관심이 많은지에 대한 파악을 할 수 있다. 더 나아가서 어떻게 데이터 파이프라인을 구성했을 때 가장 효율적으로 데이터를 수집하고 분석을 빠르게 할 수 있는지에 대한 고민도 필요하다. 데이터를 수집할 수 있는 외부 서비스에는 050 시스템, Appsflyer, Adbrix가 있는데, 050 시스템의 경우에는 상담사가 전화한 내용을 전부 Web Hook을 통해서 DB서버에 저장을 하고, 저장된 데이터를 텍스트로 변환하는 작업까지 한다.그 외의 두 개의 서비스는 다채널 마케팅으로, 어느쪽에 광고를 실었을대 가장 효율이 좋은지 데이터적 근거를 통해 분석할 수 있다. Google Analytics와 같은 서비스를 통해서도 데이터를 파악 및 분석할 수 있지만, 데이터를 내제화해서 데이터를 일괄적으로 모은 다음에 디테일하게 분석하기 위해서는 별도의 파이프라인 구성이 필요하다.특정 사용자가 네이버 블로그를 통해서 앱을 설치했다고 가정하자. 사용자가 앱을 설치한 뒤에 앱을 삭제할 수도 있고, 앱을 설치하고 앱을 통해 원하는 정보를 확인할 수도 있다. 이러한 디테일한 사용자의 니즈를 파악하기 위해서는 데이터를 내제화할 필요성이 있다. 그리고 클라우드에는 이러한 데이터 내제화에 도움이 되는 여러 서비스들이 있다. 데이터 파이프라인을 위한 AWS 서비스앞으로 할 실습에 필요한 AWS 서비스에 대해서 구체적으로 알아본다. [EC2] EC2는 AWS의 가상 컴퓨팅 환경 인스턴스로, 서버를 띄울 수 있는 다양한 AMI(Amazon Machine Image)를 제공한다. (1) AMI(Amazon Machine Image) : 서버에 필요한 운영체제, 소프트웨어들이 적절히 구성된 상태로 제공되는 템플릿으로 EC2 인스턴스를 쉽게 생성할 수 있도록 도와준다. (2) 인스턴스 타입(유형) : 인스턴스 이미지에서 위한 CPU, 메모리, 스토리지, 네트워킹 등의 여러가지 구성을 제공한다. (3) 과거에는 아이디/패스워드로 로그인을 했던 반면에, 현재는 Key pair를 사용해서 인스턴스 로그인 정보 보호(AWS는 공용키를 저장하고 사용자는 개인키를 안전한 장소에 보관하는 방식)로 계정을 관리할 수 있으며, 웹 콘솔에서도 조작을 할 수 있다. (3) 인스턴스 스토어 볼륨 : 임시 데이터 를저장하는 스토리지 볼륨으로 인스턴스 종료시 삭제된다. (4) Amazon EBS(Elastic Block Store) : Amazon EBS를 사용해서 영구 스토리지 볼륨에 저장한다. 스토리지는 사용되지 않을때에는 중지상태로 만들어두면 저렴하게 사용할 수 있다. (5) 인스턴스와 Amazon EBS 볼륨 등의 리소스를 다른 물리적 장소에서 액세스할 수 있는 리전 및 가용 영역 (6) 보안 그룹을 사용해서 인스턴스에 연결할 수 있는 프로토콜, 포트, 소스IP 범위를지정하는 방화벽 기능 제공 (특정 IP로 부터 접근을 허용할 수 있도록 보안정책을 넣어줄 수 있다.) (7) 탄력적 IP 주소(EIP) : 동적 클라우드 컴퓨팅을 위한 고정 IPv4 주소(EC2를 띄우게 되면 설정되는 Public IP의 경우에는 중지 후 재 실행시에 DHCP에 의해 Public IP가 변경될 수 있다. 이 경우에는 EIP를 설정해서 인스턴스에 할당을 해주면 Public IP가 변경되지 않는다. 다만 사용되지 않을 때 비용이 발생한다.) (8) 태그 : 사용자가 생성하여 Amazon EC2 리소스에 할당할 수 있는 메타 데이터 모든 서비스에 태그를 할 수 있는데, 태그를 통해 비용산정/특정 서비스/특정 팀에서 사용하고 있는 리소스를 파악할 수 있다. (NAME TAG) 해당 인스턴스가 어떤 용도로 쓰이고 있는지 태깅하는 습관을 들이도록 한다. (9) AWS 클라우드에서는 논리적으로 격리가 되어있지만, 원할때마다 고객의 네트워크와 간편히 연결할 수 있는 가상 네트워크,(Virtual Private Clouds(VPC)) [S3] 지속적으로 파일을 관리해줘야되기 때문에 생명주기를 얼마나 유지해줘야 할지 라이프사이클을 정의하고, 일정 기간이 지나면, AWS S3 Gracier(온프레미스의 테입 드라이브와 같은 서비스)서비스를 활용하여 Archieving을 해준다.cost는 저렴하고, 테입 드라이브처럼 아카이빙하기는 좋은데, 읽을때에는 느리다는 단점을 가지고 있다. 버킷Amazon S3에 저장된 객체에 대한 컨테이너로, 모든 객체는 특정 버킷에 포함이 된다. 일종의 윈도우 폴더와 같은 개념으로 이해하면 된다. 객체Amazon S3에 저장되는 기본 객체로, 객체는 객체 데이터와 메타 데이터로 구성이 된다. 일종의 윈도우 폴더내의 파일로 생각하면 된다. 키버킷 내 객체의 고유 식별자로, 버킷 내 모든 객체는 딱 하나의 키를 갖는다. 버킷, 키 및 버전 ID의 조합이 각 객체를 고유하게 식별하기 때문에 Amazon S3 = &quot;버킷 + 키 + 버전&quot;과 객체 자체 사이의 기본 데이터맵으로 간주할 수 있다. 그리고 형상관리 기능도 지원한다. [RDS] Amazon RDS는 Amazon Relational Database Service의 약자로 클라우드에서 관계형 데이터베이스를 더욱 간편하게 설정하고 운영/확장 할 수 있도록 도와주는 서비스이다.하드웨어 프로비저닝(=설치), 데이터베이스 설정, 버그 패치 및 백업과 같은 시간 소모적인 관리 작업을 자동화하면서 비용 효율적이고 크기 조정이 가능한 용량을 제공한다. 지원 가능한 데이터베이스 엔진으로는 위의 첨부한 그림에서와 같이 Oracle, MySQL, Microsoft SQL Server, PostgreSQL, MariaDB, Amazon Aurora (MySQL, PostgreSQL 두 오픈소스의 약점을 보완)를 제공한다. [API Gateway] 최근에 마이데이터 서비스로 인해서 API Gateway service도 많이 언급되고 있다. 한 유저가 특정 데이터를 A사이트 -&gt; B사이트로 이전해달라고 요청시, RESTFul Application이 되어있으면, API를 통해서 데이터 이전을 해줄 수 있다.AWS 서비스 내의 서비스를 연결시켜주는 관문적인 역할을 해준다.서비스와 리소스명이 들어오면, 해당되는 리소스로 switching해주는 Proxy의 역할도 해준다. 어떤 규모든지간에 개발자가 API를 생성/게시/유지 관리/모니터링 및 보호할 수있도록 해주는 AWS의 서비스이다. 모바일 및 웹 어플리케이션에서 AWS 서비스에 엑세스할 수 있는 일관된 RESTFul Application Programming Interface(API)를 제공한다.사용자는 RESTFul API를 생성, 구성, 호스팅해서 어플리케이션의 AWS 클라우드 자원에 액세스를 할 수 있다. [CloudWatch] 서비스도 중요하지만 서비스별로 모니터링할 수 있도록 만들어진 서비스이다.지금은 많이 개선되었지만, 각 이벤트마다 형식이 서비스마다 다르기때문에 모니터링에 있어서 어려움이 있다. 따라서 이에대한 고민이 필요하다. Amazon Web Service(AWS)리소스와 AWS에서 실시간으로 실행중인 어플리케이션을 모니터링한다. 리소스 및 어플리케이션에 대해 측정할 수 있는 변수인 지표를 수집하고 추적할 수 있다. 경보는 알림을 보내거나 정의한 규칙을 기준으로 모니터링하는 리소스를 자동으로 변경한다. 이전 학습에 배웠던 Amazon EMR, Amazon ES, Amazon Kinesis, Amazon Athena, Amazon Lex, Amazon Forecast등의 이벤트 로그들을 Amazon CloudWatch를 통해서 실시간으로 모니터링 할 수 있으며, delay time은 2-3분정도 된다고 한다. 리소스가 부족할때에는 프로그램으로 리소스를 확장할 수도 있고, 경보나 알림을 이메일이나 SNS로 푸쉬알람을 줄 수 있는 기능을 줄 수도 있다. 데이터를 핸들링하는 부분에 있어서는 리소스가 부족한 부분이 생길 수있다. 예를들어, Kinesis와 같은 큐도 제한(LIMIT) 용량을 가지고 있는데, CloudWatch가 모니터링을 하고있다가, 용량의 70-80%가 차는 경우, 샤드를 늘려줄 수 있도록 프로그램을 짤 수 있다.이를 통해, 로그를 수집하고 있을때 loss가 없게 파이프라인을 구성할 수 있다. CloudWatch는 1 Service - 1 Monitoring 지원을 한다.","link":"/2022/04/10/202204/220410-data-pipeline-4/"},{"title":"220410 Hadoop과 친해지기 다섯 번째 이야기(HDFS에 대한 이해)","text":"이번 포스팅에서는 HDFS의 내부구성에 대해서 알아보고, 대용량의 파일을 어떤식으로 처리하는지에 대해서 한 번 정리해보겠다. 개괄적으로 HDFS에서 대용량의 파일을 처리하는 방법에 대해서 간단히 알아보면 대용량의 파일을 여러개의 블록 단위로 쪼개서 클러스터를 구성하고있는 각 노드에 저장을 한다.각 노드는 같은 데이터 블록 정보를 저장하고 있기 때문에 노드 중에 한 개에 문제가 생겨도 복원을 할 수 있다.(Single point failure issue 대응) 이제 한 번 STEP BY STEP으로 HDFS(Hadoop Distribution File System)에 대해서 알아보자. STEP1 BIG DATA의 등장과 블록단위로 쪼개고 저장하기 자, 엄청나게 큰 데이터가 주어졌다고 가정하자.HDFS는 이러한 빅데이터를 전체 클러스터에 분산해서 안정적으로 저장해서 어플리케이션이 그 데이터를 신속하게 액세스해 분석할 수 있도록 돕는다. 좀 더 구체적으로 말하면 HDFS는 대용량 파일들을 다루기 위해서 만들어졌다. 작은 데이터는 누구든 쉽게 다룰 수 있지만, HDFS는 이런 대용량의 파일들을 작은 조각으로 쪼개서 클러스터 전체에 걸쳐 분산시키는데 최적화되어있다.그래서 분산 파일 시스템이라고 하는 것이다. 아직 빅데이터를 안다뤄봤지만, 빅데이터와 같은 대용량의 파일은 센서나 웹 서버등으로부터 받아오는 정보의 로그 등이 될 수 있다고 한다. 이러한 대용량의 데이터를 작은 조각으로 쪼개는 과정은 그 파일을 우선 데이터 블록단위로 쪼갠다. 그 개별 블록의 사이즈도 큰 편인데, 기본값으로 128MB이다.이렇게 블록단위로 나누게 되면, 더 이상 하드 드라이브의 용량에 제한되지 않는다. 그말인 즉, 각 하드 드라이브의 용량보다 훨씬 큰 파일의 데이터 정보를 담을 수 있다는 말이 된다. 각 노드는 자신에게 저장된 데이터 블록을 동시처리할 수 있으며, 처리되는 특정 블록이 저장된 곳이랑 물리적으로 가까운 거리에 있도록 조정을 할 수 있다. (데이터의 효율적 액세스) 블록들이 저장될 때에는 단 하나의 블록만을 저장하지 않고, 모든 블록마다 두 개 이상의 복사본을 다른 노드에 저장을 한다. 그래야지 특정 노드가 다운이 되더라도 HDFS가 해당 블록의 백업 복사본을 가지고 있는 다른 노드에서 정보를 불러올 수 있다. HDFS ArchitectureHDFS의 Architecture에 대해서 세세하게 알아보자.먼저 HDFS는 대용량의 파일을 여러 블록으로 쪼개서 클러스터내의 여러 노드 전반에 거쳐 데이터를 분산/저장한다고 했다. 그럼 분산된 각 데이터 블록들 중에 특정 블록을 찾아서 처리하고자 할때, 어디있는지 기억하고 있는 친구가 있어야 한다. 이 친구가 바로 이름 노드(Name Node)이다.이 Name Node는 내부에 큰 차트를 가지고 있는데, 그 곳에는 주어진 파일의 이름과 HDFS 내의 가상 디렉토리 구조 등의 정보가 있고, 파일과 관련된 모든 블록과 해당 복사본들이 어떤 노드에 저장되어있는지에 대한 정보를 기록해두고 있다.그 외에도 편집로그(Edit log)가 있는데, 무엇이 생성되었고, 어떤게 수정되어서 저장되었는지에 대한 정보가 기록이 된다. 그냥 모든 변경 사항이 생기면, 이 Name Node라는 친구의 테이블에 기록이 된다고 알면 된다.따라서 하위에 있는 Data Node에 무엇이 있는지에 대해 Name Node에 기록이 된다는 말이다. [HDFS에서 파일읽기]자 그러면 HDFS의 각 노드에 저장된 데이터 파일은 어떻게 읽는 것일까? [STEP1]어플리케이션이 HDFS에 있는 데이터에 액세스를 하고자 한다. 이 경우, 우선 Client Node에 있는 클라이언트 어플리케이션은 Name Node에 Query를 해서 &quot;내가 지금 파일 A가 필요한데, 어디로 가야되?&quot;라고 묻는다. [STEP2]그러면 Name Node는 “파일 A는 지금 Y라는 노드의 x라는 블록들에 저장이 되어있어”라고 알려준다. [STEP3]그러면 클라이언트 어플리케이션은 Name Node가 알려준 정보를 기반으로 찾고자하는 A라는 데이터를 가지고 있는 Data노드를 방문해서 해당 데이터를 가져와서 파일 A를 구성한다. 위에서 [STEP1] ~ [STEP3]에서 Client Node에서 Name Node에 요청을 하고, Client Node에서 Name Node로부터 받은 정보를 기반으로 Data Node로 가서 데이터를 가져와서 파일을 구성한다고 했는데, 실제로는 클라이언트 라이브러리가 사용된다. API가 특정 Byte를 특정 파일에서 읽고 싶다고 하면, HDFS 클라이언트 라이브러리가 내부적으로 어떤 블록을 어디서 찾아야 할지 그리고 어떻게 찾아가야 되는지에 대해서 알아낸다. 위에서 설명한 내용은 내부적으로 일어나는 일로 이해하자. [HDFS에서 파일쓰기]클라이언트 어플리케이션이 HDFS에 새로운 파일을 만들려고 할때, [STEP1]우선적으로 블록의 위치를 추적하는 Name Node라는 친구에게 우선 물어본다. [STEP2]그러면 Name Node는 새로운 파일 항목을 확인하고, 그 블록의 위치를 정해서 Client Node에게 특정 Data Node에 가서 새로운 파일 항목을 저장하라고 알려준다. [STEP3]그러면 Client Node는 지명받은 특정 Data Node에 가서 파일을 건내준다. 그러면 해당 Data Node의 주변 다른 Data node에 복사본을 전달하고 또 전달을 한다.(Broadcasting) [STEP4]최종적으로 데이터가 저장이 되면, 잘 받았다는 신호를 Client Node를 통해 Name Node에 전달을 한다.그러면 Name Node는 새로운 파일의 블록과 복사본의 위치를 테이블에 기록을 해서 기억한다. Name Node의 Single Point Failure 자 앞서 살펴보았듯이 Data Node가 여러 개인 것에 반해 Name Node는 한 개였다.그래서 우려되는 점이 바로 Single Point Failure(단일 실패 지점)이다.만약 한 개만 존재하는 Name Node가 만약에 문제가 생기게 된다면, Client Node에서 찾고자하는 파일 블록의 위치에 대해서 물어볼 수 있는 곳이 없어지기 때문에, Name Node의 Single Point Failure에 대한 해결이 필요하다. Name Node Resilience 메타데이터를 지속적으로 백업한다. Name node가 편집 로그를 Local Disk와 NFS(Network File System)에 동시에 저장하도록 구성을 하는 것이다. 이때 NFS는 다른 랙이나 데이터 센터의 백업 자장소와 연결이 되어있기 때문에 나중에 Name Node가 죽어도 NFS 백업에서 편집 로그라도 살릴 수 있으며, 새롭게 Name Node를 만들어서 되살린 편집 로그를 사용해서 boostrap할 수 있다. 하지만, 백업 데이터를 다시 작성하는데에는 렉이 좀 걸리기 때문에 어느 정도의 정보 손실은 감수해야한다. 하지만, 데이터를 완전히 잃어버린다는 것은 아니다. 약간의 다운타임이 있어도 괜찮은 상황에서는 메타데이터를 지속적으로 백업해서 Name Node의 Single Point Failure 문제를 예방하는 방법으로 괜찮다. Secondary Name Node를 운영한다.Secondary Name Node는 메인 Name Node와 동시에 운영하지 않는다. 즉, 동적 백업(hot backup)상태가 아니다.이 Secondary Name Node가 하는 일은 Main Name Node의 편집 로그의 복사본을 유지하는 역할을 한다. 이렇게 유지된 편집 로그 복사본을 사용해서 Name Node의 복구가 필요한 상황에 사용된다. HDFS 연합(Federation)은 이름에서 유추되는 것과 같이, 단일 이름 노드를 확장한다는 개념이 아니다.HDFS는 많은 수의 작은 파일보다는 대용량 파일을 다루는데 최적화되어있다.하지만 다수의 작은 파일만 있는 상황이 올 수 있는데, 이러한 경우에는 Name Node는 한계점에 다다를 것이다. 그말인즉, 단일 Name Node만으로는 충분하지 않은 시점이 올 수 있다는 것을 의미한다.위와같은 이유로 HDFS Federation은 HDFS 파일 구조 내에 namespace volume이라고 불리는 서브디렉터리마다 분리된 이름 노드를 지정한다. 그러면 각 볼륨마다 데이터 파일을 읽거나 쓸 때 어떤 이름 노드와 이야기해야 하는지 알 수 있고, Name Node의 업무를 분담할 수 있다.Name Node 중에서 하나가 다운되면 적어도 데이터의 일부분만 잃어버린다는 것이다. Single Name Node가 다운되어 전체 HDFS 클러스터가죽는 것만큼 재앙적인 상황이 아니다. 클러스터에 어떠한 경우에도 다운타임이 허용되지 않는 경우,HDFS High Availability를 사용해서 Hot Standby Name Node를 운영한다.Hot Standby Name Node는 공유되는 편집 로그(Shared edit log)를 활용하는데, Name Node가 HDFS가 아닌 다른 안전한 공유 저장소에 편집 로그를 작성한다.그리고 Name Node에 문제가 발생하여 다운이 되면, Hot Standby Name Node가 바로 업무를 이어받는다. 이전에 하둡 에코 시스템에서 살펴보았던 Zookeeper가 어떤 Name Node가 활성화되어있는지 파악을 하고, Client Node는 우선적으로 Zookeeper와 이야기해서 어떤 이름의 노드와 소통을 해야하는지 알아내고, Zookeeper는 모든 클라이언트들이 한 번에 하나의 Name Node만 사용하도록 통제한다. 예를들어 두 개의 Name Node가 동시에 작동할 수 있다고 가정하자. 이렇게 되면, 쓰기 요청을 받을때 한 Name Node는 그 데이터가 어디에 있는지 알고, 다른 Name Node는 모를 수 있다. 그 외에도 읽기 요청이 각 각 다른 Name Node로 전달되는 경우에도 안좋은 경우가 발생한다.따라서 이러한 상황을 미연에 방지하기 위해서 HDFS High Availability는 극단적인 조치를 취하는데, 한 번에 하나의 Name Node만을 사용하도록 사용되지 않는 Name Node의 전원을 물리적으로 차단시켜서 이 노드와는 소통하지 못하도록 한다.위와같은 처리를 위해서는 많은 구성이 필요하다. HDFS를 사용하는 방법HDFS는 어떻게 사용하는 것일까? (1) 유저 인터페이스를 사용하는 방법단순히 파일을 복사하고 HDFS의 디렉토리 구조를 시각화하고 싶을 때 사용한다.(마치 HDFS를 거대한 하드 드라이브처럼 활용하는 케이스)HDFS는 클러스터의 수 많은 컴퓨터에 걸쳐 분산되어 있고, 실패 회복력과 큰 가능성이 있다. 결국에는 파일 시스템을 갖춘 거대한 하드 드라이브인 것이다. HTTP로 HDFS를 다루는 방법직접적으로 HTTP로 접근하거나 Proxy 서버를 통해서 할 수 있다. Proxy 서버는 HDFS 머신과 Client 사이에 위치해있으며, 데이터를 분석하기 위해 스크립트를 작성하거나, 어플리케이션 개발을 위해 이 Proxy 서버에 웹 인터페이스도 구성할 수 있다.이 인터페이스를 구성하는 것이 매우 중요하며, HDFS는 대부분 자바로 코딩되어있고, 자바 인터페이스를 사용한다.대부분의 언어들은 자바 코드와 소통할 수 있기 때문에 C, Python, Scala등을 사용해도 무관하다. 결국에는 HDFS의 내장 Java 인터페이스와 내부적으로 소통을 할 수 있다. NFS 게이트웨이를 이용하는 방법NFS 게이트웨이를 사용해서 HDFS를 사용할 수 있다.NFS는 네트워크 파일 시스템으로, 원격 파일 시스템을 서버로 마운트한다. NFS를 통해 파일 시스템을 서버에 연결하듯이 NFS 게이트웨이를 통해 HDFS를 리눅스 박스에 연결시킬 수 있다.그러면 하드 드라이브에 또 다른 마운트 지점이 생기게 되고, HDFS와 전혀 관련없는 다른 프로그램들도 사용할 수 있게 된다.NFS 게이트웨이를 사용하는 다른 프로그램에게는 그냥 어떤 파일 시스템의 또 다른 파일처럼 보이게 된다는 의미이다.","link":"/2022/04/10/202204/220410-hadoop_bigdata_class/"},{"title":"220411 데이터 파이프라인 스터디 4일차","text":"이번 포스팅에서는 앞으로 진행하게 될 첫 실습내용에 대해서 개괄적으로 정리를 해보려고 한다.실습에 사용되는 것은 AWS의 EC2, Kafka, Logstash, Twitter이며, 간단하게 말하면 EC2 가상 서버를 기반으로 설치된 Producer와 Kafka(queue), Consumer로 구성하고, Producer가 Twitter에서 발생하는 데이터를 수집해서 Kafka queue에 저장된 Twitter의 데이터를 Consumer에서 가져와서 화면에 보여주는 형태로 실습을 진행한다. (Kafka를 온프레미스에 설치하듯이 AWS의 EC2에 설치를 해서 실습을 진행한다) Kafka가 뭐지? Kafaka란 메시지 큐이고, 분산환경에 특화되어 설계가 되어있다는 특징을 가지고 있다. 따라서 기존의 RabbitMQ와 같은 다른 메시지큐보다 훨씬 빠르게 처리한다.(여기서 RabbitMQ란 커다란 바구니 안에 여러가지 공이 있는데, 공은 queue에 있는 job을 의미한다. 공들 중에서 하나를 꺼내서 job을 수행하고, 성공적으로 job이 끝나면 해당 job을 삭제하는 일련의 과정을 거친다) Kafka가 다른 메시지큐보다 빠른 이유는 위에서 언급했듯이, 분산환경에 특화되어 설계가 되어있는데, 때문에 여러 서버 혹은 서비스에서 발생하는 데이터를 한 군데에서 수집할 수 있는 환경을 구성할 수 있다. 또한 대표적인 스트림 큐의 서비스이며, 파일 사이즈를 기준으로 설정하며, default로 1GB 정도의 데이터를 유지한다. 만약에 큐의 데이터가 1GB를 넘을 경우, 삭제처리한다.(ex. 폭포에 비유 : 사이즈를 넘는 데이터는 폭포에서 떨어지는 데이터로, 삭제 처리한다) [특징] LinkedIn에서 개발된 분산 메시징 처리 시스템이다. 파일시스템을 사용하기 때문에 데이터영속성이 보장된다. 대용량의 실시간 로그 처리에 특화되어 설계된 메시징 시스템으로, 기존 범용 메시징 시스템대비, TPS가 우수하다. 이러한 이유로 최근에 가장 많이 사용하는 스트림 큐이다. Producer &amp; ConsumerKafka 큐의 데이터를 중심으로 데이터를 생산하는 Producer와 소비하는 Consumer가 있다. Producer는 웹이나 앱 혹은 관련 IoT 또는 회사의 시스템에서 발생하는 여러 메시지를 카프카 클러스터에서 받아서 큐에 쌓고, 쌓여있는 큐의 데이터를 컨슈머들이 가져다가 사용한다. Producer는 메시지를 생산하는 주체이며, Consumer는 소비자로써 메시지를 소비하는 주체이다. Consumer는 카프카(큐)에 쌓여있는 데이터를 가져다가 DB에 저장을 하기도 하고, S3에 저장, 리얼로 저장(Elasitic search, NoSQL DynamoDB에 저장)하여 대시보드에 보여주는 형태로 사용된다. 위의 도식도를 보면 알 수 있듯이, Producer와 Consumer는 1:1 매핑이 아니며, Consumer는 데이터를 소비하며, 자기가 어느 위치의 데이터를 가져다가 처리하는지에 대한 정보를 가지고 있다.Consumer마다 각 각 해당 정보들을 가지고 있으며, 시스템이 받쳐준다면 더 많은 컨슈머들이 존재할 수 있다. 실무예시-Producer와 Consumer가 일치하지 않는다)수집량이 25TB인데 소비하는 데이터의 양은 하루에 30TB라고 한다. 수집량과 데이터의 양은 같지 않음을 알아야 한다. Kafka 실습 구성도실습은 AWS의 가상 서버인 EC2에 설치를 한다.Kafka 클러스터는 별도로 구성을 안하고, 큐 하나로써 서버에 구성을 한다.위의 도식도에서는 Producer/Kafka/Consumer 세 개의 논리적 파트로 나눠져있지만, 실제 실습에서는 EC2 가상서버를 2대로 하고, EC2 하나에서는 Kafka queue를 구성, 다른 하나에서는 Producer를 구성해서 Producer에서는 twitter에서 발생하는 데이터를 Logstash를 사용해서 수집해서 Kafka queue로 넘겨줄 것이다. kafka queue에 쌓여있는 데이터는 Kafka Consumer로 넘겨서 최종적으로 화면에 보여주는 부분까지 실습을 진행할 것이다. 간단하게 AWS에서 지원하는 카테고리별 서비스 보기 (컴퓨팅) - EC2 (스토리지) - S3, S3-Glacier (데이터베이스) - RDS, DynamoDB (마이그레이션 및 전송) - DMS(Database Migration Service) 이기종 데이터를 옮길때 사용, AWS내의 DBMS의 데이터를 전송할때에는 현재 운영중인 서비스의 리소스를 적게 쓰면서 데이터를 migration할 수 있는 서비스이다. EC2 인스턴스 생성 인스턴스 유형 c : computing g, p : GPU 사용 m : 간단하게 사용 (CPU/GPU 반반으로 사용) i : I/O 관련 성능을 좋게 만들어놓은 것 r : 스팟 사용시 많이 사용됨. 메모리 중심 유형 네트워크 설정 VPC(Virtual Private Cloud) : AWS 리소스에 경계를 설정하는 데 사용할 수 있는 네트워킹 서비스이다. AWS 서비스를 사용하는 수백만 명의 고객이 존재하고, 이들 고객이 생성한 EC2 인스턴스와같은 수백만 개의 리소스들이 리소스간에 경계가 없으면 네트워크 트래픽이 제한 없이 리소스 간에 흐를 수 있기 때문에 VPC의 개념이 필요하다. NAT Public IP private IP 태그 설정태그 설정은 default로 넣어주는 것이 좋다. 나중에 특정 cluster에서 어떤 instance가 사용이되었고, 사용량이 어떻게 되었는지 책정하는데 태깅을 하게 되면 유용하다.다른 사람들과 AWS계정을 같이 사용할때에는 내가 사용하는 EC2가 무엇인지 구분하기 위해서는 필수이다. 보안그룹과거에는 서버를 구성할때 기본으로 보안 담당자가 설정을 했지만, in-bound, out-bound가 있다.in-bound는 EC2를 띄워주는데, 어떤 port를 통해서 들어오는지 설정하는 것이고, out-bound는 EC2에서 인터넷쪽으로 나가는 방향을 설정할때 설정한다. 일반적으로 out-bound는 다 열어준다. (은행과 같이 보안이 강한 곳을 제외)보통 in-bound 컨트롤을 많이 한다. (해킹/보안) 인스턴스 시작 검토 단계에서 로그인 패스워드 설정접속할 수 있는 RSA 방식의 *.pem 파일을 하나 다운로드 받아서 해당 파일로 접속하도록 보안이 강화되었다.region별로 키페이가 다르기때문에class-hglee-seoul 형태로 region명을 넣어주도록 한다. EC2 인스턴스 구성 인스턴스 ID : 프로그래밍(Python, Java 등의 언어)으로 인스턴스에 접근을 해서 변경을 할 수 있다. Private IP Public IP항상 Private과 Public이 쌍으로 구성되어있다. 해당 EC2 인스턴스 자체가 SUBNET VPN자체가 Public으로 구성이 되어있어서 두 개가 쌍으로 구성되어있다. 내부에서의 네트워크를 할 경우에는 Private IP/Domain을 사용해서 통신을 한다. AMI ID 키 페어 이름 (키 페어 이름으로 인스턴스를 검색할 수 있다) EC2 인스턴스 연결EC2 인스턴스 리스트에서 &quot;연결&quot;을 클릭하면, 인스턴스 ID, 퍼블릭 IP 주소, 사용자 이름 정보가 표시되는 탭이 표시가 되는데, 이 부분은 브라우저에서 바로 접속할 수 있게 지원해주는 서비스이다. GCP쪽에서는 기본적으로 브라우저에서 접속이 가능하도록 지원해주는데, AWS에서도 지원을 해준다.한글입력으로 인해 SSH 클라이언트 탭을 통해 접속을 해보도록 한다. SSH 클라이언트 탭에 나와있는 설명대로, 이전에 EC2 인스턴스 비밀번호 생성시에 다운받았던 *.pem 파일의 권한모드를 변경시켜주고, 권한모드를 변경한 *.pem파일을 퍼블릭 DNS를 사용하여 인스턴스에 연결해준다. 12$chmod 400 class-hglee-seoul.pem$ssh -i &quot;class-hglee-seoul.pem&quot; ec2-user@ec2-3-34-44-134.ap-northeast-2.compute.amazonaws.com 위의 commandline 명령을 실행하면, 아래와같이 *.pem 파일 경로와 동일 경로에 사용자 초기화 파일들이 생성이 되는 것을 확인할 수 있다. Git에 있는 소스코드/분석에 진행할 로그 데이터를 다운로드 받기 위한 Git 설치 12# CentOS 환경에 Git 설치$sudo yum install git Git에서 다운받은 실습용 로그 데이터를 분석을 위해서 S3로 이전s3-cli명령어 s3-sync 명령어 사용해서 이전 서비스와 서비스 간의 권한을 획득해서 다른 서비스를 사용하기 위해서 access key, security key를 사용해야했다.서비스와 서비스간의 권한주기위해서는 access key와 security를 사용해서 해킹을 당하는 경우가 생겨서, 권한을 부여해서 사용하는 것을 권장하게 되었다.AWS console에서 IAM에서 권한을 줄 수 있게 되어있다. (사용자 역할 및 정책)IAM 대시보드에서 사용자에대한 역할 및 정책을 컨트롤할 수 있으며, 보안자격증명에 가보면, Accesskey키와 Security키를 확인할 수 있다.잃어버린 경우에는 비활성화 시킨 뒤에 재생성하면 된다. 서비스 &lt;-&gt; 서비스 간의 권한을 만들어서 하는 방식으로 실습진행한다.권한을 탈취될 경우가 적기때문에 역할을 만들어서 실습을 하도록 한다. EC2 역할 생성IAM &gt; 역할 &gt; 역할 생성 (일반 사용사례에서 EC2 선택)권한 정책 연결에서 AdministratorAccess 권한 추가해준다.AdministratorAccess를 추가해주게되면, EC2에서 다른 어떤 서비스에도 접근할 수 있는 권한을 주게 된다.역할 이름 및 태그도 어떤 EC2 인스턴스에 적용을 할 것이지 정확히 명시해주도록 한다. 생성한 EC2 역할을 EC2 인스턴스에 적용EC2 인스턴스 리스트에서 적용하고자하는 EC2 인스턴스를 체크한 다음에 인스턴스 시작 왼쪽의 드롭다운 박스에서 생성한 IAM 역할을 선택한다.[보안]-[IAM 역할 수정]-[EC2Admin]을 선택한다. EC2에 적용된 역할 확인(aws cli 명령) 및 S3 Bucket 생성1234567# s3를 읽어서 확인# 현재 생성되어있는 s3내의 bucket 리스트 확인 (해당 bucket에 역할 적용)$aws s3 ls$export bucketname=hg-myclass #bucket이름을 변수에 넣기# region option을 주지 않으면, 미국 동부 버지니아가 기본값으로 설정되서 버킷이 생성되는데, 이 경우에는 버지니아에 생성이 되었기 때문에 여기에 데이터를 넣고 서울에서 데이터를 땡기게 되면, 데이터 전송 비용이 발생을 하게 된다.# cli에서 많이 사용해야 프로그램통해서 리소스를 핸들링할때 도움이 된다.$aws s3 mb s3://$bucketname --region ap-northeast-2 생성한 S3 Bucket에 분석용 로그파일 복사12# .git/* 하위의 파일들을 제외하고 s3에 새로 생성한 버킷에 현 위치에 있는 폴더를 복사한다. (sync 명령어 사용)$aws s3 sync . s3://$bucketname --exclude '.git/*'","link":"/2022/04/11/202204/220411-data-pipeline-study/"},{"title":"220412 Hadoop과 친해지기 일곱 번째 이야기(MAPREDUCE 이해)","text":"이번 포스팅에서는 MapReduce의 정의에 대해서 알아보고, 어떻게 작동을 하는지 그리고 분산처리하는 방법과 간단한 MapReduce 연습문제를 통해 데이터를 분류해보도록 하겠다. MapReduce?이전에 Hadoop의 Core System에 대해서 전반적으로 알아볼때 이미 학습했지만, MapReduce는 HDFS -&gt; YARN -&gt; MapReduce 순으로 구조화되어있다. 즉, MapReduce는 HDFS, YARN과 함께 제공되는 Hadoop의 핵심기술이다.Hadoop을 이해하기 위해서는 MapReduce를 잘 이해하는 것이 필수이다. MapReduce는 Hadoop에 내제된 기능으로, 클러스터에 데이터의 처리를 분배하는 역할을 한다.데이터를 파티션으로 나눠서 클러스터 전반에 걸쳐서 병렬 처리되도록 한다.이러한 일련의 과정에서 관리를 하고 실패하였을때 감당하는 역할을 한다. Data Mapping -&gt; Mapper -&gt; Data Transformation데이터를 변형(Transformation)하는 과정으로, 데이터가 한 줄씩 들어오면, Mapper는 해당 데이터를 변형시키게 된다. 들어오는 데이터에서 필요한 정보를 추출하고, 이해할 수 있는 구조로 정리를 해서 해결하고자하는 문제를 해결한다. 데이터를 집계하고자 하는 중요한 데이터를 키-값의 형태로 연관시켜서 구조화시킨다. (키:집계하고자하는 항목)input line -&gt; (mapper) -&gt; Data 추출 및 구조화해서 한 줄로 출력 Mapper의 처리 순서는 아래와 같다. INPUT DATA -&gt; Mapper -&gt; K1:V, K2:V, K3:V, K4:V, K5:V …. Data Reducing -&gt; Reducer -&gt; Data AggregationReducer는 데이터를 집계한다. Mapper를 통해 key:value의 형태로 변형된 데이터가 shuffle &amp; sort 과정을 거쳐서 Reducer는 얻고자 하는 결과 데이터를 기준으로 key:value에서 value 데이터를 집계한다. [MapReducer를 통한 문제해결]Mapper와 Reducer를 창의적으로 조합하면, 놀랄만큼 다양한 분석문제를 해결할 수 있다. MapReduce의 실패처리 메커니즘클러스터를 다룰때에는 항상 어떤 특정 노드가 다운되거나 잠기는 등의 상황을 다룰 줄 알아야 한다. MapReduce 예시 - MovieLens Dataset문제인지 : 현재 영화 평점관련 데이터가 주어졌다. 이 데이터를 활용해서 어떤 사용자가 몇 편의 영화를 평가를 했는지에 대한 데이터를 추출해야한다. 12345678USER ID | MOVIE ID | RATING | TIMESTAMP196 242 3 88125949186 302 3 88125959196 377 1 88123949244 51 2 88122949166 346 1 88121949186 474 4 88125949186 265 2 88125449 실제로 클러스터 네트워크에서 오고 가는 데이터가 적을수록 좋기 때문에 현재 필요한 USER ID와 MOVIE ID를 제외한 나머지 데이터는 필요가 없다. 이는 최적화를 위해서 매우 중요하다. 클러스터 네트워크에서 오고 가는 데이터가 적을 수록 좋다! [STEP1] Mapper를 통한 필터 : Mapper의 주된 목적은 필요한 정보를 추출하고 적절하게 구조화하는 것이다. Mapper함수가 작업을 끝내게 되면, MapReduce 프레임워크는 아래와같은 key:value 쌍의 목록을 클러스터의 어딘가에 보관하게 된다. 1196:242 186:302 196:377 244:51 166:346 186:274 186:265 [STEP2] MapReduce의 Shuffle &amp; Sort별도의 코딩없이 MapReduce가 섞고 정렬해주는 작업을 해준다. 각 고유 키에게 주어진 값들을 묶어서 집계를 해주고, 더 나아가 키를 정렬해준다. 여기서 중요한것은 각 각의 값에 대한 데이터가 클러스터 내의 각기 다른 컴퓨터의 Mapper에서 왔을 수도 있다. MapReduce는 클러스터 전체에 걸치 모든 값들을 집계하고 한 자리에 가져와서 필요에 따라 처리하는 강력한 힘을 가지고 있기 때문이다. 1166:346 186:302,474,265 196:242,377 244:51 [STEP3] ReducerMapReduce의 Shuffle &amp; Sort과정을 거쳐서 새롭게 필터된 값이 나왔는데, 이 값은 각 키마다 Mapping된 값의 목록이다.Reducer는 이 출력으로 무엇을 할지 특정하는 역할을 한다.여기서 각 사람마다 평가한 영화의 편 수를 구하는 것이 문제상황이므로, 간단하게 각 value list에 len()이라는 연산자를 사용해서 처리하면 된다.Reducer는 데이터의 고유 키를 한 번씩만 호출한다. 만약에 key가 전체 클러스터에 걸쳐 저장이 되어있다면, 다시 호출될 수 있으며, 이 말은 여러 Reducer가 여러 대의 컴퓨터에 걸쳐 동시에 병렬로 작업중이라면, 개별 Reducer는 주어진 양의 키를 처리해야하고, 최종적으로 Reducer는 각각의 키를 한 번만 호출한다는 의미이다. 1166:1 186:3 196:2 244:1","link":"/2022/04/12/202204/220412-hadoop_bigdata_class-1/"},{"title":"220412 Hadoop과 친해지기 여섯 번째 이야기(HDFS 다뤄보기)","text":"이번 포스팅에서는 sample 데이터를 활용해서 HDFS를 실제로 다뤄보는 실습을 해본다. AmbariAmbari는 Hadoop Eco System에서 구조상 최 상위에 위치해있는 어플리케이션으로, Hadoop과 관려된 서비스들 전체를 모니터링할 수 있는 웹 인터페이스 환경을 제공한다. 웹 인터페이스를 사용해서 HDFS 다뤄보기 HTTP 인터페이스로 간단하게 HDFS에 파일을 업로드, 로컬 디스크에 다운로드, HDFS에 업로드한 파일들을 삭제해보았다. HDFS에 파일을 업로드할때에는 단순히 파일을 업로드 되는 것처럼 보이지만, 실제로는 파일을 올릴때 HDFS 내부의 Name Node에 어느 Data Node에 저장을 할지 확인하는 절차를 통해 클러스터내의 특정 Data Node에 저장을 하고, 정상적으로 저장이 완료되면 Name Node에 결과를 알려줘서 Name Node가 관리하고 있는 저장 데이터와 관련된 메타 데이터 테이블에 새롭게 저장된 데이터 정보를 추가하게 된다.(내부적으로 이러한 일련의 과정을 거치고 있다는 것을 인지한 상태에서 계속해서 학습을 이어가도록 하자) Hadoop 클러스터와 연결된 명령 프롬프트의 명령줄을 활용해서 HDFS 다뤄보기master node, client node에 접근해서 HDFS를 읽는다. 가상머신의 OS INSTANCE를 PUTTY로 원격 연결12Host Name : maria_dev@127.0.0.1Port : 2222 HDFS를 linux host commandline으로 조작$ prompt의 뒤에 hadoop fs -{linux commandline}의 형태로 command라인을 작성해주면 된다. 123456789101112131415$hadoop fs -ls$hadoop fs -mkdir ml-100k$wget http://media.sundog-soft.com/hadoop/ml-100k/u.data$hadoop fs -copyFromLocal u.data ml-100k/u.data$hadoop fs -ls ml-100k$hadoop fs -rm ml-100k/u.data$hadoop fs -rmdir ml-100k$exit","link":"/2022/04/12/202204/220412-hadoop_bigdata_class/"},{"title":"220413 데이터 파이프라인 스터디 5일차","text":"이번 포스팅에서는 웹 서버 전용 EC2 인스턴스를 생성하고, 생성한 EC2 인스턴스에 httpd 설치 및 실행을 해보고 간단한 테스트 웹 페이지를 띄워보는 실습을 해본다. Httpd우선 웹 서버 전용 EC2 인스턴스를 생성한 뒤에 해당 EC2 인스턴스에 Httpd demon을 설치해줬다. 선택한 AMI가 CentOS와 비슷한 이미지를 가지고 만들었기 때문에 설치한 httpd 서비스가 EC2 lifting시에 자동으로 로드되게 하기 위해서는 서비스를 등록해야되는데, 설치하면 default로 서비스에 등록이 되기 때문에 아래와 같은 프로세스로 서비스 설치 및 시작을 하면 된다.설치된 Httpd에 기본 Home 디렉토리(/var/www/html)가 있는데, 해당 위치에 웹 기본이 되는 파일(index.html)을 생성해서 간단하게 웹 페이지 테스트를 할 수 있다. 12345678910111213141516171819202122232425# httpd 설치$sudo yum install httpd -y# httpd service 시작$sudo service httpd startRedirecting to /bin/systemctl start httpd.service# 현재 실행되고 있는 service process에서 httpd관련 service를 출력$ps -ef|grep httpdroot 6563 1 0 02:33 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 6564 6563 0 02:33 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 6565 6563 0 02:33 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 6566 6563 0 02:33 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 6567 6563 0 02:33 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 6568 6563 0 02:33 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDec2-user 6611 3407 0 02:33 pts/0 00:00:00 grep --color=auto httpd# root 사용자로 변경 (prompt $ -&gt; #)$sudo su -# 웹 서버에 출력할 html파일 출력$cd /var/www/html/# index.html 파일 작성$vi index.html 위에서 index.html 파일내에 HTML 코드를 작성 후, 주소표시줄에 “연결” 메뉴에서 확인하였던 퍼블릭 DNS 주소를 웹에 붙여넣으면, 작성했던 HTML 페이지가 출력됨을 확인 할 수 있다.이처럼 Public DNS 주소를 통해 생성된 EC2 인스턴스의 리소스에 접근을 할 수 있다는 것을 알 수 있다. 추가적으로 위의 httpd를 설치 및 서비스 시작하는 일련의 과정을 EC2 인스턴스 생성시에 고급 세부 정보 세션에서 사용자 데이터 부분에 스크립트로 넣어서 간단하게 실습해볼 수도 있다. (웹 서버 시작 부분까지) 123$yum update -y$yum install httpd -y$sudo service httpd start Kafka 실습 구성이번 실습에서는 실제 Cluster까지는 구현을 안하고, 메시지가 어떻게 흘러가는지 이해할정도로만 구성하고, 메시지가 어떻게 흘러가는지 전반적인 흐름에 대해서 파악을 한다. 파이프라인을 구성할때 PC의 툴이나 연필로 직접 그려가면서 파이프라인을 구성하는 연습을 하면 완성도 높은 파이프라인을 구성할 수 있다. 아직 지식이 많이 부족하더라도 조금씩 이해하고 있는 지식들을 최대한 활용해서 연습해보도록 하자. (Topic name : twitter) Kafka 설치 : Kafka에 Topic을 하나 생성해서 이 곳에 데이터를 쏠 것이다. Producer에서는 Logstash가 있는데, 이 친구는 Elastic search에서 데이터를 수집해서 Elastic search 안으로 넣어주는 프로그램이다. 데이터를 임의로 생성하기 어렵기 때문에, 트위터에서 데이터를 받아서 카프카에 넣어서 처리를 해볼 것이다. Topic을 Consumer하는 부분까지 구성한다. Consumer 자체에서도 Logstash를 띄워서 카프카 큐에 있는 데이터를 가져와서 화면에 띄워주는 부분까지 작업 위의 실습은 마치 온프레미스 환경에서 서버 3대(EC2 3개)를 가지고 구성한다고 가정하고 실습하도록 하자. Kafka EC2 인스턴스 생성 및 Kafka 설치Kafka 실습 - 유형 t2.medium 이상 선택 인스턴스 세부 정보 구성 : 인스턴스 갯수 : 2 (한 번에 두 개 이상 시작 할 수 있도록 구성) Kafka가 기본적으로 자바 베이스로 돌아가기 때문에 자바를 설치하고 카프카를 설치하면 된다. Open source용 자바를 설치 권장 1$sudo yum install -y java-1.8.0-openjdk-devel.x86_64 12$wget https://dlcdn.apache.org/kafka/3.0.1/kafka_2.12-3.0.1.tgz$tar xvf kafka_2.12-3.0.1.tgz 버전이 바뀌었을때에도 논리적 링크를 가져가기 위해서 링크를 걸어서 사용한도록 한다. (kafka 뒤에 길게 붙어있는 버전명을 신경쓰지 않아도 되는 장점이 있다) 12# 압축을 해제한 디렉토리를 kafka의 alias로 link를 걸어서 재사용할 수 있다.$ln -s kafka_2.12-3.0.1 kafka Zookeeper &amp; Kafka server 시작Zookeeper에 대해서는 이전에 Hadoop의 eco system에 대해서 전반적으로 알아보면서 정리했을때 알아보았는데, Zookeeper는 클러스터의 모든 것을 조직화하는 기술이다. 이 기술을 이용하면 클러스터내에서 어떤 노드가 살아있는지 추적할 수 있고, 여러 어플리케이션이 사용하는 클러스트의 공유 상태를 안정적으로 확인할 수 있다. (Hadoop과 치해지기 세 번째 이야기 중) 123456789# 압축을 해제한 Kafka 폴더 내에서 아래 commandline을 실행한다.# zookeeper 띄우기# 클러스터간의 통신을 담당하는 친구 (=&gt; zookeeper)$./bin/zookeeper-server-start.sh config/zookeeper.properties &amp;# Kafka server 시작$./bin/kafka-server-start.sh config/server.properties &amp;# 현재 실행중인 서비스 데몬 port로 확인$sudo netstat -anp | egrep &quot;9092|2181&quot; #Kafaka server: 9092 / zookeeper: 2181 외부의 서버와 통신하기 위해서는 보안그룹에서 9092 port를 열어놔야지 다른 서버에서 producer가 메세지를 넘겨줄 수 있다. 보안그룹에서 9092 port 추가 필요.(EC2 인스턴스의 보안그룹 -&gt; 인바운드 그룹 규칙에 9092 port 추가 / kafka-producer EC2 서버에서 데이터를 넘겨줄때 해당 포트를 통해 kafka-server의 가상서버 자원으로 접근할 수 있다) Kafka에서 Topic 생성하기12345# Kafka에 Topic 생성$bin/kafka-topics.sh --create --topic twitter --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092 &amp;# 생성된 topic 확인$bin/kafka-topics.sh --list --bootstrap-server localhost:9092 메시지가 서버에 잘 쌓이는지, 서버 한 대에서 확인 1$./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic twitter --from-beginning Producer Setupkafka를 테스트용으로 producer에서도 설치를 한다. 123456789# Java 설치$sudo yum install -y java-1.8.0-openjdk-devel.x86_64# kafka 설치/압축해제/압축 해제된 디렉토리를 link 걸어서 사용/해당 디렉토리로 이동 (과정 생략)# Server로 Data를 Producing 할 것이기 때문에 Server쪽 IP의 확인이 필요하다.# 내부 통신이기 때문에 Kafka-server의 Private IP 주소를 target IP 주소로 설정한다.# topic이 twitter라는 곳에 message를 제공$bin/kafka-console-producer.sh --topic twitter --bootstrap-server 172.31.47.33:9092 위의 설정이 다 끝나고 Producer 역할의 EC2에서 메시지를 입력하면, Consumer 역할을 하는 EC2의 Terminal에 출력이 되는 것을 확인할 수 있다. 하나하나 파이프라인을 만들때마다 중간 체크를 확인해가면서 파이프라인을 구축해나가는 연습을 해야한다.ctrl+d : 신호를 보내는 것이 아닌, 특수한 바이너리 값을 나타내며, EOF(End Of File)을 의미 cf) ctrl+c : 중단 명령 (프로그램의 실행을 강제로 중단) ctrl+z : 중단 명령 (작업이 끝나지 않았으므로, 프로세스에서 끊긴 상태를 유지. fg/bg 명령을 사용하여 프론트나 백스테이지의 작업을 이어나갈 수 있다) [Producer side] Logstash 설정 및 twitter 연결하기[Logstash 설정하기]1234$wget https://artifacts.elastic.co/downloads/logstash/logstash-7.4.0.tar.gz$tar xvzf logstash-7.4.0.tar.gz$ln -s logstash-7.4.0 logstash logstash같은 경우에는 접속 정보나 hump라는 파일을 구성하는데 어느 폴더의 위치에서나 Logstash를 실행시키기 위해서 .bash_profile에 path를 추가해준다. 123456789101112131415161718192021222324252627282930313233343536373839404142$ vi ~/.bash_profileexport LS_HOME=/home/ec2-user/logstashPATH=$PATH:$LS_HOME/bin$source ~/.bash_profile# logstash 버전 확인$logstash --version# Producer_test.conf 파일 작성# input, output 두 개의 section으로 나눠져있으며,# input에는 twitter에서 정보를 가져오기 위한 consumer_key, consumer_secret, oauth_token, oauth_token_secret, keywords, full_tweet의 정보가 포함되어있다.# output은 print문으로 생각하면 된다.# keywords는 가져오고자 정보의 키워드를 문자열 배열로 구성한다.# twitter developer 사이트에 가입하고, 프로젝트를 생성하면 API Key, API Secret key, Bearer Token을 주는데,# API Key = Consumer key# API Secret key = Consumer secret key#input { twitter { consumer_key =&gt; &quot;[consumer_key]&quot; consumer_secret =&gt; &quot;[consumer_secret]&quot; oauth_token =&gt; &quot;[oauth_token]&quot; oauth_token_secret =&gt; &quot;[oauth_token_secret]&quot; keywords =&gt; [] full_tweet =&gt; true }}output{ stdout{ codec =&gt; rubydebug }}#logstash 실행하기#앞서 작성한 *.conf 파일을 logstash 명령을 사용하여 실행한다.$logstash -f producer_test.conf Twitter 개발자 페이지에서 필요한 key들(consumer key, consumer secret, oauth token, oauth token secret)을 발급받아 사용하는 과정에서 에러가 발생해서 시간이 많이 지연되었다. 결과적으로 아래의 Twitter log를 Producer로부터 취득해서 가져올 수 있었고, 내일부터는 Producer로부터 취득한 Log 데이터를 Kafka의 message queue에 담아보는 작업부터 이어서 해봐야겠다. 쉬운게 없다… 123456789101112131415161718192021222324252627282930{ &quot;quoted_status_id&quot; =&gt; 1514173632012234754, &quot;source&quot; =&gt; &quot;&lt;a href=\\&quot;http://twitter.com/download/android\\&quot; rel=\\&quot;nofollow\\&quot;&gt;Twitter for Android&lt;/a&gt;&quot;, &quot;quoted_status_permalink&quot; =&gt; { &quot;display&quot; =&gt; &quot;twitter.com/Pirate_Radio_/…&quot;, &quot;expanded&quot; =&gt; &quot;https://twitter.com/Pirate_Radio_/status/1514173632012234754&quot;, &quot;url&quot; =&gt; &quot;https://t.co/CrNaTanRd5&quot; }, &quot;filter_level&quot; =&gt; &quot;low&quot;, &quot;in_reply_to_screen_name&quot; =&gt; nil, &quot;@timestamp&quot; =&gt; 2022-04-13T11:15:32.000Z, &quot;id&quot; =&gt; 1514200610903494658, &quot;in_reply_to_user_id_str&quot; =&gt; nil, &quot;quote_count&quot; =&gt; 0, &quot;is_quote_status&quot; =&gt; true, &quot;id_str&quot; =&gt; &quot;1514200610903494658&quot;, &quot;reply_count&quot; =&gt; 0, &quot;timestamp_ms&quot; =&gt; &quot;1649848532783&quot;, &quot;geo&quot; =&gt; nil, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;quoted_status&quot; =&gt; { &quot;source&quot; =&gt; &quot;&lt;a href=\\&quot;http://twitter.com/download/iphone\\&quot; rel=\\&quot;nofollow\\&quot;&gt;Twitter for iPhone&lt;/a&gt;&quot;, &quot;filter_level&quot; =&gt; &quot;low&quot;, &quot;in_reply_to_screen_name&quot; =&gt; nil, &quot;id&quot; =&gt; 1514173632012234754, &quot;in_reply_to_user_id_str&quot; =&gt; nil, &quot;quote_count&quot; =&gt; 3, &quot;is_quote_status&quot; =&gt; false, &quot;id_str&quot; =&gt; &quot;1514173632012234754&quot;,.......... [Kafka message queue에 Twitter로부터 가져온 로그정보 넘겨주기]123456789101112131415161718192021input { twitter { consumer_key =&gt; &quot;[consumer_key]&quot; consumer_secret =&gt; &quot;[consumer_secret]&quot; oauth_token =&gt; &quot;[oauth_token]&quot; oauth_token_secret =&gt; &quot;[oauth_token_secret]&quot; keywords =&gt; [] full_tweet =&gt; true }}output{stdout{codec =&gt; rubydebug}kafka {bootstrap_servers =&gt; &quot;[Kafka server IP]:9092&quot;codec =&gt; json{}acks =&gt; &quot;1&quot;topic_id =&gt; &quot;[TOPIC ID]&quot;}} 위의 스크립트를 실행하면 producer가 twitter로 받은 log정보를 Kafka message queue로 쌓아주게 된다. 그 다음 STEP은 Kafka message queue에 쌓인 message들을 Consumer에서 읽어서 소비하는 작업을 처리한다.","link":"/2022/04/13/202204/220413-data-pipeline-5/"},{"title":"220414 데이터 파이프라인 스터디 6일차","text":"이번 포스팅에서는 현재 진행중인 실습과 앞으로 하게 될 실습내용에 대한 간단한 회고에 대한 내용을 정리해보려고 한다. 이번 실습내용과 앞으로의 실습내용에 대한 이해 및 정리우선 아직 마무리되지 않은 이번 실습과 앞으로 하게 될 한 번의 실습에 대해서 간단하게 짚고 넘어가고자 한다.그냥 단순히 실습을 진행하는 것 보다는 왜 내가 이 실습을 하고 있는지에 대한 셀프 피드백이 필요하다. 단순히 실습을 하고 기록을 남기는 것에 그친다면 학습에 있어 방향을 잃을 수 있고, 향후에 내가 포트폴리오를 만들 때 분명 AWS 서비스 기반으로 Cloud Topologies를 구상할 것이기 때문이다. 아직 극초반이지만, 좀 더 앞으로의 큰 그림을 생각하고 학습해나가보자.이러한 연습이 단순 포트폴리오 영역을 넘어서 나중에 업무에 있어서 빠르게 업무에 녹아드는데 도움이 될 것이라고 생각한다. 자 그럼 지금 하고 있는 첫 실습 내용과 앞으로 하게 될 실습내용을 연관지어서 한 번 정리를 해보자. (실습1) EC2 환경에서 Kafka를 이용한 Pipeline 구성 (실습2) AWS API Gateway - Kinesis - S3 구성의 Pipeline 구축 앞선 실습1, 실습2는 모두 데이터 수집을 위한 데이터 파이프라인 구축 실습이다. 표면적으로 실습1, 실습2에서 사용되고 있는 AWS의 서비스 구성만 보더라도 실습1의 경우에는 on-premise 환경에서의 구성에 가까우며, 아직은 잘 모르지만 EC2 기반으로 했기 때문에 시스템에 대한 모니터링이 필요하다고 한다.반면에 실습2의 경우에는 실습1에 비해 더 쉽고 빠르게 적용할 수 있다고 한다. 클라우드에서는 관리의 대상을 늘리기보다는 서비스에 집중할 수 있고, 관리적 요소가 적은 AWS 서비스로 구성하는 것이 좋다고 한다.Kafka를 사용한 서비스 구성이 필요하다면, Amazon MSK(Managed Streaming for Apache Kafka)를 사용하는 것이 권장된다고 한다. 이 부분에 대해서 미리 짚고 넘어가는 것은, 간혹 실습하는 것에 집중한 나머지 본질을 잊는 경우가 생겨서이다. 현재 진행중인 실습과 앞으로 진행하게 될 실습은 위에서 언급한 중요한 내용에 유념하면서 진행해보도록 하자. 2022/04/15 업데이트 Consumer 전용 EC2 생성이전에 Producer, Kafka Server 전용 EC2 instance를 생성했다. Producer에서는 Twitter로부터 로그정보를 받아서 Kafka server 전용 EC2 instance의 topictwitter로 받은 로그 정보를 방출해주고, kafka message queue는 받은 로그 정보를 queue에 쌓아준다.이번 마무리 실습에서는 consumer.conf 파일에서 input 부분에 Kafka server에 대한 정의를 topic에 대한 정보와 함께 해줌으로써 queue에 쌓인 정보를 Consumer에서 받아서 console에 출력해줄 수 있다.최종적인 구성도는 아래와 같다. 첫 번째 데이터 파이프라인 실습 회고이번 첫 번째 데이터 수집관련 파이프라인 구축 실습을 하면서 좋았던 부분은 기본적인 AWS 서비스를 좀 더 알아가면서 실습 할 수 있었던 부분이었다. 이전에 프론트엔드 관련 사이드 프로젝트를 할 때 잠깐 AWS 관련 서비스를 만져본적은 있는데 겉훑기식으로 알고 만들어서, 많이 아쉬웠었는데, 이번 기회가 많이 도움이 되었다.그래서 이참에 AWS의 가장 기초가 되는 AWS Practitioner 자격증 공부도 같이 병행을 하고 있다. 지금 공부한지 한 6일정도 되어가는데, 공식 사이트에서 제공하는 시험관련 콘텐츠를 지금 모듈5 부분을 공부하고 있다. 공부를 하면서 물론 만져보지 못한 AWS 서비스도 많지만, 이번 데이터 파이프라인 구축 실습을 하면서 만져보았던 AWS 기본 서비스 관련 내용도 많이 나와서 이론 공부와 실습을 같이 병행한 느낌이라 많이 도움이 되었다.앞으로 남은 실습들도 같이 열심히 진행하고 학습한 내용을 토대로 반복하면서 연습을 해봐야겠다.","link":"/2022/04/14/202204/220414-data-pipeline-study/"},{"title":"220415 AWS 서비스-1","text":"이번 포스팅에서는 AWS의 가장 기초적인 자격증인 Practitioner 자격증 공부를 하면서, 기록할 필요가 있다고 생각되는 내용을 정리해보려고 한다. 현재 데이터 파이프라인 구축관련 공부를 하면서 AWS 서비스를 활용하여 실습을 하고 있는데, Storage와 Database 부분이 중요하다고 생각되기 때문에 우선적으로 이 부분에 대한 정리를 시작으로 다른 중요한 AWS 서비스들도 정리를 해나가보려고 한다. Amazon EBS(Amazon Elastic Block Storage) 데이터 파이프라인을 실습하면서 복수 개의 EC2 인스턴스를 생성하고, 계속 구동시켜놓으면 비용이 발생하기 때문에 사용하지 않을때에는 잠시 중지상태로 전환시켜두었다. 그런데 다시 구동시켰더니 내부에 저장해두었던 리소스들이 사라져있었다.그 이유는 바로 EC2 인스턴스 가상 서버로, 내부 리소스는 Instance store volume이라는 블록 저장 공간에 저장을 하게 되는데, 중지 후 다시 재구동을 하게 되면 가상 서버는 베이스가 되는 부분이 재 생성되면서 기존에 사용되었던 instance store가 삭제되게 된다. (EC2의 임시 블록 수준 스토리지)이러한 이유로 EC2 인스턴스를 중지후 재구동을 하여도 지속적으로 남기게 하기 위해, Amaozn EBS라는 블록 스토리지 솔루션이 등장했다.Amazon EBS는 EC2 인스턴스를 중지한 뒤에 다시 실행하여도 내부에서 저장하였던 리소스를 보존시켜준다.EC2 인스턴스에 EBS를 연결하려면 동일한 가용 영역(AZ)상에 있어야 한다. 저장은 하지만, 2GB EBS 볼륨을 프로비저닝해서 가득 채우게 되면, 볼륨이 자동으로 확장되지 않는다. Snapshot을 지원해는데, 나중에 Snapshot에서 data를 복구할 수도 있다.(최대 16TiB, SSD, HDD 모두 제공)cf.SI 접두어 방식으로 하면, TB(테라바이트)인데, 이를 이진 접두어 방식으로 하면, TiB(테비바이트)가 된다. Amazon S3(Amazon Simple Storage Solution) 이전에 데이터 파이프라인 실습하면서 간단하게 정리했지만, Bucket이라는 디렉토리 개념의 공간에 객체라는 파일 개념의 형태로 데이터를 저장하는 곳을 Amazon S3라고 했다. 최대 업로드 객체 크기는 5TB이며, 버전관리도 지원하기 때문에 실수로 삭제되는 일도 사전에 예방할 수 있다. 그리고 접근 권한 설정도 할 수 있어서 객체에 액세스할 수 있는 사람을 제한하는 설정도 별도로 할 수 있다.S3의 종류에는 S3 Standard가 있으며, 높은 내구성을 제공한다. 그리고 HTML 및 Assets을 업로드하면 정적 웹 사이트를 호스팅 할 수 있다.그 외에도 액세스 빈도가 낮지만 필요에 따라 빠르게 액세스를 해야되는 데이터의 경우에 사용되는 S3-IA(Infrequent Access)가 있다. 백업, 재해 복구 파일 또는 장기 보관이 필요한 모든 객체에 적합하다.(무한대의 스토리지 제공, 개별 객체크기 5,000GB, WORM에 특화) Amazon S3 Gracier 이전에 데이터 파이프라인 실습을 하면서 이 Gracier라는 서비스에 대해서 언급이 되었었는데, 데이터베이스의 데이터를 Archieving하는데 사용된된다고 배웠다.이 Amazon S3 Gracier는 감사 데이터와 같이 수년간 데이터를 저장하고, 그리 자주 검색이 되지도 않는 경우, 이 Amazon S3 Gracier를 사용해서 데이터를 보관하게 된다.Gracier를 사용하는 방법은 데이터를 Gracier로 옮기거나 저장소를 만든 다음에 Archive로 채우면 된다.유지 기간 같은 데이터 유지 관련 규정 준수 요구 사항이 있다면, S3 Gracier 저장소 잠금 정책을 적용하여 저장소를 잠글 수도 있으며, WORM(Write One, Read Many) 제어정책을 지정해서 잠글 수도 있다.(한 번 잠긴 정책은 변경 불가) 또한 S3의 수명 주기 정책을 사용하여 데이터를 여러 계층으로 자동으로 이동하게 할 수 있게 할 수 있다.ex.객체를 S3 Standard에서 90일 동안 유지 -&gt; S3-IA에서 30일 동안 유지 -&gt; (120일 후) -&gt; 객체를 S3 Glacier로 이전 Object storage(S3) VS Block storage(EBS)Object storage의 경우에는 업로드되는 모든 파일들을 개별 객체로 취급하기 때문에 파일을 수정한 다음에 업로드하게 되면, 수정된 객체 파일 전체가 다시 업로드된다.반면에 Block stoarge의 경우에는 일부 파일이 수정이 되면, 수정된 파일의 수정된 일부만 다시 업로드된다.이처럼 완성된 객체를 사용하거나 변경 횟수가 적다면 S3를 사용하는 것이 좋으며, 복잡한 읽기, 쓰기, 변경 기능을 수행한다면 EBS가 적합하다.상황에 따라 적합한 서비스를 선택할 수 있어야한다. Amazon EFS(Elastic File System) Amazon EFS는 관리형 파일 시스템으로, 여러 인스턴스 동시 읽기 및 쓰기가 가능하며, EBS와 같이 쓰기 작업을 할 수 있는 빈 하드 드라이브 개념이 아니다. Linux를 위한 진정한 파일 시스템으로, Region resource이다. Region의 모든 EC2 인스턴스를 EFS 파일 시스템에 쓸 수 있다는 의미이다. EFS는 EBS와 달리 추가 데이터를 쓰게 되면, 자동으로 공간이 확장되며, 별도로 추가 볼륨을 프로비저닝할 필요가 없다. Block storage(EBS) VS File storage(EFS)파일 스토리지에서는 여러 클라이언트가 공유 파일 폴더에 저장된 데이터에 엑세스할 수 있다.이 접근 방식은 Storage server가 Block storage를 Local file system과 함께 사용하여 파일을 구성한다.클라이언트는 파일 경로를 통해 데이터에 접근한다. Block storage 및 Object storage와 비교하면, File storage는 많은 수의 서비스 및 리소스가 동시에 동일한 데이터에 접근해야 되는 경우에 이상적이다.AWS 클라우드 서비스 + On-Premise 리소스를 함께 사용되는 확장 가능한 파일 시스템으로, 어플리케이션을 중단하지 않고, On-demand로 페타바이트 규모로 확장할 수 있다.","link":"/2022/04/15/202204/220415-aws-study/"},{"title":"220414 Hadoop과 친해지기 여덟 번째 이야기(MAPREDUCE 실습)","text":"이번 포스팅에서는 Hadoop의 생태계에서 핵심이 되는 MapReduce의 세부동작에 대해서 정리해보려고 한다. MapReduce에 동작에 대해서 이전 포스팅에서 정리를 했듯이 Mapper와 Reducer가 하는 일은 그렇게 복잡해보이지 않는다. 다만 Hadoop의 Cluster내에서 MapReducer가 동작하는 방식이 복잡하기 때문에 이 부분에 대해서 정리가 필요하다. 이전 포스팅에서 다뤘던 내용은 입력받은 데이터를 Mapper가 Key-Value 쌍으로 데이터를 Transformation해주고, 그 결과를 MapReduce가 자체적으로 셔플과 정렬(Shuffling and Sorting)해준다.이후에 Reducer는 구조화된 정보를 전달받아서 최종 출력물을 생산해내는 역할을 한다. 만약에 정말로 큰 데이터 세트를 가진 클러스터를 운영한다면, 아마 처리 과정을 여러 컴퓨터에 배분하거나 각 노드에서 여러 작업에 걸쳐 진행을 할 것이다.MapReduce 작업을 세 개의 노드에 나눠서 Mapping작업을 한다고 가정한다면, 데이터에서 몇 줄은 첫 번째 노드에 보내서 처리하고, 나머지 데이터는 나머지 노드에 배분해서 처리할 것이다.입력 데이터를 여러 파티션에 끼워 맞추고, 각 파티션에 작업 할당을 하게 되는 것이다.바로 여기서 다른 파티션에 있는 데이터는 신경쓰지 않아도 되기 때문에 작업의 병렬화가 가능해진다.최종적으로 각 각 작업 배분한 컴퓨터에서 작업이 끝나면 Hadoop은 정보를 잘 받아오고 마무리가 된다. [MapReduce procedure 반복 숙지][STEP1 - Mapping]클러스터의 각 노드로 대량의 데이터를 작은 블록으로 배분함으로써 key:value 쌍으로 전환하는 Mapping 처리과정을 배분할 수 있다. [STEP2 - Shuffling and Sorting]그 다음으로는 Shuffle &amp; Sort작업인데, 앞서 key:value 쌍으로 Mapping하는 과정에서 복수 개의 key를 갖는 데이터가 생기기 때문에 이 과정을 통해서 같은 키 값끼리 모아서 Reducer로 보낼 데이터로 가공하게 된다. 이 과정은 MapReduce가 대신 수행을 하게 된다. 단순히 네트워크상에서 데이터를 주고 받는 형태가 아닌, 모든 정보를 merge sort하는 형태로 처리한다. [STEP3 - Reducing]최종적으로 Reducer 단계에서는 데이터를 Key 값을 기준으로 정렬된 세트를 Reducing하게 된다. 다시 정리하자면, Mapping단계에서는 입력된 데이터를 작은 block으로 쪼개서 클러스터 내의 여러 노드로 분산시키고, 셔플과 정렬 작업 이후에는 각 노드가 각 각의 세트를 담당하여 reducing하게 된다. 클러스터 내의 각 노드에서 reducing 작업까지 완료된 데이터들은 Hadoop에 의해 최종적으로 수집되어 마무리된다. MapReduce의 세부 동작 MapReduce 작업의 내면을 자세하게 살펴보자. [MapReduce Sequence](1) MapReduce 작업은 Client Node(Terminal prompt)가 작업을 지시한다. (2) Client Node는 먼저 YARN Resource Manager와 이야기를 한다. 이전에 하둡과 친해지기 관련 포스팅(세 번째 이야기)에서 언급했듯이, YARN(Yet Another resource Negotiator)은 또 다른 리소스 교섭자의 의미로 데이터의 처리 부분을 담당하며, HDFS는 데이터의 저장부분을 담당했다. YARN은 HDFS에 저장된 데이터를 처리하기 위한 일종의 데이터 관리자의 역할을 해준다. YARN은 어떤 노드가 추가 작업을 할 수 있는지 없는지 그리고 클러스터를 작동하는 심장박동과 같은 역할을 한다.그 외에도 어떤 노드의 성능은 얼마인지 등의 정보를 기억하고 있기 때문에 Client Node는 YARN Resource Manager와 먼저 상담을 하는 것이다.그 결과, Client Node는 YARN Resource Manager에게 이러이러한 MapReduce 작업이 필요하다고 전달을 하게 된다. (3) 그러는 동시에 필요한 데이터를 HDFS에 복사를 한다. 이 작업들을 수행하는 노드들이 데이터에 접근할 수 있도록 데이터를 복사하는 것이다. (4) MapReduce Application Master가 작동하기 시작하는데, NodeManager 하위에 있다. 기본적으로 MapReduce가 작동하는 모든 것은 NodeManager가 관리한다고 볼 수 있다. *NodeManager는 어떤 노드가 무엇을 하고 사용가능한지, 그리고 작업중인지에 대한 정보를 관리한다.Application Master는 개별 Mapping과 Reducing 작업을 주시하고, YARN과 협업해서 작업을 클러스터에 걸쳐 배분을 한다.상단의 구조도와 같이 YARN과 Application Master를 제외하고 두 개의 노드가 있다고 하자. 둘 다 Mapping 작업을 하고 있고, 서로 다른 컨테이너이지만 JVM(Java Virtual Machine)에서 실행이 되고 있다. 하지만, 같은 노드 관리자를 두고 있기 때문에 Application Master와 서로 소통을 하면서 어떤 노드가 어디서 무엇을 하는지 추적할 수 있습니다. (5) 이러한 Mapping과 Reducing 작업이 진행되는 동안에 HDFS 클러스터와 소통을 하면서 필요한 데이터를 받고, 작업이 다 끝나면 결과 데이터를 HDFS 클러스터로 출력을 하게 됩니다. [중요]여기서 중요한 내용은 YARN이 Mapping이나 Reducing작업을 최대한 데이터와 가까운 곳에서 실행을 한다는 것이다. 다시말해, YARN은 입력 데이터에 대한 Mapping작업을 웬만하면 그 데이터 블록을 가지고 있는 노드가 수행하도록 조율을 한다는 것이다. 만약 그것이 불가능하다면, 네트워크상에서 최대한 가까이 두도록 한다. 이 말은 불필요한 네트워크 전송을 최대한 줄이려고 한다는 것이다. MapReduce의 생김새MapReduce는 원래 Java로 작성이 되어있고, Hadoop도 Java로 작성이 되어있다.만약 MapReduce 어플리케이션을 직접 작성하고 싶다면, Java를 사용해서 필요한 Mapper와 Reducer가 있는 jar 파일을 만들어야한다.Streaming은 Pyton과 같은 간단한 언어로 MapReduce를 사용할 수 있게 하기 때문에 Python을 사용해서 MapReduce를 작성할 수 있다. MapReduce의 Streaming을 사용하면, 표준 입/출력을 사용해서 Mapping작업을 시작할 수 있다. 진행중인 프로세스와 소통하며 Mapper와 Reducer를 실행할 수 있다.Java에 Mapping, Reducing function을 작성하는 대신에 Python이나 컴퓨터 클러스터 노드에서 작동하는 프로세스에 대신 작성하고 표준 입출력을 사용해서 소통을 하는 것이다. (표준 입/출력을 통해 데이터 받기 -&gt; Mapper 실행 -&gt; 표준 출력으로 key-value pair 받기) MapReduce와 Hadoop의 Single Point FailureHadoop은 범용 PC로 구성된 거대한 클러스터로, 단점으로 범용 하드웨어들이 종종 다운될 수 있다는 점이다. 작업을 관리하는어플리케이션 마스터가 특정 작업물에서 오류를 발견하면 간단히 그 작업을 재 시작할 수 있다. 이처럼 어플리케이션 마스터는 리소스 관리자에 의해서 작동을 한다.따라서 어플리케이션 마스터가 다운되면 YARN이 어플리케이션 마스터를 재시작한다.하지만, 리소스 관리자가 다운이 되면 어떻게 될까? 고가용성 MapReduce라는 것이 있는데, 이것을 이용해서 이전에 다뤘던 Zookeper를 사용해서 동적얘비 리소스 관리자를 유지하는 것이다. 이렇게 되면, MapReduce는 먼저 ZooKeeper와 이야기를 해서 어떤 리소스 관리자를 생성할지 결정하게 되고, 리소스 관리자 사용을 결정 및 다운이 되었다면 Zookeeper는 자동으로 두 번째 백업 리소스 관리자를 가릔다. 나중에 개인적으로 더 찾아 볼 하둡의 개념들(1) 카운터(counter) : 클러스터 전반에 걸쳐 공유된 총 수 를 유지하는 기능 (2) 컴바이너(combiner) : Mapper Node를 줄여서 최적화해 간접비를 줄여준다. 최근 MapReduce의 입지요즘에는 MapReduce를 예전만큼 많이 사용하지 않고 있다. 클러스터에 SQL Query를 허용하는 Hive나 Spark같은 고수준 도구로 대부분 대체가 되었기 때문이다. MapReduce는 Hadoop의 필수 요소로써 이해해야 할 중요한 가치를 가지고 있지만, MapReduce를 직접 사용하는 것보다는 MapReduce가 어떻게 작동하는지 실제로 코딩을 해보면서 이해해보는 것이 중요하다. [실습]영화 평점에 따른 영화 편수 분류주어진 영화정보 데이터에는 userID, movieID, rating, timestamp 정보가 있다.이번 실습에서는 각 영화의 평점별 영화의 분포를 분류해보고자 한다. [Mapper]1234def mapper_get_rating(self, _, line): (userID, movieID, rating, timestamp) = line.split('\\t') # mapper -&gt; MapReduce F/W로 돌려보내는 것을 의미한다. (yield) yield rating, 1 [Reducer]123# Shuffle &amp; Sort 과정은 MapReduce에서 별도의 처리를 해주는 부분이기 때문에 바로 Reduce에 대한 작성을 한다.def reducer_count_ratings(self, key, values): yield key, sum(values) [Python MapReduce Code]mrjob은 Python package로, MapReduce를 작성하기 위해 사용된다. Streaming Interface를 다루는 복잡함을 희석시켜주는 역할을 해준다고 이해하면 된다. 여기서 Streaming Interface에서 Streams란 file과 같은 객체라고 이해하면 된다. I/O module에서 정의하고 있는 툴을 사용해서 객체를 읽고 쓸 수 있으며, 모듈들은 interface를 제공해주기 때문에 만약 정의하고 싶은 stream 객체가 있다면 제공해준 interface를 implement해주면 된다. 아래 코드에서 mrjob을 사용한 이유는 앞서 정의한 stream object를 정의해서 사용하기 위함이다. mrjob.job의 MRJob을 상속받아서 steps 메서드를 재정의(method overriding)하고 있는데, 이 메서드에서 maper와 reducer역할을 하고 있는 method에 대해서 정의를 해주고 있다. 다음에 자바로 작성한 MapReduce 코드도 같이 비교해서 살펴보자. 1234567891011121314151617181920from mrjob.job import MRJobfrom mrjob.step import MRStepclass RatingsBreakdown(MRJob): def steps(self): return [ MRStep(mapper=self.mapper_get_rating, reducer=self.reducer_count_ratings) ] def mapper_get_rating(self, _, line): (userID, movieID, rating, timestamp) = line.split('\\t') yield rating, 1 def reducer_count_ratings(self, key, values): yield key, sum(values)if __name__ == '__main__': RatingsBreakdown.run()","link":"/2022/04/14/202204/220414-hadoop_bigdata_class/"},{"title":"220416 데이터 파이프라인 스터디 7일차","text":"이번 포스팅에서는 앞으로 실습할때 사용하게 될 AWS인 API Gateway,Kinesis Stream, Kinesis Firehose에 대해서 공부한 내용을 정리해보겠다. 두 번째 실습내용이번 실습에서는 AWS 서비스인 Object Storage 서비스 S3를 이해하고, 대규모 데이터 스트림을 실시간으로 수집하고 처리하는 Kinesis Stream을 사용하고 이해한다.더 나아가 이전 포스팅에서 정리한 것처럼, 1주차에 배운 데이터 온프레미스 수집방법과 클라우드상에서 데이터를 수집하는 방법의 차이를 이해한다. (1) 실습내용 : AWS 패키지들을 이용한 데이터 수집 실습 (2) 실습 구성 : Api-Gateway, Kinesis Stream, Firehose, S3의 이해(Simple한 데이터 수집방법 / 구체적으로 왜 이렇게 파이프라인을 구성했는지에 대해 이해한다) (3) 실습환경 설정 : Kinesis Stream, Firehose, S3 설정하기 (4) 구성한 실습환경에서 데이터 수집 : AWS 서비스에서 데이터 수집하기 개인 실습에서는 웹/앱에서 데이터를 발생시킬 수 없기 때문에 간단하게 EC2에서 웹 접속하는 것과 똑같은 환경을 구성한다. (CLI에서 curl명령을 통해서 데이터를 생성하고, 테스트용으로 만든 데이터를 S3에 저장해서 실습한다) API Gateway AWS에서 앱이나 웹을 개발할 경우에는 외부의 이벤트들(모바일/서비스/웹)을 AWS 서비스로 받아들이는 관문으로 이해하면 된다. 이벤트 같은 경우에는 백엔드에서 처리해도 되지만, 앱에서 발생하는 이벤트 중에서 백엔드의 자체 프로세스에서 처리되지 않는 이벤트(화면상에서 전환되는 이벤트)같은 이벤트도 처리하기 위해서 API Gateway를 사용해서 이벤트를 AWS 클라우드 안으로 받아들이도록 처리하기도 한다. 관련 서비스 050, 애드브릭스 등과 같이 최근 클라우드상에서 Parse 형태로 서비스되고 있는 서비스들이 있는데, 이벤트가 발생하면 바로 회사로 이벤트를 넘겨주는 데이터들도 있다.외부에서 발생하는 이런 데이터들을 내제화할때 사용을 꼭 해야되는 시스템으로 API Gateway가 사용된다. 모바일 및 웹 어플리케이션에서 AWS 서비스에 액세스할 수 있는 RESTFul API를 제공을 하며, 사용자는 RESTFul API를 생성, 구성, 호스팅하여, 어플리케이션의 AWS 클라우드 액세스를 지원한다. Kinesis Stream Kinesis Stream은 대규모 데이터 레코드 스트림을 실시간으로 수집하고 처리할 수 있다. 이벤트가 일정하지 않고 튀는 경우가 있어서 에러가 발생하는 경우가 있어서, 이벤트성으로 로그 데이터나 서비스가 폭주하는 경우에는 Kinesis를 사용하는 것이 좋다. 하나의 통로에 여러 병렬처리를 할 수 있도록 제공하는 것이 Shard이다. Kinesis Stream은 Shard를 늘릴때 숫자만 바꿔주면 병렬처리될 수있게 늘려줄 수 있고, 거기에 쌓인 데이터를 Kinesis Firehose를 통해서 위에 있는 Amazon S3, DynamoDB, Amazon Redshift, Amazon EMR 서비스로 보낼 수 있는 장점이 있다. 실무에서는 하루에 2500만건 발생을 하고, 이벤트는 하루에 한건 이상 발생을 하고, 이벤트 발생 포함해서 2500만건 데이터를 처리할때, 2개의 Shard가 사용이 되었다.비용은 200불정도 소모되었다. (kinesis + Firehose 포함 비용) Kinesis FirehoseQueue에 있는 데이터를 S3에 쉽게 저장할 수 있도록 도와주는 AWS 서비스이다. Kinesis Firehose는 자체적으로 큐(Queue)의 기능을 가지고 있지만, 큐(Queue)라기보다는 버퍼(Buffer)이다. 버퍼사이즈가 그렇게 크지 않기 때문에 대규모 서비스에 사용하기에는 무리가 있다. 따라서 앞단에 kinesis stream을 두고 뒷 단에 Kinesis firehose를 사용한다.이벤트가 크지 않은 경우에는 특정 버퍼링도 가능하기 때문에 firehose만으로도 가능하다. Firehose가 나오기 전에는 Lambda를 통해서 S3에 저장을 하거나, DynamoDB나 MySQL DB에 저장을 하는 서비스를 운영하였다.Lambda의 경우에는 정상처리 코드(200)가 떨어지지 않으면 4번까지 re-try를 하기 때문에 데이터 중복이 발생할 수 있기 때문에 데이터 중복에 대한 가정을 하고 파이프라인을 운영하는 것이 중요하다.(클렌징 작업 필요(주의)) Amazon S3 대상인 경우에는 스트리밍 데이터가 S3 bucket으로 전송되고, 데이터 변환이 활성화되면 선택적으로 소스 데이터를 다른 S3 bucket으로 백업할 수 있다. (format이 JSON 형태일때에는 Redshift나 Elasticsearch에 저장이 가능하다) Amazon Redshift 대상인 경우에는 스트리밍 데이터가 먼저 S3 bucket으로 전송이되고, 그 다음에 Kinesis Data Firehose가 Amazon Redshift 발행 Copy 명령을 사용하여 S3 bucket의 Amazon Redshift 클러스터로 데이터를 로드한다. 데이터 변화이 활성화되면, 선택적으로 소스 데이터를 다른 Amazon S3 bucket으로 백업할 수 있다. Elasticsearch cluster 대상인 경우, 스티리밍 데이터가 Elasticsearch cluster로 전송이 되며, 동시에 선택적으로 S3 버킷에 백업을 할 수 있다. Splunk 대상인 경우, 스트리밍 데이터가 Splunk cluster로 전송이 되며, 동시에 선택적으로 S3 버킷에 백업할 수 있다. (2022/05/10 업데이트) 참고)splunk란 모든 소스의 머신 데이터(machine data)를 실시간으로 수집하고 분석하는 가장 뛰어난 운영 인텔리전스 플랫폼이다.수집하는 데이터의 비즈니스 유형에는 제약을 두고 있지 않으며, 모든 장비/모든 서버/모든 장치에서의 정형/비정형 데이터를 수집할 수 있다. 수집된 데이터를 규칙에 따라 분석하고, 분석된 자료를 기반으로 시각화하여 사용자에게 객관적인 지표를 제공할 수 있다.데이터 수집부터 보고까지 전 과정을 제공하며, 수십 TB까지 다양한 규모의 데이터 처리가 가능하다. splunk는 빅데이터를 통해 인사이트를 얻을 수 있도록 도와준다. 모니터링 툴이나 third part tool과의 연동도 많이 지원을 한다. Third part tool과 연동을 할때에도 백업은 S3에 한다. [출처] : https://docs.aws.amazon.com/ko_kr/firehose/latest/dev/what-is-this-service.html 저장 Architecture 저장 용량이 무제한 (S3) 요청에 의한 데이터 파이프라인을 쉽게 구성하고 데이터 수집 및 저장이 가능 요청이 많아져도 Kinesis Stream의 Shard 조정만으로 빠른 scalability 처리 데이터 유실에 대한 kinesis Stream에서 기본적으로 24시간의 데이터보존기능 S3에 반 정형화된 JSON 형식의 데이터로 저장하므로, 가변적 값의 데이터 수집(Query Parameter 값이 변화에 대해서)에 대한 유연한 분석이 가능(이벤트가 발생하는 데이터를 저장할때 저장하는 포멧을 JSON으로 저장하자. 데이터 분석가와 같이 일을 하면, 앱에서 발생하는 이벤트 중에서 특정 이벤트에 데이터를 같이 넘겨줬으면 하는 요구사항이 있는 경우가 있다.(혹은 삭제) JSON 형태로 저장을 한다면, 손쉽게 데이터 조작이 가능하다. (과거에는 테이블 형태로 관리되어 데이터 조작이 어려웠다) Serverless의 운영비용 감소","link":"/2022/04/16/202204/220415-data-pipeline-study/"},{"title":"220416 AWS 서비스-2","text":"Amazon Redshift Amazon Redshift는 빅 데이터 분석에 사용할 수 있는 데이터 웨어하우징 서비스이다. 이 서비스는 여러 원본에서 데이터를 수집하여, 데이터 간의 관계 및 추세를 파악하는데 도움이 되는 기능을 제공한다. 때로는 비즈니스 요구 사항은 현재 진행중인 일이 아닌, 과거에 일어난 일과 연결되기도 한다. 물론 모든 곳에 단일 데이터베이스를 사용하는 만능 모델을 사용할 수도 있지만, 빠른 속도와 실시간 수집 및 쿼리를 고려해 설계된 현대적인 데이터베이스가 적절하지 않은 경우도 있다.기록/분석의 문제점은 쿼리를 요청한 시점에 데이터가 수집을 멈추지 않고 지속한다는 것이다. 그리고 최신 원격 분석과 IoT의 폭발적 증가 때문에 데이터는 결국 최고 용량의 기존 관계형 데이터베이스도 감당할 수 없는 양이 될 것이다.더 나아가 데이터의 다양성도 문제가 될 수 있는데, 재고,금융 및 소매 영업 시스템과 같은 다양한 데이터 장소에서 오는 데이터를 상대로 BI(Business Intelligence) 프로젝트를 실행하는 경우, 여러 데이터 베이스에 단일 쿼리를 사용하는 것은 기존 데이터베이스에서는 쉽게 처리하지 못한다. 데이터가 너무 복잡해서 기존 관계형 데이터베이스로 처리하기 어려워지면, DW로 처리하게 되는데, DW는 이러한 유형의 빅데이터용으로 특별히 제작되었고, 사용자는 운영 분석이 아닌 기록 분석을 살펴보게 된다. 기록이란, “지난 1시간동안 모든 점포에서 기록한 매출정보를 출력”과 같이 지난 1시간은 이제 과거이므로 현재 판매하는 건은 포함되지 않는다는 것을 의미한다.“지금 모든 점포에서의 매출은 어떻게 되지?”라는 질문과 비교해보면, 말하는 이 순간에도 결과는 바뀔 수 있으며, 비즈니스 질문이 과거를 향하다면 해당 BI에는 DW가 올바른 솔루션이 될 수 있다. DW팀이 엔진관리 대신에 데이터에만 집중할 수 있도록 나온 서비스가 바로 Amazon Redshift이다.서비스로서의 데이터 웨어하우징 제품이다. 확장성이 매우 뛰어나며, 수 Petabyte 크기의 Redshift 노드도 흔하다. 실제로 Redshift Spectrum을 통해 데이터 레이크에서 실행되는 수 Exabyte 바이트의 비정형 데이터를 대상으로 단일 SQL 쿼리를 실행할 수 있다.여기서 핵심읁 빅데이터 BI 솔루션이 필요할 때 Redshift를 이용하면 단일 API 호출로 작업을 시작할 수 있다는 것이다.","link":"/2022/04/16/202204/220416-aws-study/"},{"title":"220415 Hadoop과 친해지기 아홉 번째 이야기(HDP2.6.5에서 Python으로 작성한 MapReduce 코드 실행하기 (Local&#x2F;Hadoop))","text":"이번 포스팅에서는 이전에 작성했던 Python으로 작성한 MapReduce 코드를 HDP 2.6.5 환경에서 구동시켜볼 것이다. Hadoop을 사용하지 않고, local에 copy된 dataset을 가지고 로컬에서 실행해보고, Hadoop을 기반으로 실행해보는 두 가지 방법으로 실습을 해볼 것이다. HDP2.6.5에서 MapReduce 실습환경 구축[STEP1] 가장 먼저 VirtualBox에 올린 HDP2.6.5 OS Image를 구동시킨다.[STEP2] Putty를 사용해서 가상 OS환경에 접속한다.[Putty host/port configuration] 12host: maria_dev@127.0.0.1port: 2222 [STEP3] PIP 설치를 위한 SETUP12345678910111213# root 계정으로 switch$su root # root account initial password : hadoop$yum-config-manager --save --setopt=HDP-SOLR-2.6-100.skip_if_unavailable=true # HDP Solar라는 저장소를 무시하고 설치하기 위한 configuration$yum install https://repo.ius.io/ius-release-el7.rpm https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # IUS package를 수동으로 설치$yum install python-pip # python-pip 설치$pip install pathlib # pathlib 설치 *설치를 안하면 MRJob이 정상적으로 설치가 안됨$pip install mrjob==0.7.4 #설치한 pip를 이용해서 MRJob 설치 [STEP4] 실습에 필요한 DATASET 및 Python source code 준비12345678# nano editor 설치 (or vi editor 사용해도 됨)$yum install nano# 실습에 필요한 MovieLense dataset file 설치$wget http://media.sundog-soft.com/hadoop/ml-100k/u.data# Python MapReduce source code download$wget http://media.sundog-soft.com/hadoop/RatingsBreakdown.py [STEP5] MRJob프레임워크를 사용해서 MapReduce 실행하기MRJob 프레임워크를 사용해서 MapReduce를 실행하는 방법에는 총 두 가지가 있다.첫 번째는 Hadoop을 전혀 사용하지 않고 로컬에서만 실행하는 방법이다. MRJob 프레임워크를 사용해서 MapReduce를 시뮬레이션한다. 개인 PC에서 여러가지를 테스트하기에 유용한 방법이다.Hadoop 클러스터에서 개발하는 대신에 텍스트 에디터(nano, vi 등)를 사용해서 데스크톱에서 개발을 할 수 있다. 아래는 로컬 환경에서 MRJob 프레임워크를 사용해서 MapReduce를 실행하는 방법이다. 12# 스크림트 파일과 입력 데이터 파일을 입력$python RatingsBreakdown.py u.data (개인 PC에서 실행하는 경우에는 방대한 데이터 세트 전체를 사용하지 않고, 때때로 테스팅이나 개발 목적으로 데이터의 부분집합을 만드는 것도 좋은 방법이다.) 두 번째는 Hadoop에서 MapReduce를 실행하는 방법이다. 12345678910$python MostPopularMovie.py -r hadoop --hadoop-streaming-jar/usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jaru.data#(1) -r hadoop : 'MRJob' 프레임워크에게 이 스크립트를 실제 Hadoop 클러스터에서 실행할 것이라고 알려준다. (Resource Manager와 Application Master를 사용해서 작업을 실제로 분배한다)#(2) '--hadoop-streaming-jar'은 Hortonworks의 특유 문구인데, 'MRJob'은 Hadoop Sandbox의 Hadoop Streaming 위치를 모르기 때문에 수동으로 어디에 있는지 알려주는 역할을 한다. Amazon EC2나 Elastic MapReduce 서비스 플랫폼에는 이러한 별도의 옵션이 필요 없다. 그 이유는 Hadoop streaming을 어디서 찾아야되는지 알고 있기 때문이다.#(3) 입력 데이터의 마지막 매개 변수가 있는데, 이것은 HDFS에 자동으로 복사될 로컬 파일이 될 수 있고, 대부분의 실제 상황에서는 클라이언트 노드에는 존제하지 않는 거대한 대용량 데이터를 다루기 때문에 'hdfs://' 형식으로 HDFS 파일 시스템 경로를 지정해준다. 위의 commandline에서는 local file을 HDFS에 복사를 하고, 복사한 파일 데이터를 여러 노드에 걸쳐서 분배가 된다. 하지만 현재 실습하고 있는 dataset의 경우에는 아주 작은 작업이고, Virtual Machine도 한 개만 존재하기 때문에 HDFS에 복사해서 다른 여러 노드들로 분배하는 과정을 진행하지 않을 것이다.이를 Uber Job이라고 불리는데 모든 걸 하나의 인스턴스에서 작동하는 작업을 말하는 용어이다. 데이터가 작기 때문에 가능한 이야기이다. [Uber job]Uber job이란 Mapper나 Reducer container를 생성하기 위해 RM(Resource Manager)와 서로 소통하기 보다는 MapReduce ApplicationMaster내에서 작업들이 실행되는 것을 말한다.앞에서 말했듯이 처리되는 데이터가 작은 경우에는 별도로 Map/Reduce 처리를 위한 container를 생성해서 처리할 필요가 없기 때문에 AM(ApplicationMaster) 자체 내에서 Map과 Reduce 작업을 전반적으로 처리한다. 이러한 것을 Uber job이라고 한다.","link":"/2022/04/15/202204/220415-hadoop_bigdata_class/"},{"title":"220418 데이터 파이프라인 스터디 8일차 (Kinesis Stream &#x2F; 구축한 수집 파이프라인에서 테스트)","text":"이번 포스팅에서는 Kinesis Stream + Kinesis Firehose + S3 전반적인 서비스 구성하는 방법에 대해서 정리를 해보고, 구성한 수집 파이프라인에서 sample data를 발생시켜서 수집 파이프라인에서 테스트해보는 과정까지 정리해보려고 한다. Kinesis Stream 시스템 구성API Gateway에서 어느 스트림으로 내보낼지 설정했던 Stream이름으로 Kinesis Stream 생성시에 이름을 정의한다. API Gateway에 비해 Kinesis Stream은 매우 간단하게 구축을 할 수 있다. Kinesis Firehose 시스템 구성Firehose는 Delivery streams 메뉴에 정의가 되어있다. Delivery stream 메뉴에서 [Create delivery stream]을 클릭한 뒤에 아래의 하위 선택 목록들을 설정해준다. [Choose source and destination] Source: Amazon Kinesis Data Streams Destination: Amazon S3 [Source settings] - Kinesis data stream Browse - 앞서 생성한 Kinesis data stream Name을 선택 [Delivery stream name] 관리하기 용이하게 앞의 Kinesis data stream의 이름과 동일하게 재정의해준다. [Transform and convert records] - optional 들어온 데이터를 Lambda에서 처리할 수 있도록 설정할 수 있다. (별도의 Buffer size 도달시 Lambda가 실행되도록 설정) [Record format conversion] AWS Glue에 데이터 카탈로그에 데이터를 생성한 다음에 JSON방식으로 데이터를 삽입할 수 있는 옵션이다.JSON에 Key값을 기준으로 각 Column에 데이터를 넣어준다. Apache Parquet, ORC는 빅데이터용 압축형식의 데이터이다. 칼럼형태의 포멧을 가지고 있으며, 조회할때 읽기 용이하게 되어있다. Apache Parquet : 라이브러리를 안쓰고도 쓸 수 있게 상용화되어있다. ORC : 속도면에서 더 좋다 [Destination settings]생성했던 S3 bucket을 선택해준다. [S3 bucket prefix] - S3 bucket에서 저장될 구체적인 위치를 지정해준다.rawdata/ 이벤트가 발생하면 하위 path에 YYYY/MM/dd/HH filename 포멧으로 Firehose 통해서 저장이 된다. (UTC 시간 기준이기 때문에 일 단위의 데이터를 분석할 때 2일 단위로 데이터를 쌓아서 분석한다) [Advanced settings] Permission 설정 부분으로, IAM 자체를 구성하기 힘들기 때문에, 자동으로 IAM을 생성해줄 수 있다.(Firehose S3에 데이터를 쓰는 과정(서로 다른 서비스 간에 데이터를 쓰기 위해서)에서는 IAM 권한을 부여해줘야 Firehose에서 S3에 데이터를 쓸 수 있기 때문이다. 하지만, 회사에서는 반드시 IAM을 별도로 생성해서 자동으로 생성하지 않도록 해야한다) [Tags] 이름을 구분하거나 리소스에 대한 사용을 파악하기 위해 각 서비스에 대해 Tag를 달아서 사용하는 것이 좋다. (파이프라인에서 발생하는 금액에 대한 파악에도 용이) EC2 - API Gateway - Kinesis Stream - Kinesis Firehose - S3 수집 파이프라인에서 테스트재기동하면 Public IP가 변경되기 때문에, 왼쪽 메뉴에서 Elastic IP (사용 안할때만 비용이 발생)를 사용하면 고정 IP를 사용할 수 있다. 12345# json 형태의 데이터를 POST 방식으로 API Gateway END POINT로 쏴주는 명령어curl -d &quot;{\\&quot;value\\&quot;:\\&quot;30\\&quot;,\\&quot;type\\&quot;:\\&quot;Tip 3\\&quot;}&quot; -H &quot;Content-Type: application/json&quot; -X POST https://xxxxxxxxxxx.execute-api.ap-northeast-2.amazonaws.com/PROD/v1# 정상 발송시 (output){&quot;SequenceNumber&quot;:&quot;49628695723364114399951164086273603527143171538215239730&quot;,&quot;ShardId&quot;:&quot;shardId-000000000003&quot;} [데이터가 정상적으로 발송되었는지 CloudWatch에서 확인]CloudWatch - [로그] - [로그 그룹] - API-Gateway-Execution-Logs 아래와같이 200코드를 받았다면, 정상적으로 API Gateway로 정상적으로 데이터가 넘어갔다는 의미이다. [Kinesis Stream에서 메시지 데이터가 전달되었는지 확인]데이터 스트림 선택해서 API Gateway로부터 넘겨받은 메시지 데이터 확인 [Kinesis Hose에서 메시지 데이터 전달되었는지 확인]Kinesis hose에서는 총 1개의 데이터 스트림을 받고, S3 object storage에는 2개의 데이터가 전달되었다는 것을 알 수 있다. [S3(Destination)에 데이터가 제대로 전달이 되었는지 확인]Kinesis stream에서 전송 스트림(kinesis hose)메뉴에 보면, Destination source로 이동할 수 있는 링크가 있는데, 이동하면 S3로 이동한 최종 데이터 파일을 확인할 수 있다.파일명 format을 보면 알 수 있듯이, UTC 기준으로 시간이 되어있기 때문에, 시계열 그래프를 그리게 되면, 그래프가 짤리게 된다.(우리나라는 UTC+9) 따라서 2일 분의 데이터를 기준으로 시계열 그래프를 그려야한다. [S3(Destination)에서 파일 다운받아서 열기]해당 데이터 파일을 클릭해서 열기를 클릭하면 브라우저상에서 다운로드가 되는데, 다운받은 파일을 열면 아래와 같이 전송한 연속된 두 개의 문자열 데이터가 쌓인 것을 확인할 수 있다. [테스트 파일을 활용해서 sample data 전송해보기]실제로 대량의 데이터를 전송하는 경우, 비용이 많이 발생하기 때문에, 소량의 데이터로 테스트를 해본다. (\\*.jar 파일을 어떻게 작성하는지 한 번 공부해보기) 1java -jar sendPost.jar -f ../ods/danji_master.csv -u https://xxxxxxxx.execute-api.ap-northeast-2.amazonaws.com/PROD/v1","link":"/2022/04/18/202204/220418-data-pipeline-study-1/"},{"title":"220419 데이터 파이프라인 스터디 9일차 (AWS EMR - 이론)","text":"이번 포스팅에서는 Amazon EMR에 대해서 알아본다. 이전 포스팅까지는 Kafka, Kinesis와 같은 message queue를 활용해서 분석할 데이터를 수집하는 것에 대해서 직접 파이프라인을 구성해보고 실습해보았다. 이번 포스팅에서는 Apache Hadoop과 Apache Spark와 같은 빅데이터 프레임워크 실행을 간소화하는 관리형 클러스터 플랫폼인 EMR(Elastic MapReduce)에 대해서 실습해본다.분석할 대상이 되는 데이터가 요구하는 컴퓨팅 파워에 따라 클러스터를 쉽게 확장하고 축소할 수 있는 장점을 Amazon EMR은 가지고 있다. 그래서 이름에도 Elastic이 붙어있다.이 Amazon EMR에서 파이프라인의 핵심 축이 되는 Spark의 활용법에 대해서도 집중적으로 학습을 하고 정리를 해보려고 한다. DataPipeline flow streaming이나 batch에 의해서 발생한 파일 데이터는 Bronze 데이터로써 저장이된다.cf. Ingestion : (음식물 등의) 섭취 / Refined : 정제된이 Bronze 데이터는 한 번 가공해서 테이블 형태로 저장을 하는데 이 데이터 형태는 DW(Data Warehouse)형태라고 하며, Sliver 데이터라고도 한다.이 Sliver 데이터를 다른 사람이 분석하거나 시각화가 가능한 상태의 데이터로 가공하게 되면, 이 데이터를 DM(Data Mart)형태 혹은 Gold 데이터가 된다. DataPipeline Sequence(STEP 1) S3 bucket에 있는 데이터는 웹/앱/Batch에 의해 발생한 데이터이며, Bronze 데이터라고 한다.상품/고객의 Transaction, 고객이 상품을 보았는지에 대한 데이터 등이 이에 해당한다. (STEP 2) 앞서 STEP1에 있는 데이터가 한 번 가공을 해서 Silver 형태로 저장을 하게 되는데, 이때 사용되는 것이 바로 Amazon EMR 안의 Spark이다. (Sliver의 데이터는 Amazon S3, Amazon RDS에 저장이 된다) (STEP 3) 상품/고객 같은 경우에도 마스터성 데이터가 있다. 그 데이터를 S3에 넣고, 이벤트성 데이터는 보통 ID가 들어가있는데, ID를 푸는 작업을 한다. 그 이후에 Gold 타입의 데이터로 변화한다.(Gold 데이터는 Amazon S3, Amazon RedShift, Amazon RDS에 저장이 된다.) EMR 구조Hadoop 구조를 가지고 있는 관리형 클러스터 플랫폼이다.Master Node의 하위에는 Core와 Task Section으로 나뉘어진 구조로 되어있다. Master Node 클러스터를 관리 노드간에 데이터 및 작업의 분배를 조정 작업 상태를 추적하고 클러스터의 상태를 모니터링 Core Node(최근에는 Core Node없이 Master Node 자체만으로도 Jupyter notebook으로 구성할 수 있다) Data Node로, 클러스터의 HDFS에 데이터를 저장하는 노드이며, 하나 이상의 Core node가 있어야 한다. Task Node Core Node와 비슷한 역할을 한다. Task Node는 HDFS가 없으며, 오직 compute resources만을 가진다. 따라서 Scale In/Out하기에 유용하다. (Optional) HDFS로 S3를 사용하면 좋은점 Computing Node(EMR)와 Data Node(S3)를 서로 독립된 객체로써 운영함으로써 좋은 Architecture를 구성할 수 있다. 기존 HDFS의 경우에는 용량이 부족한 경우에는 용량에 대한 확인을 해서 확장을 해줘야했는데, S3의 경우에는 확장에 대한 신경을 쓰지 않아도 된다. 높은 내구성을 가진다. 유연하게 클러스터 내(S3)에서 필요에 따라 노드를 추가하거나 삭제할 수 있다. 클러스터를 종료 후에 다시 클러스터를 구성해도 기존의 데이터를 다시 읽을 수 있다. 여러개의 클러스터 서비스에서 데이터를 읽을 수 있다.(운영 EMR과 분석용 EMR에서 모두 사용 가능) Storage 부분과 Compute 부분을 분리함으로써 복수 작업자 간의 종속성 분리 Storage 부분과 Amazon EMR(computing part)을 분리/구성하고, Storage의 데이터를 Spark를 활용해서 동시 접근 및 처리하는 형태로 구성을 할 수 있다.(이전에는 큰 서버에 여러 분석가들이 접속을 해서 작업을 했었다. 그래서 큰 작업을 하는 작업을 하는 사람이 있는 경우에는 부하가 걸려서 다른 작업자에게 영향을 주었다. 하지만 위와같이 Storage와 Compute영역을 분리를 시키게 되면, 각 각의 작업자간의 종속성을 분리시킬 수 있다) Amazon EMR(Elastic MapReduce)내의 다양한 서비스들Amazon EMR 내에는 데이터 전처리 및 분석을 위한 여러 서비스들이 존재한다. 대표적으로 아래의 서비스들이 존재하며, 시간이 될때 찾아서 각 서비스들의 특징에 대해서 살펴보도록 하자. Presto Ganglia Hue oozie mahout TensorFlow","link":"/2022/04/19/202204/220419-data-pipeline-study/"},{"title":"220418 데이터 파이프라인 스터디 8일차 (API Gateway)","text":"이번 포스팅에서는 앞으로 하게 될 실습의 전체 구성도를 정리하고, Amazon API Gateway를 구성했던 내용에 대해서 정리해본다. (앞으로 API Gateway를 설정할때 remind를 하기 위해 참고하도록 하자) 두 번째 데이터 수집 파이프라인 구축 실습 구성도이번 실습에서는 EC2(외부 데이터 발생 source 부분)를 하나 준비해서 데이터를 발생시키고, 발생시킨 데이터를 API Gateway를 통해서 Amazon Kinesis Stream으로 넘겨주고, Amazon Kinesis Firehose에서 Stream의 데이터를 받아서 Amazon S3의 특정 폴더에 최종적으로 저장을 해주는 flow를 구성한다. (Mac/Window환경에서는 curl명령을 사용해서 외부에서 데이터를 발생시킬 수 있다) API Gateway 구성하기시스템을 구성할때에는 버전관리가 필요하다. 나중에 버전관리에 따른 업그레이드된 버전을 적용해야되는 경우가 생기기 때문이다.새로운 API를 생성한 다음에리소스 - [작업] - [리소스 생성] - [새 하위 리소스] - 리소스 이름 : v1 (버전 이름 지정) [API 메소드 생성]외부에서는 오는 이벤트를 받을때에는 POST 방식으로 받는다. 기본적으로 들어온 데이터는 Lambda로 처리하기 때문에 Lambda가 default로 선택이 되어있다. 하지만, 이번 실습에서는 Kinesis Stream에 연결을 해줄 것이기 때문에 AWS 서비스를 선택해준다. (통합 유형) AWS 리전 : ap-northeast-2 AWS 서비스 : Kinesis HTTP 메서드 : 내부 서비스간에 통신하는 부분 정의(POST) 작업 : API Gateway에서 Kinesis로 넘길때 약속된 작업 명령어 (대소문자 구분) PutRecord 실행역할 : IAM (AWS안에서 실행되는 서비스들도 role을 가지고 있기 때문에 권한 없이 다른 서비스에 명령어를 날리거나, 실행을 요청할 수 없다. 따라서 항상 role을 만들어서 서비스에 권한을 줘야 다른 서비스에 action을 할 수 있다) [역할 만들기]=&gt; IAM - [역할 만들기] - 역할 이름 : apigatewayToKinesis역할이 추가되면, 생성된 역할에서 [정책 생성] - &quot;정책 연결&quot;을 선택해서, &quot;AmazonKinesisFullAccess&quot;를 선택 - &quot;정책 연결&quot; - 생성된 역할 페이지에서 ARN 주소를 복사를 해준다. 통합요청 부분이 메시지를 처리하는 부분이고, Kinesis에서 정상처리되면, 200 Response code를 반환해준다. [통합요청 부분 추가 처리]통합 요청부분은 메시지가 왔을 때 어떻게 처리할지 정의하는 부분으로 추가적인 작업이 필요하다. (1) Type은 JSON으로 정의하도록 한다. HTTP 헤더에 아래 정보를 추가해준다. 12이름 : Content-TypeMapped from : 'application/x-amz-json-1.1' (2) 매핑 템플릿에서 패스스루 방식에서 매핑 템플릿 추가로 application/json를 해준다. (3) velocity programming language를 사용해서 템플릿을 생성해준다. 아래의 스크립트를 참고하도록 하자. 12345678#set ( $enter = &quot;&quot;)#set($json = &quot;$input.json('$')$enter&quot;){ &quot;Data&quot;: &quot;$util.base64Encode(&quot;$json&quot;)&quot;, &quot;PartitionKey&quot;: &quot;$input.params('X-Amzn-Trace-Id')&quot;, &quot;StreamName&quot;: &quot;class-stream&quot;} enter값을 넣는 이유는 메시지 데이터값이 오면, queue에 담겼다가 오기 때문에 enter가 없으면 다음 row로 넘어가지 않기 때문에 넣어준다.한 줄 한 줄씩 받아온 메시지 데이터를 읽어서 Kinesis에 RESTFul 방식으로 넘겨주는 Script이다. Data 자체를 base64로 encoding한다. (이유는 우리가 흔히 쓰는 이메일도 base64로 encoding되어있다. web contents는 특수한 기호가 많기 때문에 base64로 encoding하지 않으면 전문이 갈때 특수문자에 의해서 원하지 않은 부분에서 짤리는 상황이 발생한다) Partitionkey는 Shard를 3개를 나눈다고 가정할 때 Kinesis가 Shard의 index 순서에 맞게 만들어주게 하기 위해서 정의한다. StreamName은 어느 스트림으로 보낼지에 대한 이름을 정의한다. (4) API 배포 작업상단에 작업 드롭다운 메뉴에서 API 작업 - API 배포를 클릭해서 생성한 API 메서도를 배포해준다. 배포 스테이지 : [새 스테이지]스테이지 이름 : PROD 최종적으로 서비스에 대한 END POINT도 확인할 수 있다. (5) curl명령어를 사용해서 END POINT로 생성한 데이터를 API Gateway END POINT를 통해 Kinesis Stream으로 넘겨준다. 1curl -d &quot;{\\&quot;value\\&quot;:\\&quot;30\\&quot;,\\&quot;type\\&quot;:\\&quot;Tip 3\\&quot;}&quot; -H &quot;Content-Type: application/json&quot; -X POST https://xxxxxxxx.execute-api.ap-northeast-2.amazonaws.com/PROD/v1 (6) 서비스 모니터링서비스를 한 개 만들때마다 모니터링을 해줘야한다. API Gateway와 같은 경우에는 로그가 CloudWatch에 떨어진다.Api Gateway에 CloudWatch에 대한 설정을 해줘야한다. [설정] - CloudWatch 로그 역할 ARN 입력 (IAM에서 CloudWatch 권한이 추가되어있는 정책 ARN 입력) (7) 배포된 API 경우에는 스테이지 항목에 올라가있기 때문에 스테이지에서 API 항목을 클릭한 뒤에 CloudWatch 설정에서 CloudWatch 로그 활성화를 체크하고 INFO 수준으로 체크해준다. 현재는 실습단계이기 때문에 CloudWatch에 로그가 쌓으는 것만 확인하고 바로 서비스를 disable한다. (별도 비용발생)","link":"/2022/04/18/202204/220418-data-pipeline-study/"},{"title":"220419 Hadoop과 친해지기 열 번째 이야기(MapReduce Challenge)","text":"이번 포스팅에서는 MapReduce 실습 도전과제에 대해 직접 해본 내용을 정리해보려고 한다. 이전 포스팅에서는 영화 정보 데이터가 주어졌을때, 각 평점을 기준으로 몇 편의 영화가 분포되어있는지 확인하는 MapReduce 코드를 작성해보았다. 이번 Self-Challenge 과제는 총 2개이며,첫 번째 챌린지 과제는 영화별 인기순위를 분류하기 위한 MapReduce 작성하는 것이다.두 번째 챌린지 과제는 평가 횟수를 기준으로 영화를 정렬하는 MapReduce를 작성하는 것이다. 영화별 인기순위 분류하기 위한 MapReduce 작성영화 인기순위는 영화 평가 횟수를 기준으로 보았을 때, 시청 횟수의 대용물로 평가가 될 수 있기 때문에 movieID를 기준으로 mapper를 작성해주고, reducer에서 해당 영화 평가 횟수를 종합해주면 된다는 결론이 나온다. 123456789101112131415161718from mrjob.job import MRJobfrom mrjob.step import MRStepclass NumOfEvaluateBreakdown(MRJob): def steps(self): return [ MRStep(mapper=self.mapper_get_num_of_evaluate, reducer=self.reducer_count_num_of_evaluate) ] def mapper_get_num_of_evaluate(self, _, line): (userID, movieID, rating, timestamp) = line.split('\\t') yield movieID, 1 def reducer_count_num_of_evaluate(self, key, values): yield key, sum(values) if __name__ == '__main__': NumOfEvaluateBreakdown.run() 여기서 확인해야 될 중요한 부분은 Streaming은 모든 input과 output에 대해서 문자열로 다룬다는 것이다. 따라서 위의 출력 값에서 볼 수 있듯이, movieID를 numeric값이 아닌 string 값으로 정렬이 됨을 볼 수 있다.따라서 다음 챌린지에서 평가 횟수를 기준으로 정렬할 때에는 zfill() method를 사용해서 일정 자리수로 맞춰주고, 나머지 자리수의 경우에는 0으로 padding값을 넣어 맞춰줘야 문자열로도 numeric으로 정렬한 것과 같은 동일한 결과값을 얻을 수 있다. 평가 횟수를 기준으로 영화를 정렬하는 MapReduce를 작성1234567891011121314151617181920212223242526272829from mrjob.job import MRJobfrom mrjob.step import MRStepclass NumOfEvaluateBreakdown(MRJob): def steps(self): return [ MRStep(mapper=self.mapper_get_num_of_evaluate, reducer=self.reducer_count_num_of_evaluate), # Shuffle and Sort 작업이 한 번 더 일어나며, sort_by MRStep( reducer=self.sort_by_rating_count ) ] def mapper_get_num_of_evaluate(self, _, line): (userID, movieID, rating, timestamp) = line.split('\\t') yield movieID, 1 # 평가된 횟수를 총 5자리로 숫자로 만들어서 그 값을 기준으로 movieID를 정렬한다. def reducer_count_num_of_evaluate(self, key, values): yield str(sum(values)).zfill(5), key # 현재 복수 개의 동일한 movieID가 movies 값 배열에 들어가 있기 때문에 # movies 배열을 순회하면서, 매핑되는 count 값을 출력해준다. def sort_by_rating_count(self, count, movies): for movie in movies: yield movie, count if __name__ == '__main__': NumOfEvaluateBreakdown.run() [Multi-stage jobs] 복수 개의 map/reduce 작업을 chaining 하는 방법은 아래와 같이 MRStep으로 연속해서 ,(comma)로 구분해서 이어주면 된다. 1234567def steps(self): return [ MRStep(mapper=self.mapper_get_ratings, reducer=self.reducer_count_ratings), # MapReduce에 의해 Shuffle and Sort가 이뤄진다. MRStep(reducer=self.reducer_sorted_output) ]","link":"/2022/04/19/202204/220419-hadoop_bigdata_class/"},{"title":"220421&#x2F;220422 데이터 파이프라인 스터디 10&#x2F;11일차 (AWS EMR - 실습)","text":"이번 포스팅에서는 이전 시간에 이론적인 부분을 학습한 EMR을 직접 실행해보고 모니터링해보면서 학습한 부분에 대해 포스팅해보려고 한다. EMR 서비스처음 EMR 서비스 대시보드에는 아무것도 보이지 않지만, 기본적으로 보이는 클러스터 history 정보의 경우에는 default로 두 달 정도 보여주며, optional하게 노출 기간의 변경이 가능하다. 저렴하게 EMR 서비스 이용 고급 옵션으로 이동 EMR은 관리형 클러스터이기 때문에 여러가지 소프트웨어를 옵션으로 설치할 수 있다.(emr-6.x : Docker/Kubernetes를 같이 실행할 수 있는 환경을 지원) 소프트웨어 선택 Hadoop, Ganglia, Hive, Zeppelin, Spark, Pig 선택 cf. Zippelin : 웹 환경에서 Spark에 여러가지 command를 통해서 job을 던진다. cf. Livy : 어플리케이션 간에 Spark에 job을 던질때 사용되는 것이 Livy이다. AWS Glue catalog setting Glue는 S3에 저장된 정보의 meta정보를 테이블 형태로 관리하는 것을 말한다. 클러스터 생성 하드웨어 구성 Networking VPC : 하나의 건물로 이해 / subnet은 각 층으로 이해 (스팟을 제공하는 subnet이 따로 있는 경우가 있기 때문에 확인 필요) Cluster Nodes and Instance Instance type : spark 자체는 메모리 기반 데이터를 분석하는 툴이다. 따라서 메로리에 좀 더 최적화된 인스턴스 타입을 선택한다. (r-&gt; ram -&gt; memory 기반의 인스턴스, c -&gt; computing, g -&gt; gpu, i -&gt; disk 최적화(데이터 분석시에도 고려되면 좋음)) worker node(코어 + 작업 노드)의 합은 홀수가 되는 것이 좋다. job을 수행할 때 경쟁이 생기지 않게 하려면, 홀수가 되도록 설정하는 것이 좋다고 data bricks에서 recommend하고 있다. on-demand : EC2를 구매해서 사용하는 형태 spot : 각 zone에 남아있는 유효장비를 사용하고자 하는 고객들에게 bidding을 붙여서 사용하게 하는 것(70~90% 저렴하게 사용 가능) 기본적으로 Amazon EMR은 관려형 서비스이기 때문에 생성한 Master, Core, Task node의 수만큼 EC2 dashboard에 가보면 인스턴스가 생성된 것을 확인할 수 있다. [EMR Master node 작업을 위한 EC2 키 페어 설정]Amazon EMR 클러스터에서 작업을 할때에는 Master node에 접근을 해서 작업을 하는 경우가 많다. 모든 설정이 완료된 후에는 7 - 10분정도 후에 클러스터를 쓸 수 있는 상태로 전환이 된다. Zeppelin에서 Spark coding을 위한 port settingweb 상에서 Zeppelin을 통해서 Spark을 coding하려면 8890 port에 접근할 수 있어야 한다. 이 부분에 대해서 Master node의 security group의 inbound rule에 새로운 규칙을 추가하도록 한다. [보안 및 액세스 - 마스터 / 코어 및 작업 노드 보안 그룹 관리]Master/Core 및 Task node의 security group 관리를 위해서는 클러스터 대시보드에서 요약탭의 &quot;보안 및 액세스&quot; 부분을 통해 관리한다. [단계] 탭클러스터를 구성한 이후에클러스터를 구성한 다음에 특정 TASK(드라이브 설치 혹은 job 실행)를 실행하도록 할 수 있다. ex) DB에 접속할 수 있는 드라이버를 자동으로 설치하고 싶은 경우, TASK 추가 [부트스트랩 작업] 탭클러스터를 구성하기 이전에미리 설치하기 위한 어플리케이션이나 스크립트가 있는 경우 사용 하둡 클러스터 종료하둡이 HDFS로 구성이 되어있는데, 클러스터가 종료가 되면, 모든 데이터는 사라지게 되고, 구성했던 내용만 남게 된다.종료되면 EC2 인스턴스가 하나씩 내려가게 된다. 2022/04/22 업데이트 Amazon EMR 모니터링Amazon EMR 구성요소 중에서 Spark를 사용한 데이터 분석을 Ganglia를 사용해서 모니터링을 할 수 있다. [monitoring을 통한 확인 및 판단] Process는 총 몇 개가 돌고 있는지에 대한 확인 어느경우에 scale-out / scale-in 해줘야되는지? [EMR on EC2] - Clusters클러스터 이력(history) 목록에서 종료된 클러스터 항목을 clone함으로써 종료된 클러스터를 재 실행시킬 수 있다. (주의해야될 점은 설정 언어가 한글인 경우에는 Cluster node instance의 구매 옵션이 spot으로 변경해서 설정한 부분이 수정되지 않은 경우가 있는데, 이 부분은 원래대로 spot으로 수정을 시켜줘야한다) 만약에 에러가 발생하는 경우에는 Cluster의 &quot;Steps&quot; 탭에서 새로운 Step을 추가해준다. 이미 이전에 언급했듯이, 모든 클러스터 구성을 완료된 후에 새로운 라이브러리를 추가 설치하는 과정을 위한 부분이 이 Steps 탭이다. 아래는 Ganglia에 접속이 안될 경우 해결을 위한 script이다. deny를 allow로 설정을 바꿔주고 httpd서비스를 재구동한다. (아래 스크립트는 위의 Arguments의 S3 Bucket내의 *.sh파일의 내용이다) 12345if [ -f &quot;/etc/httpd/conf.d/ganglia.conf&quot; ]; then echo &quot;Setting up ganglia on master node&quot; sudo sed -i 's/Order deny,allow/#Order deny,allow/g' /etc/httpd/conf.d/ganglia.conf sudo service httpd reloadfi JAR 위치는 아래와 같이 region에 대한 값을 제외하고 주어진다.SHELL을 실행할 수 있게 해주는 script-runner.jar 파일에 대한 정의부분이다. 1234s3://&lt;region&gt;.elasticmapreduce/libs/script-runner/script-runner.jar#seoul regions3://ap-northeast-2.elasticmapreduce/libs/script-runner/script-runner.jar Ganglia 모니터링Ganglia에서 좌측의 Server Load Distribution 부분에서 서버가 과도하게 과부하가 걸리게 되면, 주황색이나 빨색으로 바뀌게 되는데 scale out을 통해서 순조롭게 돌도록 구성을 해주면 된다. Spark같은 경우는 중간에 노드를 늘리더라도 그 노드의 마스터 노드가 Task를 저압을 가지고 있다면 새로 생성된 노드에 작업할당을 해주기 때문에 중간에 클러스터를 scale out을 해줘도 괜찮다. Apache Zeppelin 이전에 하둡과 친해지기 네 번째 이야기 블로그 포스팅에서 외부 데이터 스토리지 관련 내용에 대해서 정리를 하면서 잠깐 Apache Zeppelin에 대해서 언급을 했었다.Apache Zeppelin은 클러스터와의 상호작용과 사용자 인터페이스를 노트북 유형으로 접근하는 새로운 관점을 채택했다는 내용이었는데, 이 부분에 대한 실습을 해보았다. Spark, DB, Presto와도 연동이 가능하다. 말 그대로 Zeppelin은 interpreter이기 때문에 Spark CLI에서 처리해야되는 부분도 손쉽게 웹 기반으로 처리할 수 있도록 도와주는 친절한 녀석이다. Python, Scala, R 등의 언어를 섞어가며 분석코드를 표현할 수 있고, 바로 실행하여 결과를 볼 수 있을 뿐만 아니라 생성된 데이터를 그래프로 시각화하여 볼 수 있다. Bronze data —-(Amazon EMR)—-&gt; Sliver dataAmazon S3에 적재되어있는 Bronze 데이터를 Amazon EMR을 활용해서 Sliver 데이터로 convert하는 작업 파일 분석시 데이터 포멧온프레미스의 DBMS 데이터도 CSV 포멧으로 내려받고 DataFrame으로 convert하면, Spark SQL로 분석을 할 수 있는 환경이 된다. CSV 파일 여러개인 경우, 각 각을 테이블로 보고 조인을 해서 분석을 할 수도 있다.결과는 RDBMS Data로 쓸 수 있고, Json Data, CSV Data 등의 데이터로 write할 수 있다. Parquet Data format : 칼럼 정보 뿐만 아니라 데이터가 가지고 있는 속성/타입도 줄 수 있다. 더 나아가 압축도 제공하며, 빅 데이터를 빠르게 분석할 수 있게 연구하는 단계에서 발생한 압축 파일이며, Zip파일에 비해서 떨어지지만, Spark에서 데이터를 읽을때 성능이 좋다. Zeppelin Syntax 및 Analysis ErrorSQL의 경우에는 둘 다 Runtime에서 검사를 해주며,DataFrames는 Syntax Error의 경우에는 Compile Time에서 검사를 해주고, Analysis Error는 Runtime에서 검사를 해준다.Datasets의 경우에는 둘 다 Compile Time에서 검사를 해준다.","link":"/2022/04/22/202204/220421-data-pipeline-study/"},{"title":"220422 Hadoop과 친해지기 열 번째 이야기(Pig로 Hadoop 프로그래밍)","text":"이번 포스팅에서는 구조상으로 보면, MapReduce의 상위에 위치해있는 Pig라는 친구를 사용해서 실습을 해보도록 하겠다. 실습은 Ambari를 사용해서 해보도록 한다. Ambari에서 관리자 계정 접근 활성화1234$su root# switch to root account$ambari-admin-password-reset# type the password for admin account Pig ?Ambari 웹 브라우저의 화면 우측 상단에 격자 무늬 아이콘을 클릭하면, Pig View 메뉴를 확인할 수 있는데, 이곳에 Pig Scripts 추가하고 실행해볼 수 있다. 여지까지 Hadoop에 대해서 학습하면서 Hadoop의 가장 핵심기술인 MapReduce에 대해서 중점적으로 배웠다. MapReduce는 Hadoop의 시작을 함께 한 기술이긴 하지만 오래된 데이터 처리 기술이고, 어렵다. 이러한 어려운 데이터 처리 기술을 Pig는 좀 더 쉽게 할 수 있도록 도와준다. Pig는 정확히 말하면, Pig Latin이라는 스크립팅 언어이고, Hadoop과 MapReduce 위에 구축되어있다.(신속한 반복처리(iteration)가 가능한 편리한 개발환경에서는 다양한 실험을 통해 데이터를 창의적으로 사용할 수 있다) Pig를 사용하면 Mapper와 Reducer를 별도로 작성하지 않아도 Pig Latin script로 MapReduce 작업을 대체할 수 있다. 정확히 말하면 Pig script가 MapReduce로 번역이 되지만, 성능상에 문제는 되지 않는다.MapReduce의 가장 큰 문제는 개발 사이클 타임인데, MapReduce 프로그램을 개발해서 실행하고, 원하는 작업을 수행하기까지 오랜시간이 걸리기 때문에 이 Pig라는 친구가 등장하게 된 것이다.Pig Latin script는 SQL과 비슷하고, 절차형 언어이며, 아주 간단한 스크립트 양식을 사용해서 단계별로 데이터 간에 여러 관계를 설정한다. (데이터 선택/필터/변형) Pig는 MapReduce 위에 구축되어있다고 했지만, 정확히 말하면 MapReduce와 TEZ 사이에 있다. 즉, MapReduce외에도 TEZ를 기반으로 사용하기도 한다. TEZ는 MapReduce보다도 훨씬 빠르기 때문에 Pig를 사용함으로써 얻는 장점이 많다.Pig Latin은 UDF(User Define Function)을 지원하기 때문에 사용자 지정함수를 사용해서 스크립트를 작성할 수 있다. 또한 Pig는 Hadoop 클러스터나 종속된 기술읕 사용하지 않고도 독립적으로 간단한 작업을 수행할 수 있다. TEZ는 기본적으로 작업을 훨씬 효율적으로 체계화하는 방식(방향성 비사이클 그래프)을 가지고 있으며, MapReduce의 매퍼와 리듀서가 있고, 처리된 데이터를 또 다른 매퍼와 리듀서가 받아서 데이터를 출력하면 다음 매퍼와 리듀서가 받는 등의 순차적인 관계가 있는 것과 상반되는 특성을 가지고 있다.TEZ는 상호 의존성을 파악하고, 의도한 바를 이루기 위한 가장 효율적인 경로를 계산한다. (MapReduce보다 10배 빠른 속도) Pig 사용법(1) Master node에서 ‘pig’입력, Grunt prompt 후 스크립트 입력(2) 스크립트를 파일에 저장, commandline에서 Pig 명령으로 실행(3) 브라우저 상 편집기에서 작성/실행/저장 작업예제문제를 통해서 Pig Latin script 실습[실습] 평균 평점 4점을 초과하는 영화를 출시일 기준으로 정렬하기1234567# HDFS Cluster location (path)ratings = LOAD '/user/maria_dev/ml-100k/u.data' AS (userID:int, movieID:int, rating:int, ratingTime:int);metadata = LOAD '/user/maria_dev/ml-100k/u.item' USING PigStorage('|')AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdbLink:chararray);# DUMP command는 디버깅 등에 굉장히 유용하고, 내가 생각한대로 데이터가 추출되는지 확인할 수 있다.DUMP metadata; 다른 관계 데이터에서 또 다른 관계 생성하기; FOREACH / GENERATE123metadata = LOAD '/user/maria_dev/ml-100k/u.item' USING PigStorage('|')AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdbLink:chararray);nameLookup = FOREACH metadata GENERATE movieID, movieTitle, ToUnixTime(ToDate(releaseDate, 'dd-MMM-yyyy')) AS releaseTime; [Reduce in Pig] Group By이전에 각 평점 점수에 따른 영화의 분포를 알아보기 위해서 count를 했었는데, 평점 점수가 기준이 되어 Grouping되었었다.Pig에서 Group By는 MapReduce에서 Reduce에서와 같은 처리를 해준다. 123456789101112131415161718ratingsByMovie = GROUP ratings BY movieID;DUMP ratingsByMovie;#(1,{(.....),(.....),(.....),.....})#(2,{(.....),(.....),(.....),.....})avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID, AVG(ratings.rating) AS avgRating;DUMP avgRatings;DESCRIBE ratings;DESCRIBE ratingsByMovie;DESCRIBE avgRatings;# 관계성 스키마 출력# ratings: {....}# ratingsByMovie: {....}# avgRatings:{....} FILTER1fiveStarMovies = FILTER avgRatings BY avgRating &gt; 4.0; JOIN12345678DESCRIBE fiveStarMovies;DESCRIBE nameLookup;# movieID를 기준으로 fiveStarMovies와 nameLookup 테이블을 JOIN한다.fiveStarsWithData = JOIN fiveStarMovies BY movieID, nameLookup BY movieID;DESCRIBE fiveStarsWithData;DUMP fiveStarsWithData; ORDER BY123oldestFiveStarMovies = ORDER fiveStarsWithData BY nameLookup::releaseTime;DUMP oldestFiveStarMovies;","link":"/2022/04/22/202204/220419-hadoop_pig_bigdata_class/"},{"title":"220423 데이터 파이프라인 스터디 12일차 (종합복습 및 세미 프로젝트 준비)","text":"오늘은 데이터파이프라인 1일차부터 11일차까지 학습했던 내용들은 종합적으로 복습하면서 앞으로 개인적으로 진행하게 될 세미 프로젝트는 어떤 방향으로 진행을 하게 될 것인지에 대해서 구상을 해보았다. (참고로 개인프로젝트를 채워넣을 개인 포트폴리오 웹 사이트를 아주 심플하게 만들었다. 뭔가 좋은 동기부여가 될 것 같아 하나 하나씩 채워가기 위해서 만들었는데, 확실히 좋은 동기부여가 되는 것 같긴하다) 아직은 총 11차 중에 2일분 밖에 복습을 진행하지 못했다. 하지만, 2일차까지 내용을 천천히 되새기면서 정리를 해보니 정말 주옥같은 내용들이 많았고, 내가 새로운 개념들을 배워가다가 잊고 있었던 부분들도 있었다. 앞으로 남은 복습분량들도 충분히 시간을 투자해서 진행할 계획이다. 우선 총 11일 동안 진행했던 데이터 파이프라인 학습은 나름 만족스러운 진행상태이다. 11일 동안 총 학습 목표의 44.7% 달성을 했고, 그 사이 사이에 블로깅이며, 다른 공부와 병행을 하고, 개인적으로 복습시간을 갖았던 것을 고려하면, 나름 괜찮은 성과이다. 세미 프로젝트의 방향(별도의 공간에서 정리를 하며 진행하고 있다)","link":"/2022/04/23/202204/220423-data-pipeline-study-1/"},{"title":"220423 AWS Practitioner 자격증 취득 준비","text":"이번 포스팅에서는 요즘 재미를 붙여가면서 공부하고 있는 AWS 공부와 관련된 이야기 좀 하려고 한다. AWS 공부를 하게 된 계기는 데이터 파이프라인 구축과 관련된 실습을 AWS 가상 클라우드 환경에서 하고 있기 때문이다.이번 실습을 하면서 이전에는 단순히 S3 Bucket에 리소스를 올려서 사이드 프로젝트에서 사용하고, EC2 가상 서버 인스턴스를 올려서 단기적으로 사용하던 것을 넘어서서 이번에는 각 서비스에 대해서 제대로 이해하고 사용하기 시작했다.그 이유는 지금 공부하면서 실습하고 있는 내용이 데이터 파이프라인 구축에 관한 내용인데, 데이터 수집, 전처리, 분석 및 시각화, 저장해주는 각 각의 일련의 과정에서 사용되는 AWS의 서비스들은 많으며, 다양한 구성도가 나올 수 있고, 상황에 맞게 잘 구성해서 사용해야되기 때문이다.심지어 운영적 측면도 고려하여, cost도 생각해서 pipeline에서 사용할 AWS의 서비스를 구성 및 설계해야한다. 생각보다 한 서비스를 활용한다는 것은 정말 복잡하다. 데이터 파이프라인 실습 중간 중간에 공부를 하면서 AWS와 관련된 여러 서비스들이 등장을 하는데, 뭔가 학습에 있어 로드맵이 필요하다는 생각이 들어서 찾아보던 중, AWS 관련 자격증 중에서 AWS Practitioner라는 자격증이 있다는 것을 알게 되었다. 가장 기초가 되는 자격증이라는 점이 가장 마음에 들었고, 지금 시점에 배우는 서비스들 중에 아닌 서비스들도 있지만, 대부분의 서비스들이 AWS 서비스들 중에 가장 기본이 되는 서비스들이 많기 때문에 병행해서 학습을 한다면 좀 더 좋을 거라고 생각을 했는데, 2주간 같이 병행해서 학습을 해보니, 역시 내 예상대로 정말 많은 도움이 되었다. 오늘부로 AWS 공식 사이트에서 제공해주는 AWS Cloud Practitioner Essentials 코스 완료했다. 최종 테스트에서 87% 정답률을 달성했는데, 틀린문제는 다시 풀어서 다 맞췄다. 생각보다 수업내용과 AWS 공식 사이트에서 제공해주는 무료 강좌의 커리큘럼에서 순차적으로 등장하는 AWS의 서비스들이 겹치는 순간이 좀 많이 있어서 복습의 효과까지 얻게 되었던 것 같다. 이제 남은 시험날까지 13일정도 남았는데, 그 날까지 기출문제를 계속 풀어보면서 AWS에 있는 서비스 및 기술들에 대해서 다시 복습하고, AWS 공식 사이트에서 알려준 아래의 중요도 순으로 시간을 배분해서 공부를 해봐야겠다. 영역3(기술) &gt; 영역1(클라우드 개념) &gt; 영역2(보안 및 규정준수) &gt; 영역4(결제 및 요금)","link":"/2022/04/23/202204/220423-aws-study/"},{"title":"220423 데이터 파이프라인 스터디 12일차 (Zeppelin notebook을 사용한 분석)","text":"이번 포스팅에서는 Zeppelin을 사용해서 데이터를 처리실습을 한 내용을 정리해볼 것이다. Zeppelin 메뉴 구성Zeppelin 페이지를 보면, Job 메뉴를 통해서는 실행되고 있는 좌표들을 모니터링할 수 있다. 그리고 Interpreters 메뉴를 통해서는 다른 Interpreter를 추가할 수도 있으며, Angular, File, Flink, Kotlin, R, Python, Spark 등에 대한 Interpreter를 default로 지원해주고 있음을 확인할 수 있다. 1234567891011121314val csvDF = spark.read // 첫 번째 row가 header인 경우, .option(&quot;header&quot;, &quot;true&quot;) // delimiter가 comma로 구분된 경우, .option(&quot;delimiter&quot;, &quot;,&quot;) // schema를 구성을 할 때 알지 못하는 데이터의 경우에는 문자열로 정의한다. // 전체 데이터를 읽고 숫자 데이터만 있는 경우에는 Integer로, 문자만 있는 경우, String만 정의한다. // 단, 파일이 큰 경우에는 전체를 다 읽어야 되기 때문에 이 부분을 고려해서 작업을 진행해야된다. .option(&quot;inferSchema&quot;,&quot;true&quot;) .csv(&quot;s3://[s3-name]/[...]/[filename].csv&quot;)csvDF.show // 파일 확인하기csvDF.printSchema // DataFrame Schema 출력/확인 최초 실행을 하게 되면 약간 느린데 Spark과 통신 Session을 열어줘야되기 때문이다. SparkSQL, Streaming, ML, Graph 처리를 위한 기본 제공 모듈이 있는 대규모 데이터 처리용 통합 분석 엔진이다. Spark는 Cloud의 Apache Hadoop, Apache Mesos, Kubernetes에서 자체적으로 실행될 수 있으며, 다양한 데이터 소스에 대해서 실행될 수 있다. Apache Spark와 Apache Hadoop은 둘 다 분산 시스템이며, Hadoop은 주로 디스크 사용량이 많고, MapReduce 패러다임을 사용하는 작업에서 사용이 된다. 반면에 Spark는 더 유연하게 사용이 되며, 대체로 더 많은 비용이 드는 In-Memory Architecture라는 차이가 있다. Spark를 사용하면 구조화 또는 구조화되지 않은 대량의 실시간 또는 아카이브 데이터를 처리 및 분석하는 까다롭고 계산된 집약적인 Task를 간소화시킬 수 있다.Spark는 많은 언어로 프로그래밍 할 수 있는 옵션을 갖추고 있으며, 이러한 이유로 Spark을 사용해서 데이터 처리작업을 코딩하고 빌드한다. 12345678// View Table 정의csvDF.createOrReplaceTempView(&quot;data_master&quot;)// View Table 확인하기%sql // bind중에서 &quot;sql을 사용한다&quot;고 정의!show tables// isTemporary column(true)은 세션이 끊기면 사라지는 테이블이라는 의미이다. Spark SQL 수행생성한 View Table을 참조해서 일반 SQL query를 수행할 수 있다. CSV -&gt; JSON파일로 쓰기Spark 자체가 여러 노드를 가지고 분산 컴퓨팅을 하기 때문에, 아래와같이 coalesce(1)라는 옵션을 주지 않으면 복수 개의 JSON파일이 생성될 것이다. 123456csvDF .coalesce(1) .write .mode(&quot;Overwrite&quot;) .format(&quot;json&quot;) .save(&quot;s3://[s3 bucket name]/silver/&quot;) JSON -&gt; DataFrame으로 읽기1234val jsonDF = spark.read.json(&quot;s3://[s3 bucket name]/silver/&quot;)jsonDF.show # JSON 데이터 DataFrame으로 convert해서 출력하기jsonDF.printSchema # DataFrame Schema 확인 DataFrame -&gt; Parquet format으로 쓰기Spark같은 경우에는 파일이 큰 경우에는 동시성이 떨어진다. 1234jsonDF.write .mode(&quot;Overwrite&quot;) .format(&quot;parquet&quot;) .save(&quot;s3://[s3 bucket name]/target/master/parquet/&quot;) AWS에서 Parquet로 저장을 하게 되면, snappy라는 형태로해서 저장을 하게 된다. (snappy도 gzip과 동일한 압축파일 확장자 형태로 보면 된다) 빅데이터가 생겨나면서 압축률은 줄이면서, 읽는 것을 빠르게 하기 위해서 나온 압축형태이다. Parquet format data -&gt; DataFrame으로 읽고, View Table 생성하기12345val parquetDF = spark.read.parquet(&quot;s3://[s3 bucket name]/target/master/parquet/&quot;)parquetDF.printSchemaparquetDF.createOrReplaceTempView(&quot;data_master_pq&quot;) DataFrame data를 CSV format 파일로 쓰기12345parquetDF.coalesce(1) .write .made(&quot;Overwrite&quot;) .format(&quot;csv&quot;) .save(&quot;s3://[s3 bucket name]/target/master/parquet/csv/&quot;)","link":"/2022/04/23/202204/220423-data-pipeline-study-2/"},{"title":"220424 Hadoop과 친해지기 열한번째 이야기(Pig로 Hadoop 프로그래밍)","text":"이번 포스팅에서는 앞서 배운 Pig Latin script 작성방법을 기반으로 Ambari를 통해서 Hadoop 클러스터에서 Pig script를 실행해보고 결과값을 확인해볼 것이다. Ambari에서의 Pig script 실행 Hadoop stack을 사용해서 Pig Latin script를 실행해보았다.결과는 Ambari에서 결과, Logs에 대한 기록, 작성한 Script Details 정보를 각 카테고리별로 분류해서 확인할 수 있었다. [TEZ를 기반으로 Pig script 실행]이전 포스팅에서 하둡의 생태계에 대해서 배울때 배웠듯이, Pig는 MapReduce를 사용하지 않고, TEZ를 사용해도 된다. TEZ는 비정형 사이클 그래프를 사용해서 여러 단계의 상호관계를 분석해서 최적의 경로를 찾아낸다.최종 경로부터 역순으로 각 단계의 종속성으로부터 가장 최적의 경로를 찾아낸다. 결과적으로 보았을 때, Pig script를 MapReduce보다는 TEZ 기반으로 실행했을때 약 2배정도 처리 속도가 빨랐음을 알 수 있었다.이처럼 하둡의 생태계에는 여러 기술들이 집약되어있는데, 어떤 기술과 어떤 기술을 조합해서 사용하느냐에 따라서 다른 퍼포먼스를 보여준다. Pig Latin script에 대해서 더 알아보기STORE 관계성 이름 INTO 파일이름 USING PigStore('[delimiter]');FILTER DISTINCT FOREACH/GENERATE MAPREDUCE STREAM SAMPLE MAPREDUCE : Pig와 MapReduce를 따로 사용하지 않고 혼합해서 사용 STREAM : Pig의 결과물을 STREAM 형태로 출력 JOIN COGROUP GROUP CROSS CUBEORDER RANK LIMITUNION SPLITDIAGNOSTICS (1) DESCRIBE : 관계성의 스키마 출력 (2) EXPLAIN : 주어진 쿼리를 어떻게 실행할지에 관한 설명 출력 (3) ILLUSTRATE : 관계성에서 표본을 가져와서 각 데이터 조각들을 가지고 무엇을 하는지에 대한 것을 출력 Pig 내부적으로 무슨 일이 일어나는지 구체적으로 알고 싶은 경우에는 (2)EXPLAIN과 (3)ILLUSTRATE를 사용하고, 테이블의 관계성 스키마를 확인하기 위한 목적에서는 (1)DESCRIBE를 사용하도록 한다. UDF(User Define Function) REGISTER : UDF가 포함된 jar 파일을 가져오는데 사용 DEFINE : 함수에 이름을 부여해서 Pig 스크립트 내에서 사용할 수 있도록 정의하는데 사용 IMPORT : Pig 파일의 매크로를 호출하는데 사용 (재사용 가능한 Pig 코드를 매크로로 저장한 경우) 그 외 함수들과 로더들 AVG, CONCAT, COUNT, MAX, MIN, SIZE, SUM PigStorage TextLoader JsonLoader AvroStorage ParquetLoader OrcStorage HBaseStorage [Challenge] 1점을 가장 많이 받은 최악의 영화 찾기 (1) 평균 평점이 2.0 미만인 모든 영화를 찾는다. (2) (1)에서 출력한 영화 리스트를 총 평점 수를 기준으로 정렬한다. 1234567891011121314151617181920212223ratings = LOAD '/user/maria_dev/ml-100k/u.data' AS (userID:int, movieID:int, rating:int, ratingTime:int)metadata = LOAD '/user/maria_dev/ml-100k/u.item' USING PigStorage('|') AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdbLink:chararray);# metadata에서 movieID와 movieTitle만 GENERATE# movieID 기준으로 JOIN해서 영화 제목을 알아내기 위해서nameLookup = FOREACH metadata GENERATE movieID, movieTitle;groupedRatings = GROUP ratings BY movieID;averageRatings = FOREACH groupedRatings GENERATE group AS movieID, AVG(ratings.rating) AS avgRating, COUNT(ratings.rating) AS numRatings;badMovies = FILTER averageRatings BY avgRating &lt; 2.0;# badMovies와 nameLookup 테이블을 movieID를 기준으로 JOIN한다.namedBadMovies = JOIN badMovies BY movieID, nameLookup BY movieID;finalResults = FOREACH namedBadMovies GENERATE nameLookup::movieTitle AS movieName, badMovies::avgRating AS avgRating, badMovies::numRatings AS numRatings;# 1점대로 평점 평가가 몇 번되었는지에 대한 count값을 기준으로 내림차순 정렬한다.finalResultsSorted = ORDER finalResults BY numRatings DESC;DUMP finalResultsSorted;","link":"/2022/04/24/202204/220424-hadoop_pig_bigdata_class/"},{"title":"220425 데이터 파이프라인 스터디 14일차 (AWS Glue 서비스)","text":"이번 포스팅에서는 AWS Glue라는 서비스에 대해서 배웠던 내용을 정리하려고 한다. 메타 데이터를 관리할 수 있는 서비스인데, 최근에 여러 회사에서 governance에 대한 관심과 여러 tool들이 많이 나오고 있고, data에 대한 정의(metadata)에 대해서 관심이 높아지고 있다. 이전에 데이터파이프라인에 대해서 가장 처음 전체적인 흐름에 대해서 배웠을때, 데이터 수집과 전처리의 중간 지점에 있는 관련 AWS 서비스에 Glue라는 서비스가 있다고 배웠는데, AWS Pipeline의 복잡해짐에 따라서 관리 및 운영이 어려워지고, 이러한 문제를 개선하기 위해서 등장한 친구라고 간단하게 배웠었다.AWS Glue에는 파이프라인의 기본적인 서비스들이 추가가 되어있고, 가장 비용 효율적으로 잘 활용되고 있는 부분이 메타 스토어 정보가 포함되어있는 부분인데, meta store의 정보에는 데이터 위치, 포멧, 버전의 변경사항등에 대한 정보를 포함하고 있다고 한다. (2022/04/27 업데이트) Glue의 가장 큰 특징은 서버리스로, 설정하거나 관리할 별도의 인프라가 없다는 것이다. 그리고 메타 데이터만으로 ETL작업이 가능하기 때문에 원본 데이터의 변경 및 변경 데이터의 저장을 위한 별도의 저장소가 필요가 없다.(왜 데이터 파이프라인이 복잡해졌을때 Glue라는 서비스가 유용한지 이해가 잘 안됐었는데, 원본 데이터의 사용없이 메타 데이터만으로 ETL작업이 가능하다는 점에서 Glue라는 서비스는 별도의 파이프라인을 통하지 않고 메타 데이터만으로 분석이 가능하도록 해주는 아주 유용한 친구라는 것을 다시 개념정리를 하면서 알게 되었다.)스케쥴링 기능으로 주기적인 작업을 실행하고 자동화할 수 있으며, 북마크 기능으로 작업상태를 저장하여 중단된 시점부터 작업 재개하는 것이 가능하고 작업에 대한 모니터링 또한 지원을 한다. AWS Glue에 대해서 알아보기기능 : ETL Workflow를 정의하고 관리할 수 있다.[세부 서비스] Data Catalog : AWS Glue 데이터 Catalog는 영구적 Meta Data Store이다.(가격이 저렴) AWS GLUE Data Catalog는 Amazon Redshift, S3, RDS, EC2 기반의 데이터베이스에 저장된 데이터에 대한 metadata 정보를 담고 있다.이렇게 담겨진 정보는 adhoc하게 분석을 할 수 있도록 도와주는 AWS Athena 분석툴이나 좀 더 심층분석이 가능한 Amazon Redshift Spectrum, Amazon EMR 서비스들을 통해서 AWS GLUE DATA CATALOG에 접근해서 ETL 분석을 할 수 있다. AWS Glue Crawlers : 리포지토리에서 데이터를 스캔하고 분류, 스키마 정보를 추출 및 AWS Glue Data Catalog에서 자동적으로 Metadata를 저장하는 Crawlers를 설정할 수 있다.(S3에 저장되어있는 데이터를 SQL로 분석을 한다는 것을 이해하기 위한 실습) S3에 있는 데이터를 쉽게 분석할 수 있다는 이점이 있었지만, 서비스 --- 서비스 사이에는 IAM을 생성해서 IAM을 통해서 Glue Crawler에서 S3에 접근을 하도록 한다. 접근을 해서 S3에 있는 object value file을 읽어오도록 한다.그리고 읽어 온 값을 Glue Data Catalog에 저장을 시킨다. RDS, Redshift에 있는 값은 JDBC를 통해서 metadata를 가져올 수 있다. S3에서 가져온 값은 직접적인 query가 가능하고, RDS, Redshift는 단순 metadata값만 참조가 가능하다. AWS Glue ETL 연산 : AWS Glue Jobs System, Trigger 기능","link":"/2022/04/25/202204/220425-data-pipeline-study-2/"},{"title":"220424 데이터 파이프라인 스터디 13일차 (Zeppelin notebook을 사용한 분석 및 Spark로 가공한 데이터를 Amazon RDS(MySQL)에 저장)","text":"이번 포스팅에서는 저번 시간에 이어서 Zeppelin을 사용해서 데이터를 처리실습을 한 내용을 정리해볼 것이다.Zeppelin은 Interpreter로, 다양한 Interpreter를 제공해준다고 했는데, Spark도 그 중에 하나로 포함이 되어있다. 그래서 이전 시간에 실습했던 것처럼 s3 bucket에 있는 csv format의 파일을 spark.read에 여러 option을 붙여서 DataFrame 객체로써 저장을 할 수 있었다. 이 Apache Zeppelin에서 DataFrame을 조작해보면서 느낀점은 이전에 해봤던 Pandas의 DataFrame 조작과 매우 비슷한 부분이 많다는 것이다. Spark로 만든 DataFrame 변수로는 View Table을 정의할 수 있는데, 이 임시로 정의한 View Table을 활용해서 SQL query를 실행시킬 수도 있다. 그리고 DataFrame format의 데이터를 JSON format의 데이터로 S3 bucket에 저장을 할 수 있으며, S3에 저장된 JSON format의 데이터를 spark.read.json()으로 다시 DataFrame 형식으로 불러올 수 있다. 이정도의 내용을 저번 시간에 간단하게 실습을 해보았다. TextFile 변환JSON, Parquet, CSV의 경우에는 기본적으로 DataFrame으로 만들기 쉬운 골격을 가지고 있는 반면에, 텍스트 파일 형태의 데이터의 경우에는 DataFrame으로 만들기 위해서 많은 작업이 필요하다. (비정형 데이터 -&gt; DataFrame) 이번 시간에는 TextFile을 DataFrame으로 변환하는 부분부터 실습해보겠다. [Text File 읽기]1234567import spark.implicits._import org.apache.spark.sql.Encoder val log1=spark.sparkContext.textFile(&quot;s3://[s3 bucket name]/[...path]/logs/*.gz&quot;)// 10개의 항목을 가져와서 foreach로 출력 log1.take(10).foreach(println) [비정형 데이터의 Line Parser method 작성]아래의 데이터는 sample 텍스트 데이터(비정형 데이터)를 읽고 출력했을 때의 결과값이다.마지막 |(pipe)의 JSON 값은 새롭게 추가/삭제되는 요소에 대한 정보를 참조하기 용이한 JSON format의 데이터로 보낸 것이다. 1234567891011121735166793836|328ff2fd-b86b-49ea-a26e-14309dafd5f1|1aa50dae-ab5e-4bea-978b-f766bb1bb68f|oneroom_filter_results|ZigbangApp/4.13.22 (com.chbreeze.jikbang4a; build:239; Android 7.0; SM-J727S; RELEASE)|101.235.133.71| 1000dd3db588ea6703bea25be752350cf735e74e98e71dd01e237ecc1d66cc8d52a35ed 1d5b9609d13401e2085b797a70a5c7423bed06b2a05ee8767fb84bd55| 1516949292197|2018-01-26 15:48:13|{&quot;screen_name&quot;:&quot;원룸 필터&quot;, &quot;room_type&quot;:&quot;one_open,one_double,three&quot;,&quot;rent_max&quot;:-1, &quot;deal_type&quot;:&quot;rent&quot;,&quot;deposit_max&quot;:-1,&quot;rent_min&quot;:0, &quot;activity_name&quot;:&quot;SearchRoomActivity&quot;,&quot;park_only&quot;:0,&quot;manage_cost_in&quot;:0, &quot;floor_option&quot;:&quot;ground,underground-rooftop&quot;, &quot;previous_activity&quot;:&quot;ListItemsAdActivity&quot;,&quot;deposit_min&quot;:0} 이제 마지막 추가/삭제되는 parameter 용도의 JSON 데이터를 제외한 부분을 DataFrame format의 데이터로 transform해주기 위한 함수를 작성해줘야 한다. 아직 scala에 대한 구체적인 문법은 모르지만, 대략적으로 현재 작성되는 코드가 어떤 부분을 위한 작업인지는 알 수 있기 때문에, 차후에 scala 문법을 배우면서 코드를 다시 작성해보기로 하자.scala로 작성되어있는 코드는 Python 코드로 작성을 해보자. 123456789101112131415161718192021222324252627282930313233343536373839import org.json4s._import org.json4s.jackson.JsonMethods._import org.json4s.JsonDSL._// class를 정의해준다.case class Cflog(base_date: String, adid: String, uuid: String, name: String, timestamp: String, gtmTimes: String, screen_name: String, item_id: String, content_type: String, item_category: String, is_zb_agent: String, building_id: String, area_type_id: String, agent_id: String, status: String , button_name: String)def parseRawJson(line: String) = { // '|'(pipe)를 기준으로 각 line을 split해주고, val pieces = line.split(&quot;\\\\|&quot;) // 각 index 요소의 값을 별도의 변수에 담아 저장한다. val adid = pieces.apply(1).toString val uuid = pieces.apply(2).toString val name = pieces.apply(3).toString val timestamp = pieces.apply(8).toString val gtmTimes =pieces.apply(7).toString //JSON Parse val jsonString = pieces.apply(9).toString implicit val formats = DefaultFormats val result = parse(jsonString) var screen_name = (result \\ &quot;screen_name&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) var item_id = (result \\ &quot;item_id&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val content_type = (result \\ &quot;content_type&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val item_category = (result \\ &quot;item_category&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val is_zb_agent = (result \\ &quot;is_zb_agent&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val building_id = (result \\ &quot;building_id&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val area_type_id = (result \\ &quot;area_type_id&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val agent_id = (result \\ &quot;agent_id&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val button_name = (result \\ &quot;button_name&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val status = (result \\ &quot;status&quot;).extractOpt[String].getOrElse(&quot;NULL&quot;).replaceAll(&quot;nil&quot;, &quot;NULL&quot;) val base_date = timestamp.substring(0,10) Cflog(base_date, adid, uuid, name, timestamp, gtmTimes, screen_name, item_id, content_type, item_category, is_zb_agent, building_id, area_type_id, agent_id, status , button_name)} 데이터 분석을 하면서 Ganglia 모니터링하기 Zeppelin을 통해서 데이터 분석을 하면서도 중간 중간에 현제 workload가 어떻게 되는지, 현재 작업 노드로는 부하상태는 괜찮은지에 대해서 분석을 하면서 진행해야한다.위의 캡처를 보면 하단에 총 3개의 노드가 실행되고 있다. Master node, Core node, Task node 각 각 1개씩 할당이 되어있기 때문에 위와같이 총 3개의 노드가 모니터링 되고 있는 것이다.만약 클러스터의 부하에 문제가 생기면, 노드를 새롭게 추가(scale-out)해주는 등의 해결책을 내어서 문제를 해결하면서 데이터 분석작업을 이어나가야 한다. Spark + RDSSpark에서 가공한 데이터를 RDS의 MySQL에 저장하는 작업을 실습해볼 것이다. RDSAmazon Aurora MySQL 생성 데이터베이스 인스턴스를 생성하게 되면, region cluster 하위에 writer instance가 하나 생성이 됨을 확인할 수 있다. 생성된 database의 writer instance의 엔드포인트를 사용해서 Spark를 사용해서 전처리한 데이터를 RDS 데이터베이스에 저장을 할 수 있다. 이제 생성한 Amazon RDS EndPoint를 host로 해서 client program에서 cloud의 RDS DB에 접속을 한다.접속을 한 뒤에 DW(Data Warehouse) 개념의 DB 저장소를 하나 만들고, DM(Data Mart) 개념의 DB 저장소를 하나 만들어서 기본 환경구성을 해놓는다. 이제 Amazon EMR에서 가공한 데이터를 내보낼 target에 대한 setting이 끝났기 때문에 이제 다시 Amazon EMR에서 어떻게 전처리한 데이터를 target 지정을 해서 내보내는지에 대해서 알아보도록 하겠다. Amazon EMR에서 전처리한 데이터를 Amazon RDS로 내보내기","link":"/2022/04/24/202204/220424-data-pipeline-study/"},{"title":"220425 데이터 파이프라인 스터디 14일차 (Spark로 가공한 데이터를 Amazon RDS(MySQL)에 저장)","text":"이번 포스팅에서는 Amazon EMR(정확히는 EMR내의 Zeppelin의 Spark Interpreter를 통해서)에서 가공한 데이터를 내보낼 target에 대한 준비가 끝났기 때문에 이제 다시 Amazon EMR에서 어떻게 전처리한 데이터를 target 지정을 해서 내보내는지에 대해서 알아보도록 하겠다.뭔가 데이터 파이프라인 구축은 정말 말 그대로 구성할 파이프를 각 각 만들어놓고, 특정 파이프의 output을 다음 공정의 파이프의 input으로 넣어주는 과정을 만드는 것 같다는 느낌에 재미있는 것 같다. 물론 각 파이프를 구성하고 전체적인 파이프라인 구성도를 생각해내는 것은 아직도 연습이 많이 많이 필요하지만, 그래도 이런 흥미를 붙였다는 것에 앞으로의 공부에 있어 좋은 원동력이 될 것 같다.자 그럼 이제 본격적으로 Amazon RDS에 Spark로 가공한 데이터를 넣어주는 처리를 해보도록 하자. Amazon EMR에서 전처리한 데이터를 Amazon RDS로 내보내기과거에는 Zeppelin이 Maven의 기능을 그대로 가지고 있어서 필요한 Repo에 대한 설치를 쉽게 할 수 있도록 구성이 되어있었는데, 이 부분이 변경이 되었다. 따라서 jdbc mysql driver를 별도로 아래의 스크립트를 통해서 다운받아줘야한다. Spark의 라이브러리에 MySQL driver를 다운받아서 설치해줘야한다. 이 부분은 EMR의 “단계”탭에 새로 스크립트가 실행될 수 있도록 새로운 단계를 추가시켜주면 된다. 123#!/bin/bashwget https://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.40/mysql-connector-java-5.1.40.jarsudo mv ./mysql-connector-java-5.1.40.jar /usr/lib/spark/jars EMR의 단계 탭에 새로운 단계를 추가해준 후에는 EMR에 적용시켜주기 위해 Zeppelin과 Spark이 접속된 Session을 재기동시켜줘야 제대로 적용된다. Zeppelin session restartZeppelin은 Interpreter이기 때문에 접속 정보를 확인할 수 있는 부분이 있는데, 헤더의 최 우측 상단의 annoymous 메뉴 - Interpreter - Spark - restart RDS 읽어오기123456789val url = &quot;jdbc:mysql://[AWS RDS End Point]:3306/dw?characterEncoding=UTF-8&quot;val user: String =&quot;admin&quot;val pass: String=&quot;xxxxx&quot;// 접속 정보를 property를 통해서 설정을 해놓는다.val prop = new java.util.Propertiesprop.setProperty(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)prop.setProperty(&quot;user&quot;, user)prop.setProperty(&quot;password&quot;,pass) DB에 쌓을 데이터 Load하기123456// CSV 파일을 읽어서 DataFrame 형태의 데이터로 가공한다.val csvDF = spark.read .option(&quot;header&quot;, &quot;true&quot;) .option(&quot;delimiter&quot;, &quot;,&quot;) .option(&quot;inferSchema&quot;, &quot;true&quot;) .csv(&quot;[DB로 쌓을 CSV 데이터 파일 PATH (In S3)]&quot;) SQL Query를 돌릴 대상 view Table을 생성123456csvDF.createOrReplaceTempView(&quot;data_master_pq&quot;)%sql// 원하는 column 데이터만 추려서 테이블 출력select id, namefrom data_master_pq Spark SQL / RDS WriteSpark SQL로 전처리한 데이터 테이블을 DB에 table로 저장한다. overwrite : 덮어쓰기 (DBMS의 경우에는 기존의 Table을 삭제하고 다시 Table을 생성한다)append : 데이터 이어쓰기 12345678val table1 = &quot;data_master_pq&quot;val df = spark.sql(&quot;&quot;&quot; select id, name, from data_master_pq &quot;&quot;&quot;)df.write.mode(&quot;append&quot;).jdbc(url, table1, prop) 생성된 테이블을 보면, string에 해당되는 부분은 모두 text 타입으로 되어있는 것을 볼 수 있다.(Spark에서 쓸때는 text type으로 정의해서 사용한다.분석을 할때 column자체가 크기때문에 mysql자체가 느려질 수 있다.따라서 modify table을 통해서 column에 맞는 사이즈로 변경해주는 것을 권장한다.) RDS로부터 읽기Amazon RDS로부터 저장된 데이터를 읽어보자. 1234567891011121314// 반드시 alias를 붙여줘야한다. 아니면 에러발생val areaQuery = &quot;&quot;&quot;( select * from dw.data_master_pq a) a&quot;&quot;&quot;val dataMasterDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;,url).option(&quot;dbtable&quot;, areaQuery).option(&quot;user&quot;, user).option(&quot;password&quot;, pass).option(&quot;useUnicode&quot;, &quot;true&quot;).option(&quot;characterEncoding&quot;,&quot;utf8&quot;).load()// DataFrame 최종확인// Spark의 경우에는 action, transformation, function이 나눠져있기 때문에// compile과정에서는 DB에서 데이터를 가져오지 않고, show를 실행했을 때 비로소 실제// 데이터를 DB로부터 가져온다.dataMasterDF.show","link":"/2022/04/25/202204/220425-data-pipeline-study/"},{"title":"220425 Hadoop과 친해지기 열두번째 이야기(Spark와 RDD(회복성 분산 데이터))","text":"이번 포스팅은 Spark에 대해서 정리를 해보려고 한다. 데이터 파이프라인 실습에서 이미 Spark interpreter를 사용해서 데이터 전처리를 하고 있다. 그런데 아직 정확히 Spark에 대한 개념적 정의가 되지 않았기 때문에 이 부분에 대해서 정리를 하고 넘어가려고 한다. Spark ?Spark는 거대한 양의 데이터를 합리적인 방법으로 처리하고, 더 나아가 ML이나 Graph 분석 그리고 Data streaming 등의 멋진 기능을 포함하고 있는 친구이다.이전에 데이터 파이프라인 구축 실습에서 데이터 batch 처리(ETL)나 실시간 스트리밍 데이터를 처리할때도 모두 이 Spark라는 친구를 사용한다고 했으니, 정말 대단한 친구임에는 틀림이 없다.이처럼 Spark의 역량과 속도는 엄청나며, Java나 Scala, Python과 같은 실제 프로그래밍 언어를 사용해서 스크립트를 작성할 수 있는 유연성을 제공하고, 복잡한 데이터를 조작이나 변형, 분석할 수 있는 능력자이다. 앞서 실습해보았던 Pig같은 기술과의 차이점은 Spark 위에는 또 다른 엄청난 생태계가 존재한다는 것이다. 그 생태계에는 ML, 데이터 마이닝, 그래프 분석, 데이터 스트리밍과 같은 복잡한 일을 해결할 수 있다. 이처럼 Spark는 강력하고 아주 빠른 프레임워크이다. 다른 Hadoop 기반의 기술처럼 확장성도 가지며, Spark는 드라이버 프로그램(작업을 어떻게 진행할지 통제하는 스크립트)의 패턴을 따른다. Spark는 Hadoop위에서 작동할 수도 있지만, Hadoop 위에서 반드시 동작할 필요는 없고, 내장된 클러스터 관리자를 사용하거나 Mesos라는 클러스터 관리자를 사용할 수도 있다.어떤 클러스터 관리자를 사용하든지 범용 컴퓨터 클러스터 매니저(Spark, YARN)에 의해서 작업을 분배하게 되고, 데이터를 아래와 같이 병렬로 처리하게 된다. 각 작업 집행자들은 Executor프로세스라고 불리며, 각 각 Cache와 Task를 갖는다. Cache는 Spark 성능의 키(키의 일부분)이며, HDFS와 항상 접촉하는 디스크 기반 솔루션과 달리 Spark는 메모리 기반 솔루션이다. 따라서 Spark는 정보를 최대한 RAM에 유지하며 처리한다. (속도의 비결)또 다른 키로는 방향성 비사이클 그래프인데, MapReduce와 비교하면 메모리 내에서 작동할때 최대 100배까지 빠르고 디스크 액세스에 제한되면 10배까지 빠른 속도를 낸다. MapReduce로는 Mapper와 Reducer의 관점을 생각해야 되기 때문에 할 수 있는 일이 제한이 되지만, Spark는 프레임워크로 제공해서 이러한 고정적인 관점의 단계를 건너뛸 수 있다.원하는 결과가 무엇인지와 프로그램에 좀 더 집중할 수 있다는 장점을 가지고 있다. 이전에 TEZ와 Pig를 같이 사용했을 때 방향성 비사이클 그래프에 대해서 언급을 했었는데, Spark에서도 같은 것이 내장되어있기 때문에 Spark도 최종 결과로부터 시작해서 거꾸로 돌아가면서 작업의 흐름을 최적화하고 최정결과에 다다르는 가장 최적의 방법을 계산한다. 거대한 데이터 세트와 현실의 문제해결많은 거대 기업들이 거대한 데이터 세트를 활용해서 현실의 문제점들을 해결하기 위해 노력하고 있다. 이러한 노력의 과정속에서 사용되고 있는 프레임워크가 바로 Spark인데, Amazon, eBay, NASA, Yahoo등의 많은 기업들이 이미 Spark를 사용하고 있다.Spark는 단 몇 줄의 코드만으로 클러스터에 굉장히 복잡한 분석을 할 수 있다는 매력을 가지고 있다. RDD(Resilient Distributed Dataset, 회복성 분산 데이터)회복성 분산 데이터란, 기본적으로 데이터 세트를 나타내는 객체이다. 그리고 RDD객체에 다양한 함수를 사용해서 이를 변형하거나 reducing하고 분석해서 새로운 RDD를 생성할 수 있다. 여기서 말하는 RDD 데이터 구조가 바로 스파크의 기본 데이터 구조이다.따라서 입력 데이터의 RDD를 가져와서 변형하는 스크립트를 작성하는 것이 보통이다.2016년에 최초로 출시된 Spark2.0에는 RDD위에 데이터 세트를 생성할 수 있도록 만들어졌다. SQL을 의식한 RDD개발이었지만 결국 RDD를 중심으로한 구축이 되었다. Spark Core내의 RDD에 프로그래밍을 하면, Spark 위에 구축된 라이브러리 패키지가 있어서 함께 사용이 가능하다. Spark Streaming도 그 패키지들중에 하나인데, 데이터를 일괄 처리(Batch processing)하는 대신에 실시간(RealTime processing)으로 데이터를 입력할 수 있다. Spark Streaming의 매력웹 서버 여러 대가 운영 중이라고 가정해보면, 동시 다발적으로 배출되는 로그 데이터를 실시간으로 가져와서 일정 시간 안에 분석을 하고, 데이터베이스나 NoSQL 데이터 스토어에 그 결과를 저장할 수 있다.그리고 위의 일련의 처리들을 Spark Streaming에서 단 몇 줄의 코딩으로 처리할 수 있다. Spark SQLSpark SQL은 Spark의 SQL 인터페이스로 데이터에 그냥 SQL 쿼리를 작성하거나, SQL 유사함수를 사용해서 Spark의 데이터 세트를 변환할 수 있다.이것이 앞서 언급하였던 Spark의 방향성이며, Spark SQL를 활용해서 데이터 세트를 최적화를 하게 되면, 방향성 비사이클 그래프 이상의 최적화를 가능하게 한다. MLLibMLLib도 흥미로운 기술인데, Spark의 데이터 세트에 실행하는 ML이나 데이터 마이닝같은 도구의 라이브러리이다.대략적인 클래스만 만들어놓으면 Spark MLLib이 데이터의 의미를 추출해서 회귀 분석이나 클러스터링 같은 머신 러닝 문제를 해결해준다. (아직은 잘 모르겠지만 일단 이런 라이브러리가 있다는 것만 알아두자) GraphX그래프 이론에서 사용되는 그래프인데, 사람과 사람간의 촌수 관계를 연결해서 계산을 할 수도 있고, 그 외에 재미있는 것들을 할 수도 있다. Python과 ScalaPython은 테스트용으로 사용하기에 무난하지만 결국 Scala로 옮겨가게 될 것이라고 한다. Spark를 개발한 언어가 바로 Scala이고, Spark의 작동방식을 구성한 기능적인 프로그래밍 모델이기 때문이다.Scala는 훨씬 빠르고 안정적인 성능을 가지고 있는데, Python처럼 오버헤드가 필요가 없으며, Scala는 Java의 바이트 코드에서 바로 컴파일을 하기 때문에 효율적이다. Python code12nums = sc.parallelize([1,2,3,4])squared = nums.map(lambda x:x*x).collect() Scala code12val nums = sc.parallelize(List(1,2,3,4))val squared = nums.map(x=&gt;x*x).collect() RDDRDD는 Resilient Distributed Dataset의 약자로, Spark 내에서 발생하는 클러스터내에서의 작업 분산과 작업 중간에 발생하는 실패에 따른 회복이 가능함을 나타내는 용어이다.실제 사용자에게는 데이터 세트로 출력을 해주기 때문에 사용자가 고려할 것은 많지 않으며, RDD 객체는 키-값 정보나 일반적인 정보를 저장하는 수단이며 클러스터가 알아서 작업을 해준다. RDD의 생성(1) 드라이버 프로그램이 SparkContext라는 것을 생성SparkContext는 Spark 환경 내에서 드라이버 프로그램이 작동하는 환경이자 RDD를 만드는 주체이다. (2) 새 RDD에 데이터 텍스트파일 호출텍스트 파일을 호출하면, RDD의 모든 행을 입력 데이터로 가진 텍스트 파일을 갖게 된다. 12# file://, s3n://, hdfs://sc.textFile(&quot;file:///c:/users/frank/gobs-o-text.txt&quot;) (3) hiveContext 사용hiveContext를 사용해서 SQL 쿼리를 사용할 수 있는데, 아래와 같은 형태로 할 수 있다. 123hiveCtx = HiveContext(sc)rows = hiveCtx.sql(&quot;SELECT name, age FROM users&quot;) (4) 그 외 convert 가능한 data format들 JDBC Cassandra HBase Elasticsearch JSON, CSV, sequence files, object files, 다양한 압축 포멧들 (5) RDD Transforming하기 map flatmap filter : RDD에서 데이터를 빼낼때 사용 distinct sample : 무작위 표본을 만들때 사용 union, intersection, subtract, cartesian RDD 조작 예시map 예시123rdd = sc.parallelize([1,2,3,4])squaredRDD = rdd.map(lambda x:x*x)# output : 1, 4, 9, 16 RDD actions (1) collect : RDD의 데이터를 가져와서 Python 객체를 반환하고 Python 드라이버 스크립트를 사용해서 출력하거나 텍스트 파일로 저장하는 등의 작업을 할 때 사용 (2) count : RDD에 얼마나 많은 행이 있는지 세어준다. (3) countByValue : 각 각의 고유 값이 RDD에 얼마나 있는지 총계를 내준다. (4) take : 특정 시점에 RDD에 뭐가 있는지 재빠르게 확인하거나 디버깅할 때 유용하게 사용된다. (5) top : 예) 상위 10개의 결과를 구하는데 사용된다. (6) reduce : MapReduce에서 Reducing작업처럼 각 고유키와 관련된 값들을 집계하는 함수들을 정의한다. … and more …Spark에서는 위의 명령어가 호출되기 전까지는 아무 일도 일어나지 않는다. 다시말하면, RDD에 이 함수를 호출하기 전까지 드라이버 스크립트에서 아무것도 실행하지 않는다는 것을 의미한다.스크립트가 실행되면 비로소 종속성 관계를 통해 가장 빠른 경로를 찾아내고 그 후에 가장 빠른 경로를 찾아낸다.그 다음에 최종적으로 클러스터에서 작업을 시작한다. 평균 평점이 제일 낮은 영화 찾기123456789101112131415161718192021222324252627282930313233343536373839404142# SparkConf와 SparkContext를 불러와서 RDD를 생성하고, 작업하는데 필요한 것을 준비from pyspark import SparkConf, SparkContextdef loadMovieNames(): movieNames = {} with open(&quot;ml-100k/u.item&quot;) as f: for line in f: fields = line.split('|') movieNames[int(fields[0])] = fields[1] return movieNamesdef parseInput(line): fields = line.split() # u.data의 각 line을 (movieID, (rating, 1.0)) tuple 데이터로 convert return (int(fields[1]), (float(fields[2]), 1.0))if __name__ == &quot;__main__&quot;: conf = SparkConf().setAppName(&quot;WorstMovies&quot;) sc = SparkContext(conf = conf) # movie name lookup table movieNames = loadMovieNames() lines = sc.textFile(&quot;hdfs:///user/maria_dev/ml-100k/u.data&quot;) # Convert to (movieID, (rating, 1.0)) movieRatings = lines.map(parseInput) # reduce to (movieID, (sumOfRatings, totalRatings)) ratingTotalsAndCount = movieRatings.reduceByKey(lambda movie1, movie2: ( movie1[0] + movie2[0]), movie1[1] + movie2[1]) # Map to (movieID, averageRating) averageRatings = ratingTotalsAndCount.mapValues(lambda totalAndCount : totalAndCount[0] / totalAndCount[1]) # sort by average rating sortedMovies = averageRatings.sortBy(lambda x: x[1]) results = sortedMovies.take(10) # print for result in results: print(movieNames[result[0]], result[1]) 123#spark-submit shell을 사용하면 Spark환경을 구성하고#로컬 머신의 단일 프로세스가 아닌 클러스터 전체에 걸쳐 실행하도록 할 수 있다.$spark-submit LowestRatedMovieSpark.py","link":"/2022/04/25/202204/220425-hadoop_pig_bigdata_class/"},{"title":"220426&#x2F;27 데이터 파이프라인 스터디 15&#x2F;16일차 (Glue의 Crawler&#x2F;Glue를 활용한 분석 Table 생성)","text":"이번 포스팅에서는 AWS Glue의 Crawler에 대해 실습한 내용을 정리해보려고 한다.데이터에 대한 트랜스폼 (CSV-JSON, JSON-PARQUET) 실습을 했는데, 쌓여있는 데이터를 크롤링을 통해서 glue에 있는 data catalog에 등록하는 작업을 실습해보고, 이렇게 Data Catelog에 쌓인 metadata를 분석하기 위해서 Atena를 사용해서 조회해보는 것 까지 실습해본다. AWS Glue 데이터베이스 추가테스트용 데이터베이스를 AWS Glue 하위의 데이터베이스에서 추가해준다. 데이터베이스 내의 테이블이 초기 생성에서는 비어있는 상태인데, 크롤러 추가 버튼과 함께, 크롤러를 통해 DB내의 테이블을 생성할 수 있음을 알 수 있다. 크롤러 추가(1)Crawler source type / (s3에서 할 것이기 때문에) Data stores 선택 (2)Repeat crawls of S3 data stores / Crawl all folders 선택 (3)데이터 스토어 추가 데이터에 대한 transform (CSV-JSON, JSON-PARQUET) 실습을 했을 때 적재했던 S3의 폴더의 경로를 지정하면 된다. (4)IAM 역할 선택 S3에 저장된 데이터에 Glue의 Crawler가 접근해서 가져오는 것이기 때문에 IAM 권한 설정이 필요하다.(IAM 역할 생성)AWSGlueServiceRole - crawlerclass (5)크롤러의 일정 생성S3에 주기적으로 데이터가 올라오는 것이면, 빈도에 대한 설정이 필요하다. 크롤러를 실행하게 되면, 해당 s3폴더로 가서 파일을 다 읽어서 유형이 어떤 타입이고, 무엇을 판별하고, 각 칼럼에 어떤 값이 있는지 읽어온다. 위와같이 크롤러를 실행한 뒤에 3개의 새로운 테이블이 생성되었음을 확인할 수 있다.(JSON/PARQUET format 데이터의 테이블의 경우에는 크롤러가 column정보를 쉽게 읽을 수 있다. 반면에 csv format 데이터의 테이블의 경우에는 column정보가 col1, col2...로 정의가 되어있음을 알 수 있다.) Parquet는 데이터에 대한 데이터 타입까지 다 가지고 있기 때문에 좀 더 빠르게 데이터 값을 읽을 수 있다. 12입력 형식 org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat출력 형식 org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat Partitioning파티션 값도 Data store에 저장이 되는데, 파티션 값이 업데이트가 되지 않으면, 데이터를 제대로 조회할 수 없다. 이 부분에 대해서는 좀 더 자료 찾아보기 아직은 좀 이해가 되지 않는 부분이 있다. 날짜별로 Partitioning이 필요한 이유 등.. 이 부분에 대해서는 아래 부분에서 추가 학습을 통해 알게 되었다. Catalog Table을 생성할때 PARTITIONED BY (base_date) 옵션을 줘서 생성하게 되면, 날짜를 기준으로 폴더가 생성되고, 그 하위에 생성된 파일이 위치하게 되는데, 읽을 때에는 빠르게 읽을 수 있다는 장점이 있지만, 일괄적으로 저장할때에는 느릴 수 있다는 단점이 있다.각 파티션 값은 AWS Glue 서비스의 데이터 카탈로그 테이블에서 확인할 수 있으며, 파티션 값만 따로 관리하는 메뉴도 있다.그리고 나중에 조회를 할 때 조건을 주면 해당되는 날짜의 폴더에 있는 데이터만 호출해서 읽을 수 있다. (SQL의 퍼포먼스를 높일 수 있는 조건)Redshift spectrum, Athena통해서 조회할때도 파티셔닝을 주게되면, 해당되는 데이터만 읽어오기 때문에 조회할때 금액적으로 저렴하게 처리할 수 있다. Amazon Athena이제 Crawler가 S3에 있는 데이터를 가져다가 AWS Glue Data Catalog에 저장을 했다. 이제 Glue Data Catalog Server는 Metadata repository로써 Amazon Athena와 같은 분석 툴을 사용해서 분석을 할 수 있다. Athena는 query를 실행했을 때 Loading 양에 따라서 charging이 되는 방식으로 요금이 부과된다.Athena의 엔진은 Presto를 사용하고, 최근에는 기본적인 ML을 수행할 수 있게 Query에서 처리할 수 있게 업그레이드 되었다. Athena query result output settingsAthena에서의 쿼리 실행결과를 담을 공간에 대한 setting을 초기에 해줘야한다. UI 구성 살펴보기데이터 원본으로 AwsDataCatalog이고, 데이터베이스는 이전 테스트 실습을 위해 생성한 이름으로 default 설정된 것을 확인할 수 있었다.그리고 테이블은 Crawler를 통해 추가 생성된 테이블이 리스트업된 것을 확인할 수 있다.각 테이블을 통해 어떤 데이터를 가지고 있는지 query를 통해 조회가 가능했다. (2022/04/27 업데이트) 데이터 파이프라인의 전체 flow 잡고 들어가기(flow1)우선 S3에 저장된 Bronze 데이터를 EMR에 있는 Spark를 활용해서 전처리를 한 다음에 Sliver 데이터의 형태로 변환을 한다. 변환된 데이터의 경우에는 과거에는 RDS에 저장을 하였으나, 요즘에는 S3에 저장을 하고, S3에 저장을 할 때에는 기본 metadata는 앞서 실습을 했듯이 AWS Glue Data Catalog라는 곳에 테이블을 정의 및 데이터를 저장해서 sliver 데이터를 관리하도록 할 것이다. (flow2)생성된 sliver 데이터를 기준으로, EMR의 Spark를 활용해서 RDS나 Redshift에 Gold 데이터를 저장하는 형태로 flow가 진행이 된다.최근에는 Gold 데이터도 S3에 저장을 한 다음에 Athena와 같은 분석 tool을 활용해서 분석을 한다. AWS Glue를 활용한 분석 Table 생성 위에서 표시한 부분은 AWS Glue Option으로, Catalog setting에 대한 옵션이다. metastore의 역할을 하는 data catalog를 활용을 할 것인데, 외부 DB에 metastore를 운용하는 경우도 있는데, 관리 point를 줄이기 위해서 Glue에 있는 data catalog를 사용하는 것을 권장한다. Hive table(Hadoop의 metastore 기능) 선언 후 데이터를 넣을 때 필요한 옵션선언한 Hive table에 데이터를 insert해서 넣을 때 필요한 옵션이다.Dynamic partitioning(여러 폴더가 있으면 각 폴더에 데이터를 나눠서 넣을 수 있도록 하는것) 아래와같이 SparkSession에 option을 추가설정해준다. 123456789val spark: SparkSession = SparkSession .builder() .appName(&quot;StatsAnalyzer&quot;) .enableHiveSupport() .config(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;) .config(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;) .getOrCreate() log file 내의 내용을 확인하고 parsing하는 function이 별도로 필요한 경우에 function을 짜지만, 파일의 사이즈가 커지더라도 JSON format으로 파일을 저장해주는 것이 관리적 측면에서 좋다. 12logsDFAll.createOrReplaceTempView(&quot;logs&quot;)sqlContext.cacheTable(&quot;logs&quot;) 위의 코드는 Ganglia의 Cache 영역의 표시를 확인할 수 있는데, 이 부분에 데이터를 올릴때, 옵션을 따로 줄 수 있는데, 이 캐시 주는 부분은 데이터 프레임 자체에 캐시를 주거나 위의 코드와 같이 view table 자체에 캐시를 주는 경우가 있다. caching을 하면, S3에 있는 데이터를 대해서 캐시로 로드를 해야한다. 이러한 이유로 아래와같이 count(*)로 일괄적으로 캐시 데이터를 로드해준다. (캐싱하려는 데이터가 master성 데이터라면, 캐싱하는 과정을 통해서 이후 과정이 수월하게 진행될 수 있게 하는 것이 좋다) 123%sqlselect count(*)from logs 왜 Zeppeling의 Spark SQL을 통해서 전처리한 데이터를 단순 테이블 형태 말고도 각종 그래프 형태로 시각화시켜서 시계열 그래프로 데이터의 기간별 추이를 살펴볼 수 있다. Parquet 포멧으로 S3에 저장 및 확인metaStore가 없던 시절에는 Parquet 포멧 형태로 데이터를 저장해서 많이 사용되었었다. 1234567891011121314151617val dataDF = sqlContext.sql(s&quot;&quot;&quot; select base_date, uuid, timestamp as base_date, building_id as build_id from logs where category = 'apt'&quot;&quot;&quot;)// Parquet 형태로 s3에 저장한다.dataDF.write .mode(&quot;overwrite&quot;) .format(&quot;parquet&quot;) .save(&quot;s3://[S3 Bucket Path]/silver/parquet-test/&quot;)val danji_viewDF = spark.read.parquet(&quot;s3://[S3 Bucket Path]/silver/parquet-test/&quot;)dataDF.show 하지만 위와같이 파일을 쌓는 방식은 File I/O가 발생하기 때문에 속도면에서 많이 느리다는 단점이 있다. Hive Table(Hadoop의 metastore 기능)을 생성하고 삽입하는 방식(방식1) Zeppelin에서 SQL Query로 Hive테이블 추가 - AWS Glue에 Table 정의Spark data catalog로 연결을 해놓은 상태이기 때문에 아래와같이 테이블을 쿼리로 생성해도 생성이 된다. Storage를 하둡이 아닌 외부(EXTERNAL)에 저장을 하기 위해서 아래와 같이 CREATE EXTERNAL로 정의한다. 12345678910111213%sqlCREATE EXTERNAL TABLE class.data_view_silver( base_dt timestamp, data_id int, uuid string)PARTITIONED BY ( base_date date )STORED AS PARQUETLOCATION 's3://[s3 bucket name]/silver/'tblproperties (&quot;parquet.compress&quot;=&quot;SNAPPY&quot; ,&quot;classification&quot;=&quot;parquet&quot;); PARTITIONED BY 옵션은 날짜별로 폴더가 생성이 되고, 그 하위에 생성된 파일이 위치하게 된다. (속도있게 읽기 위한 옵션/단, 저장할때는 느릴 수 있다) 생성된 Hive table은 AWS Glue 서비스의 데이터 카탈로그 테이블에서 확인할 수 있다. 열 이름 중에는 Partition 키의 역할을 해주는 칼럼이 있는데, 이 파티션을 통해 등록된 내용을 통해서 데이터가 참조된다. INSERT 명령을 통해서 생성한 테이블에 데이터 입력하기1234567891011sqlContext.sql(s&quot;&quot;&quot; insert overwrite table class.data_view_silver PARTITION(base_date) select base_date, uuid, timestamp as base_date, building_id as build_id from logs where category = 'apt'&quot;&quot;&quot;) Parquet로 데이터 파일을 추출하는 것보다 위와같이 생성한 메타데이터 테이블에 데이터를 넣고 관리하는 것이 속도면에서 빠르고 효율적이다. (방식2) AWS Glue 서비스에서 직접 수동으로 테이블 추가(방식3) Athena에서 query를 통해서 테이블을 추가(결론) 전처리한 데이터를 Parquet 포멧으로 S3에 저장하고 저장된 데이터를 DataFrame 형식으로 호출해서 사용하는 방식과 HiveTable을 생성(target 테이블은 Glue의 catalog table)하고, metadata 형태로 데이터를 저장해서 호출하는 방식을 비교했을때, 후자의 방식이 더 효율적임을 알 수 있었다. Glue의 스키마에 설명 포함시키기메타정보를 관리할때 테이블 스키마의 설명정보를 관리하는 것이 좋다. (한글명/부가설명) Sliver data를 통해서 Master성 데이터와 Gold 데이터로 정제하기정형화된 데이터를 Data Catalog에 정의를 하고, Master성 데이터를 DBMS에 별도로 구성(ODS, Operation Data Store)해서 넣는다.이렇게 따로 DBMS에 구성한 마스터성 데이터와 정제한 데이터를 잘 조합해서 다른 GOLD Data/DM(시각화가 가능한 형태의 데이터)로 만들어서 저장을 한다. RDS에 Gold 데이터를 저장123456789101112131415val localDF = sqlContext.sql(s&quot;&quot;&quot;select a.base_date, a.idx_id, b.b_name, count(*) as total_countfrom class.data_silver ajoin data_master bon a.idx_id = b.idgroup by a.base_date, a.idx_id, b.b_name&quot;&quot;&quot;)val tableName = &quot;dm.data_gold&quot;localDF.write.mode(&quot;append&quot;).jdbc(url, tableName, prop) 특정 시간에 Zeppelin notebook을 실행시켜줄 수 있도록 설정(cron job) 우선 어플리케이션은 기본적으로 master node에 설치가 되어있기 때문에 master node로 접속을 한다. 1 yum update를 하게 되면 라이브러리가 업데이트가 되서 문제가 될 수 있으니, 업데이트 하지 않는 편이 좋다. 12345678910111213$cd /usr/lib/zeppelin/conf$$$ ls zeppelin-*zeppelin-env.sh zeppelin-env.sh.template zeppelin-site.xml zeppelin-site.xml.template$ sudo zeppelin-site.xml# zeppelin-site.xml 파일에 cron에 대한 설정 부분이 주석처리되어있기 때문에 이 부분의 주석을 cron.enable의 property의 상단으로 올려준 다음에 enable의 false 부분을 true로 변경시켜줘야한다.# 변경한 뒤에는 zeppelin 서비스를 재기동해준다.$ sudo systemctl stop zeppelin$ sudo systemctl start zeppelin# zeppelin 프로세스 실행 확인$ ps -ef|grep zeppelin 이제 Zeppelin 노트북에서 노트북 파일을 열면 우측 상단에 시계모양의 아이콘이 보이게 되고, 해당 아이콘으로 cron 설정을 해줄 수 있다.정해진 시간 간격으로 Zeppelin notebook에 작성한 script를 통해서 데이터 분석을 실행 할 수 있다. 그 외의 데이터 분석 방법방법1) S3에 데이터를 놓고, EMR을 띄워서 SQL을 통해서 데이터를 분석방법2) Athena를 통해서 Adhoc하게 데이터를 분석방법3) Redshift spectrum 통해서 deep dive하게 데이터를 분석(아직 해보지 않은 방법)","link":"/2022/04/26/202204/220426-data-pipeline-study/"},{"title":"220426 개인학습 회고 - 인출연습의 필요성","text":"이제 데이터 엔지니어와 관련된 학습을 한지 한 달 정도 된 것 같다.한 달하고도 하루 전쯤인 3월 25일에 데이터 엔지니어로 입사를 하려면 어떤 것을 준비해야되는지, 조건에 대해서 조사를하고 리스트업을 해서 공부방향을 잡았었다. 물론 지금도 지식이 많이 부족하지만, 그때는 뭔가 배경지식도 없었고, 뭔가 추상적이고 범접하기 어려운 분야라고만 생각을 해서, 우선 필요한 배경지식을 최대한 쌓아보자는 생각으로 지금까지 달려왔던 것 같다.그렇다고해서 단순히 수동적으로 학습을 했다는 이야기는 아니다. 능동적으로 관련 문제들도 인터넷에서 찾아서 풀어보고, 손코딩도 해보고 블로깅을 해보면서 공부했던 내용도 정리를 해보고, 정리했던 내용을 다시 보면서 복습도 했다.그런데 지금 한 달이 된 이 시점에서 학습하는 방법에서의 변화가 필요하다는 것을 느꼈다. 이제는 어느정도 학습을 하면서 데이터 엔지니어 분야와 관련된 기초적인 지식을 조금 쌓았기 때문에 이제는 수동적인 학습의 비율을 줄이고, 좀 더 능동적으로 내가 여지까지 학습했던 내용에 대해 다시 꺼내보는 연습을 하면서 앞으로의 학습을 진행하는 것이 좋을 것 같다는 생각이 들었다. (능동적인 학습에는 포트폴리오 준비를 위한 준비도 포함이 되어있다) 가끔은 수동적으로 인터넷에서 제공되는 강의 콘텐츠만을 보고 반복하면서 학습을 하다가 나도 모르게 안다고 착각하는 부분이 생기게 되는 것 같다.막상 배운 내용을 설명해보려고 하거나 몇 일 뒤에 세부 내용에 대해서 써보려고 하면 아는 것은 써도 안다고 생각했던 중요한 내용은 쓸 수 없는 경우가 많았다.이러한 익숙함과 배움을 착각하지 않기 위해 이제부터는 수동적으로 학습하는 방법을 적게 배치하고 앞으로 두 달동안은 능동적인 학습방법의 비율을 높여서 학습을 진행하려고 한다. 그래야 나중에 면접에 가서도 좀 더 자신있게 내가 학습한 내용에 대해서 대답을 하고 정리해서 말할 수 있을 것 같다. 주기적인 인출연습 = 부담되지 않는 선에서 시험보기부담이 없는 선에서 시험을 자주 보면서 능동적으로 주기적인 인출연습을 하는 것은 매우 중요하다고 한다. 이는 학습효과를 높이는데 매우 효과적이고, 평가의 목적이 아닌 내가 학습했던 내용에 대해 인출을 하기 위한 목적의 잦은 시험은 학습효과와 자신감을 높여주고, 불확실함에 대한 불안감은 낮춰준다. 인출연습 방법주기적으로 시험을 보는 방법 이외에도 배운 내용을 써보는 것과 직접 문제를 만들어보는 것, 그리고 다른 사람을 가르쳐보는 것이 가장 좋은 인출연습 방법이라고 한다.그 외에도 항상 수동적인 학습인 강의를 들을때도 끊임없이 자문자답을 통해서 자신이 이전에 학습했던 관련 입력 지식을 끄집어내려고 노력해야한다.심지어 책이나 내가 정리한 내용을 읽으면서도 중간 중간에 “방금 어떤 내용이었지?” “이 부분은 왜 이렇게 분류를 했었지?”와 같은 자문자답을 통해서 내가 어떤 부분을 알고 있고, 어떤 부분은 모르고 있는지 확인하는 과정을 거치면서 진행해야 한다. 복습 방법일단 이전에 배운 지식이 어느정도 망각이 된 상태에서 복습을 진행한다.해당 파트의 새로운 부분을 이어서 공부하기 전에 이전에 공부했던 파트의 내용을 자기만의 말로 바꿔서 빈 종이에 떠올르는대로 적고, 내가 이전에 정리했던 내용과 비교해서 이상한 내용은 없는지, 고쳐야 될 부분은 없는지 찾아서 교정을 한다. 아 그리고 포모도로라는 시간관리법도 있다는데, 25분 학습하고 5분 휴식하는 식으로 학습을 진행하면, 초두 효과와 최신 효과를 극대화 시킬 수 있다고 한다.아무튼 열심히 달려온 한 달, 오늘 하루는 좀 쉬어가는 하루로써 지난 한 달을 돌아보고, 내일부터는 앞서 정리한 학습방법이나 포모도로 시간관리법을 적용시켜서 새로운 마음으로 즐겁게 학습을 이어가야겠다.지금 이 공부습관이 앞으로 직장생활을 다시 시작했을때에도 꾸준히 이어갈 수 있는 좋은 습관으로 남을 수 있도록 좀 더 힘내서 노력해보자.","link":"/2022/04/26/202204/220426-memoirs/"},{"title":"220428 데이터 파이프라인 스터디 17일차(Amazon EMR기반 Presto 기본 개념 및 Presto-cli 실습)","text":"이번 포스팅에서는 수집한 데이터를 전처리 및 저장한 뒤에 gold 데이터를 활용하여 분석 및 시각화하는 실습을 하는 부분에 대해서 정리를 해보려고 한다.분석하기 편한 데이터 형태로 데이터를 변환한 다음에 해당 데이터를 시각화 작업을 통해서 분석작업을 한다.Presto는 HDFS와 S3를 비롯한 여러 데이터 소스의 처리를 할 수 있는 분산 SQL query engine이다.Presto는 Data Visualization Tool과 연동을 해서 작업을 할 수도 있다. [핵심] Presto를 사용해서 S3에 저장되어있는 Hive Table Data를 BI툴에 제공할 수 있다. Presto와 Tableau를 연동하여 Dashboard를 구성할 수 있다. Near Realtime 분석툴 ElasticSearch를 활용할 수 있다. 분석 및 시각화 전체 흐름EMR 내의 Spark을 통해서 정제된 데이터가 S3에 저장되고, 기본적으로 AWS에서 서비스를 운영하면 전반적인 데이터가 S3에 저장이 된다. 그리고 S3에 저장된 데이터에 대한 metadata 정보는 metastore인 AWS Glue에 저장이 된다. (순환구조) 분석가분들은 Glue에 저장된 meta정보를 기반으로 데이터 분석을 시작한다.분석을 할때에는 EMR내의 Presto cluster(cluster이기 때문에 복수 개의 노드에 거쳐서 iterative하게 분석이 가능)나 Athena(serverless 분석툴)를 사용해서 진행한다. (Presto와 Athena 둘 다 SQL 분석 지원)Tableau, Elastic search에서 가장 많이 사용하고 있는 Kibana를 통해서 시각화를 해 줄 수 있다. [STEP1] S3에 저장되어있는 물리적인 Hive Table Data를 Presto를 이용해서 BI툴(Tableau)에 제공 PrestoPresto는 분산형 SQL 쿼리 엔진으로, 빅데이터에 대한 빠른 분석을 위해 등장한 기술이다. 과거의 DBMS처럼 별도의 repository를 가지고 있지 않으며, 이기종 데이터간에 JOIN을 할 수 있는 기능을 제공한다. (ANSI 호환 SQL문을 실행) 위에서 언급했듯이, Presto는 cluster로, 여러 worker로 분할되어 작업이 진행된다. 작업에 대한 내용은 Presto Coordinator가 Hive MetaStore로부터 땡겨오는 구조로 되어있다.최초 접속은 Presto CLI를 통해서 Presto Coordinator에 접속하는 형태로 진행한다. Coordinator (=&gt; Master node) SQL 구분 분석 SQL query plan 어떤 작업 노드에 Job을 분류하고 실행할 것인지에 대한 전반적인 것을 조정 REST API를 사용해서 Worker 및 Client와 통신 작업자의 결과를 가져와서 최종 결과를 클라이언트에게 반환 Worker 작업 실행 및 데이터 처리 데이터를 가져오고 중간 데이터 교환 REST API를 사용하여 다른 작업자 및 Presto coordinator와 통신 Presto관련 용어ConnectorHive나 RDS와 같은 DBMS에 접속하기 위한 Connect Adapter로, Catalog의 물리적 정의 부분에 해당한다. CatalogConnector의 논리적 이름으로, 실제 SQL 조회시에 Catalog의 이름(논리적 이름)을 사용한다. Schema유사한 성격의 Table들의 그룹 Table행과 열로 구성된 데이터 Athena와 Presto 사용의 차이Athena도 Presto에서 실행되었던 SQL query를 실행시킬 수 있으며, serverless이기 때문에 전역에 필요한 배치처리를 돌리기에 좋은 환경을 제공한다.다만, Athena는 RDS의 데이터와 JOIN해서 분석하는 기능은 제공을 하지 않기 때문에 EMR의 Presto를 사용하는 것이 권장되며, EMR Presto는 computing을 사용한 시간을 기준으로, Athena는 데이터용량으로 비용을 charging하기 때문에 상황에 따라 적절하게 서비스를 선택해서 사용하는 것이 좋다. Presto cluster 생성Presto cluster를 생성하기 위해서 EMR cluster를 생성하는데, AWS Amazon EMR에서 기존에 클러스터를 생생했던 방식은 똑같지만 세부옵션 내역은 조금 차이가 있다. 소프트웨어 구성 : Ganglia, Zeppelin, Presto AWS Glue 데이터 카탈로그 설정 : Presto 테이블 메타데이터에서 사용 포함 전체 선택 이기종 DB간에 데이터를 테이블을 JOIN하는 경우를 위한 setting을 위해 mysql-driver 설치를 위한 &quot;단계 추가&quot; setting은 추가해주는 것이 좋다. Presto cli 접속1234567891011121314151617#presto의 bin folder 하위의 presto-cli executable file로 command 실행 (버전은 상이하므로 확인 필요)#/usr/lib/presto/bin/resto-cli~$./presto-cli-0.266.amzn.0-executable --server localhost:8889 --catalog hive --schema classpresto:class&gt; show tables;presto:class&gt; select * -&gt; from hive.class. -&gt; master_data_silver -&gt; ;#Data aggregationpresto:class&gt; select id, count(*) as v_count -&gt; from hive.class.master_data_silver -&gt; group by id -&gt; order by count(*) desc; Spark와 Presto의 데이터 query 방식은 서로 다르다.우선 Spark는 초기 메모리로 Cache를 하고, query 작업을 한다.반면에 Presto는 분산된 노드에 sql로 query를한 뒤에 데이터를 취합한다. (Tip: 실제로는 속도면에서 별로 차이가 안나지만, Bronze 데이터로 정제한 뒤에 Presto를 통해 데이터를 분석하면 좀 더 빠르게 데이터를 분석할 수 있다) (2022/04/29 업데이트) Presto-MySQL 연동Presto는 이기종간의 데이터를 JOIN해서 새로운 데이터 산출물을 만들어낼 수 있다. 따라서 이번 section에서는 Presto와 RDS 의 MySQL DB와 연동을 시키는 부분에 대해서 정리를 해볼 것이다. (computing 시간을 기준으로 비용이 책정되기 때문에 이부분에 유의해서 실습해보도록 하자) 123456$cd /etc/presto/conf#catalog folder 안에 정의를 하게 되면, presto가 기동할때 해당 정보를 끌고 온다.#mysql end point userid, passwd 정보 알고 있기#catalog folder로 이동 후 작업$sudo vi mysql.properties mysql.properties 파일내용1234connector.name=mysql #catalog nameconnection-url=[RDS END POINT]connection-user=adminconnection-password=[pssword] mysql DB를 읽기 위한 작업/etc/presto/conf 123456789$sudo vi config.properties#node-scheduler.include-coordinatgor=true 로 변경해주기#presto-server 재기동$ sudo systemctl stop presto-server$ sudo systemctl start presto-server$ /usr/lib/presto/bin/presto-cli-0.266.amzn.0-executable --server localhost:8889 --catalog mysql --schema dw 바로 presto prompt가 떠서 테이블을 출력해보려고 했는데, 생각보다 Presto 엔진을 재기동하는데 시간이 걸린다. 123456presto:dw&gt; show tables;Query 20220429_035026_00000_bnnvk failed: Presto server is still initializing$ use schema dm;$ show tables from mysql.dm; 123456789101112131415#[catalog name].[schema name].[table name]presto:dw&gt; select * -&gt; from mysql.dm.data_goldpresto:dw&gt; select a.base_data, -&gt; a.id, -&gt; b.name, -&gt; b.addr -&gt; b.count(*) as view_count -&gt; from hive.class.silver_data a -&gt; join mysql.dw.master b -&gt; on a.id = b.id -&gt; group by a.base_date, -&gt; b.name, -&gt; b.addr; 위의 query예시처럼 hive(data catalog)내의 테이블과 mysql(RDS master data) 테이블을 서로 JOIN해서 새로운 데이터의 형태로 가공할 수도 있다. Presto를 통해서 새로운 테이블을 만들 수는 있지만, 용도자체가 분석용 툴이기 때문에 추천하지 않는 방법이라고 한다. Zeppelin을 통한 Presto를 활용한 분석분석가에게는 console에서 데이터를 분석하는 것은 사용감에 불편함을 주기 때문에 Presto를 Zeppelin과 연동시켜서 분석하는 방법을 채택하기도 한다. [STEP1] Zeppelin 헤더의 우측 상단의 annoymous - [Interpreter] 선택 [STEP2] “Create” 버튼을 클릭해서 새로운 Interpreter로 Presto를 추가해준다.MySQL이나 Presto를 대부분 연동할때 JDBC Interpreter를 통해서 연동하기 때문에 CLI를 통해서 JDBC Interpreter를 설치하고, 다시 Zeppelin으로 돌아와서 작업한다. 12345678910111213141516171819$sudo su - #export JAVA_TOOL_OPTIONS=&quot;-Dzeppelin.interpreter.dep.mvnRepo=http://insecure.repo1.maven.org/maven2/&quot;#대부분의 zeppelin echo system은 아래 lib/zeppelin 하위에 설치가 되어있다.$cd /usr/lib/zeppelin#jdbc Interpreter inatallation$./bin/install-interpreter.sh --name jdbc --artifact org.apache.zeppelin:zeppelin-jdbc:0.10.0# Interpreter 설치 후 아래 STEP으로 Zeppelin에 Interpreter 설치하기# 1. Restart Zeppelin# 2. Create interpreter setting in 'Interpreter' menu on Zeppelin GUI# 3. Then you can bind the interpreter on your note#zeppelin 서비스 재기동$sudo systemctl stop zeppelin$sudo systemctl start zeppelin#zeppelin 서비스 프로세스 확인$ps -ef|grep zeppelin Zeppelin interpreter에 Presto Interpreter 추가1234567891011Interpreter Name : prestoInterpreter group : jdbc[Properties]default.url : jdbc:presto://localhost:8889default.user : hivedefault.driver : com.facebook.presto.jdbc.PrestoDriver[Dependencies] - 아래에 지정해준 드라이버를 참조해서 presto의 interpreter를 실행해준다.Artifact : /home/hadoop/presto-jdbc-0.225-SNAPSHOT.jar presto에 접속을 하기 위해서 driver를 설치해줘야한다. 1234$cd /home/hadoop$sudo wget https://github.com/gabrielrcouto//releases/download/0.225/presto-jdbc-0.225-SNAPSHOT.jar$sudo chown hadoop:hadoop presto-jdbc-0.225-SNAPSHOT.jar Zepplin에서 새로 노트를 추가해보면, Default Interpreter에 추가해준 presto Interpreter가 리스트되어있는 것을 확인할 수 있다.","link":"/2022/04/28/202204/220428-data-pipeline-study/"},{"title":"220428 Hadoop과 친해지기 열세번째 이야기(PySpark 실습) (작성중...)","text":"이번 포스팅에서는 RDD 객체를 DataFrame Dataset으로 convert하고, Spark SQL로 데이터를 전처리한 실습내용에 대해서 정리해보려고 한다. RDD =&gt; DataFrameRDD를 DataFrame으로 변경함으로써 Spark SQL을 통해 데이터를 쉽게 가공할 수 있다.아래는 샘플 코드로, RDD데이터를 spark.createDataFrame을 통해서 DataFrame으로 convert하고, convert된 DataFrame 객체를 활용해서 Spark SQL을 활용해서 groupBy, avg, count, join, orderBy, take, 등을 수행한다. 123456789101112131415161718192021from pyspark.sql import SparkSessionfrom pyspark.sql import Rowfrom pysql.sql import functions# Convert RDD to DataFramedf = spark.createDataFrame([RDD DATA])# movieID 기준으로 grouping하고, rating에 대한 평균값 column을 표시한다.averageRatings = df.groupBy(&quot;movieID&quot;).avg(&quot;rating&quot;)# 각 movieID에 대해서 rating을 count한다.counts = df.groupBy(&quot;movieID&quot;).count()# movieID, avg(rating), count 세 개의 column을 join한다.averagesAndCounts = counts.join(averageRatings, &quot;movieID&quot;)# top 10 results를 출력한다.topTen = averagesAndCounts.orderBy(&quot;avg(rating)&quot;).take(10)# Stop the sessionspark.stop()","link":"/2022/04/28/202204/220428-hadoop_pig_bigdata_class/"},{"title":"220501 데이터 파이프라인 스터디 20일차(ELK)","text":"이번 포스팅에서는 말로만 들어왔던 ElasticSearch에 대해서 학습한 내용에 대해서 정리하고, 실습한 내용을 정리해보려고 한다. 실습내용이전에 실습으로 EC2 인스턴스 3개를 두고, Producer에서 Logstash를 통해서 Twitter의 로그를 가져다가 Kafka(이하 메시지 큐)에 쌓았다가 Consumer로 넘겨주는 흐름으로해서 실습한 적이 있었다.이번 실습에서는 Amazon S3에 있는 데이터 정보를 LogStash(EC2)에서 뽑아서 Amazon OpenSearch Service(ElasticSearch)로 넘겨주고, 이를 Kibana라는 시각화 툴로 시각화까지 시켜주는 부분까지 실습을 해보려고 한다. (Amazon S3 -&gt;(Logstash(EC2))-&gt;Amazon OpenSearch Service -&gt; Kibana) Amazon Kinesis Firehose -&gt; Amazon OpenSearch Service(ElasticSearch)로 Direct로 데이터를 넣어줄 수 있다. (2022/05/01 업데이트) Amazon OpenSearch Service(ElasticSearch) 설정 및 실습준비create domain (1)도메인 이름: class(2)배포 타입: 개발 및 테스트(3)인스턴스 타입: t3.medium.search(4)네트워크 : Public Access (실습이기 때문에)(5)세분화된 액세스 제어(6)마스터 사용자 생성 - 계정정보 입력(7)액세스 정책 : 세분화된 액세스 제어만 사용(어디에서든 접속 가능하도록 설정)(8)고급 클러스터 설정 - 최대 절 수: 1,024 (생성시간은 대략 10분정도 소요) Logstash를 위한 EC2 인스턴스 생성(1) 인스턴스 타입: t3-medium[태그 추가](2) Key: Name / Value: elasticsearch-logstash EC2 인스턴스에 필요한 환경 구성하기[Open JDK 설치 및 logstash 설치]1234567891011121314151617$sudo yum install -y java-1.8.0-openjdk-devel.x86_64$wget https://artifacts.elastic.co/downloads/logstash/logstash-7.4.0.tar.gz$tar xvzf logstash-7.4.0.tar.gz$ln -s logstash-7.4.0 logstash#어느 폴더에서도 logstash를 실행가능하도록 .bash_profile 파일 수정$ vi ~/.bash_profile# export LS_HOME=/home/ec2-user/logstash# PATH=$PATH:$LS_HOME/bin$source ~/.bash_profile#logstash version 확인$logstash --version 실습 순서STEP1 : [ElasticSearch에 쏴줄 데이터 준비하기]EMR에서 Zeppelin notebook을 통해서 이전에 Text형태의 파일을 JSON 파일로 convert해서 ElasticSearch로 쏴줄 데이터를 준비할 것이다. Logstash에서 ElasticSearch로 데이터를 전달할때 to_timestamp형태가 timestamp 타입의 데이터 형태여야 시계열 형태로 그래프를 그릴 수 있다. 1234// 이전시간에 연습했던 것처럼 아래의 spark 코드로 지정한 S3 Bucket 위치에 SQL query 결과 데이터를 JSON 형태로 저장을 한다.spark.sql(&quot;&quot;&quot;[SQL query statement]&quot;&quot;&quot;).write.mode(&quot;overwrite&quot;).json(&quot;[S3 Bucket Location]&quot;) STEP2 : Elastic Search 활용하기 위의 OpenSearch 대시보드 URL은 Kibana로 로그인/접속하기 위한 URL이며, 도메인 엔드포인트는 프로그램에서 데이터를 뿌려주기 위해 사용되는 엔드포인트이다. Kibana?Kibana란 ElasticSearch 데이터를 시각화하고 Elastic Stack을 탐색하게 해주는 무료 오픈 소스 인터페이스이다.쿼리작업량 추적부터 앱을 통한 요청 흐름 방식에 대한 파악까지 다양한 것들을 시도해볼 수 있고 한다. STEP3 : [Logstash에서 S3에 저장된 데이터를 뽑아서 ElasticSearch에 던져줄 수 있도록 config 파일 작성]우선 S3에 있는 데이터를 일괄적으로 ElasticSearch에 던지기 전에 우선적으로 간단하게 message를 주고 받는 것을 확인하는 형태로 테스트용 producer_test.conf 파일을 작성한다. 12345678910111213141516171819input { s3 { access_key_id =&gt; &quot;[ACCESS KEY ID]&quot; secret_access_key =&gt; &quot;[SECRET ACCESS KEY]&quot; bucket =&gt; &quot;[BUCKET NAME]&quot; region =&gt; &quot;ap-northeast-2&quot; prefix =&gt; &quot;temp/json&quot; }}filter { json { source =&gt; &quot;message&quot; }}output { stdout {codec =&gt; rubydebug}} CLI에서는 S3 bucket에 있는 데이터를 읽을때에는 access key나 secret key 없이도 서비스별로 권한을 부여하여 데이터에 접근할 수 있었다. 하지만 위의 경우는 어플리케이션 레벨에서 S3 bucket에 접근을 하는 것이기 때문에 위와같이 access_key_id와 secret_access_key에 대해서 정의를 해줘야한다.prefix는 데이터를 가져올 대상 폴더의 경로를 의미하며, filter에서는 가져온 json 데이터를 처리하는 부분을 담당한다. producer.conf 1234567891011121314151617181920212223242526272829input { s3 { access_key_id =&gt; &quot;[ACCESS KEY ID]&quot; secret_access_key =&gt; &quot;[SECRET ACCESS KEY]&quot; bucket =&gt; &quot;[BUCKET NAME]&quot; region =&gt; &quot;ap-northeast-2&quot; prefix =&gt; &quot;temp/json&quot; }}filter { json { source =&gt; &quot;message&quot; } mutate { remove_field =&gt; [&quot;[filter할 column명]&quot;] }}output { elasticsearch { hosts =&gt; [&quot;[ElasticSearch 도메인 엔드포인트]&quot;] ssl =&gt; true index =&gt; &quot;class-%{+YYYY.MM.dd}&quot; user =&gt; &quot;[사용자명]&quot; password =&gt; &quot;[비밀번호]&quot; ilm_enabled =&gt; false }} output의 index 부분은 partitioning과 동일한 부분으로, ElasticSearch는 데이터를 많이 가져가면 비효율적인 부분이라고 이전에 학습을 했었다. 그래서 2주에서 최대 1달정도의 기간동안의 데이터를 취합해서 분석할때 유용하다고 했다.index에서 이름에 기간을 넣어서 유동적으로 표시를 한 이유는 나중에 특정 기간 이전의 데이터를 삭제할때 조건처리로 할 수 있기 때문에 위와같이 기간을 index명에 포함을 시켰다. logstash에는 다양한 기능이 있는데, filter에서 mutate는 그 중 하나의 기능으로, 분석에 필요하지 않는 column이 있는 경우, script에서 간단하게 column 명을 넣어서 해당 column을 삭제처리할 수 있다. STEP4 : [Kibana에서 데이터 확인하기]지정해준 ElasticSearch domain endpoin에 데이터를 보냈다면, Kibana에서 해당 데이터를 확인할 수 있다. 햄버거 버튼에서 Management - Dev Tools 메뉴를 클릭하면, query를 날려서 현재 데이터를 확인할 수 있는데, Amazon OpenSearch Service의 Domain에서 클러스터 상태 메뉴에 들어가면, 클러스터 상태를 확인할 수 있는데, 상태에 따라서 클러스터의 노드를 늘려주거나 디스크를 확장해서 해결해 줄 수 있다. Tip : 외부에서 발생하는 데이터를 API Gateway를 통해서 Kinesis stream -&gt; Kinesis firehose -&gt; S3의 형태로 외부 데이터가 수집되어 분석되는 것을 이전에 실습을 해보았는데, 이 경우에는 ElasticSearch에서 데이터를 수집했을때와 비교하면 느리다.따라서 좀 더 빠르게 새로 런칭한 신규 서비스의 사용자 이용추이 데이터를 분석할때에는 이번 포스팅에서 다룬 ElasticSearch가 활용하기에 좋다. STEP4 : [들어오는 데이터에 인덱싱해주기]OpenSearchDashboard에서 햄버거 버튼을 클릭하고, Management -&gt; Stack Management를 클릭 -&gt; Index Pattern 안에 생성된 data object를 확인할 수 있다. 이것을 분석할 수 있는 하나의 테이블 뷰 형태로 만들어주면 된다.Index pattern name : class* -&gt; NEXT -&gt; (시간 기준 데이터 Time field 선택) 우측 상단의 별을 클릭하면 default index로 사용을 할 수 있다. 이제 햄버거 버튼 클릭후 OpenSearch Dashboards - Discover를 클릭하면, 상단에 필터된 기간에 맞게 그래프에 데이터가 출력이 된다. 세부 데이터 관련 내용은 하단에 세부 항목으로 확인을 할 수 있다. 이외에도 Visualize 메뉴를 통해서 빠르게 들어오는 데이터(예시-신규 서비스 당일 접속자 데이터)를 그래프로 시각화를 할 수도 있고, Search Engine기반이기 때문에 매우 빠르게 처리를 해준다.하지만 너무 많은 데이터를 ElasticSearch를 통해서 너무 많은 데이터를 처리하려고 하면 비용이 너무 많이 들기 때문에 이 경우에는 다른 구조로 파이프라인을 구성해서 데이터를 분석하는 것이 좋다.","link":"/2022/05/01/202205/220501-data-pipeline-study/"},{"title":"220429&#x2F;30 데이터 파이프라인 스터디 18&#x2F;19일차(Presto와 Tableau 연동해서 Dashboard 구성)","text":"이번 포스팅에서는 Presto와 Athena에 이어서 Tableau에 대해서 정리를 해보려고 한다. 최근 학습한 내용에 있어서, 데이터 전처리 및 시각화하는데 사용되는 서비스들이 많이 등장하였는데, 가장 최근에 배웠던 Presto와 Athena, 그리고 Tableau 이 세 개의 기술에 대해서 다시 한 번 전체적으로 개념을 잡고 Tableau를 실습한 내용을 정리하려고 한다. Presto / Athena / Tableau우선 Presto -&gt; Athena -&gt; Tableau 순으로 살펴보면, Presto는 짧은 시간의 임시 데이터 분석에 최적화된 Open source로, 분산 SQL query engine으로, 이종 데이터간의 JOIN을 지원한다. Athena는 Presto와 같이 SQL query를 실행시킬 수 있고, Serverless이다.단, Presto와는 다르게 RDS 데이터와 JOIN(이종 데이터간의 JOIN)은 지원하지 않는다.하지만, 분석할때에는 Presto가 좋다고 하더라도 항상 구동시켜놓기에는 Presto는 부담이 있기 때문에 Serverless인 Athena를 Tableau와 연동해서 시각화를 하는 것도 좋다. 자 앞서 살펴보았던 Presto와 Athena는 둘 다 SQL Query engine으로, 데이터를 SQL Query로 분석할 수 있다는 공통점이 있었지만, 이종 데이터간의 JOIN을 할 수 있기도(Presto) 없기도한(Athena) 차이가 있었다. Tableau그럼 Tableau는 무엇인가? Tableau는 BI(Business intelligence) 툴로, 뭔가 어려워 보이는 툴이지만, 사람들이 데이터를 보고 이해할수 있도록 돕는다.는 경영철학을 가지고 만들어진 소프트웨어이다.이 BI 소프트웨어는 3명의 스탠포드 대학 출신의 Christian, Chris, Pat이 설립한 Tableau 기업에서 만든 소프트웨어로, Pat과 Chris가 데이터 베이스에 저장되어있는 데이터의 이해를 돕고자 추친된 프로젝트에 참여하여 당시 가장 상용화된 BI 소프트웨어를 사용하였는데, 박사출신인 본인들이 사용하기에도 너무 어렵게 느껴져서 이 부분에 문제점을 자각하게 되면서 만든것이 바로 이 Tableau라는 BI 툴이다. 당시 후발주자로 BI 업계에 들어왔지만, 빠른 속도로 시장을 장악하였으며, 현재 BI 소프트웨어 업계에서 부동의 1위를 차지하고 있다.BI(Business Intelligence) : 책, 저널, 문서, 이미지, 메일, 파일, 기타 비즈니스 소스를 포함한 내/외부 시스템에서 많은 양의 비정형 데이터를 수집하고 처리하는 어플리케이션 소프트웨어 형식이다. 주로 쿼리를 통해 정보를 찾기 위해 데이터를 수집하는 방법을 제공한다. 대시보드 및 데이터 시각화를 만들 수 있도록 분석할 데이터를 준비하는데 도움이 된다. 자 이렇게 Presto와 Athena는 데이터 분석툴이고, Tableau는 BI 툴이라는 정리가 되었다.이제 본격적으로 Presto와 Tableau를 연동해서 Dashboard를 구성해보는 실습을 해보자. Tableau Tableau Desktop 14일 평가판을 다운받는다. Tableau driver를 다운받아서 설치한다. 이하 Tableau Desktop에서 AWS Aurora MySQL Connector를 사용하기 위해 필요한 driver를 설치한다. https://www.tableau.com/support/drivers Tableau Desktop을 다시 열고 AWS Aurora MySQL Connector로 Database resource를 import한다. Tableau의 연결방식에는 Live와 Extract 방식이 있는데 아래와 같은 차이가 있다. Live 방식 : 그때 그때 SQL query를 날려서 데이터를 가져와서 처리한다. (DBMS/Presto에 부하를 줄 수 있다) Extract 방식 : DBMS의 데이터를 File로 내리고, 내려받은 File의 데이터를 기반으로 시각화(Visualization)을 한다. (Tableau 전용 서버를 별도로 운영한다면, 부하를 줄 수 있다) Tableau는 Drag&amp;Drop 방식으로 손쉽게 UI상에서 시각화가 가능하다. (2022/04/30 업데이트) Tableau와 Presto연동하기Tableau내에서 Presto driver를 다운받아서 분산형 클러스터 형태로 SQL 쿼리로 데이터 처리를 빠르게 할 수 있는 Presto를 Tableau 내에서 함께 사용해보도록 할 것이다. [STEP1] JDBC DRIVER 다운받아서 Tableau와 연동 https://prestodb.io/download.html 다운받은 presto *.jar 파일은 /Library/Tableau/Drivers 하위에 옮겨준다.(Presto와 Tableau 연동) [STEP2] Tableau에서 Presto connector를 사용해서 접속 아래와같이 EMR의 Presto cluster의 master end point 주소, port, catalog, schema, Authentication method, Username을 입력한다. 별도의 Password 입력 설정이 없기 때문에 8889번 Port에 대한 Inbound rule에서 보안설정을 strict하게 해준다. Tip)처음 연결되면 아무것도 보이지 않는데, 스키마 드롭다운 리스트를 클릭하면, 돋보기 모양 아이콘이 보이는데, 아이콘을 클릭하면 schema리스트가 나오고, shema를 선택한 다음에 테이블에서도 돋보기 모양 아이콘을 클릭하면, 세부 데이터 테이블 리스트가 출력된다. Silver 데이터 테이블을 우측의 workspace에 Drag&amp;Drop을 해주게 되면, 작성했던 silver 데이터 테이블 정보를 볼 수 있다. (sample data 해석)언제 - 어떤 사용자(id)가 - 어떤 단지의 id를 클릭했는지에 대한 정보를 확인할 수 있다. 위의 hive catalog정보를 mysql cataqlog정보의 테이블과 Join해서 새로운 데이터로써 사용할 수 있다.(JOIN시, 기준 column 선택) [지도(맵)에 데이터 시각화하기] 이 부분은 다음에 공공기관의 데이터를 활용하여 한 번 실습을 해보자.데이터 테이블에서 시/군/구에 해당되는 데이터가 있다면, 해당 칼럼 데이터 항목을 우측 클릭해서 지리적 역할 - 시(도시) 혹은 시군구를 클릭하여 맵핑해주고, 해당 칼럼 데이터 항목을 새로운 시트에서 worksheet에 Drag&amp;Drop해주면 지도상에서 데이터의 분포를 표시해준다. [Gold data/Silver data와 BI 툴의 조합]일반적으로 GOLD 데이터를 BI tool과 1:1로 mapping 시켜서 데이터를 시각화시키는 방법도 있으나, 여러가지 시각으로 분석을 해야되는 경우,SILVER 데이터를 BI tool + Presto(빠른 엔진)과 조합을 하여, 다각도로 SILVER 데이터를 시각화해서 분석할 수 있다. Tableau와 Athena 연동하기Athena는 serverless 데이터 분석 툴로, 데이터는 data catalog의 metadata만을 사용(S3의 Glue기반의 metadata)한다. 사용한 데이터 용량을 기준으로 charging되기 때문에 상황에 따라 적절하게 사용되어야 한다. Athena에서는 RDS의 master성 데이터를 import해서 사용할 수 없기 때문에, 임의로 data catalog에 master성 데이터 테이블을 넣어서 JOIN을 하는 형태로 데이터를 조작해서 전처리해야 한다. https://docs.aws.amazon.com/ko_kr/athena/latest/ug/connect-with-previous-jdbc.html (JDK 7.0호환 드라이버 다운받기) Athena도 Presto와 동일하게 다운받은 jar 파일을 /Library/Tableau/Drivers 하위에 옮겨준다. (Tableau와 Athena 연동) Amaqzon Athena 서비스에서 쿼리 결과 위치 경로 설정하기Amazon Athena -&gt; 쿼리 편집기 -&gt; 설정에서 해당 경로를 추가해줄 수 있다. Tableau에서 Amazon Athena Connector 설정(1) server: ena.ap-northeast-2.amazonaws.com (2) port: 443 (3) S3 Staging directory: [Amazon Athena의 쿼리결과 위치 S3 PATH] (4) Access key ID: Access Key ID의 경우에는 다른 계정의 Access Key를 입력할 경우, 해킹의 문제가 될 수 있기 때문에, 반드시 Athena 사용에 대한 권한만 부여받은 별도의 사용자 계정을 만들어서 해당 사용자의 Access Key ID를 입력하도록 한다. 사용자 권한 추가시에 Athena 사용에 대한 권한 이외에도 AmazonS3FullAccess 권한을 함께 부여해줘야 한다.(S3 Staging directory read/write) (5) Secret Access Key: (Optional)","link":"/2022/04/29/202204/220429-data-pipeline-study/"},{"title":"220502 코딩 테스트 문제 리스트 Web crawling해보기","text":"이번 포스팅에서는 코테준비를 하기 위해서 코테문제를 풀다가 집중도 안되고, 원래 체크리스트를 만들어 놓고 하나씩 지워나가면서 하면 뭔가 성취감도 느끼고 해서 프로그래머스의 연습문제 페이지에 있는 문제 리스트를 Selenium + BeautifulSoup + Pandas + Python의 조합으로 빠르게 크롤링을 해서 엑셀파일로 뽑아보았다. 206개의 코딩 테스트 문제파이썬 관련 코딩테스트 문제를 뽑아보니 총 206개의 문제가 나왔다. Level1부터 5까지 전체를 뽑아본거라 난이도가 많이 어려운 문제도 있을거라고 생각되지만, 일단 다 풀어보는 것을 목표로 한 번 시작해보려고 한다.아무래도 혼자 공부하다보니 성취감이나 동기부여가 필요한데, 푼 문제는 엑셀파일에 하나씩 지워가면서 진행해볼 생각이다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# -*- coding: utf-8 -*-from selenium import webdriverfrom selenium.webdriver.common.by import By # CSS선택자 서치 담당from selenium.webdriver.chrome.service import Service # 크롬드라이버의 동작 시작, 끝 담당from bs4 import BeautifulSoupimport timeimport pandas as pdPAGES = 13options = webdriver.ChromeOptions()options.add_experimental_option('excludeSwitches', ['enable-logging'])# webdriver# 크롬을 기준으로 현재 사용하고 있는 버전에 맞춰서 webdriver를 다운로드service = Service(executable_path=&quot;./chromedriver&quot;)browser = webdriver.Chrome(service=service, options=options)url = 'https://programmers.co.kr/learn/challenges'browser.get(url)problems = []def extractCodingTestProblems(search_result_items): for item in search_result_items: problem_item = dict() language_anchor = item.find(&quot;div&quot;, &quot;languages&quot;).find_all(&quot;a&quot;, href=True) for lang in language_anchor: if &quot;python&quot; in lang.get('href'): title = item.find(&quot;h4&quot;, &quot;title&quot;).text.replace('\\n', '').strip()[5:].strip() level = item.find(&quot;span&quot;, &quot;level-badge&quot;).text.strip() info = item.find(&quot;h6&quot;, &quot;level&quot;).text.strip() problem_item['title'] = title problem_item['level'] = level problem_item['info'] = info problem_item['link'] = f&quot;https://programmers.co.kr{lang.get('href')}&quot; problems.append(problem_item) breakfor idx in range(1, PAGES+1): pagination_btn = browser.find_element(By.CSS_SELECTOR, f&quot;a[href='/learn/challenges/filter_lessons?page={idx}']&quot;) pagination_btn.click() time.sleep(1) html = browser.page_source soup = BeautifulSoup(html, 'html.parser') search_result = soup.find(&quot;div&quot;, &quot;list-positions&quot;) search_result_items = search_result.find_all(&quot;div&quot;, &quot;col-item&quot;) extractCodingTestProblems(search_result_items)# Python의 List 타입의 데이터를 DataFrame으로 만들고, Excel 파일로 추출problems_df = pd.DataFrame(problems)problems_df.to_excel('./problems_excel.xlsx',sheet_name=&quot;Algorithm problems&quot;, index=True, header=True)print(problems_df)","link":"/2022/05/02/202205/220502-web-crawling/"},{"title":"220507 학습 전반에 대한 회고 및 AWS Practitioner 자격증 취득후기","text":"이번 포스팅에서는 3월 중순부터 5월 초까지 한 학습 전반에 대한 회고 및 어제 응시한 AWS Practitioner 시험에 대해 간단하게 후기를 작성하려고 한다. 약 한달 반 동안의 학습 회고이 한달 반이라는 시간동안 앞으로의 방향설정에 대해 많은 고민을 했었다. 그런데 너무 감사하게도 좋은 분들을 뵙게 되고, 직간접적으로 조언을 들으면서 앞으로 나아갈 방향에 대해서 방향설계를 할 수 있게 되었다. (나중에 이 분들은 따로 뵙고 감사인사를 드릴 것이다.)너무 많은 일이 있었기 때문에 시간이 한 반년 흘러간 것 같은 느낌이었는데, 지난간 시간을 계산해보니, 대략 한달하고도 보름정도 지났다니 그래도 나름 알차게 시간을 보낸 것 같다. 이 한달 반동안에 있었던 일 중에 가장 큰 성과는 앞으로 나아갈 데이터 엔지니어라는 새로운 도메인을 정하게 된 것과 앞으로 어떤 것을 공부하면서 준비해야되는지, 그리고 기간을 어느정도 두고 준비할지에 대한 목표설정에 대한 부분이었다. 이제 목표로 설정한 부분과 이미 진행한 공부내용들을 기반으로 앞으로 남은 3개월의 기간동안 열심히 달려 볼 계획이다. [앞으로 3개월동안 할 공부 리스트] 블로그 관리 꾸준히 하기 파이썬 / Scala 활용능력 키우기 코딩테스트 준비(프로그래머스/백준 알고리즘 문제 풀면서 파이썬 활용능력을 키우면서 문제해결능력도 키우자) 포트폴리오 준비 틈틈이하기 (대단한 프로젝트가 아니어도 괜찮다. 과정을 기술하고 그 일련의 과정에서 내가 어떤 것을 배웠고, 앞으로 어떤 방향으로 나아갈지에 대해 보여줄 수 있으면 된다.) Kafka를 활용한 간단한 프로젝트 구성하기 (완료 - 차후에 살붙이기식으로 프로젝트에 내용추가해가면서 확장해갈 예정) 데이터 파이프라인 학습 및 클라우드 환경에서의 응용 &amp; 실습 (+AWS) Hadoop Ecosystem 지식 및 관련기술(Spark, MapReduce)에 대한 활용능력 키우기 SQL로 데이터 분석 꾸준히 하기 Docker/Kubernetes 학습하기 AWS SAA-C02 자격증 시험 준비하기 (6월 말 예정) AWS Practitioner 자격증 시험 준비하기 (취득 완료) 앞으로 할 공부 리스트를 작성해보니 많아보이지만, 각 항목들이 서로 연관되어있는 것들이 많아서 한 가지 항목을 하면서도 부수적으로 다른 항목들도 같이 공부되는 것들이 많다.서로 시너지 효과를 내는 부분도 많이 있기도 하고, 나중에 데이터 엔지니어로서 지원을 했을 때 나를 좀 더 빛내줄 수 있는 항목들이기 때문에 제대로 준비해보도록 해야겠다. AWS Practitioner 자격증 취득 후기 2022년 5월 6일 금요일 13시에 선정릉역 쪽에 있는 SRTC라는 테스트 센터에서 AWS Practitioner 자격증 시험을 보았다. AWS 공식 Skill Builder 사이트에서 무료로 제공해주는 강의를 약 2주동안 수강을 하고, 나름 수강했을때 나왔던 AWS 기술과 관련된 내용과 클라우드 개념, 보안 및 규정 준수, 결제 및 요금과 관련된 내용들을 간단하게 정리해서 해당 용어가 나왔을때 어떤 내용인지 매칭될 정도로 개념정리를 하였다.그리고 인터넷에서 덤프문제도 다운을 받아서 400문제정도 풀어보았는데, 역시 개념적으로 이해를 했어도 막상 문제를 풀어보니 틀리는 문제들이 있어서 역시 이해를 제대로 못했구나 싶어서 다시 개념을 정리를 하였다. 아래는 초반에 문제풀이를 했을때 실제 체크하면서 공부했던 PDF 100 페이지 중에 한 페이지인데, 쉬운 개념이라고 생각되었던 부분이었음에도 틀려서 좀 당황했었다. 그래도 다시 개념으로 돌아가서 반복할 수 있었던 좋은 계기가 되었던 것 같다. 시험이 끝나고 다시 훑어봤는데 나름 열심히 그리고 꾸준히 공부했던 것 같다.앞으로 학습할 기술적인 내용에 대해서도 지금처럼 한결같이 학습하도록 하자. 그런데 반전은 실제 시험에서는 400문제의 덤프문제에서 10문제정도 유사한 문제가 나오고 나머지는 다 새로운 문제들이 출제되었다.(ㅋㅋㅋㅋㅋㅋ) 시험보기전에 인터넷에 후기를 보면 덤프문제에서 뭐 절반이상 나온다 비슷하게 나온다 이런 말들이 있는데, 내가 유료도 아닌 무료로 다른 고마운 분이 올려주신 자료를 다운받아서 그래서인지 몰라도 후기와는 다른 상황에 조금 당황했지만, 그래도 나름 AWS에서 제공해주는 공식 강의와 개인적으로 AWS Practitioner 시험 각 Section별 개념을 따로 정리해서 개념정리 및 공부를 했기 때문에 문제없이 시험에서 패스할 수 있었다. (그래도 덤프문제를 풀면서 시험이 어떤 식의 문답으로 출제가 되는지 유형에 대해서 익숙해질 수 있는 좋은 기회가 되었고, 알게모르게 문제는 달라도 비슷한 의도의 문제는 다수 출제가 되었을 것이라고 생각된다.그래서 혹시 시험보실 분이라면 풀어보실 것을 추천!) 시험보고나서 느낀점AWS Practitioner 시험을 보고나서 느낀점은 마냥 자격증 취득해서 좋다는 것보다는 시험에서 출제되었던 AWS 기술 Section에서 다뤘던 기술들에 대해서 심도있게 다뤄보지 못했다는 아쉬움이 컸다. 그래서 시험이 끝나고 집에 돌아오는 길에 AWS Certified Solutions Architect Associate Certificates SAA-C02 자격시험 관련 Udemy 강의를 결제하였다.SAA-C02 시험부터는 정말 상황에 맞게 AWS의 각 기술들을 어떻게 사용해서 대처해야되는지에 대한 능력도 보는 것 같다. 아직 학습을 시작도 하지 않았지만 공식 사이트에서 제공해주는 sample 문제를 풀어보니, 그런 느낌을 받았다.강의 수강목록들을 보니, AWS Practitioner에서 다뤘던 기술들의 항목들을 직접 hands-on 해보면서 실습해보는 과정이 많아서 나중에 실무에서 AWS를 활용해서 업무를 할때에도 많은 도움이 될 것 같다.그럼 대략 두 달정도 열심히 준비해서 6월 말일경에 SAA-C02 자격시험에 응시해야겠다.","link":"/2022/05/07/202205/220507-memoirs/"},{"title":"220509 AWS Certified Solutions Architect Associate Certificates (SAA-C02) (작성중...)","text":"이번 포스팅에서는 이번에 취득한 AWS Practitioner 자격시험에 이어서 SAA-C02 시험에 대비하기 위해 Udemy에서 강의를 수강하면서 직접 AWS에서의 각종 서비스들을 실습해보면서 본격적으로 시험에 대비해보려고 한다.이번 AWS Practitioner 시험과 AWS 클라우드 환경에서 데이터 파이프라인 구축해보는 실습을 통해서 일부 AWS 서비스들은 다뤄보았지만, 아직 기존에 다뤘던 서비스들도 그렇고, 다른 서비스들에 대해서도 잘 알지 못하는 부분이 많다. 그래서 이번 기회에 자격증 시험준비를 하면서 나중에 실무에서도 프로페셔널하게 직접 클라우드환경을 구축하고 사용할 수 있도록 이론과 실습을 많이 해보기로 결심했다.아무래도 스스로 환경을 만들어서 익혀야되는 환경이다보니 자격증과 같은 동기부여 요소가 있어야 좀 더 신경써서 현재 내 지식 수준을 체크하면서 진행할 수 있는 것 같다. :) 오늘은 SAA-C02 시험 대비 겸 AWS 실습 첫 날로, 아래의 내용들에 대해서 개괄적으로 살펴보았다. 앞으로 실습하게 될 AWS의 서비스들 위의 서비스들은 앞으로 약 두 달간 하루에 조금씩 실습을 해나가면서 완성해나갈 AWS의 서비스들 중에 일부이다. 이따가 아래에서 정리를 하겠지만, 오늘 잠깐 Global service 중에 하나인 IAM 서비스에 대해 실습을 했었는데, AWS Practitioner 자격시험을 공부하면서 알던 서비스였지만, 다시 실습을 하면서 다시 머리속에 차곡차곡 정리되는 느낌이었다.첫 시작이 이렇게 좋으니 앞으로 학습하게 될 서비스들에 대한 기대도 크다. 이전에 데이터 파이프라인 구성에 대한 실습을 하면서 Kinesis stream도 다뤄본 적이 있는데, 뭔가 또 다시 복습하면서 정리하면 또 다른 느낌일 듯 싶다. AWS Cloud의 역사2002년에 AWS 내부적으로 Cloud 서비스가 런칭되었다. 이후 2003년에 Amazon Infrastructure가 그들의 핵심 강점 중 하나로 여겨져서 시장에 등장하였다.2004년에 Amazon SQS(Simple Queue Service)는 완전관리형 메시지 대기열 서비스를 출시하였다.2006년에는 SQS, S3 &amp; EC2 서비스를 출시하였고, 2007년에는 유럽에서 출시를 하였다. 그 이후에는 Dropbox, NETFLIX, Airbnb, NASA 등에서 AWS 서비스를 사용하기 시작했다. AWS 클라우드 서비스를 사용하면, 복잡하고 확장 및 축소 가능한 어플리케이션을 제작하는 것이 가능하다. AWS Global InfrastructureAWS Infrastructure는 대륙과 대륙이 여러 Region들로 열결이 되어있으며, Region내에는 기본적으로 세 개의 AZ(Availability Zone)s이 존재한다. AWS Regions 개념 및 선택시 고려사항대부분의 AWS 서비스들은 region scope이다. Region은 data center들의 클러스터이며, 전 세계적으로 분포한다. AWS Region의 선택시에 영향을 주는 요소에는 아래 4가지 요소가 있다. 데이터 관리와 합법적 요구에 따른 준수(Compliance)에 따라 데이터는 명시적 허가없이 region을 절대 벗어나서는 안된다. (거리, 시간상의)고객으로의 가까움, 근접 (Proximity)지연을 줄일 수 있는 지리적 region을 선정한다. 선택한 Region에서 사용가능한 서비스(Available service)인지에 대한 확인이 필요하다. AWS에서 제공해주는 서비스들이 모든 region에서 제공되는 것이 아니다. 따라서 사용하려는 서비스가 특정 region에서 사용가능한지에 대해 확인해야한다. 가격(Pricing) : region에 따라서 가격은 달라진다. 따라서 내가 운영하는 서비스의 특성을 잘 파악해서 최적의 가격을 맞출 수 있도록 region을 선택해야 한다. AWS Availability Zones 개념AWS Points of Presence(Edge Locations)AWS Global-scoped service와 Region-scoped serviceIAM 서비스 IAM : Users와 Groups IAM : Permissions IAM 정책 상속 IAM 정책 구조","link":"/2022/05/08/202205/220508-aws-saa-study/"},{"title":"220510&#x2F;11 AWS Certified Solutions Architect Associate Certificates (SAA-C02)(작성중...)","text":"이번 포스팅에서는 IAM 서비스와 관련된 실습과 AWS 액세스 키, AWS CLI, SDK를 설정하고 활용하는 방법에 대해서 배운내용에 대해서 간략하게 정리를 하려고 한다. IAM 정책 실습IAM MFA 개요IAM MFA 실습AWS 액세스 키, CLI 및 SDKAWS CLI 설정(2022/05/11 업데이트) AWS CLI 실습 AWS CloudShell Region 가용성AWS CloudShellAWS Service에 대한 IAM 역할 설정AWS IAM 역할 실습AWS IAM 보안 도구","link":"/2022/05/10/202205/220510-aws-saa-study/"},{"title":"220513 AWS Certified Solutions Architect Associate Certificates (SAA-C02)","text":"이번 포스팅에서는 EC2 인스턴스의 활용에 대해서 실습한 내용에 대해서 정리해보려고 한다. 이번 포스팅까지만 기본적인 부분을 실습하고, 이제 다음 포스팅부터 본격적으로 AWS SAA(Solutions Architect Associate) level에 맞는 파트에 대해서 학습을 시작할 것이다. AWS 예산 설정예산 초과시에 경고를 받을 수 있도록 설정을 하는 것이 좋다. My Billing Dashboard에서 좌측의 메뉴에서 Cost Management - Budgets 메뉴를 통해서 설정한 예산에서 특정 임계값을 넘었을때 이메일이나 SNS메시지로 알람이 올 수 있도록 할 수 있다. EC2 기초 EC2 인스턴스 생성을 통해 정적 웹 사이트 실습 EC2인스턴스 유형 기본 사항보안 그룹 및 클래식 포트 개요보안 그룹 실습SSHEC2 인스턴스 연결EC2 인스턴스 역할 데모EC2 인스턴스 시작스팟 인스턴스 및 스팟 집합EC2 인스턴스 launch type 실습","link":"/2022/05/13/202205/220513-aws-saa-study/"},{"title":"220512 데이터 엔지니어가 되기 위한 준비와 노력","text":"이번 포스팅에서는 인터넷에서 우연히 보게 된 현업 데이터 엔지니어의 “데이터 엔지니어가 되기 위한 준비”에 대한 영상을 보고 난 후의 후기에 대해서 작성해보려고 한다.결과적으로는 너무 많은 도움이 되었다. 아직은 많이 부족하지만, 개인적으로 데이터 파이프라인 구축이나 Hadoop, Spark, AWS 등 데이터 엔지니어가 되기 위해 필요한 기술 스택에 대해서 공부를 해왔기 때문에 이번 영상이 나에게 많은 도움이 되었던 것 같다. 가장 많은 도움이 되었던 부분은 데이터 엔지니어라는 직무적 측면에서 지원하고자 하는 기업을 볼때 어떻게 봐야하는가였다. 데이터 엔지니어 직무로서 바라 본 기업의 업무 프로세스이 부분은 내가 기업에 지원할때 실제 그 기업에서 어떤 데이터가 발생하는지, 그리고 그 데이터를 수집하고 관리하며, 최종적으로 데이터가 어떤식으로 활용이 되는지에 대한 이해는 필수인 것 같다는 생각을 하였다. 예를들어, A라는 기업에서 아래의 업무 프로세스를 가지고 있다고 가정하자. [업무 프로세스]연구 및 개발 -&gt; 제조 -&gt; 배송 -&gt; 온/오프라인 판매 위의 전체 업무 프로세스를 통해 여러 데이터들이 발생할 것이다. 최종 판매 단계에서는 매일 다양한 경로로 판매가 이루어지고 있다는 것을 분석할 수도 있을 것이고, TB규모이상의 데이터가 처리될 것이다.(위의 각 업무 프로세스상에서 발생하는 데이터들을 추출하기 위해서는 데이터 엔지니어의 필수역량인 데이터 파이프라인 구축을 해야한다.) 기업마다 업무상에서 추출 및 분석하는 데이터는 다르며, 게임회사의 경우에는 게임 플레이할 때 발생하는 로그 데이터나 게임 아이템 마켓에서 구매내역과 관련된 데이터도 있다. 이처럼 데이터를 수집(추출)하고 가공하고, 적재하는 일련의 과정을 통틀어서 ETL(Extract/Transform/Load)라고 한다. 기업은 매일 생산해내는 여러 데이터를 수집하고 관리하며 데이터가 필요한 곳에 전달해서 의사결정에 활용한다. 데이터 엔지니어 직무데이터 엔진지어의 역할은 앞에서 언급했듯이 데이터 파이프라인을 구축하는 것인데, 데이터 유실없이 튼튼한 데이터 파이프라인을 구축하는 것이다. 뭐 데이터 그냥 엑셀파일로 저장해서 관리하면 되는 거 아닌가? 라는 생각을 가진 사람들도 있겠지만, PC 한 두 대가 아닌 클러스터 형태로 여러 노드들을 사용해서 핸들링해야되는 데이터를 다뤄야되기 때문에 이 데이터 파이프라인이 필요한 것이다. 추가적으로 회사마다 데이터 엔지니어의 업무 범위가 다르기 때문에 지원하기 전에 반드시 확인을 해야된다고 한다. Data Lake관계형 데이터의 경우에는 RDB에 저장을 하고, 비 관계형 데이터는 NoSQL DB에 저장하는 식으로 나눠서 따로 따로 저장하지 않고, 정형 데이터와 비정형 데이터를 한 곳에 모아서 Data Lake를 형성하게 되는데 이때 사용되는 것이 바로 AWS S3(객체 스토리지)이다.이렇게 한 곳에 일괄적으로 모으게 되면, 분석에 용이하다. 데이터 엔지니어의 스킬셋 (1) AWS, GCP데이터 파이프라인 구축을 클라우드 기반으로 하기 때문에 AWS나 GCP의 서비스를 활용하는 방법에 대해서 알아두는 것이 좋다. (2) Apache SparkSpark는 여러 컴퓨터에 대량의 데이터 처리를 나눠서 하는 분산처리 엔진이다. (3) KafkaKafka는 메시징 큐로, 실시간 데이터 처리에 많이 사용이 된다. (4) Apache Airflow데이터가 흘러다니는 데이터 파이프라인에 x는 언제 실행이 되고, y는 x 다음에 실행이 되고, 이런 식으로 스케쥴링을 orchestration 해주는 엔진이다. (5) Docker / KubernetesDocker는 Container이고, Kubernetes는 Container를 orchestration 해주는 엔진이다.이러한 컨테이너 관련 서비스를 이용하는 이유는 앱이 모두 동일한 환경에서 작동하는 것을 보장하고, 같은 환경을 배포하도록 하기 위해서이다. (6) HadoopHadoop은 분산 컴퓨팅 에코 시스템이다. (7) ELK Stack여러 데이터 관련 툴을 제공하는 Stack이다. (8) SQL데이터베이스에 접근해서 데이터를 저장하고 호출하거나, 수정하고 삭제하는데 사용되는 구조화된 질의 언어이다. 아직 위의 기술셋 중에서 Docker/Kubernetes, Apache Airflow는 아예 해보지도 못했는데, 지금 하고 있는 공부를 하면서 한 번 배워봐야겠다. 그런데 유행하는 기술이나 다양한 기술셋들은 빠르게 변한다. 따라서 아래의 항목들에 대해서 유의해서 학습하면 좋다고 한다. (1) 필요한 기술을 빨리 습득하고, 업무에 사용할 수 있는 능력 (2) 분산 컴퓨팅, 컨테이너 기술에 대한 이해 (3) 하나의 언어와 SQL 능력 필요 어쩌면 지금 단기간에 여러가지 기술들을 스스로 배우고 정리하고, 반복학습을 하면서 내것으로 만드는 과정이 나중에 실무에서 새로운 기술을 만났을때 스스로 찾아서 공부하고 내것으로 만들 수 있는 좋은 습관으로 자리잡을 수 있도록 열심히 노력해야겠다. 가장 먼저 해야되는 일 내 활동과 스펙들을 쭉 정리를 해보자. 채용공고들을 쏵 훑어보자. 나에게 맞는 포지션을 고른다. 포트폴리오 준비 포트폴리오의 주된 목적은 내 스킬셋을 보여주기 위한 것이다. 구구절절 적지 말고, 한 눈에 잘 들어올 수 있도록 작성하도록 하자. (포트폴리오는 선택이 아닌 필수) PDF나 URL로 준비를 해서 제약 없이 어디서든 접속해서 볼 수 있는 형식으로 제출하자. 코딩 테스트 문제를 푸는 것 이상으로 효율적이고 좋은 코드를 내 것으로 만들기 내가 한 번 풀어보고, 좋은 풀이 보기 하다보면 실력이 생긴다! 여러 사이트를 통해서 다양한 플랫폼을 경험해보는 것이 중요! (Leetcode, HackerRank, Programmers(프로그래머스 문제에서 고득점 kit을 풀 수 있으면 탑티어 기업을 제외한 기업들의 코테를 어느정도 통과할 수 있다)) 면접 개발자 성향 (습득이 빠르고, 새로운 기술에 대한 호기심) 협업 경험 면접 태도에서 보여지는 커뮤니케이션 능력 기술 능력 이력서에 ~ 는 써 봤는데 어느 정도로 사용해보셨나요? (버전, 어느정도 설정까지 해봤는지에 대해 설명할 수 있어야 한다) 이슈, 장애상황에 대한 설명 (어떻게 문제상황을 대처했는지에 대한 설명을 할줄 알아야함) 구글링을 통해 개발자 기술 면접 질문 리스트, 데이터 엔지니어 면접 질문 리스트 뽑아서 연습하기 최대한 많은 질문들을 뽑아서 한 번씩 생각해보고 가기(1분 자기소개도 준비해가기) 여러 면접 팁 관련 영상자료 많이 보고 가기(실제로 많이 도움이 된다) - 자투리시간 활용해서 시청하기 편안하게 내가 노력한 것들에 대해서 보여주기 반드시 준비해야되는 것들 내가 개발자 성향인지 보여줄 수 있는 것 커뮤니케이션 능력, 프로그래밍 실력 포트폴리오를 통해서 내가 지원한 직무에서 얼마나 적합한 사람인지 보여줄 수 있어야 한다. 취득하면 좋은 자격증 AWS 클라우드 자격증 쿠버네티스 자격증 마지막으로 정리 채용공고 상시보기 내 자신을 스스로 평가하지 말고, 서류를 직접 제출하고 결과로써 평가받기 목표는 포트폴리오를 채우기!","link":"/2022/05/12/202205/220512-data-pipeline-study/"},{"title":"22040523 SQL NL Join &#x2F; Sort Merge Join &#x2F; Hash Join에 대한 이해","text":"이번 포스팅에서는 이번에 SQLD 자격증 시험 준비를 하면서 새롭게 알게 된 NL Join, Sort Merge Join, Hash Join에 대한 내용을 간략하게 정리하려고 한다.이번 SQLD 자격증 시험은 단순 자격증 취득 목적이 아닌, 내가 SQL에 대해 이론적으로나 실제 SQL 쿼리의 사용에 대한 이해가 어느정도 되는지 확인하고, 부족한 부분을 확인하고, 부족한 부분은 개인적인 공부를 통해 채워나가기 위해서 준비하게 되었다. SQLD 시험 공부는 2022/05/09에 시작했으니, 오늘부로 14일(2주)째인데, 데이터 모델링의 이해부터 SQL 활용부분까지 전체적으로 이론공부도 했고, 직접 부분 실습도 해보았다.공부를 하면서 실제로 내가 잘 알지 못하는 부분에 대한 내용도 알게 되었고, 문제풀이를 통해서 내가 알고 있다고 착각하고 있는 부분에 대해서 다시 복습할 수 있는 좋은 계기가 되었다.그리고 이전에 SQL 성능 향상을 위한 튜닝에 대한 내용에 대해서 얼핏 들어보았었는데, 시험에서 SQL 최적화 기본 원리에 대한 내용을 통해서 SQL 튜닝이 뭔지, 내가 작성한 SQL 쿼리의 실행계획을 어떻게 확인하고 분석하는지에 대해서 아직 기본적인 내용만 알고 있지만, 대략적으로 어떤 내용인지 알게 되었고, 나중에 직접 SQL 쿼리를 작성하면서 실행계획 부분도 한 번 꼼꼼하게 확인해봐야겠다고 느꼈다.자 그럼 이번 포스팅에서는 NL Join, Sort Merge Join 그리고 Hash Join에 대해서 한 번 정리해보도록 하겠다. 문제를 풀면서 NL Join, Sort Merge Join, Hash Join에 대한 이해가 부족하다고 느껴서 급하게 이론 내용을 찾아 정리를 하였다. NL JoinNL Join은 Nested Loop Join으로, 프로그래밍에서 중첩 For-loop와 유사한 방식이라고 이해할 수 있다.예를들어, 부서와 직원 테이블이 있다고 가정하자. NL Join에서는 Outer Table(부서 테이블)에서 Inner Table(직원 테이블)로 1차적으로 부서 테이블의 첫 행에 해당하는 부서에 속한 직원들에 대한 정보를 가져오고나서 Outer Table로 돌아오고, Join이 된다.이러한 일련의 과정이 부서테이블에 있는 부서의 갯수만큼 N번 발생한다.(N번 For-loop 돈다고 생각하면 됨) 따라서 위의 내용을 통해 이해한 NL Loop의 특성에 따라 Outer Table과 Inner Table에는 조건이 필요하다. [Outer Table] 소량의 데이터를 가진 테이블을 Outer Table로 지정하는 것이 성능향상에 좋다. [Inner Table] 만약에 Inner Table의 Join Column에 Index가 걸려있지 않으면, 굉장한 비효율이 발생하게 된다.그 이유는 Outer Table에서 Inner Table로 일치하는 데이터를 찾기 위해서 갈때, 한 건 한 건 가게 되는데, Join Column에 Index가 걸려있지 않다면, 매번 Full scan을 해야되는 경우가 생긴다. 건 by 건으로 Join하는 방식이기 때문에 대량의 테이블을 Join하는 방식으로는 적합하지 않다. OLTP성 환경의 쿼리에 적절하다. 흔히 말하는 트랜잭션 처리에 OLTP(Online Transaction Processing)의 의미가 포함되어있다. 트랜잭션의 주 특징은 연산 실패시 Rollback이 지원된다는 것이며, 주로 대규모의 처리보다는 소규모의 정교한 데이터 구성이 필요한 데이터의 처리가 중점이 된다. ref. OLAP(Online Analytical Processing)는 Database 자체적으로 운용되는 시스템이 아닌, DW 등의 시스템과 연관되어 Data를 분석하고 의미있는 정보로 치환하거나, 복잡한 모델링을 가능하게 하는 분석 방법이다. Sort Merge Join Sort Merge Join 방식도 NL Join과 같이 중첩 For문과 유사한 방식으로 처리된다. (단, 차이점은 Join하고자 하는 두 테이블을 Join 기준 칼럼을 기준으로 Sort하고나서 Join을 시킨다) 적절한 index가 없어서 NL Join을 쓰기에 너무 비효율적인 경우에 사용된다. 동등조인(Equal join)이 아니라 범위로 Join을 하는 경우, 이 경우에 Sort Merge Join은 적절한 수행원리이다.(동등 연산자(=)가 아닌, 범위로 두 테이블을 조인할때 사용되는 수행원리) Table Random Access가 발생하지 않고, Sorting 작업이 PGA 영역에서 수행되다. ref. PGA(Program Global Area)영역과 SGA(System Global Area)영역에 대한 정의 PGA는 사용자마다 공유하지 않고, 개별적으로 사용하는 영역을 말한다. 데이터베이스에 접속하는 모든 사용자에게 할당되는 각 각의 서버 프로세스가 독자적으로 사용하는 오라클의 메모리 영역이다. SGA는 각 각의 서버 프로세스에 할당되는 메모리 영역인 PGA와는 다르게 모든 사용자가 공유 가능한 메모리 공간이다. 경합이 발생하지 않아서 성능에 유리하다는 장점을 가지고 있다. Hash Join Batch에서 쓰면 좋은 수행원리이다. 대용량 테이블을 조인할때 좋다. Hash Join에서는 작은 집합인 Outer Table(직원 테티블)을 Build Input으로 삼아서 Hash 영역으로 올라간다. (Hash 영역 = PGA영역으로 처리속도가 매우 빠르다) Hash 영역으로 작은 집합이 Build input으로 올라갈때는 Join 칼럼을 기준으로 해시함수가 적용되기 때문에 Key column에 중복이 없을 수록 성능에 유리하다. 큰 집합(프로브 테이블(Probe table))에 해당하는 테이블이 해시함수를 통해 해시 테이블을 탐색하면서 조인을 한다. 동등조인(Equal Join)만 가능하고, Sort Merge Join과 같이 Random Access에 따른 부하가 없다. Hash 영역에 들어가는 테이블(Outer table)의 크기가 충분히 작아야 한다. 그 이유는 Hash 영역의 사이즈가 정해져있는데, 테이블 용량이 정해진 사이즈를 초과할 경우, 디스크 영역을 추가적으로 사용해야되는 경우가 발생하기 때문에 성능에 매우 불리하게 된다. 수행빈도가 높은 OLTP 환경에서는 좋지 않은 수행방법이다. 그 이유는 Hash Join으로 수행하게 되면, 오히려 CPU나 메모리 사용량이 늘어서 성능이 저하될 수 있다. SQL 성능 향상을 위한 튜닝(1) 내가 작성한 쿼리가 어떻게 실행되고 있는지 실행계획을 확인한다.(2) NL Join / Sort Merge Join / Hash Join을 실행계획은 통해 확인해서 상황에 맞지 않은 수행원리인 경우, Oracle의 Hint를 써서 다른 방식으로 쿼리를 유도를 해본다.(3) I/O나 실행시간이 어떻게 변화하는지 확인을 해서 더 좋은 방향으로 튜닝을 한다.","link":"/2022/05/23/202205/220523-sql-study/"},{"title":"220528 데이터 파이프라인 구축 오프라인 수업 수강 전 준비","text":"좀 더 많은 것을 얻어가기 위해서앞선 블로그 포스팅에서 이미 언급했듯이, 내일 드디어 데이터 파이프라인 구축과 관련된 오프라인 수업을 들으러 간다.여지까지 인터넷으로도 그렇고 수업을 들으면서 느낀거지만, 같은 시간, 같은 수업을 통해서 더 많은 것을 얻어가기 위해서는 사전에 이 수업을 통해서 무엇을 얻고자 하는지, 전체적인 수업 커리큘럼에 대한 이해가 필요하다. 아직 구체적인 수업내용에 대해서는 전달받지 못했지만, 그래도 표면적으로라도 어떤 것들을 배울 것인지에 대해 대략적인 정리를 하고 수업에 임한다면 좀 더 많은 것을 얻을 수 있을 것이라고 생각한다. 수업 사전 준비사항수업전에 ssh 접속을 위한 터미널 프로그램을 설치해오라는 사전공지가 올라왔다.나는 기존에 그냥 Mac에서 기본으로 제공해주는 터미널을 통해 ssh 접속을 했었는데, 이번기회에 iTerm2를 사용해보려고 한다. iTerm2 설치 및 커스텀iTerm2는 터미널과 같은 기능을 하지만, 더 다양한 기능을 제공해주는 친구이다.처음 딱 설치를 해봤더니 기본으로 bash shell로 설정이 되어있다. 그래서 bash의 확장된 shell인 zsh을 설치해보려고 한다. homebrew를 사용한 zsh 설치 우선 homebrew를 설치한다. 1$/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; Oh-my-zsh 이게 뭔데? 구글링을 하던 도중에 zsh 설치와 함께 Oh-my-zsh을 설치해서 사용하면 좋다는 글들을 보았다. 찾아보니 Oh-my-zsh은 Zsh 환경설정을 관리하기 위한 프레임워크라고 한다.이런 shell도 프레임워크를 통해 관리를 하는구나.. 처음 알게 되었다.oh-my-zsh에는 많은 플러그인과 테마가 있어서 zsh을 좀 더 편하게 사용할 수 있게 해준다고 한다.아무튼 그냥 일반 터미널만 사용해온 나 같은 사용자에게 아주 개쩌는 사용경험을 선서해 줄 그런 친구라는 말이다.여지까지 개발관련 일을 했음에도 oh-my-zsh의 존재를 이제 알게 되었구나. 반성한다. $ oh my zsh 공식 홈페이지 https://ohmyz.sh/ 우선 설치한 homebrew를 사용해서 zsh과 oh-my-zsh을 설치한다. 123$brew install zsh$sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; oh my zsh을 설치하면 위와같이 알록달록한 텍스트들이 출력된다. 색상 Theme 설정하기 https://iterm2colorschemes.com/ 페이지로 가면, iTerms에 적용할 수 있는 다양한 색상 테마들이 나오는데, 그 중에서 나는 synthwave 테마가 마음에 들어서 선택했다. 12345$brew install curl$mkdir util &amp;&amp; cd util$curl -LO https://raw.githubusercontent.com/mbadolato/iTerm2-Color-Schemes/master/schemes/synthwave.itermcolors 이제 다운받은 iTerm color theme 파일을 iTerm의 preferences 메뉴-Colors에서 import를 해준다. 이후에 터미널 theme과 Font를 변경해주었다. 내가 이 수업을 통해서 얻고자 하는 것한 달 반 동안 인터넷 강의를 통해 대략적으로 데이터 파이프라인이 무엇이고, AWS를 활용해서 구축을 하게 되면, 어떤 이점이 있는지 그리고 AWS의 어떤 서비스를 이용해서 데이터 수집, 분석 파이프라인을 구성할 수 있는지에 대해서 배웠고, 직접 실습을 하면서 익숙해지고 있는 중이다.나는 이번 오프라인 수업을 통해서 실제 데이터 엔지니어로써 현업에서 많이 사용되는 기술 스택과 현업에서 겪을 수 있는 문제점 등에 대해서 배우고 싶다. 아직 심화된 내용을 알기에는 지식이 부족하지만, 대략적으로 데이터 파이프라인을 구축에 대해서 학습한 현 상황에서 의문이 드는 부분이 AWS의 각종 서비스를 활용해서 데이터 파이프라인을 만들었는데, 실제 현업에서는 데이터 파이프라인을 구축한 다음에 어떻게 관리를 하고, 유지보수를 하는지 궁금했다.그리고 이전에 배웠던 데이터 파이프라인은 개괄적인 내용들이어서 오프라인 수업에서는 커리큘럼에 명기되어있듯이, 현업에서 가장 많이 사용되고 있는 Apache Kafka와 ELK 스택(Elasticsearch, Logstash, Kibana)에 대해서 좀 더 알아보고 싶다.대략적으로 각 기술 스택들이 어떤 역할을 하는지, 그리고 어떻게 구성되는지에 대해서는 개괄적으로 알고 있지만, 이번 수업을 통해 각 기술 스택들에 대해 알지 못했던 내용과 이미 알고 있던 내용이라도 좀 더 정제된 내용을 알고싶다. 이번 강의가 기대되는 이유중에 하나는 나와 같은 입문자들을 위해 최대한 핵심을 설명하고, 현업에서 겪을 수 있는 문제들에 대해서도 다룬다는 점이다. 단순히 데이터 파이프라인을 구성하는 각 컴포넌트에 대한 설명 뿐만 아니라, 각 설정 값들을 바꿔보면서 하나씩 실습을 해나간다는 점에서 매력적인 것 같다.그리고 수업이 데이터 엔지니어 뿐 아니라 백엔드 개발자나 DevOps엔지니어 영역에서 사용되는 기술들도 다룬다고 하니, 향후에 가지치기 하면서 학습할때 방향설정에 많은 도움이 될 것 같다. 수업에서 배우게 되는 기술 스택들우선 수업을 듣기 전에 수업에서 다루게 될 각종 기술 스택들에 대해서 배경지식을 쌓는 것이 좋을 것 같다. 이미 데이터 파이프라인 구축에 대해서 학습해본 경험으로 미루어보면, 각 종 기술 스택들이 갑자기 밀려들어오면, 나도 모르게 갑자기 혼란스러워지는 타이밍이 온다. 그래서 우선 전체적인 데이터 파이프라인의 flow에 대해서는 이해를 하고 있으니, 각 종 기술 스택을 현재 가지고 있는 지식 수준에서 연결고리를 만들어서 배경지식을 쌓아놓으려고 한다. 데이터 수집 파이프라인 이미 한 번 학습을 한 내용 : logstash, Amazon EC2, Kafka, Amazon Kinesis Data Streams, Amazon Kinesis Data Firehose 새롭게 학습하게 될 내용 : beats, fluentd, AWS Lambda 데이터 저장 및 분석 파이프라인 이미 한 번 학습을 한 내용 : Amazon S3, Elasticsearch, hadoop 새롭게 학습하게 될 내용 : Apache Flink, Amazon REDSHIFT","link":"/2022/05/28/202205/220528_datapipeline_study-1/"},{"title":"220528 데이터 파이프라인 구축 관련 학습의 앞으로의 계획","text":"새로운 수업 그리고 앞으로의 계획내일 드디어 한 달전에 등록한 데이터 파이프라인 구축 관련 수업을 들으러 간다. 이번 수업이 기대가 되는 이유는 실제 실무에서 데이터 엔지니어로 근무를 하시는 실무자 분께서 해주시는 수업이라 너무 기대가 된다. 여지까지 인터넷 강의를 통해서 AWS를 활용한 데이터 파이프라인을 구축에 대해서 학습을 했었다. 그리고 이전에 가지고 있었던 데이터 엔지니어의 업무 중 하나인 데이터 파이프라인 구축에 대해 가지고 있던 궁금증이 많이 해소가 되었다.지금도 그 때 인터넷 강의로 공부하면서 남겼던 블로그 글들을 다시 보고 직접 실습을 하면서 내가 놓친 부분이 없지는 않은지, 다시 확인하고 기억이 안나면 다시 강의를 보고 다시 정리하고 하는 과정을 반복하고 있다. 사실 처음 듣고 정리를 해두면, 어느 정도 이해했다 싶다가도 시간이 흐르고 다시 내용을 되새겨보면 약간 흐릿하게 기억 속에 남아있는 경우가 많다. 이 시점에 다시 복습을 해주면, 정말 이전보다는 좀 더 오래 기억에 남고, 이전에는 간과했던 세부적인 내용들까지 눈에 들어와서 더욱 학습이 잘 되는 것 같다. 물론 인터넷 강의에서 수업해주신 강사님도 너무 잘 가르쳐주셔서 많은 도움이 되었지만, 좀 아쉬웠던 부분이 나 같은 입문자들을 위한 강의여서 그래서인지 단편적인 예시가 많았고, 실습에서도 이미 제공된 파일을 통해 빠르게 실습을 하고 넘어가는 부분이 꽤나있어서, 많이 아쉬웠다.아 그리고 또 아쉬웠던 부분은 인터넷 강의여서 그런지, Q&amp;A 게시판에 질문을 올리면 답변이 너무 안올라와서 ㅎㅎ 좀 많이 답답했다. 그래서 강의를 듣고 난 후에 개인적으로 공식 사이트나 구글링을 해서 궁금한 부분을 해소하기도 하고, 제공된 *.jar파일을 툴을 사용해서 코드를 까보고, 작성된 코드(Java)를 Python 코드로 다시 작성을 해보면서 데이터 파이프라인에 데이터를 흘려보내보기도 하였다.이렇게 학습을 하니 뭔가 플러스 알파로 더 많이 알게 되는 내용들도 생겨나서 학습하는 내내 기분 좋게 공부를 할 수 있었다. 위와 같은 이유로 이번에는 오프라인 수업으로, 실제 현업자 분이 오셔서 진행하는 수업을 신청하였다. 생각보다 가격이 많이 비쌌지만, 개강 한 달 전에 미리 알람 신청을 해두고, 한 달 일찍 수강신청을 해서 좀 더 저렴해진 가격에 시청을 할 수 있었고, 여담이지만, 수업을 들을까 고민을 많이 하면서 사이트를 하루에 여러번 들어가서 강의 커리큘럼도 다시 읽어보고, 가격도 다시 확인하면서 들락날락 거렸는데, 요즘에는 이런 사용자 데이터도 관리가 되는지, 이메일로 강의신청할까 말까 고민 많으시죠? 라는 내용과 함께 무려 15% 할인 쿠폰이 제공되었다! ㅎㅎ그래서 훨씬 저렴해진 가격에 수업을 신청할 수 있었다. 저렴해져도 여전히 나에게는 비싼 가격이지만, 비싼 수강료만큼 많이 배우면 된다고 생각한다. 그래서 이전에 조금이나마 공부했던 데이터 파이프라인 구축 관련된 내용을 다시 복습하고, 모르는 내용이 생기면 수업시간에 강사님께 질문 할 생각이다. 이번 수업을 통해 앞으로 데이터 파이프라인 구축과 관련하여 개인학습을 이어가고, 포트폴리오에 넣을 프로젝트들도 어떻게 구성을 할지에 대해서 많이 고민해보는 유익한 시간이 되었으면 좋겠다. 이전에 유튜브 영상을 보다가 박지성 선수와 관련된 이야기가 소개가 되었었는데, 그렇게 빛나보이고 슬럼프 같은 것도 없을 것 같았던 박지성 선수도 나름의 슬럼프가 있었고, 그 과정에서 축구를 하면서 같은 팀 선수에게 볼을 패스하는 매 순간마다 자기 자신에게 &quot;자, 봐 할 수 있잖아&quot;라고 자기 스스로에게 사소한 것 하나 하나에 칭찬을 했다고 한다.그리고 그 슬럼프를 극복했다고 한다. 이 이야기를 듣는데, “프로 선수임에도 저런 과정을 겪는구나”라고 생각했고, 뭉클했다.솔직히 나도 몇 주 전까지만 해도 나름의 슬럼프가 있었다.그럼에도 지금은 계속해서 학습을 하고 있고, 무언가를 새롭게 배우는데 있어 즐겁다.그래서 스스로 꾸준하게 학습을 하고 있는 나 자신에게 칭찬을 해주려고 한다.&quot;잘 하고 있고, 잘 될꺼고, 지금처럼 꾸준하게 한 발 한 발 나아가자.&quot;","link":"/2022/05/28/202205/220528_datapipeline_study/"},{"title":"220528 SQLD 시험후기","text":"SQLD 자격시험 후기오늘 SQLD 자격시험을 보고 왔다.마침 고사장이 서울 지방 병무청 근처 성남고등학교였는데, 오늘이 신림선 경전철 노선이 운행을 하게 되는 첫 날이라 보라매역에서 신림선 경전철을 타고 시험을 보고 왔다.운행 첫 날인데 이용하는 사람들이 생각보다 많았던 것 같다. 자, 이제 시험 이야기로 돌아와서 시험은 객관식 40문제에 주관식 10문제로, 주관식은 평이하게 나왔고, 객관식에서 헷갈리는 문제가 꽤나 많았지만, 전반적으로 풀만한 문제들이 많았다.이번 시험을 준비하면서 앞으로 SQL 공부를 어떻게 더 해나갈지 방향을 잡는데 도움이 되었던 것 같다. 사실 이번 SQLD 자격시험을 준비하게 된 계기가 기존에 너무 기본적인 SQL지식만 알고 있어서, 나중에 데이터 가공할때 SQL 쿼리문을 많이 사용할텐데 어떻게 하면 앞으로 좀 더 깊이있게 공부 할 수 있을까? 라는 고민에서 시작하게 되었다. 생각보다 SQL도 공부해야되는 양이 만만하지 않다는 것을 느꼈고, 실제로도 많이 사용해보고 익숙해지는 것이 제일 중요하다는 것을 알게 되었다.최근에 포트폴리오에 넣을 데이터 파이프라인 구축 관련 프로젝트를 준비중인데, 프로젝트 중에 하나가 지금 Data Lake 구축까지 끝난 상태여서 Bronze data를 Sliver나 Gold Data로 가공하는 과정에 들어가려는 시점이다. Zepplin에서 Spark Interpreter를 사용해서 데이터를 가공해야하는데, Spark SQL에서 쿼리를 사용해서 가공을 하기 때문에 여기서 다양한 쿼리를 많이 사용해봐야겠다고 느꼈다. 작은 프로젝트지만, 시험 대비를하면서 직접 실습한 부분도 있지만, 그렇지 못한 부분도 꽤나 되기 때문에 개인 프로젝트에서 학습했던 쿼리문들을 최대한 활용해서 Data Lake에 쌓인 데이터를 분석해봐야겠다. 혼자 공부를 하면서 가장 힘든 점이 꾸준히, 그리고 지속적인 동기부여를 얻는 것이 어려운데, 자격증 준비가 어떻게 보면, 지속적으로 동기부여를 갖고 학습을 이어갈 수 있는 원동력이 되는 것 같다. 앞으로도 다음 이직전까지 데이터 엔지니어가 되기 위해 필요한 지식을 쌓기 위한 목적으로 자격증 3개 정도 더 취득할 예정이다. 요즘에는 단순 문제풀이가 아닌 좀 더 실용적인 목적의 자격증들이 많이 나온 것 같아 내가 학습을 한 내용을 정말 이해를 하고 있는지에 대해 확인해보면서 학습을 꾸준히 이어갈 수 있을 것 같다.그럼 앞으로 좀 더 힘내보자!","link":"/2022/05/28/202205/220528_sqld_exam/"},{"title":"220529 데이터 파이프라인 구축 오프라인 수업 &#x2F; 1주차","text":"이번 포스팅에서는 5월 29일날 오프라인 강의로 수강하였던 데이터 파이프라인 구축과 관련된 수업내용에서 대해서 정리해보려고 한다. 3시간 동안의 수업을 듣고 나서 느끼고 배웠던 점이 많았기 때문에 기대 이상의 수업이었던 것 같다. 사실 이전에 인터넷 강의로 관련 강의를 수강하였기 때문에 데이터 파이프라인을 AWS 클라우드 환경에서 구축하는 부분에 대해서는 약간의 배경지식을 가진 상태였다.그래서 아마 오늘 이 오프라인 수업 시간이 더욱 유익하게 느껴졌는지도 모른다. 이번 수업을 통해 기존에 내가 잘 못 알고 있던 기본 개념과 단편적으로만 알고 있던 데이터 파이프라인 컴포넌트들에 대해서 좀 더 구체적으로 알게 되었던 것 같다. 그리고 가장 좋았던 점은 앞으로 어떤식으로 추가적으로 학습을 하면 좋은지에 대해서 강사님이 방향 제시를 해주신 부분이 좋았다. 그러면 이번 포스팅에서는 배웠던 이론적인 부분과 내가 잘 못 알고 있었던 부분, 그리고 앞으로 추가적으로 학습할 내용에 대해서 정리를 해보겠다. 데이터 엔지니어링 vs BE/FE 데이터 엔지니어링 분야는 FE/BE 분야와는 다르게 정확히 정해진 F/W가 없다. 다만 빅데이터 아키텍처가 존재하며, Lambda Architecture와 Kappa Architecture, 두 아키텍처가 있다. 이 두 아키텍처에 대해서는 아래에서 정리를 해보겠다. Big Data의 3V학문적인 자료에서의 빅데이터 정의 = 3V Volume(용량) : TB, PB 수준의 대용량 데이터 Variety(다양성) : 데이터 내의 속성이 다양해짐 (성별 외에도 취향 등의 다양한 속성들을 갖는다) Velocity(속도) : 다양한 데이터들이 엄청난 속도로 쏟아지는 것 Veracity(정확성, 진실성) : 과거에는 통계적 관점에서 데이터를 분석했기 때문에 정확성에 대한 고려가 안되었으나, 최근에는 정확(좀 더 데이터의 세부적인 조건에 맞는 데이터)하고 신뢰성이 있어야 한다. 내가 서비스하는 플랫폼의 사용자로에게 좀 더 편리함을 제공해주기 위해서 데이터 기반으로 의사결정 및 추천을 하고, 이러한 의사결정 및 추천을 위해서 가치있는 데이터를 뽑아내는 것이 바로 데이터 엔지니어의 역할이다. 수업시간에 강사님이 소개해주신 넷플릭스 오리지널 시리즈의 “거대한 해킹(The Great Hack)”에 나오는 말이라는데, 인상깊어서 글을 남겨본다. &quot;우리의 온라인 활동에서 나오는 데이터가 그냥 사라지진 않는다. 우리의 디지털 흔적들을 모으고 분석하면 매년 1조 달러 규모의 사업이 된다.&quot; 이전에 데이터가 제2의 석유라는 말을 들은 적이 있는데, 정말 데이터들이 엄청난 가치를 창출해내는 요즘 데이터 엔지니어 분야는 정말 매력적인 것 같다.(데이터 기반의 의사결정) 데이터 기반 테크 기업(1) snowflake : DW 데이터를 효율적으로 적재하고, 적재적소에 빼서 사용할 수 있도록 하는 솔루션 제공(2) Xplenty / iTechArt / AdTech : 광고 관련 서비스를 하는 기업으로, 데이터를 기반으로 한 광고 추천(3) Cloudera : Hadoop 빅데이터 서비스를 상용서비스로 제공해주는 업체(이전에 Hortonworks로 실습을 했을 때 Cloudera에 합병되었다는 이야기를 들었다)(4) databricks : 아파치 스파크를 유지보수하고 솔루션을 만드는 업체(5) elastic : Elasticsearch를 만든 업체 (유지보수, Saas형 클라우드 서비스 제공)(6) TREASURE DATA(7) teradata 데이터만 다루는 큰 규모의 회사들이 많이 생겨난다. 그리고 많은 기업들이 데이터를 이용해서 문제를 해결해나가려고 한다. 수업시간에 배우게 될 내용이번 오프라인 수업에서는 아래의 내용들에 대해서 학습하게 될 예정이다. 데이터 엔지니어링 개요 / 데이터 파이프라인 데이터 수집기 (Logstash, Filebeat, Fluentd) AWS (S3, Athena, Kinesis, Glue, Lambda 등) Elasticsearch / Kafka (BE 영역에서도 많이 사용된다) Apache Flink (Batch processing + Stream processing / Python으로 코딩 할 수 있는 인터페이스를 제공) Apache Spark (관련 AWS 서비스를 활용해서 수업) 데이터 엔지니어링 ? 데이터를 수집, 저장, 처리하기 위한 시스템을 구축 및 운영 데이터 파이프라인을 개발/구축/운영 전통적인 관점에서의 ETL(Extract/Transform/Load) 작업을 수행 빅데이터 시스템을 유지보수/관리하기 위함 데이터 플랫폼을 개발 및 운영(기업에 맞게 기존 서비스들을 커스텀해서 개발) 데이터 엔지니어를 위한 공부 반복되는 작업에 대해서는 직접 API로 개발을 해서 자동화를 해야한다. 그리고 실시간으로 대량의 데이터를 처리하는 곳에서는 gRPC를 도입하는 경우도 많다. (gRPC 개별적으로 공부하기) 좋은 데이터 엔지니어가 되기 위해서는 프로그래밍을 잘 할 줄 알아야 한다. Scala로 개발된 Spark(빅데이터를 분석하기 위해 사용)도 Python으로 제공 Flink 실시간 처리 플랫폼에서 Python으로 코딩할 수 있는 인터페이스를 제공한다. SQL 지식 데이터베이스에 대한 지식이 필요하다. OLTP(On-Line Transaction Processing: MySQL과 같은 RDB에서 트랜젝션 처리 , OLAP(On-Line Analytical Processing): 온라인 기반 분석을 위한 DB Elasticsearch / MongoDB Data warehouses : Snowflake Object storage : AWS S3 Cluster computing 기본 : Hadoop, HDFS, MapReduce, Amazon EMR Data processing에 대한 지식 : Hybrid(Apache Flink), Streaming(Apache Kafka, Amazon Kinesis) Workflow scheduling, Monitoring data pipeline에 대한 지식 Container / Orchestration 툴 : Docker, Kubernetes, Apache Mesos 데이터 파이프라인 데이터를 순차적으로 전달하는 시스템 (출력이 다음 단계의 입력으로 이어지는 구조) 요청된 데이터를 특정 Platform을 통해서 제공하는 것을 Pipeline이라고 한다. (직접 데이터를 추출해서 전달하는 것은 Pipeline의 특성으로 보기 어렵다) ETL 작업들을 데이터 파이프라인이라고 한다. 수집된 데이터를 정해진 시간 텀으로 처리를 해야되는 경우, 스케쥴러를 사용한다. (현업에서는 Airflow를 가장 많이 사용한다)-&gt; Airflow는 복잡한 구조의 스케줄링을 할 때 사용된다. (개인적으로 Apache Airflow를 학습할 것을 권장) 데이터 엔지니어의 전체 업무 프로세싱이 원유를 시추해서 가공하는 과정과 유사하다. 데이터 간섭/속도를 최적화하기 위한 처리를 소프트웨어적으로 처리하는 것을 데이터 엔지니어가 담당한다. ETL : 전통적인 데이터 파이프라인 과거에는 기존 RDS에서 데이터를 추출해서 분석을 했기 때문에 ETL이라는 용어가 사용되었다. 추출된 데이터를 정제해서 새로운 분석용 DB를 생성해내는 일련의 과정을 ETL이라고 한다. 데이터 기반이 아닌 초기 스타트업 같은 경우에는 정재된 데이터가 없기 때문에 데이터 엔지니어의 업무가 필요가 없는 경우가 많다. ETL 작업이란 큰 범주로 보면, 복수의 DB를 일괄적으로 모아서 Data Warehouse를 구성하고 그 데이터를 Data Mart로 다시 구성하는 각 각의 중간 과정을 ETL 작업이라고 한다. (다층구조로 설계) ETL과 ELT 수집 - 처리 - 저장 - 분석 - 시각화 과정에서 처리 - 저장 순의 경우에는 ETL, 저장 - 처리 순의 경우에는 ELT(일단은 쌓아놓고 처리 - 빅데이터 처리 컨셉)라고 한다. 스케줄러스케줄러는 데이터를 처리하는 일련의 과정을 특정 시간 주기로 합계, 평균을 구하거나 지난 데이터를 가져와야되는 경우에 사용된다. Airflow는 복잡한 스케줄링을 사용하는 경우에 사용된다. 간단한 스케줄링 작업의 경우에는 Amazon EventBridge를 사용한다. 원유 시추 과정 = 데이터 파이프라인 과정(1) 유전에서 원유 채취 : Device/DB로부터 데이터를 채취 (2) 선박으로 운반 : Kafka (3) 원유 탱크에서 저장 : S3나 HDFS에 저장 (4) 석유 정제(정유정제) : 저장된 데이터를 정제/가공해서 정보(쓸만한 insight)를 뽑아내는 과정 앞으로 개인적으로 공부하면 좋은 것 OLTP / OLAP Beam : 여러 언어와 Runner를 제공하고, 각 서비스들을 proxy해주는 역할을 한다. 순서상 Flink나 Spark를 우선적으로 학습하고, Flink와 Spark 배경지식을 기반으로, 나중에 학습하는 것을 권장ref. 찾아보니깐 Beam은 데이터 프로세싱 파이프라인을 개발할 때 유용한 환경을 제공해주는 통합 프로그래밍 모델이다.빔에서는 다양한 언어들과 러너를 제공하고 있으며, Flink가 제공되는 여러 러너 중에 하나이다. BigQuery의 사용으로 인해서 AWS -&gt; GCP로 넘어가는 경우가 많다. (ML을 많이 하는 경우가 많다)-&gt; BigQuery가 데이터 웨어하우스 분석시에 많이 사용되기 때문에 우선적으로 BigQuery를 학습하고 나중에 GCP를 학습하는 것이 좋다. 대표적인 빅데이터 아키텍처 빅데이터 아키텍처 (1)Lambda Architecture Batch processing : 데이터를 특정 시간동안 모아서 처리하는 방식(민첩성이 떨어진다)으로, 대부분의 업무처리가 이 배치 처리방식으로 처리된다. 그리고 배치처리는 영속성이라는 특성을 가진다. (이미 클러스터에 데이터를 저장하고 있기 때문에 사후 분석이 가능한 형태이다) Stream processing : 건 바이 건으로 들어오는 데이터를 즉시 처리하는 방식으로, 실시간 데이터셋을 처리해야되는 경우 유용하다.실시간으로 matching 시켜야되는 데이터 처리를 할 때 많이 사용되며, Queue에 저장된 데이터를 개별적인 공간에 저장을 해서 데이터의 영속성이라는 특성을 갖게 할 수는 있다.네트워크 문제로 데이터의 loss가 발생(Unbounded data processing)할 수 있지만, 최근 기술로는 데이터의 loss가 생기지 않는다. 그래서 과거에는 대략적인 결과를 얻기위한 목적으로 사용되었다.(Approximate result)그리고 과거의 데이터나 재처리를 크게 고려하지 않는다는 특징을 가지고 있다.또한 네트워크 issue로 인해 Stream processing을 통해 유입된 데이터가 예상된 순차적 데이터로 처리되지 않는 경우도 있다.(배치처리보다 정확도가 떨어진다 - 이전 기술상의 문제) 위의 issue를 통해서 나온 아키텍처가 람다 아키텍처(Lambda Architecture)이다.람다 아키텍처는 Twitter의 Nathan Marz에 의해 처음 고안되었고, 과거에는 스트림 처리에 대한 지식이 부족했기 때문에 데이터 유실의 문제가 많았고, 배치처리에서는 데이터가 다 쌓인 뒤에 일괄적으로 처리하기 때문에, 결과를 보기 위해서는 모든 데이터가 처리된 이후에 가능했다. 이런 각 각의 Processing상의 문제들을 보완하기 위해 나온 아키텍처가 람다 아키텍처이다. 람다 아키텍처의 구조 배치 레이어(Batch Layer) : 과거 데이터를 장기 저장소에 축적하여 여러 번 다시 집계할 수 있다.(재처리 가능), 대용량 데이터를 처리하지만 1회 처리에 오래 걸린다. 서빙 레이어(Serving Layer) : 정기적으로 데이터가 업데이트되어 정보를 출력한다. (단, 실시간 정보는 아니다)배치 레이어의 경우에는 서빙 레이어에 별도의 DB를 위치하기도 한다. (DW/DM) 스피드 레이어(Speed Layer) : 스트림 데이터를 처리하며, 데이터 보관 기간이 짧1아서 일정 기간이 지난 데이터는 삭제된다. 람다 아키텍처의 처리 과정 데이터 수집기를 통해 들어온 데이터는 모두 일괄적으로 배치/스피드 레이어에 공통으로 유입되게 된다. 배치 레이어 + 서빙 레이어 = 배치 처리의 결과를 서빙 레이어에 정기적으로 저장이 해서 결과를 출력한다. Pros이전에 학습했을때는 Lambda Architecture에서 각 각 개별적인 processing으로 분류되서 데이터가 처리되는지 알았는데, 이 부분은 잘 못 알고 있던 부분이었다. 람다 아키텍처는 스트림 처리와 배치 처리 작업시에 각 각의 처리가 가지고 있는 문제점을 상호 보완하는 데이터 아키텍처이다.배치 처리의 처리시에 생길 수 있는 latency 동안에는 실시간 뷰를 사용(정확성이 다소 떨어짐)하고, 배치 뷰가 업데이트 되면 배치 처리를 사용한다.(상호보완적 처리 과정) Cons 각 각의 독립되어 구성된 파이프라인을 유지/보수해야되기 때문에 효율성이 떨어진다. 두 파이프라인에서 나온 결과를 병합하거나 양쪽에 query를 해야 되기 때문에 번거롭다. 빅데이터 아키텍처 / (2) Kappa Architecture람다 아키텍처상에서 배치 처리 레이어를 제거한 아키텍처를 말한다. Kafka(실시간 message queue) 개발자인 Jay Kreps에 의해 제안된 아키테처 모델이다. PC의 Performance(CPU/MEMORY 성능향상)가 좋아짐에 따라 오직 스트림으로 데이터를 처리할 수 있게 되었다. 데이터 전달이 보장됨에 따라 스트림으로 전달되는 데이터의 유실 가능성이 낮아지고, 데이터의 신뢰성이 높아졌다. ref. 데이터 전달 보장 유형에 따른 분류at-most-once : 최대 한 번 보장(더 전달), 메시지 유실 가능성이 있음at-least-once : 최소 한 번 보장(더 전달), 메시지 유실 가능성 없음, 중복 가능성 있음 (Kafka 지원)exactly-once : 전달 보장, 메시지 유실 가능성 없음, 중복 가능성 없음 (Flink 지원) 과거 메시지 누락에 대한 문제를 보완한 replayable(재현가능한) 메시지 브로커인 Kafka가 등장하였다. ref,과거에 흘려보낸 스트림 데이터를 다시 흘려보낼 수 있다. (과거 TV 방송과 IP TV의 방송 다시 보기 비교 예시) 실시간 데이트 앱이나 우버와 같이 사용자와 실시간으로 매칭시켜줘야되는 needs가 많아지기 때문에 중요해진 아키텍처 대표적인 스트림처리 엔진은 Kafka, Apache Flink(배치 + 실시간 스트림 처리 가능)가 있다. Pros 스트림 처리로만 파이프라인을 구성하기 때문에 단순화 두 처리 방식의 결과물을 병합처리하는데 사용되는 Computing resource가 절약 단일 파이프라인과 단일 뷰로 개발/운용에 효율적이다. Cons 데이터 재처리시에 리소스가 많이 요구된다.=&gt; 데이터 재처리시(리소스가 많이 요구되는 case)에는 클라우드 서비스를 사용해서 instance를 임시로 띄워서 처리하고 생성한 instance를 없애는 형태로 기존 리소스의 한계를 보완하고 있다. 실사간성, 빠른 바능성으로 인해 CPU 집약적인 작업(computation intensive)(데이터를 쌓아놓고 모델링을 학습시키는 ML/DL)에는 적합하지 않을 수 있다.단, Flink를 활용해서 실시간으로 받은 데이터를 활용해서 ML/DL을 할 수는 있다. Cloud Service 확장성 : 늘어나는 데이터를 손쉽게 대응할 수 있다. (컴퓨팅 자원, 스토리지, 신규 기능) 효율적인 자원 활용 및 비용 효율성 관리형 서비스를 사용함으로써 얻게되는 이점 (개발/구축을 위한 인력의 필요성이 줄어든다) Compliance issue로부터 constraint되는 문제를 해결 할 수 있다. (클라우드의 개인정보 관련 법적/제도적 이슈를 인증해서 해결할 수 있는 방법이 생겨나고 있다) 대체 불가능한 서비스 (Bigquery, S3) 추가적으로 학습하면 좋은 것들 Docker / Kubernetes Hadoop / Hive / Presto / Trino Apache Spark Airflow AWS IaC(Infrastructure as Code): ansible, terraform, AWS CDK, AWS CloudFormation 코드로 인프라를 관리, 구축할 수 있도록 하는 기술 CI/CD 플랫폼에 대한 지식 첫 실습(Beats + Logstash + Elasticsearch)실습에 사용할 Beats, Logstash는 elastic이라는 기업에서 오픈소스화한 프로젝트인데, 이러한 오픈소스 프로젝트로 유명해진 기업이다. 이전에 Logstash를 통해 외부의 로그 데이터를 받고, 받은 데이터를 Kafka를 통해 Consumer instance로 흘려보내주는 일련의 과정을 실습한 적이 있다. 이때는 Logstash에 파일에 정의된 input, filter, output을 통해서 데이터를 Kafka로 넘겨주는 역할을 한다고만 알고 있었는데, Logstash의 input, filter, output에는 다양한 plugin이 존재한다. Logstash의 과거와 현재 그리고 Flink의 필요성 과거에는 Logstash가 단순 collector 역할을 했지만, 최근에는 filter해주는 다양한 플러그인 기능들을 제공한다. 사용 용도에 따라 찾아서 사용해보자. Logstash에서 제공해주는 다양한 플러그인 중에 S3로 넘겨받은 데이터를 저장할 수도 있고, Kafka로 데이터를 넘겨줄 수 있도록 도와주는 플러그인도 있다. (input/output plug-in을 다양하게 사용해서 조합할 수 있다)그럼 Flink와 Kafka가 필요한 이유는 무엇인가? logstash는 단일 Agent(Single node)로 데이터의 병렬 처리가 불가능하기 때문에 Flink를 사용해서 클러스터 내의 여러 노드로 대량의 데이터를 분산시켜서 처리할 수 있다. [다양한 input/output 조합]file - logstash - elasticsearchfile - logstash - kafkakafka - logstash - elasticsearchkafka - logstash - hadoop… ref. Logstash Input/Filter/Output의 다양한 플러그인들 (1) Input : https://www.elastic.co/guide/en/logstash/current/input-plugins.html (2) Filter : https://www.elastic.co/guide/en/logstash/current/filter-plugins.htmlparsing, transform도 가능하다. (3) Output : https://www.elastic.co/guide/en/logstash/current/output-plugins.html Beats - 경량 데이터 수집기 이번 실습에서는 Beats라는 경량 수집기를 사용한다. logstash는 데이터 파이프라인으로, 여러 소스로부터 데이터를 긁어서 여러 데이터 저장소에 저장을 할 수 있는 다양한 기능을 제공하므로, 매우 무겁다.따라서 빠르게 데이터만 수집할 수 있는 Beats라는 경량 데이터 수집기가 등장하였다. Logstash보다는 제한된 기능을 하지만, 리소스를 적게 먹는 경량 데이터 수집기의 역할을 한다. Beats를 container에 심어서 사용하는 경우도 있다. 그럼 logstash를 사용하지 않고, beats만 사용해도 되지 않나?Logstash와 같은 Aggregator가 존재하는 이유는 다수의 Server cluster의 노드들(beats)에 공통된 로직을 적용하기 위함이다. 또한 Storage에 복수의 connector로 연결시키게 되면, 과부하가 발생된다.(개별 노드에서 처리를 하게 되면 리소스가 그만큼 많이 소모된다.) 실습1) 구체적인 실습 과정 Logstash - Logstash도 가능한데, 실습에서는 각 서버에 beats를 설치하고, 각 beats를 Logstash에서 모아서 Store에 저장을 하는 구조로 구성을 한다. EC2 Instance 생성하기 logstash는 java base 이기 때문에 jdk를 필수로 설치해줘야 한다. 오늘 날짜를 기준으로 file name format을 맞춰서 log파일을 생성해준다. 일반적으로 vm이나 container에서 환경에서 앱을 띄워놓고 생성된 로그정보를 local에 로그파일 형태로 떨구고, 떨궈진 로그파일을 logstash가 읽어서 처리하게 된다. 따라서 logstash에서 File을 읽는 처리는 중요하다.","link":"/2022/05/29/202205/220529_datapipeline_study/"},{"title":"220531 Docker 스터디 1일차","text":"이번 포스팅에서는 도커에 대해 학습한 내용에 대해서 정리하려고 한다.도커를 이해하기 위해서는 서버를 관리하는 것에서부터 시작해야 되는데, 서버관리는 내부적으로 매우 복잡하고, 각 각의 과정이 서로 종속되어있다. 전통적인 서버 관리 방식으로 비춰보면, 서버에 특정 서비스를 설치하고자 할 때, 환경적인 제약으로 인하여 예상하지 못한 문제가 발생하기도 하고, 지속적으로 바뀌는 서버 및 개발 환경으로 인해 지속적으로 기존의 서버를 다시 설정해야 되는 경우가 생겨서, 서버의 유연성이 많이 떨어진다. Docker의 등장위와같은 전통적인 서버 관리 방식에서의 환경적 제약으로 인한 설정 오류와 유연한 서버 환경 교체에 대한 솔루션으로 등장하게 된 것이 바로 도커(Docker)라는 친구이다.Docker를 사용하면, 어떠한 서비스(프로그램)을 하나의 컨테이너로 만들어서 관리를 할 수 있다. 그리고 만들어진 컨테이너는 어떤 환경적인 제약도 없이 어디서든 돌아가게 된다.(AWS, AZURE, GOOGLE CLOUD, KT UCLOUD, NAVER CLOUD PLATFORM 등…) Docker가 등장하기 전에는 서버내에서 각 각의 서비스가 다른 버전의 Package를 사용하는 경우, 버전 변경에 있어 어려움이 있었다. 그로인해 배포를 위한 전체 과정 중에 한 부분이라도 문제가 생기게 되면 서비스가 구동이 되지 않는 문제에 직면하기도 하고, 전통적인 서버관리의 방법에는 많은 문제가 있었다. 서버 관리 방법의 변천위의 전통적인 서버관리 방식에서 생겨나게 된 문제를 개선하기 위해서 가장 먼저 도입된 것이 바로 &quot;서버배포를 위한 메뉴얼화(문서화)&quot;였다. 하지만, 잘 정리된 문서를 보고 따라하더라도 제대로 동작하지않는 경우도 있었고, 문저 정리가 중간에 잘 안되는 경우도 많았다. 그리고 특정 OS를 타겟으로 작성된 문서의 경우, 다른 OS에서 예외가 발생하는 경우도 많이 생겼다. 이러한 문서관리에 문제가 많아 생겨나게 된 것이 바로 &quot;상태관리 도구(CHEF, PUPPET LABS, ANSIBLE)&quot;이다. 이 상태관리 도구는 각 서버 설정의 단계를 스크립트로 작성을 하고 관리를 하기 때문에 스크립트를 실행함으로써 마치 서버 관리자가 STEP BY STEP으로 명령어를 작성한 것과 같은 효과를 준다.이렇게 코드로 관리함으로써 다른 관리자들과 협업도 가능하고, 버전관리도 가능하며, 코드이기 때문에 깃과 같은 저장소로 공유도 가능하다. 하지만 상태관리 도구를 사용함에 있어 러닝커브가 높다는 문제와 한 서버에 다른 버전의 동일 서비스를 설치할 때 문제가 있다는 것을 알게 되었다. 위의 문제로 인해 다음에 고려된 것이 바로 VM(Virtual Machine)이다. VM을 사용하면 하나의 서버에 여러 개의 OS를 얹어서 격리가 필요한 각 각의 서비스를 각 각의 OS에 설치하면 된다.이렇게 관리하면 좋은 점이 snapshot 기능으로 현재 서버의 상태를 저장할 수도 있다는 장점이 있다.(하지만, 용량이 엄청 크고, 이미지 공유가 어렵다.)하지만 서버를 처음부터 다시 셋팅하려면, 기존에 서버를 구축했던 사람을 통하지 않으면 어려웠고, 많이 느리다는 단점을 가지고있었다. 위의 VM으로 Hypervisor위에 Client OS를 얹고 그 위에 서비스 프로그램을 얹어서 사용하는 방식도 결국에는 하나의 OS의 자원을 각 각의 Client OS로 격리시켜서 사용하지만, 서버에 부하를 많이 주는 방식이기 때문에, Linux의 Kernal을 활용해서 직접적으로 각 각의 프로세스를 기준으로 사용하는 자원을 격리하도록 하는 고급 기술이 등장하였다. 이 기술의 등장으로 서버에서 동작하는 각 각의 프로세스를 가상으로 분리하고, 파일 및 디렉토리도 분리를 할 수 있게 되었고, CPU와 메모리, 입/출력 또한 각 각의 그룹별로 구분을 할 수 있게 되었다. (리눅스 기능을 활용한 빠르고효율적인 서버관리)하지만 너무 고급 기술이며, 일부 소수의 고급 엔지니어들만 가능한 기술로, 기술 사용에 있어 많은 어려움이 있었다. 위의 고급 기술을 고급 엔지니어가 아니어도 사용할 수 있도록 나온 것이 바로 Docker라는 Container 기반의 오픈소스 가상화 플랫폼이다. 위의 고급 기술과 같이 리눅스 커널을 여러 기술을 활용하여 구현하였고, 하드웨어 상에서의 가상화 기술보다 훨씬 가볍고, 이미지 단위로 각 프로세스의 실행환경을 구성하기 때문에 사용하기 쉽다는 장점을 가지고 있다. Docker의 장점우선 확장성(Scalability)과 이식성(Portability)이 높다는 장점이 있다.도커가 설치되어있는 서버라면, 어디서든 컨테이너를 실행할 수 있으며, 오픈소스이기 때문에 특정 회사나 서비스에 종속적이지 않다.그리고 쉽게 개발 서버를 만들 수 있고, 테스트 서버 생성에도 매우 편리하다. 또한 표준성으로 각 각의 배포 방식이 다른 서비스들을 컨테이너라는 표준으로 배포하기 때문에 모든 서비스들의 배포과정이 통일된다.($docker-compose up 명령으로 컨테이너 서비스를 올릴 수 있다) Docker의 특징 도커는 이미지에서 컨테이너를 생성하기 때문에 반드시 이미지를 만드는 과정이 요구된다. (Dockerfile을 이용하여 이미지를 생성/처음 배포상태부터 재현가능) 빌드 서버에서 이미지를 만들고, 만들어진 이미지를 이미지 저장소에 저장하고 운영 서버에서 이미지를 호출해서 사용할 수 있다. 도커의 설정은 환경변수로 제어를 하며, 환경변수 제어를 통해 컨테이너를 통해 구동되는 서비스의 환경설정을 할 수 있다.(MYSQL_PASS=password) 하나의 이미지가 환경변수에 따라 동적으로 설정 파일이 생성된다. 도커 컨테이너의 자원은 컨테이너가 삭제되면 모든 데이터가 초기화된다. 따라서 업로드하는 파일을 외부 스토리지와 링크하여 사용하거나 AWS의 S3와 같은 별도의 저장소에 저장하여 사용한다. 또한 세션이나 캐시를 memcached나 redis와 같은 외부로 분리하여 사용한다. Kubernetes의 특징Kubernetes는 여러 서버와 여러 서비스를 관리하기 쉽도록 해준다.그 중 첫 번째로 &quot;스케줄링&quot;기능이 있는데, 만약에 서버들 중에 놀고 있는 자원이 있다면 적절한 서버를 선택해서 배포를 해 줄 수 있다.컨테이너 갯 수가 많다면, 적절히 나눠서 배포를 해주고, 실행중인 서버가 죽으면, 실행 중이던 컨테이너를 다른 서버로 띄워 줄 수 있도록 한다. 여기서도 대용량의 데이터를 각 각의 노드로 분산시켜서 분산저장,분할처리해주는 클러스터링 방식을 사용하고 있다. 클러스터 내에 여러 서버 노드들이 존재하고, 도커의 컨테이너들을 클러스터 내의 각 각의 서버들에 띄워주는 것이다.클러스터링 형태로 사용하게 되면, 여러대의 서버들을 마치 하나의 서버처럼 관리하여 사용할 수 있으며, 여기 저기 흩어져있는 컨테이너들도 가상 네트워크를 통해서 마치 같은 서버에 있는 것처럼 쉽게 통신할 수 있다. 만약 특정 서버에서 동작하고 있는 MySQL서비스 컨테이너를 클러스터 내의 다른 서버에서 동작하고 있는 PHP 서버관련 컨테이너와 Mapping시켜주기 위해서는 어떻게 해야 될까? Kubernetes에서는 이러한 서비스 디스커버리 기능도 제공을 한다.클러스터 환경에서 특정 컨테이너가 어느 노드(서버)에서 실행중인지 알 수 있으며, 이러한 정보(컨테이너 생성/중지시, IP 및 Port 정보를 업데이트)는 Key-Value 스토리지에 저장이 되며, 내부 DNS 서버를 이용할 수 있다. (IP가 아닌 Docker container의 이름으로 해당 서비스를 실행하고 있는 서버를 찾을 수 있도록 할 수 있다)","link":"/2022/05/31/202205/220531_docker_study/"},{"title":"220301 인공지능(AI) 기초 다지기 1편","text":"위에 첨부한 사진은 머신 러닝이라는 용어를 처음으로 사용하고 주변 사람들에게 전파한 아서 사무엘(Arthur Samuel) 교수님이시다. 우선 인공지능이라는 분야에 대해 공부를 시작했기 때문에 시초에 머신러닝이라는 용어를 가장 처음 사용한 사람에 대해서는 상식적으로 알고 있어야겠다고 생각했다. 자, 그럼 이번 포스팅에서는 입문자를 위한 인공지능/머신러닝 관련 강의를 듣고 개인적으로 학습한 내용을 정리해보겠다. 머신러닝(Machine Learning) ? 기계학습이란 명시적인 규칙없이 데이터 간 관련성을 스스로 찾아낼 수 있는 컴퓨터 프로그램을 말한다. 이는 1959년에 Arthur Samuel 교수가 한 말이다. 1998년에 Tom Mitchell은 특정 Task가 특정 경험을 통해 Perfermance measure를 얻었다면, 이러한 데이터의 반복 축적을 통해 컴퓨터가 더 나은방향으로 개선해나가게 되는데, 여기서 컴퓨터는 경험을 통해 자동으로 증진해나감에 집중한다. E - DATA, T - CLASSIFICATION, P - ACCURRACY(정확도를 높이기 위해서는 데이터의 갯수를 늘린다) 전통적인 프로그래밍과 머신러닝의 접근법 차이 전통적인 프로그래밍에서는 프로그래머가 명시적으로 INPUT 값과 PROGRAM(RULES/ALGORISM)을 명시하고 결과 값으로 OUTPUT(ANSWER)을 도출해낸다. 반면 머신러닝은 INPUT 값과 OUTPUT값의 DATASET들을 명시하고 이를 통해 새로운 함수(RULES/ALGORISM)을 도출해낸다. 새롭게 도출해낸 함수를 통해 다른 값들이 들어왔을 때 결과값으로 어떤 값이 도출되는지 예측할 수 있다. 머신러닝의 예시 그렇다면 머신러닝의 예시로는 어떤 것들이 있을까?(1) 강아지 사진이 주어졌을 때, 해당 영상이 어떤 레이블인지 분류(2) 자연어 텍스트의 평점 예측ex) 너무 재미있어요 : 9.0/10.0(3) vector값에 대한 영상 출력 위의 예시 모두 f:x → y 라는 관계가 성립된다. 여기서 x는 이미지, 자연어, vector가 될 수 있고, y는 레이블, continuous data, 영상이 될 수 있다. 머신러닝의 타입 머신러닝의 타입에는 크게 두 분류로 나뉠 수 있다. feedback이 필요한 경우와 그렇지 않은 경우, teacher가 존재하는 경우와 그렇지 않은 경우로 나뉠 수 있다. (1) Teacher가 존재하지 않는 경우는 Unsupervised Learning으로, feedback의 유/무와 관계없이 주어진 data(X)만으로 학습을 해서 결과 값(Y)를 추론하는 것이다.구체적인 방법으로는, Dimensionality Reduction(차원 축소)가 있는데, 이는 입력된 고차원의 데이터를 의미있게 표현하는 방법에 대한 구현 방법이다. 또 다른 방법으로는 Clustering이 있는데, 이는 Supervised Learning의 Classification과 유사한 방법(단, y값이 주어지지 않는다)이다.케이크 모형으로 살펴보면, Unsupervised Learning은 케이크의 거의 전체를 포함하는 비중으로, 전체 머신러닝에서 상당한 비중과 난이도를 포함한다.f(x)에서 값 x만으로 함수 f(x)의 결과 값 y를 도출해내야 하기 때문에 어렵다. (2) Teacher가 존재하는 경우는 Supervised Learning과 Reinforcement Learning이 있다. Supervised Learing은 가장 많이 사용되는 형태의 머신 러닝으로, 직접적 피드백(Direct feedback)이 있다. (X,Y) PAIR 형태로 Labeling 작업된 데이터가 존재한다. 입력 데이터를 통해 결과 값을 추론하게 되므로 다른 방법에 비해 쉽게 해결될 수 있는 방법이다.y = f(x) 구체적인 방법으로는 Classification과 Regression(회귀)이 있으며, pair로 존재하는 (X,Y) 데이터에서 Y데이터의 성격에 따라 그 방법을 달리한다.데이터 Y가 Categorical한 성격을 가지고 있다면, Classification, Continuous한 성격을 가지고 있다면 Regression(회귀)의 방법을 택한다.케이크 모형으로 살펴보면, Supervised Learning은 케익의 데코레이션 부분인 icing 부분으로, 머신러닝에서 매우 적은 부분을 차지하고 있음을 알 수 있다. (3) 마지막으로 Teacher가 존재하는 경우 중 간접적인 피드백(Indirect feedback)을 주는 경우인 Reinforcement Learning은 여러번의 action을 수행하여 간접적으로 어떤 action이 더 나은지 판단하는 방법이다.f(s,a) = r의 형태를 가지며, r(rewards)의 값이 최대화되는 a(action)을 도출해낸다. pair로 존재하는 (S,A) 데이터는 (입,출)을 담당하며, 강화학습의 주된 목적으로 사용된다. Reward가 최대화 되는 방향으로 매 Step마다 Action을 선택해서 문제를 푸는 기법으로, Sequential data 환경에서 사용된다.사용되는 분야로는 Game AI / Robot Navigation / Realtime decisions 등이 있다. 케이크 모형으로 살펴보면, Reinforcement Learning은 케이크의 데코로 올라간 체리와 같이 아주 작은 비중으로, 그 난이도는 가장 어렵다.y = f(x) that maximizes rPredict action y based on ob servation x to maximize a future reward r. 케이크를 통해 바라본 머신러닝의 타입","link":"/2022/03/01/202203/220301-ai-basic-skku/"},{"title":"220301 인공지능(AI)에 한 걸음 다가가기","text":"(2022/03/01) 작성 이번 포스팅에서는 이전에 궁금했지만 어려운 분야라서 선뜻 다가가기 어려웠던 인공지능이라는 분야에 대해 한 번 개념적인 부분을 공부해보면서 공부한 내용을 정리해보려고 한다.완전 입문이기 때문에 생활코딩의 머신러닝 강의를 듣고 학습한 내용을 정리해보려고 한다. 머신러닝(Machine Learning) ? 인공지능이라는 말과 함께 빠지지 않고 나오는 말이 바로 이 머신러닝이라는 말이다. 한국말로는 기계학습인데, 왜 기계가 학습을 하는가 생각을 해보면, 인공지능이라는 것이 인간의 판단을 기계에 위임하기 위한 기술이기 때문이다. 전염병 판단이나 자율주행 등과 같이 무언가 인간을 대신해서 의사결정을 해야되는데, 이를 위해서는 이 머신러닝, 즉 기계학습이 필요하기 때문이다. 문제 + 공부 = 문제 의 밸런스 우리의 일상에는 많은 문제들이 존재하며, 이러한 문제들을 해결하기 위해 우리는 공부(학습)를 한다. 가장 이상적인 형태가 바로 해결하려는 문제에 비해 해야되는 공부의 양이 적고, 이를 통해 문제의 양의 줄어드는 것인데, 이러한 형태는 우리의 일상에서 일반적이지 않다. 이와 반대로 문제의 크기에 비해 공부하는 양의 크기가 크고, 공부한 양에 비해 문제는 해결되지 않고 오히려 문제의 크기가 더 커지는 부작용이 발생하기도 한다.따라서 공부의 효용적 측면에서 우선 문제의 양적 측면을 최대한 부각시켜서 상대적으로 공부해야되는 양이 작게 보이도록 함으로써 공부의 양적 부담을 줄이는 방향으로 한 번 머신러닝이라는 분야에 접근해보도록 하겠다. 결정 = 비교 + 선택 결정은 비교와 선택의 합이다. 우리의 일상에는 다양성으로 인해 무언가를 선택함에 있어 비교해야되는 항목이 엄청나며, 이로인해 우리는 무언가 결정을 하는데 있어 어려움을 겪는다. 따라서 이러한 결정을 기계에게 부여함으로써 기계가 대신 결정할 수 있도록 할 수 있다. 여기서 오해가 있을 수 있는데, 망원경이 있다고 해서 눈이 필요없는 것이 아니고, 포크레인이 있다고 해서 손이 필요없는 것이 아니듯, ML이 있다고 해서 우리의 두뇌가 필요없는 것이 아니다. 우리의 두뇌가 더 나은 가치 판단 능력으로 확장시키기 위해 ML이 필요한 것이다. 소비자가 아닌 생산자의 입장에서 머신러닝 바라보기 이전에는 불가능하다고 했던 것들이 요즘 머신러닝을 통해 가능해지고 있다. 이러한 편안함을 느끼고 사용하는 단순 소비자에 그치지 않고, 우리의 일상에 잠재되어있는 각종 불편한 것들을 어떻게 해결할지 항상 궁리하는 습관을 가지고 해결하고자 노력하는 생산자의 입장에서 앞으로 머신러닝과 관련된 개념과 기술들을 학습해보도록 하겠다. (2022/03/06) 작성 우선 스마트 폰의 사용자가 되어보고, 차후에 필요한 지식을 쌓아나가자. 머신러닝 이라는 분야를 공부하기 위해서는 원리 + 수학 + 코딩 이 세 가지 부분은 필수적으로 알아야 한다. 하지만 우리가 스마트폰을 처음 사용했을 때 스마트 폰의 원리에 대해서 전부 이해한 상태였는가? 아니다. 우리는 우선 사용자적 관점에서 스마트폰을 사용해보고 좀 더 심도있게 스마트 폰을 사용해보고자 개인적으로 원리나 코딩을 공부해본다.요즘 수학을 몰라도 머신러닝 서비스를 사용해서 개발할 수 있도록 나온 Teachable machine https://teachablemachine.withgoogle.com/이라는 서비스가 있다. 이 서비스를 이용하면 이미지, 오디오, 포즈 등의 데이터를 컴퓨터에 학습시킬 수 있다. 예를들어 A 또는 B라는 포즈의 사진을 학습시키기 위해서는 학습력을 높이기 위해 여러개의 사진을 학습시켜야 한다.각 각의 Class 사진들이 입력이 되었다면, Training 시켜주면 되고, Training 결과 판단력의 결과로 model이 출력된다.좋은 model일수록 좋은 추측 결과를 도출하며, model을 만드는 과정이 학습의 과정이다. (2022/03/07) 작성 모델(model), 머신러닝(machine learning)의 중요 열쇠 일반적으로 사람이 먹을 수 있는 것과 먹을 수 없는 것을 판단 할 때에는 겸험에 의해 판단을 한다. 경험해보지 못한 음식이어도 유사 경험을 통해 추측 및 예측을 통해 결정을 할 수 있다.과학에 있어서도 특정 현상이 관찰되면, 일단 해당 현상에 대해서 추측 및 가설을 세우게 되고, 검증을 위해 실험을 하게 된다. 실험을 통해 해당 가설의 진위를 가리게 된다. 여기서 무언가를 최종적으로 결정하는데 있어 판단하는데 있어 판단력이라는 것이 필요한데, 이 판단력을 기계에 부여하는 것이 바로 기계학습이며, 여기서 판단력은 model이다.좋은 model일수록 좋은 추측과 좋은 결정이라는 결과를 도출해낼 수 있다. (2022/03/08) 작성 머신러닝으로 문제해결하고 싶은 일상의 문제 생각해보기 https://bit.ly/ml-other-plan 자영업자 환경 불만족 꿈","link":"/2022/03/01/202203/220301-ai-daily-coding/"},{"title":"220302 인공지능(AI) 기초 다지기 2편","text":"이번 포스팅에서는 이전 포스팅에서 개괄적으로 분류해보았던 Supervised Learning과 Unsupervised Learning에 대해서 좀 더 구체적으로 정리를 해보고자 한다. Supervised Learning ? 이전 포스팅에서 알아보았듯이 Supervised Learning이란 (X(input), Y(output))에서 Y(output)에 대한 값이 labeling을 통해 주어지고, 이를 통해 X(input)값이 유추되는 형태이다.Y의 값이 direct feedback으로 주어지고, Y 데이터의 형태에 따라 두 가지 형태로 분류된다. Y labels are continuous = RegressionRegression은 크게 Linear Regression과 Polynomial regression으로 분류된다. Polynomial regression은 레이블이 다항식 관계에 있는 경우이며, 2차 3차 함수의 계수 형태로 존재한다. Y labels are discrete = Classification Y 데이터가 아래와 같이 discrete형태로 분류된다면 이는 supervised learning의 classification로 분류된다. ex) (1) SVM (Support Vector Machine) : 기계학습의 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도학습 모델이며, 주로 분류와 회귀 분석을 위해 사용된다. (2) 신경망(Neural Network) : 신경망은 인간의 뇌 행동을 반영한다. 인간의 뇌 행동에 있어, 컴퓨터 프로그램은 특정 패턴을 인지하고 AI, 머신러닝, 딥러닝 분야에서의 일반적인 문제들을 해결하는 것 또한 허용한다.신경망은 Artificial Neural Networks(ANNs) 또는 Simulated Neural Networks(SNNs)로 알려져있다.이 신경망 개념은 머신러닝의 부분집합 개념이자, 딥러닝 알고리즘의 핵심적인 부분이다. 이 신경망이라는 이름과 구조는 인간의 뇌로부터 영감을 받았으며, 생물학적인 뉴런들의 시그널을 주고받는 방식을 모방하였다. - 퍼셉트론(Perceptron) 인공 신경망은 수많은 머신 러닝 방법 중 하나로, 딥 러닝을 이해하기 위해서는 우선 인공 신경망에 대한 이해가 필요하기 때문에 초기 인공 신경망인 퍼셉트론(Perceptron)에 대한 이해가 필요하다. [참고] https://wikidocs.net/24958 - 다층 퍼셉트론(MultiLayer Perceptron, MLP) (3) 의사결정 나무(Decision tree) : 의사결정나무는 DTs라고도 하며 여러가지 규칙을 순차적으로 적용하며서 독립 변수 공간을 분할하는 분류 모형이다. 분류(Classification)와 회귀 분석(Regression)에 모두 사용될 수 있기 때문에 CART(Classification And Regression Tree)라고도 한다. (4) 나이브 베이즈 분류(Naive Bayes classification) : 스팸메일 필터, 텍스트 분류, 감정 분석, ㅊ투천 시스템 등에서 광범위하게 활용되는 분류기법이다. 나이브 베이즈 모델인 용어에서 알 수 있듯이, Naive(단순한, 순진한, 전문성이 없는)한 모델임을 알 수 있다. 나이브 베이즈 모델은 모든 feature(속성)가 동등하게중요하고 독립적이라고 가정한다. 하지만 실생활에서 각 feature(속성)들 간에 독립성을 보장하기는 쉽지 않기 때문에 실생활에 바로 적용하기에는 어려움이 있다. 참고 - 머신러닝 관련 블로그 글 (bkshin tistory) (5) K-nearest neighbors(K-NN) : 지도 학습 기반에 한 종류로 거리기반 분류분석 모델이라고 할 수 있다. 기존 관측지의 Y값(Class)가 존재한다는 점에서 비지도학습인 클러스터링(Clustering)과 차이가 있다. 유유상종이라는 말이 있다. 같은 날개를 가진 새들끼리 함께 모인다는 말인데, K-NN 알고리즘을 설명하기 적합한 속담이다. 머신러닝에서 데이터를 가장 가까운 유사 속성에 따라 분류하여 데이터를 분류하는 기법이 바로 K-NN 알고리즘 기법이기 때문이다. Classification model 예시 분류 모델의 예시로는 대표적으로 문서 분류 Sports, Politics, science … topic 별로 분류하는 것과 e-mail 중 ham vs spam SNS 내용중에 sentiment: negative, confidence: 99%, trend: boring 과 같은 감성적인 내용으로의 분류의 예시가 있다. 이외에도 아래의 분류 관련 예시들이 있다.(1) Image Classification : ImageNet Competetion(2) Video Classification : 영상의 frame을 쪼개서 분석(3) Object Detection(4) Video Summarization : 범죄 현장에서 수사에 활용되는 기법으로 다른 시간대의 물체가 동시간대에 시간만 다르게 labeling 되어 표시된다.(5) Image Captioning : 이미지에 대한 설명 label X와 Y의 관계를 규명 (Image caption)(6) Speech Recognition : X(Speech data), Y(대응 텍스트 - 영상에 대한 설명) 이러한 머신러닝의 기법 중에 하나인 Supervised learning 기법은 우리의 실생활은 물론 현업에서도 많이 활용되고 있다.","link":"/2022/03/02/202203/220302-ai-basic-skku/"},{"title":"220302 AI를 위한 Pandas 본격적으로 시작하기 (데이터의 중요성과 Pandas series 속성)","text":"이번 포스팅에서는 인공지능 모델을 개발할 때 필요한 필수 요소인 데이터를 준비하고 정제(전처리 및 결합)하는데 필요한 Pandas에 대한 학습을 하기 전에 데이터의 중요성과 왜 내가 지금 이 공부를 해야되는지에 대한 이유에 대해서 정리를 해보고자 한다. 데이터의 중요성 방대한 데이터를 유의미한 데이터로 가공해서 그 데이터를 통해서 가치있는 insight를 얻어서 매출 증가와 생산력을 증가시키는 기업들이 많다. 우리가 요즘 많이 시청하는 넷플릭스 또한 영화 추천 시스템을 제공하는데 이 또한 고객의 데이터를 활용해서 끊임없이 고객에게 고객이 선호하는 콘텐츠를 추천하고 제공한다. 내가 데이터 준비 및 정제에 대한 공부가 필요한 이유? 인공지능 모델을 개발 할 때 DATA + MODEL + COMPUTE 이 세 가지는 필수적인 요소이다. 이 세가지 요소는 인공지능 모델을 개발 할 때 충족해야되는 요소들로, 여기서 DATA란 어떻게 정제하고 합치고 시각화하는지가 중요시되며, MODEL은 DATA를 이용해서 인공지능을 만들 수 있는 마치 수학식과 같은 개념이라고 할 수 있다.그리고 DATA를 사용해서 MODEL을 학습시키기 위해서는 COMPUTE의 과정이 필요하다. 앞으로의 공부계획 앞으로 데이터의 정제 및 시각화를 어떻게 하는지에 대해 학습 (수치계산에 특화된 numpy, 데이터 분석에 필요한 pandas, 데이터 시각화에 필요한 matplot, seaborn 활용) 머신러닝, 데이터 사이언스에서 사용되는 대용량 데이터 세트를 다루는 방법에 대해서 학습 대용량 데이터의 샘플로는 KAGGLE, UCI, ImageNet에서 제공하는 dataset을 활용해 볼 것이다. (1)KAGGLE : https://www.kaggle.com/datasets(2)UCI : https://archive.ics.uci.edu/ml/index.php(3)ImageNet : https://www.image-net.org/ Pandas Pandas는 데이터 조작 및 분석 툴로써, Numpy를 기반으로 한다.(Numpy는 파이썬을 통해서 데이터 분석을 할 때 기본적으로 사용되는 라이브러리로, C언어로 구현이 되어있는 파이썬 라이브러리이다. 고성능의 수치계산을 위해서 제작이 되었으며, Numerical Python의 줄임말이다. Numpy는 백터 및 행렬 연산에 있어서 매우 편리한 기능을 제공한다는 장점을 가지고 있으며, 데이터 분석을 사용 할 때 사용되는 라이브러리인 Pandas와 Matplotlib의 기반으로 사용되기도 한다.) Pandas는 DataFrame으로 알려져있는 데이터 구조(MS Office excel의 Python 버전이라고 생각하면 된다)를 사용하며, DataFramems은 프로그래머로 하여금 표로 정리된 행과 열과 열 데이터의 저장과 조작을 용이하게 해준다. (Series는 DataFrame 데이터 구조에서 단일 칼럼(Single column)으로 간주한다) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import pandas as pdmy_favorite_movie = ['Dead Poets Society', 'Good Will Hunting', 'The Pursuit of Happyness']my_series = pd.Series(data = my_favorite_movie)print(my_series)#0 Dead Poets Society#1 Good Will Hunting#2 The Pursuit of Happyness#dtype: object# index는 자동으로 생성이 되게 할 수도 있지만, Series의 두 번째 인수로 index 값을 넘겨주어 사용자 정의 index 값을 정의해줄 수도 있다.my_favorite_movie = ['Dead Poets Society', 'Good Will Hunting', 'The Pursuit of Happyness']idx = ['movie#1', 'Movie#2', 'movie#3']my_series = pd.Series(data = my_favorite_movie, index = idx)print(my_series)#movie#1 Dead Poets Society#Movie#2 Good Will Hunting#movie#3 The Pursuit of Happyness#dtype: objecttype(my_series)#pandas.core.series.Series# 위에서는 pandas.Series의 인수로 리스트(data)를 넘겨서 series type의 데이터를 생성하였는데, dictionary 타입의 데이터도 넘겨서 처리할 수 있다.stock_dict = {'Nvidia': 10000, 'Microsoft': 20000, 'Facebook': 15000, 'Amazon': 20000, 'Boeing': 30000}type(stock_dict)stock_dict_series = pd.Series(stock_dict)stock_dict_series#Nvidia 10000#Microsoft 20000#Facebook 15000#Amazon 20000#Boeing 30000#dtype: int64 Data source / Type Data source는 image, audio/sound, time series/signals 등 다양한 형태로 존재한다.이러한 뎅니터 리소스를 얻는 것은 어려운데, 데이터를 얻기 위해서는 데이터 스크래핑 과정을 거쳐서 스크래핑한 데이터를 구조화시키고, 이를 분석해서 마케팅이나 기획에서 활용된다. Data scientist가 실무에서 80% 정도의 시간을 소요하는 부분이 바로 데이터 준비 및 정제 / 데이터 전처리 및 결합이다. 데이터의 타입에는 크게 labeled data와 unlabeled data로 나뉘게 되는데, 별도로 supervisor가 시간과 비용을 들여서 labeling 과정을 거쳐 raw data에 labeling을 하는 경우에 labeled data가 만들어지게 된다. Pandas series 속성 123456789101112131415161718192021222324252627my_list = ['NVDA', 'MSFT', 'FB', 'AMZN', 'BA']my_series = pd.Series(data = my_list)print(my_series)#0 NVDA#1 MSFT#2 FB#3 AMZN#4 BA#dtype: objectprint(my_series.values)#array(['NVDA', 'MSFT', 'FB', 'AMZN', 'BA'], dtype=object)print(my_series.index)#RangeIndex(start=0, stop=5, step=1)print(my_series.dtype)#dtype('O') ('O' stands for 'object' datatype)print(my_series.is_unique)#True (모든 값이 Unique하기 때문에 True)print(my_series.shape)#(5,) (Pandas series가 One dimensional이기 때문에)print(my_series.size)#5 (Pandas series의 size를 구하기 위해서는 size method를 사용해야 한다)","link":"/2022/03/02/202203/220302-pandas_master_class/"},{"title":"220305 인공지능(AI) 기초 다지기 3편","text":"이번 포스팅에서는 이전 포스팅에서 개괄적으로 분류해보았던 Unsupervised Learning에 대해서 좀 더 구체적으로 정리를 해보고자 한다. Unsupervised Learning ? Supervised Learning이란 (X(input), Y(output))에서 Y(output) 값에 대한 레이블이 제공되지 않은 상태로, 최소한의 사람의 개입을 통해 데이터를 해석해서 숨겨진 데이터의 패턴을 찾아낸다. 그 예시로 각기 다른 모양과 색상을 가진 사과와 바나나가 나열되어있다고 가정해보자. 어떤 기준으로 나열을 할 것인가? 앞서 이미 언급을 했듯이 Y라는 label이 주어지지 않은 상태에서 기계는 주어진 데이터들을 분류해야한다. 기계는 사람과 같이 직관력이 없기 때문에 모양이 동그란지 혹은 길죽한지에 따라서 분류를 하기도 하고, 색상이 알록달록한지 아닌지에 따라 분류하기도 한다. Unsupervised Learning 예시 그렇다면 비지도 학습의 예시로는 어떤것들이 있을까? 우선 첫 번째, 클러스터링(lustering)이 있다. Clustering은 Classification과 비슷하지만 Y labeling data가 주어지지 않는다는 점이 다르다. Clustering은 말 그대로 data duples을 clusters로 grouping하는 것을 말한다. Similar/Dissimilar를 정의함에 있어서는 사람이 개입을 하게 되는데, 이 부분에 있어서만 최소한으로 사람이 개입하게 된다. 두 번째, 차원축소(Dimensionality reduction)가 있다. 대표적인 예시로 PCA(Principal Component Analysis)가 있으며, 2차원의 데이터를 1차원 데이터로, 3차원 데이터를 2차원 데이터로 축소를 하게 된다. 이렇게 차원을 축소하게 되면 어떤 효과가 있을까?우선 첫 번째 고차원에서는 알 수 없었던 correlation(상관관계)를 효과적으로 알 수 있고, 불필요한 noise를 제거할 수 있으며, 저장공간을 효율적으로 사용 할 수 있다. 그리고 마지막으로 시각화와 해석능력이 좋아진다. 세 번째, 밀집도 판단(Density estimation) : sample 모양이 규칙적이지 않고, 복잡한 모양인 경우가 많다. 이러한 복잡하고 규칙적이지 않은 모양의 영역에서 실제 데이터 영역을 추출해내야되는 경우가 많은데, 우리는 Manifold Hypothesis라는 용어에 집중해야 한다. 이 용어는 고차원 데이터라 할지라도 실질적으로 해당 데이터를 나타내주는 저차원 공간인 manifold 존재한다는 것을 의미한다. (sparse한 고차원 데이터를 간추려서 보다 저차원 공간으로 나타낼 수 있다는 의미) manifold란, 다차원 데이터에서 실질적으로 의미를 가지는 특징을 모아둔 조밀한 특성 공간을 뜻하며, 실제 데이터를 통해 스스로 학습하는 딥러닝 비지도 학습이 이러한 manifold를 스스로 찾아낸다. 이러한 Manifold Hypothesis를 통해 가능한 가능해진 기술이 바로 GAN(Generative Adversarial Network)로, 실제로 존재하지 않는 사람을 예상으로 생성해내는 기술이다. 최근에는 사람 이외에도 자연 이미지도 예상/예측해서 생성해내고 있다.이외에도 style-based GAN, video-to-video synthesis, source to tareget GAN 등이 있다.","link":"/2022/03/05/202203/220305-ai-basic-skku/"},{"title":"220307 AI를 위한 Pandas (Pandas series의 method, Pandas를 활용한 csv파일 읽기, Pandas 빌트인 함수, Pandas series 정렬(index, values 기준), Pandas series 연산)","text":"이번 포스팅에서는 Pandas series의 method에 대한 내용을 정리해보고자 한다. Pandas series의 method 12345678910111213141516171819import pandas as pd#Series.sum() : 합계my_series = pd.Series(data = [100, 200, 500, 1000, 5000])#Series.product(): 곱my_series.product()#Series.mean(): 평균my_series.mean()#Series.head(n): 위에서부터 n개 출력my_series.head(2) #최 상단에서 2개까지 출력#Series.tail(n): 아래에서부터 n개 출력my_series.tail(2) #최 하단에서 2개까지 출력#Series.memory_usage(): Pandas series가 사용하고 있는 용량 확인(bytes)my_series.memory_usage() #example:168(bytes) Pandas 활용해서 CSV 파일 읽기 123456789101112#Pandas의 read_csv method를 활용해서 csv파일을 읽기 위해서는 squeeze=True option은 필수이다.#squeeze=True : One dimentional table data를 읽기 위한 옵션sp500 = pd.read_csv('S&amp;P500_Prices.csv', squeeze=True)type(sp500) #pandas.core.series.Series#squeeze=False : Multi dimentional(다차원) table data를 읽기 위한 옵션이다.sp500 = pd.read_csv('S&amp;P500_Prices.csv', squeeze=False)#squeeze=False or defaulttype(sp500) #pandas.core.frame.DataFrame 위의 각 결과를 보면 알 수 있듯이, Pandas series의 경우에는 formatting 없이 단순하게 (1차원의)하나의 열로 테이블화 된 형태로 데이터가 출력이 되고, DataFrame의 경우에는 각 셀에 스타일이 적용되어 테이블이 formatting되어 데이터가 출력된다.데이터 프레임(DataFrame)이란 1개의 열이 아닌, 여러 행과 열을 가진 다차원 데이터 테이블이다. Pandas 빌트인 함수 Pandas는 이미 존재하는 python의 수 많은 함수들과 상호동작한다. 123456789101112131415161718192021222324252627sp500 = pd.read_csv('S&amp;P500_Prices.csv', squeeze=True)#Pandas의 Data type을 얻을 때type(sp500)#Pandas series의 data길이를 얻을 때len(sp500)#Pandas series에서 최대값을 얻을 때max(sp500)#Pandas series에서 최소값을 얻을 때min(sp500)# 주어진 Pandas series type의 데이터(리스트)에서 모든 양의 값은 음의 값으로 Python의 built-in function을 활용하여 구하시오.# 주어진 Pandas series의 값에서 중복되지 않은 Unique한 값을 Python의 built-in function을 활용하여 구하시오.my_series = pd.Series(data = [-10, 100, -30, 50, 100])result = [ -i for i in my_series ]set(result)#solution#모든 값을 양의 값으로 치환하는 문제였다.abs(my_series)set(my_series) Pandas series sorting(ascending order) 123456789101112131415161718import pandas as pdsp500 = pd.read_csv('S&amp;P500_Prices.csv', squeeze=True)print(sp500)#sort the values in the dataframes#최솟값 ~ 최대값 순으로 정렬sp500.sort_values()#sort_values()의 결과값은 별도로 변수에 저장해줘야한다. (메모리 정렬)sp500.sort_values(inplace = True)#index 순으로 정렬sp500.sort_index(inplace = True)#challenge#8 (descending sorting)sp500.sort_values(ascending=False, inplace=True) Pandas Series Math operation 12345678910111213141516171819202122232425262728293031323334353637383940sp500 = pd.read_csv('S&amp;P500_Prices.csv', squeeze=True)#Sum method on Pandas seriessp500.sum()#Count method on Pandas seriessp500.count()#Obtain the maximum valuesp500.max()#Obtain the minimum valuesp500.min()#새로운 dataset을 다룰때 describe method를 사용한다.# count, mean, std(표준편차), min, 25%, 50%, 75%, max 정보의 summary 정보를 알 수 있다.sp500.describe()#Challenge#9 Obtain the average price of the S&amp;P500 using two different methodsp500.mean()sp500.sum()/sp500.count()#주어진 요소가 값으로 존재하는지 체크1295.500000 in sp500.values#주어진 요소가 인덱스로 존재하는지 체크1295.500000 in sp500.index#주어진 요소가 값이 아닌 인덱스로 존재하는지 체크 (in 은 기본적으로 index를 기준으로 찾는다)1295.500000 in sp500#Challenge#103349 in sp500.values#sp500의 values를 반올림한다.rounded_sp500 = round(sp500)#반올림된 값 리스트에서 3349가 값이 존재하는지 확인한다.3349 in rounded_sp500.values","link":"/2022/03/07/202203/220307-pandas_master_class/"},{"title":"220307 인공지능(AI) 기초 다지기 4편","text":"이번 포스팅에서는 이전 포스팅에서 개괄적으로 분류해보았던 Reinforcement Learning에 대해서 좀 더 구체적으로 정리를 해보고자 한다. 강화학습(Reinforcement Learning)? 강화학습이란 다른 머신러닝(지도학습, 비지도학습)과는 달리 agent라는 개념이 도입된다. 이 agent는 environment(환경)에서 행동을 하는 주체이며, 행동에 의해 새로운 action을 갖으며, 이에 따라 새로운 state(상태)를 갖는다. 이 상대적인 action에 따라 environment(환경)은 새로운 state(상태)와 함께 보상(rewards)을 행동의 주체인 agent에게 부여한다. 예시) 미로에서 치즈를 찾는 생쥐 - 환경(미로) / action(쥐의 미로 속 이동) / rewards(치즈) / state(미로 내 쥐의 위치) Supervisor가 없고, 환경(Environment)에 의해서 보상을 받고 최상의 rewards를 얻기 위한 action을 선택하도록 한다. Feedback이 delay될 수 있다. Action이 순차적으로 미래에 주어지는 보상에 영향을 줄 수 있다. 환경이 어떤 새로운 상태를 반환할지 예측할 수 없기 때문에 Black box 상태로, 어떤식으로 rewards, state를 나타내는지 알지 못한다. The key challenge is to learn to make good decisions under uncertainty. (핵심 과제는 불확실성에서 최고의 결정을 도출해내는 것을 배우는 것이다.) OpenAI 사용해보기 OpenAIhttps://gym.openai.com/ 12345678import gymenv = gym.make(&quot;Taxi-v1&quot;)observation = env.reset()for _ in range(1000): env.render() action = env.action_space.sample() #your agent here (this takes random actions) #결과(observation) observation, reward, done, info = env.step(action) 강화학습 예시1) Frozen Lake Source에서 Goal까지 이동을 하는 Agent가 존재한다. 위의 그림과 같이 얼은 빙판은 F(Frozen), 구멍이 뚫린 부분은 H(Hole)로 표기가 되어있다. 1234567import gymenv = gym.make(&quot;FrozenLake-v0&quot;)observation = env.reset()for _ in range(1000): env.render() action = env.action_space.sample() observation, reward, done, info = env.step(action) 강화학습 예시2) 갤러그 게임 왼쪽, 오른쪽, 총 쏘기와 같은 액션(동작)이 주어지고, 이 액션에 따라 보상(점수)가 주어진다. agent인 player의 상태(state)는 지속적으로 업데이트를 해주며, 이러한 학습을 통해 갤러그와 같은 총 쏘기 게임을 머신러닝의 강화학습 형태로 학습할 수 있다. 강화학습 예시3) 벽돌깨기 게임 사람만이 할 수 있는 전략도 컴퓨털가 학습하여 게임에 적용할 수 있다. 강화학습 예시4) 알파고 강화학습 예시5) 자율주행 Artificial Neural networks 딥러닝(Deep Learning)은 머신러닝의 여러 방법 중 중요한 방법론이며, 인공 신경망(Artificial Neural Network)의 한 종류이다. 즉, 인공지능 ⊃ 머신러닝 ⊃ 인공신경망 ⊃ 딥러닝 관계가 성립한다.우선 딥러닝에 대해서 학습을 할 것이기 때문에 뉴럴 네트워크의 역사적 배경에 대해서는 상식적으로 알아두어야 한다. 그래서 큼직큼직한 사건은 알아두도록 하기 위해 정리하고 암기해둬야겠다. Neural network, 한국말로하면, 신경 네트워크이다. 이 Neural network라는 말은 요즘 들어 각광을 받고 있지만, 이 말은 1940년대부터 쓰였던 말이었다. 이 시기부터 사람들은 사람의 뇌 신경 구조를 전자기적 형태(인공 신경망)로 구현하고자 했다. 그래서 1940년대에는 Electronic brain라고 해서 X AND Y X OR Y와 같은 형태로 구조화 시키려고 했다. 하지만 현실적으로 AND, OR로 신경 구조를 만드는 것은 불가능했다. 1957년 Perceptron이라는 개념이 등장을 했는데, 이때부터 사람의 뇌 신경구조를 모방한 인공 신경망을 구현할 수 있었다. 1960년부터 1970년까지 Neural network의 황금기였다가 1970년대에 XOR 문제가 발견이 되면서 Neural network의 암흑기로 돌아간다. 1986년 조페리 힌튼(Geoffrey Hinton)이 Multi-layered Perceptron(Back Propagation)으로 XOR문제를 해결할 수 있다고 증명하였으나, 그 과정이 너무 복잡하고 긴 학습시간으로 주목받지 못하였다. 이후에 1995년 SVM(Support Vector Machine)을 활용해서 XOR문제와 같은 비선형 관계를 학습할 수 있는 모델이 등장하였고, 조페리 힌튼이 제시한 BackPropagation보다 간단하고 효과적이었다. 2006년 Deep Neural Network(Pretraining)모델이 등장을 하게 되고, 1986년 Multi-layered Perceptron(Back Propagation)을 제시했던 조페리 힌튼이 어떻게 학습하는지에 대한 제안을 하게된다. cf) 심층 신경망(Deep neural networks)(1) 입력 변수들 간의 비선형 조합이 가능하다.(2) 특징 표현(Feature representation)이 학습 과정에서 함께 수행될 수 있다.(3) 데이터가 증가함에 따라 성능이 개선될 수 있다. 이에따라 2010년 후반에는 CNN기반의 이미지 분류 모델인 AlexNet을 통해 Neural Network 모델이 복잡한 문제를 해결할 수 있음을 증명 AI vs ML vs DL 상관관계 AI(Artificial Intelligence)는 ML(Machine Learning)과 DL(Deep Learning)을 아우르는 개념이다. AI는 1990년대 전후로 나눠서 살펴 볼 수 있는데, 1990년대 전에는 명시적 룰(rule)만을 적용해서 기계가 판단하는 식이 전부였다. 하지만 1990년대 이후부터는 명시적 룰(rule)이 아닌 통계를 기반으로 기계가 판단을 하였다.DL(Deep Learning)은 수많은 parameter들을 가진 복잡한 모델을 어떻게 학습 할 것인지에 대해서 학습기법이다. 전통적인 머신러닝과 딥러닝의 가장 큰 차이는 feature extraction 과정과 classification 과정의 분리/통합 유/무 상태이다. 전통적인 머신러닝은 두 과정이 서로 분리되어있으며, feature extraction을 통해 나온 출력결과와 Output의 결과가 서로 비교되어 관계학습을 하게 된다.하지만 딥러닝에서는 feature extraction 단계가 전통적인 머신러닝에서는 필수 단계로 간주되었던 것과 달리, feature extraction(=learned feature) + classification가 통합되어 학습으로 관여된다.사용하는 측면에서 end to end learning이기 때문에 쉽지만, 이해하는 측면에서는 Black box 형태로 가려져있기 때문에 어렵다. 딥러닝 모델의 예시 Convolutional Neural Network(CNN) : 합성곱 신경망 Recurrent Neural Network(RNN) : 순환 신경망→ 순차적 관계를 가진 시계열 데이터 처리에 가장 적합한 심층 신경망 기법 Graph Neural Network(GNN) : 그래프 신경망 Image Classification SVN 25% ERROR RATE AlexNet 16% ERROR RATE Speech Recognition : 2010년 초 RNN model 적용 Machine Translation : 2010년 speech to speech model 적용, 2018 - 2019년 Transformer 적용 (Google translator/Naver Papago)","link":"/2022/03/07/202203/220307-ai-basic-skku/"},{"title":"220308 AI를 위한 Pandas (Pandas series에서 특정 요소 찾기, Pandas series slicing)","text":"Parantheses() 그리고 Brackets[] 속성을 조회할때에는 Parantheses()를 사용하지 않는다.ex)my_series.values, my_series.shape Parantheses()를 사용하는 경우에는 arguments(인자)를 포함하거나 Pandas series를 대체하거나 바꾼다.ex)my_series.tail(), my_series.head(), my_series.drop_duplicates() Brackets[]를 사용하는 경우에는 Pandas series나 DataFrame에서 특정 요소에 접근하기 위함이다.ex) Pandas series에서 특정 요소 찾기 12sp500 = pd.read_csv('S&amp;P500_Prices.csv', squeeze=True)sp500[n-1] #n번째 index 값 Pandas series 슬라이싱 1234567891011121314151617import pandas as pdsp500 = pd.read_csv('S&amp;P500_Prices.csv', squeeze=True)# index 0 ~ 4 까지 슬라이싱# 오른쪽 숫자는 범위에서 포함되지 않는다. N-1sp500[0:5]# index 0 ~ 9 까지 슬라이싱# 가장 처음 index부터 시작이기 때문에 아래와 같이 생략할 수 있다.sp500[:10]# index 5 ~ end 슬라이싱sp500[5:]# Challenge#13 마지막 세 개 요소를 제외한 Pandas series 출력sp500[:-3]","link":"/2022/03/08/202203/220308-pandas_master_class/"},{"title":"220309 인공지능(AI) 기초 다지기 5편","text":"이번 포스팅에서는 이전에 포스팅하였던 지도학습(Supervised Learning)의 학습 과정에 대해서 좀 더 구체적으로 정리를 해보고자 한다. 지도학습(Supervised Learning) 학습과정 3단계 1단계 데이터 수집 및 특징 추출하기 머신러닝의 지도학습 유형에서는 (X,Y)에서 output에 해당하는 Y(labeling data)가 필요하다. 이 labeling data는 CrowdSourcing을 통하여 labeling 작업이 되고 있다.input에 해당하는 X의 경우에는 feature의 표현으로, 주어진 X가 어떤식으로 표현이 될 것인가에 대해 결정을 되는 중요한 부분이다. 2단계 ML Model 선택하기 ML Model의 종류로는 아래와 같다.(1) Logistic regression, SVM : 분류모델(2) Decision Tree, Random forest : 트리기반, 회귀모델(3) Nearest neighbor : 비선형 모델 (근처 데이터를 통해 데이터 결정)(4) Neural Network, MLP : 딥러닝의 모체가 되는 모델 [ML Model 선택 가이드라인] 주어진 뎅티터의 특성에 따라 특정 ML Model이 더 낫다는 판단이 있을 수 있다. 많은 데이터 분석의 경험을 통해서 상대적으로 데이터가 많다면, Parameter가 많은 모델들이 Parameter가 적은 모델보다는 더 효과적으로 동작할 수 있다. 3단계 파라미터 값 최적화하기 주어진 모델하에서 모델이 가지고 있는 학습하고자하는 Parameter의 값을 결정하는 단계이다. 아래와 같이 회귀(regression)그래프가 주어져있고, 실제 우리가 학습하고자하는 Parameter값들이 아래와 같이 분포되어있다고 가정하자. f(x; w1, w2, w3)에서 각 각의 가중치 w1, w2, w3 값을 조정해서 우리가 학습하고자 하는 Parameter값들을 좀 더 잘 표현 할 수 있는 모델을 결정지을 수 있도록 조정할 수 있다. 하지만 첨부한 사진에서 보듯이 정성적 판단(사람의 눈으로 판단)으로만 본다면 명확히 어느 가중치의 값이 가장 최적의 값인지 알 수 없다.따라서 이 경우에는 정량적 판단(지표를 통한 판단)이 중요하다. f(x)와 y간의 관계에서 error를 최소화 함으로써 우리가 학습하고자 하는 Parameter와 실제 데이터 간의 오차범위가 작음을 확인하는 것이다. 아래의 공식은 에러의 정도를 정량적으로 판단하기위한 것으로, 위의 그래프에서 선형함수(예측값)와 Parameter(실제값) 사이의 차이값(actual Y - estimated Y)의 제곱된 값을 모든 데이터 (x,y)에 대해 모두 시행하여 합한 것을 의미한다. 여기서 W는 Error를 최소화하기 위한 값인데, 이 에러를 최소화시켜줄 값 W를 통해서 최고의 모델을 찾을 수 있다. 주어진 데이터를 표현하는 최고의 Parameter를 찾는 문제는 f(x)와 y사이에서 에러를 최소화 시키는 것이다 이것이 최적화 문제이며, 문제해결에는 아래의 두 가지 방법이 있다. (1) Analytic solution : 방정식 풀이를 통해 최적의 해 구하기 (2) Numerical solution : 주어진 함수를 통해 해를 구하지 못하는 경우, 수치를 통해 해와 가장 가깡툰 근사치를 구한다 이렇게 학습된 Model을 사용해서 새로운 input x를 통해 output y를 예측하거나 추론한다.","link":"/2022/03/09/202203/220309-ai-basic-skku/"},{"title":"220312 Pandas DataFrame으로 넘어가기 전 Pandas Series 최종복습 및 실습","text":"이번 포스팅에서는 학습했던 Pandas series에 대한 내용에 대해서 복습 및 실습을 하고, 그에 대한 내용을 정리해보고자 한다. 우선 Pandas란 데이터를 분석하고 각종 처리를 할 때 이용하는 파이썬 언어 기반의 라이브러리이다.Pandas로 데이터를 표현하는 두 가지 방법이 있는데 바로 여지까지 학습했던 시리즈(Series)라는 녀석과 데이터프레임(DataFrame)이란 놈이다. 이전 포스팅에서 두 개념(시리즈/데이터프레임)에 대해서 간단하게 다뤄보았지만, 아주간단하게 요약을 하자면, 시리즈(Series)란, 표(table)로 포멧팅되지 않은 리스트 형태로만 표현된 것으로, 열(Column)이 하나인 데이터 프레임이라고 생각하면 된다. 즉, 데이터 프레임은 여러 column의 series를 dictionary형태로 묶은 것이라고 볼 수 있다. 1234567# DataFrame 예시import pandas as pad# DataFrame 생성# 각 행과 열에들어가는 값은 숫자는 물론, 문자나 boolean 형태도 가능하다.pd.DataFrame({ 'Nanun' : [0, 3], 'Zoey' : [4, 2]}, index=['Apple', 'Banana']) 그래서 왜 Pandas를 사용하는데? 실제로 실무에서 분석하고 처리해야되는 데이터는 위에서 예시로 든 데이터와는 비교도 안 될 정도로 엄청 크고 방대할 것이다.따라서 한눈에 보기 어려운 많은 양의 데이터가 어떤 개요(Overview)와 성향(Features)를 가지고 있고, 그 성향을 좀 더 잘 해석하기 위해서는 어떻게 데이터가 그룹화되어 표현되어야 하는지에 대한 남다른 해석능력도 필요하다. 그리고 데이터 중에서 유독 튀는 이상한 값은 없는지, 항상 확인하고 조작하는데 이 Pandas를 활용한다. (Kaggle dataset)구글 플레이 스토어 앱 다운로드 CSV 파일을 Pandas를 이용해서 읽어보고 직접 조작해보도록 하겠다. dataset은 kaggle에서 제공해주는 데이터 중에서 개인적인 흥미에 따라 구글 플레이 스토어 앱 다운로드와 관련된 데이터셋 데이터 정보를 다운받았다. 123456789101112import pandas as pdgpstore = pd.read_csv('./googleplaystore.csv', index_col=0)#vscode에서 DataViewer로 DataFrame의 데이터 리스트를 확인하기 위해서는 데이터가 출력되는 부분에 Break Point를 두고,#&quot;실행 및 버그&quot;에서 활성화 세션의 &quot;변수&quot; 영역에서 Break Point영역 변수명에 마우스 우측클릭해서 &quot;View Value in Data Viewer&quot; 항목을 선택한다.print(gpstore)#.shape를 통해 (rows x columns)를 확인할 수 있다.print(gpstore.shape) #[10841 rows x 12 columns]print(gpstore.head(n)) #DataFrame의 상단에서부터 n개까지 출력해준다. 123456789101112131415# 구글플레이 스토어에서 다운받은 앱들 중에서 평점이 4.0 이상인 것들만 Genres, Rating, Installs, Size 항목으로 분류해서 나타낼 수 있도록 DataFrame 구성best_app = gpstore.loc[gpstore.Rating &gt; 4.0, ['Genres', 'Rating', 'Installs', 'Size']]print(best_app)# 평점이 4.0이 넘는 구글 플레이스토어의 앱의 수 (6801)print(best_app.count())# Genres 6801# Rating 6801# Installs 6801# Size 6801# dtype: int64print(best_app.groupby(['App']).count())","link":"/2022/03/12/202203/220311-pandas_master_class/"},{"title":"220326 Data warehouse vs Data lake (작성중...)","text":"이번 포스팅에서는 DW(Data Warehouse)와 DL(Data Lake)에 대한 용어 정리를 해보려고 한다.요즘 데이터 엔지니어가 되기 위한 요구조건을 찾아보다가 DW, DL이라는 약어가 많이 등장해서 궁금했는데, 이 약어가 바로 Data Warehouse, Data Lake였다.한국말로 직역하면 데이터 창고와 데이터 호수인데, 이를 통해 개괄적인 용어의 의미를 파악할 수 있다. DW(Data Warehouse) DL(Data Lake)","link":"/2022/03/26/202203/220326-data-storage/"},{"title":"220326 Hadoop과 친해지기 + 첫 하둡 실습","text":"이번 포스팅에서는 본격적으로 Hadoop이라는 녀석의 개요와 역사 그리고 생태계에 대해 알아보고 본격적으로 설치를 해보는 것에 대해 작성해보려고 한다.처음이라 마냥 어렵게 느껴지지만, 이 친구도 이 세상의 많은 기술들 중에 하나이며, 다 이유가 있어서 태어난 친구이기 때문에 한 번 친해져보려고 한다. 재미있는 것은 이 하둡(Hadoop)이라는 녀석과 나는 초면이 아니라는 것이다. (그냥 과거에 스쳐간 인연..)내가 라즈베리파이에 관심이 한창 있을 무렵에 구글링을 하다가 우연히 라즈베리파이로 슈퍼컴퓨터를 만든 사람이 올린 사진을 보았었는데, 그때 어떻게 만들었는지 너무 궁금해서 이것 저것 찾아보다가 컴퓨터 클러스터링 기술과 관련된 내용을 보았었는데, 이 곳에서 데이터 분산저장하는 기술로 이 하둡(Hadoop)이라는 친구에 대한 내용이 있었다. 딱 여기까지..그때는 컴퓨터로 클러스터링(병렬화)한다는 내용과 이런 내용들이 마냥 어렵게만 느껴져서 이 친구와의 인연은 딱 거기서 마무리되었었다.그런데 이번에 데이터 엔지니어관련 공부를 하면서 인연의 끈이 있었던 것인지, 이렇게 다시 좀 더 딥하게 공부할 수 있는 기회가 되었다. 자, 이제 컴퓨터 클러스터의 방대한 데이터 세트를 변형하고 분석하는 강력한 도구인 하둡(Hadoop)과 친해져보도록 하자. HDP(Hortonworks Data Platform)? 분산 스토리지 및 대규모 멀티 소스 데이터 세트 처리가 가능한 오픈소스 프레임워크이다. 이 프레임워크를 설치하는 이유는 이 프레임워크에 Hadoop이 이미 설치가되어있고, 관련 기술들도 일괄 설치가 되어있다.그래서 나와 같이 하둡을 처음 접하는 사람에게는 적합하다.간단하게 Virtualbox에 이미지를 올려서 실습하려고 했는데, 기본 RAM이 8GB로 설정이 되어있다.내가 휴대하면서 사용하고 있는 노트북의 RAM이 8GB라 어려울 것 같다는 느낌적인 느낌이 든다. 그래서 간단하게 AWS에 이미지를 올려서 사용을 해보기로 했다.(이번 기회에 RAM 32GB/SSD 1TB/i5 11세대 스펙이 되는 노트북 하나를 장만했다. 아무래도 향후 프로젝트를 진행하게 되면서 다양한 프로그램을 돌리게 될텐데, 사양이 안좋은 PC로 더 이상 작업을 이어가기 어려울 것 같아 이번 기회에 사양대비 저렴한 노트북 한 대를 구매하였다. 미래를 위한 일종의 투자라고 생각하자.) 앞으로의 학습 목표! 하둡(Hadoop) 생태계의 중요한 요소를 이해 & 기본 사용법 익히기 하둡과 관련된 기술들은 정말 다양하다. 따라서 각 각의 기술들이 하둡의 생태계에서 어떤 역할을 하고 있는지 전체적인 그림을 이해하고, 앞으로 어떻게 더 깊이있게 학습할 것인지에 대한 방향설계에도 도움이 될 것이다. 우선적으로 VirtualBox에 다운받은 Hortonworks Sandbox image를 올려서 실습환경을 구축하였다. Hands-on으로 흥미를 갖고, 살붙이기 식으로 개념을 심도있게 학습하기 기술에 있어, 개념과 원리 그리고 역사는 중요하다. 하지만 실제로 기술적으로 어려운 개념에 대해 학습하는데 있어, 초반에 너무 개념 익히기만 하면 금방 지치게 되는 것 같다. (나는 그렇다. 이건 사람마다 다른 것 같다) 그래서 이번 하둡 그리고 하둡과 관련된 기술들을 학습 할 때에는 직접 실습을 많이 해보고 그 원리와 개념에 대해서 심도있게 학습해보는 방향으로 해보기로 했다. 나의 첫 번째 하둡 실습 한 대의 PC와 여러 대의 PC가 연결된 클러스터 환경에서 데이터가 관리되는 것은 관리 대시보드를 보면 별 차이가 없어보인다. 다만 그 차이는 처리되는 데이터의 양과 규모의 차이일 뿐이다. 나의 첫 번째 하둡 실습은 앞서 셋팅해뒀던 Hortonworks sandbox환경에서 했다. [작업 순서](1) VirtualBox 구동——— Ambari browser Interface ————–이 곳에서는 하둡 인스턴스에서 일어나는 일들을 시각화해서 보여준다. (2) localhost:1080 페이지에서 “LAUNCH DASHBOARD” 선택(default id/password : maria_dev) (3) 페이지 최 상단의 헤더의 가장 우측 사용자 배너의 왼쪽에 위치한 격자 무늬 버튼을 클릭 -&gt; “Hive View” 선택(4) “Upload Table” 탭을 클릭해서 앞서 미리 준비한 CSV dataset을 import 해준다. u.data : 평점(rating)관련 데이터 sep=”\\t” u.item : 영화 제목 정보 sep=”|” (5) “Query” 탭을 클릭해서 아래의 query를 작성한다.123456789SELECT movie_id, count(movie_id) as ratingCountFROM ratingsGROUP BY movie_idORDER BY ratingCountDESC;SELECT nameFROM movie_namesWHERE movie_id = 50; 실제 위의 각 SQL query들을 실행시켜주게 되면, 결과값이 Ambari Browser Interface 출력이 되지만, 내부적으로는 많은 과정을 거쳐서 결과값이 도출이 된다.(내부 가정에 대한 이해 중요!) 만약 위의 작업이 한 대의 PC가 아닌 클러스터에서 진행되었다면, 데이터를 최적으로 쪼갠 후에 전체 클러스터로 배분하고 모두 병렬 처리를 했을 것이다. (6) 우측 그래프 모양(Visualization) 아이콘을 클릭하고, Positional의 X,Y axis에 각 각의 column을 넣어주면, 분포도가 출력됨을 알 수 있다. Hive 아직 Hive에 대해서 잘 알지는 못하지만, Ambari에서 Query를 실행시켜보면, Hive는 관계형 데이터베이스는 아니면서 마치 관계형 데이터베이스마냥 행세를 하는 그런 녀석이다. 이번 실습에서는 여기까지만 알고, 앞으로 차근차근 Hive에 대해서 심도있게 알아가자.","link":"/2022/03/26/202203/220326-hadoop_bigdata_class/"},{"title":"220314 Pandas DataFrame Fundamentals","text":"이번 포스팅에서는 본격적으로 Pandas의 DataFrame에 대한 학습을 하면서 학습한 내용들을 정리해보려고 한다. 12345678910111213141516171819202122232425262728293031323334import pandas as pdbank_client_df = pd.DataFrame({'Bank Client ID':[111, 222, 333, 444], 'Bank Client Name':['Laila Aly', 'Kate Steve', 'Nicole Mitch', 'Francis Morris'], 'Net Worth [$]':[35000, 3000, 100000, 2000], 'Years with Bank':[4, 7, 10, 15]})bank_client_dftype(bank_client_df) #pandas.core.frame.DataFrame#DataFrame의 상위 3개 값 출력bank_client_df.head(3)#DataFrame의 하위 1개 값 출력bank_client_df.tail(1)#DataFrame의 shapebank_client_df.shape #(4, 4)#DataFrame의 정보 출력bank_client_df.info()# &lt;class 'pandas.core.frame.DataFrame'&gt;# RangeIndex: 4 entries, 0 to 3# Data columns (total 4 columns):# # Column Non-Null Count Dtype# --- ------ -------------- -----# 0 Bank Client ID 4 non-null int64# 1 Bank Client Name 4 non-null object# 2 Net Worth [$] 4 non-null int64# 3 Years with Bank 4 non-null int64# dtypes: int64(3), object(1)# memory usage: 256.0+ bytes Pandas series는 pd.Series constructor method의 parameter로 data를 넘겨주어 Pandas series를 정의하였다. Pandas DataFrame도 마찬가지로 위와같이 pd.DataFrame constructor method의 parameter로 python의 dictionary data를 넘겨주어 정의한다.Dictionary의 key/value에서 key값이 DataFrame의 column명이 되며, value의 리스트 타입의 값이 각 row의 값이 된다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121# Challenge#1# 이번 챌린지에서는 Pandas DataFrame에서 보유 주식 수와 각 주식의 가격을 곱해서 합한 결과를 구하는 문제이다.portfolio_df = pd.DataFrame({ 'stocker ticker symbols': ['NVDA','MSFT','FB', 'AMZN'], 'number of shares': [3, 4, 9, 8], 'price per share[$]': [3500, 200, 300, 400]})portfolio_dfportfolio_df['price per share[$]']portfolio_df['number of shares']stocks_dollar_value = portfolio_df['price per share[$]'] * portfolio_df['number of shares']print(stocks_dollar_value)stocks_dollar_value.sum()print('Total Portfolio Value = {}'.format(stocks_dollar_value.sum()))1. DEFINE A PANDAS DATAFRAME[2]1초import pandas as pd[3]0초# Let's define a two-dimensional Pandas DataFrame# Note that you can create a pandas dataframe from a python dictionarybank_client_df = pd.DataFrame({'Bank Client ID':[111, 222, 333, 444], 'Bank Client Name':['Laila Aly', 'Kate Steve', 'Nicole Mitch', 'Francis Morris'], 'Net Worth [$]':[35000, 3000, 100000, 2000], 'Years with Bank':[4, 7, 10, 15]})bank_client_df[4]0초# Let's obtain the data typetype(bank_client_df)pandas.core.frame.DataFrame[7]0초# you can only view the first couple of rows using .head()bank_client_df.head(3)[8]0초# you can only view the last couple of rows using .tail()bank_client_df.tail(1)[9]0초# You can obtain the shape of the DataFrame (#rows, #columns)bank_client_df.shape(4, 4)[10]0초# Obtain DataFrame informationbank_client_df.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 4 entries, 0 to 3Data columns (total 4 columns): # Column Non-Null Count Dtype--- ------ -------------- ----- 0 Bank Client ID 4 non-null int64 1 Bank Client Name 4 non-null object 2 Net Worth [$] 4 non-null int64 3 Years with Bank 4 non-null int64dtypes: int64(3), object(1)memory usage: 256.0+ bytesMINI CHALLENGE #1:A porfolio contains a collection of securities such as stocks, bonds and ETFs. Define a dataframe named 'portfolio_df' that holds 3 different stock ticker symbols, number of shares, and price per share (feel free to choose any stocks)Calculate the total value of the porfolio including all stocks[20]portfolio_df = pd.DataFrame({ 'stocker ticker symbols': ['NVDA','MSFT','FB', 'AMZN'], 'number of shares': [3, 4, 9, 8], 'price per share[$]': [3500, 200, 300, 400]})portfolio_dfportfolio_df['price per share[$]']# 0 3500# 1 200# 2 300# 3 40# Name: price per share[$], dtype: int64portfolio_df['number of shares']# 0 3# 1 4# 2 9# 3 8# Name: number of shares, dtype: int64stocks_dollar_value = portfolio_df['price per share[$]'] * portfolio_df['number of shares']print(stocks_dollar_value)# 0 10500# 1 800# 2 2700# 3 320# dtype: int64stocks_dollar_value.sum() # 14320print('Total Portfolio Value = {}'.format(stocks_dollar_value.sum()))# 0 10500# 1 800# 2 2700# 3 3200# dtype: int64# Total Portfolio Value = 17200 2022/03/24(목) 추가 기록 INPUTS(CSV와 HTML DATA 읽기) 이번 파트에서는 Pandas를 활용하여 CSV, HTML 데이터를 읽고 해당 데이터를 DataFrame의 형태로 저장하는 부분에 대해서 정리해보겠다. 123import pandas as pdbank_df = pd.read_csv('bank_client_information.csv') 123import pandas as pdhouse_prices_df = pd.read_html('https://www.livingin-canada.com/house-prices-canada.html') CSV에 데이터 프레임 쓰기 123bank_df.to_csv('sample_output_noindex.csv', index = False)bank_df.to_csv('sample_output_noindex.csv', index = False, compression='gzip') # 용량을 줄인다.(압축 파일 *.csv.gz)","link":"/2022/03/14/202203/220314-pandas_master_class/"},{"title":"220328 Numpy TIL","text":"이번 포스팅에서는 이번에 처음 배워보는 넘파이(Numpy)에 대해서 개념 및 기본 사용법에 대해서 정리해보려고 한다. 넘파이(Numpy)? 12(1) Numpy는 C언어로 구성되었으며, 고성능의 수치계산을 위해 나온 패키지이며, Numerical Python의 약자이다.(2) Python을 활용한 데이터 분석을 수행할 때, 그리고 데이터 시각화나 전처리를 수행할 때, NumPy는 매우 자주 사용되기 때문에 중요하다. 넘파이의 기본 사용 모듈 Import123import numpy as npprint(np.__version__) 넘파이를 활용한 배열 생성 넘파이를 사용해서 배열을 생성할 수 있는데, 그 타입이 ndarray이다. ndarray는 N-dimensional array의 줄임말이다. 넘파이의 배열은 일차원 배열부터 다차원 배열의 형태를 만들 수 있다. 1234567891011121314151617181920212223242526# 1D array 정의하기 (첫 번째 depth에 배열이 들어간다.)array_1D = np.array([1,8,27,64])print(array_1D)# output:# [1 8 27 64]# 2D array 정의하기 (두 번째 depth에 배열이 들어간다.)array_2D = np.array([[1,2,3,4],[2,4,9,16],[4,8,18,32]])# output:# array([[ 1, 2, 3, 4],# [ 2, 4, 9, 16],# [ 4, 8, 18, 32]])# 3D array 정의하기 (세 번째 depth에 배열이 들어간다.)array_3D = np.array([[[1,2,3,4],[5,6,7,8]],[[1,2,3,4],[9,10,11,12]]])# output:# array([[[ 1, 2, 3, 4],# [ 5, 6, 7, 8]],# [[ 1, 2, 3, 4],# [ 9, 10, 11, 12]]]) 배열에 대한 정보 확인 메소드 - 배열의 정보를 확인하는 다양한 함수 존재 - 현재 저장된 배열에 대해 RAM의 주소를 확인 1print(array_2D.data) #&lt;memory at 0x7f445e6339f0&gt; - 배열의 구조를 확인 할 수 있다. 1print(array_2D.shape) #(3, 4) - 배열의 데이터 타입을 확인할 수 있다. 1print(array_2D.dtype) #int64 - 배열간의 간격 및 각 요소간의 간격에 대해서도 확인이 가능하다. 123456789101112131415import numpy as nparr = np.array([[1,2,3,4],[5,6,7,8]])# strides : 걸음걸이, 보폭 (Numpy에서는 각 dimensions를 건너가는데 몇 bytes나 뛰어넘어야 하는지에 대한 정보)# 8바이트인 int64형이 각 배열에 4개씩 있기 때문에, 32bytes로, dimension간의 간격은 32, 각 배열의 elements간의 간격은 8바이트이므로 (32, 8)로 표기arr.strides # (32, 8) - (dimensions 간의 간격, elements 간의 간격)# Numpy에서는 배열을 생성할때 두 번째 인자로 내부에 들어갈 데이터의 타입을 지정해줄 수 있다.# int32 - 4byte# int8 - 8bytearr = np.array([[1,2,3,5],[4,5,6,10]], 'int32')arr.strides # (16, 4) - (dimensions 간의 간격, elements 간의 간격) NumPy를 활용한 다양한 객체 생성 - NumPy 패키지내의 함수를 활용하여 다양한 방식으로 패키지 작성 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as np# Array of 1ones = np.ones((3,4)) # 행, 열print(ones)# 1로 채워진 3행 4열 배열 생성#output:# [[1. 1. 1. 1.]# [1. 1. 1. 1.]# [1. 1. 1. 1.]]print(ones.ndim) # 차원 수 확인#output:# 2# Array of 0zeros = np.zeros((2,3,5), dtype=np.int16) # 3차원, 2차원, 1차원의 개수print(zeros)#output:# [[[0 0 0 0 0]# [0 0 0 0 0]# [0 0 0 0 0]]# [[0 0 0 0 0]# [0 0 0 0 0]# [0 0 0 0 0]]]# Array with random valuenp.random.random((2,2)) # 2차원, 1차원의 개수#output:# array([[0.59266798, 0.64868564],# [0.93648129, 0.3351101 ]])# Empty 배열emptyArray = np.empty((3, 2))print(emptyArray)#output:# [[4.63755739e-310 0.00000000e+000]# [0.00000000e+000 0.00000000e+000]# [0.00000000e+000 0.00000000e+000]]# Full ArrayfullArray = np.full((2, 2), 7) # 2행 2열 배열에 숫자 7로 채워라라는 의미print(fullArray)# output#[[7 7]# [7 7]]# Array of Evenly-Spaced Values (1차원 배열)# arange 특정한 규칙에 따라 증가하는 수열을 생성한다.evenSpacedArray = np.arange(10,50,5)print(evenSpacedArray) # [10 15 20 25 30 35 40 45]# linspace &amp; logspace 명령은 선형 구간 혹은 로그 구간을 지정한 구간의 수 만큼 분할을 한다.evenSpacedArray2 = np.linspace(0,2,9)print(evenSpacedArray2)# [0. 0.25 0.5 0.75 1. 1.25 1.5 1.75 2. ]evenSpacedArray3 = np.logspace(0,2,9)print(evenSpacedArray3)# [ 1. 1.77827941 3.16227766 5.62341325 10.# 17.7827941 31.6227766 56.23413252 100. ] NumPy 배열 정렬 1# 내용 추가 정리하기 2022/03/29 추가 내용 3차원 배열 연습문제 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import numpy as np# 위와 같은 요소로 이루어진 3차원 ndarray를 만들고 a1이라는 이름으로 저장하여 출력a1 = np.array([[[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[20, 21, 22], [23, 24, 25], [26, 27, 28]], [[30, 31, 32], [33, 34, 35], [36, 37, 38]]])a1# ndaray의 차원 확인a1.ndim # 3# ndarray의 구조 확인a1.shape #(3, 3, 3)# Q1: [[30, 31, 32],# [33, 34, 35],# [36, 37, 38]] 만 출력a1[2, :, :] # 행에서 마지막 행만 출력되고 열은 모두 출력되도록 슬라이싱# Q2: [[[10, 11, 12],# [13, 14, 15]],# [[20, 21, 22],# [23, 24, 25]],# [[30, 31, 32],# [33, 34, 35]]] 만 출력되도록a1[:, :2] # 모든 행은 출력되고, 열은 마지막 열을 제외한 0,1번째만 출력이되야하므로, :2로 슬라이싱 해준다.# Q3: [20, 23] 만 출력되도록 슬라이싱하기a1[1, :2, 0] # 두 번째 행 요소에서 열의 0번째에서 2개 추출# Q4: [[14, 17],# [24, 27],# [34, 37]]a1[:, 1:, 1] 형태(Shape) 변경 12345678910111213141516171819202122232425import numpy as npnp.arange(6).reshape((3,2)) # 0~5 숫자를 요소로가지는 3행 2열 배열을 생성np.arange(6).reshape((3,-1)) # -1은 3행을 기준으로 했을 때 자동으로 열을 알아서 맞춰준다.arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])newarr = arr.reshape(2, 2, -1)print(newarr)# output:# [[[1 2]# [3 4]]# [[5 6]# [7 8]]]# NumPy에서 -1은 차원에 대한 정확한 숫자를 명시하지 않아도 된다는 의미이다. -1을 입력하게 되면, NumPy가 알아서 계산을 해줄 것이다.arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])newarr = arr.reshape(4, 3) # 1-D to 2-Darr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])newarr = arr.reshape(2, 3, 2) # 1-D to 3-D# 3차원 배열에서는 2차원 변환과는 다르게 (행, 열) 순이 아닌 (열, 행) 순이다. 2022/03/30 추가 작성 다양한 NumPy 메소드 (1) mean : 평균 1234567891011121314151617a = np.arange(2, 41, 2).reshape(2, 5, -1)# output :# array([[[ 2, 4],# [ 6, 8],# [10, 12],# [14, 16],# [18, 20]],# [[22, 24],# [26, 28],# [30, 32],# [34, 36],# [38, 40]]])a.mean()np.mean(a) (2) flatten : depth가 있는 배열을 평탄하게 만들때 사용 12a.flatten()np.flatten(a) # AttributeError flatten은 예외적으로 NumPy의 모듈 함수가 아니다. ndarray 객체의 자체 메소드로만 사용이 가능하다. 연결(Concatenate) 1234567891011121314151617181920212223242526# 2 x 2 배열과 1 X 2 배열을 합칠때에는 행 기준으로 붙여야되기 때문에 axis=0이 되어야 한다.a = np.array([[1, 2], [3, 4]])b = np.array([[5, 6]])a.ndim # 2a.shape # (2, 2)b.ndim # 2b.shape # (1, 2)np.concatenate((a, b), axis=0)# array([[1, 2],# [3, 4],# [5, 6]])np.concatenate((b, a), axis=0)# array([[5, 6],# [1, 2],# [3, 4]])# 열이 같으면 행을 기준으로 붙일 수 있고, 행이 같으면 열을 기준으로 붙일 수 있다.# 위에서 b를 reshape를 통해 a와 같은 2행으로 reshape하고, 열을 기준으로 concatenate해주도록 하자.b = b.reshape(2, -1)np.concatenate((b, a), axis=1) # axis=1은 열을 기준으로 2차원 배열 a와 b를 붙여준다는 의미이다.# array([[5, 1, 2],# [6, 3, 4]]) .T = 전치행렬 (행과 열을 교환) reshape를 사용하여 직접 행, 열의 수를 지정하여 행과 열이 교환된 배열의 모양으로 수정해줄 수도 있지만, 간단하게 .T를 사용해서 간단하게 행과 열이 교환된 배열의 모양으로 만들어줄 수 있다. 1np.concatenate((b.T, a), axis=1) 브로드 캐스팅(Broadcasting) 배열을 가지고 연산을 할 때, 배열의 모양(shape)이 서로 다른 경우에도 서로 다른 배열간의 산술연산이 가능하도록 하는 메커니즘이 바로 브로드 캐스팅이다.통신에서도 이 브로드 캐스팅이라는 용어가 있는데, 뭔가 신호나 패킷을 특정 도메인 영역에 뿌려준다는 것을 생각하면, NumPy에서의 브로드 캐스팅은 배열내의 전체적인 산술연산을 해주는 것이라고 생각하면 된다. 123[브로드 캐스팅의 조건](1) 두 ndarray 중 최소한 하나의 배열의 차원이 1 이하이면 가능(2) 두 ndarray의 차원의 축의 길이가 동일하면 가능 12345678910111213141516171819202122232425# 브로드 캐스팅 예시# 스칼라와 벡터 연산vector = np.random.randint(1,10, size=(4,1))2 * vector# output# array([[2],# [4],# [4],# [4]])vector = [[9], [8], [4]]mat = [[7, 1, 6, 3], [1, 2, 4, 5], [5, 4, 3, 5]]vector * mat# result :# array([[63, 9, 54, 27],# [ 8, 16, 32, 40],# [20, 16, 12, 20]]) 배열 조작(Array Manipulation) Adding/Removing Elementsnp.delete([target array], [row num], [0: row, 1: column])12345678910111213arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])arr# array([[ 1, 2, 3, 4],# [ 5, 6, 7, 8],# [ 9, 10, 11, 12]])np.delete(arr,1,0) #0번째 축은 행, 1행을 제거np.delete(arr,1,1) #1번째 축은 열, 1열을 제거np.mean(arr,axis=0) #0번재 축은 행, 행에서의 평균np.mean(arr,axis=1) #1번째 축은 열, 열끼리의 평균","link":"/2022/03/28/202203/220328-Numpy/"},{"title":"220328 Python TIL","text":"파이썬 클래스 12345678910111213141516171819202122232425262728class Movie: def __init__(self, name, actor, review, score): self.name = name self.actor = actor self.review = review self.score = score def update(self, name, actor, review, score): self.name = name self.actor = actor self.review = review self.score = score def all(self): print(&quot;제목: {}\\n배우: {}\\n리뷰: {}\\n별점: {}&quot;.format(self.name, self.actor, self.review, self.score)) 패터슨 = Movie('패터슨', '아담 드라이버', '아하!', 4.5) 지구를지켜라 = Movie('지구를 지켜라', '신하균', '7번 봤다 더 말이 필요한가', 5) 헤드윅 = Movie('헤드윅', '존 카메론 미첼', '위키드 리틀 타운', 4) for a in (지구를지켜라, 헤드윅, 패터슨): a.all() print('-----------') # update method 이용하여 가장 마지막 영화의 제목 뒤에 2를 넣어 수정 패터슨.update('패터슨2', '아담 드라이버', '아하!', 5) 패터슨.name 인스턴스 변수의 은닉, 클래스 변수 선언, static method 추가12345678910111213141516171819202122232425262728293031class Movie: class_name = 'Movie' def __init__(self, name, actor, review, score): self._name = name self.actor = actor self.review = review self.score = score def update(self, name, actor, review, score): # _name을 숨겨놓고 조회만 하게 하려면 update method에서 제목 자체를 생략한다. self.actor = actor self.review = review self.score = score def all(self): print(&quot;제목: {}\\n배우: {}\\n리뷰: {}\\n별점: {}&quot;.format(self.name, self.actor, self.review, self.score)) # 파이썬에서 은닉성을 만들기 위해 - 함수를 이용해서 변수명인척 숨어있는 변수를 호출하는 능(볼 수만 있고, 해당 변수명은 알 수 없으며 수정도 할 수 없음.) @property def out_name(self): return self._name @staticmethod def staticMovie(): print(Movie.class_name) @classmethod def staticMovie(cls): print(cls.class_name) 상속 자식이 부모 클래스를 상속 받고, 자식 클래스의 인스턴스 변수를 선언해주었을 때, 인스턴스 변수를 통해 자식 클래스와 부모 클래스의 공통된 메소드를 호출하게 되면, 자식 클래스의 하위에 있는 메소드가 호출된다. 자식 클래스에서 부모 클래스를 상속받고, sample이라는 이름의 method를 오버라이딩하고 있다. 12345678910class Parent: def sample(self): print(&quot;Call sample method in Parent class&quot;)class Child(Parent): def sample(self): print(&quot;Call sample method in Child class&quot;)lee = Child()lee.sample() # &quot;Call sample method in Child class&quot; 출력 1234567891011class Parent: def __init__(self): print(&quot;Constructor in Parent class&quot;)class Child(Parent): def __init__(self): print(&quot;Constructor in Child class&quot;)lee = Child()# &quot;Constructor in Child class&quot; 출력# 생성자 자체도 오버라이딩된다. 부모클래스의 생성자 호출 부모 클래스를 상속받은 자식 클래스의 생성자에서 super() 키워드를 통해 상속받은 부모 클래스의 생성자를 호출 할 수 있다. 12345678class Parent: def __init__(self): print(&quot;constructor in Parent class&quot;)class Child(Parent): def __init__(self): print(&quot;constructor in Child class&quot;) super().__init__()","link":"/2022/03/28/202203/220328-Python_til/"},{"title":"220330 데이터 시각화(Matplotlib) TIL","text":"이번 포스팅에서는 Matplotlib에 대해서 학습한 내용에 대해서 작성해보려고 한다. Matplotlib 우선 matplotlib를 어떻게 가져와서 사용하는지에 대해서 알아보자.우선 가장 기본적인 형태의 그래프를 Matplotlib 라이브러리를 사용해서 그려보려고 한다. 12345678910111213141516import matplotlib.pyplot as pltimport pandas as pdinvestment_df = pd.read_csv('crypto_daily_prices.csv')investment_df# matplotlib을 사용해서 Pandas DataFrame의 data를 시각화하기investment_df.plot(x = 'Date', y = 'BTC-USD Price', label = 'Bitcoin Price', linewidth = 3, figsize = (14, 6))plt.ylabel('Price [$]')plt.xlabel('Date')plt.title('가상화폐 시각화 실습 (matplotlib)')# 기본 범례 위치 변경해보기plt.legend(loc = 'upper right')# 그리드 그려주기plt.grid() MINI CHALLENGE #1 Ethereum 가격 정보에 해당하는 부분만 matplotlib를 사용해서 그래프로 표현해보기새로운 조건 : 그래프의 선 색상을 파란색(default)에서 빨간색으로 변경해보고, 범례의 위치는 최 우측 상단에 배치하며, 선의 두께는 4로 한다. 123456789101112import matplotlib.pyplot as pltimport pandas as pdinvestment_df = pd.read_csv('crypto_daily_prices.csv')investment_dfinvestment_df.plot(x = 'Date', y = 'ETH-USD Price', label = 'ETH Priced', linewidth = 4, color = 'r', figsize = (14, 6))plt.ylabel('Price [$]')plt.xlabel('Date')plt.title('ETH-USD Price Matplotlib graph')plt.legend(loc = 'upper right') Yahoo finance 데이터 활용하기 야후 파이낸스 라이브러리를 설치12!pip install yfinanceimport yfinance as yf 12345678910# 가져오고자하는 정보에 대한 항목 리스트를 작성한다.investments_list = ['BTC-USD']# 시작과 끝 날짜 변수를 선언한다.start = datetime.datetime(2010, 1, 1)end = datetime.datetime(2021, 5, 16)# 위에서 선언한 변수를 인수로 넣어 야후 파이넨스로부터 데이터를 다운받는다.df = yf.download(investments_list, start = start, end = end)df 날짜별 시가와 고가, 저가, 종가, 조정된 종가 등의 정보들을 확인 할 수 있다.마지막 Volume은 일일 거래 양이다. MINI CHALLENGE #2 Ethereum data를 야후 파이낸스에서 다운받아서 출력해보고, BTC, ETH 그리고 LTC 코인에 대한 정보도 야후 파이낸스에서 다운받아서 출력해보자.123456789101112131415# 출력 1) Ethereum data를 야후 파이낸스에서 다운받아서 출력investments_list = ['BTC-USD']start = datetime.datetime(2010, 1, 1)end = datetime.datetime(2022, 3, 30)df = yf.download(investments_list, start = start, end = end)df# 출력 2) BTC, ETH 그리고 LTC 코인에 대한 정보도 야후 파이낸스에서 다운받아서 출력investments_list = ['BTC-USD','ETH-USD','LTC-USD']start = datetime.datetime(2010, 1, 1)end = datetime.datetime(2022, 3, 30)df = yf.download(investments_list, start = start, end = end)df 다중 플롯(Multiple plots) 구현하기 처음 matplotlib로 간단한 그래프를 그렸을 때, df.plot()의 인수로 x축 데이터 명, y축 데이터 명, linewidth, figsize를 정의해줬다. 여기서 다중 플롯을 구현해주기 위해서는 인수 중에서 y를 문자열인 단일 인수가 아닌 배열의 형태로 넘겨주면 된다. 1234investment_df.plot(x = 'Date', y = ['BTC-USD Price', 'ETH-USD Price'], linewidth = 3, figsize = (14, 6))plt.ylabel('Price')plt.title('Crypto Prices')plt.grid() 여러 개의 그래프를 하나의 그림에 표현하기 (subplot() 함수 활용) 1investment_df.plot(x = 'Date', title = 'Crypto Prices', subplots = True, grid = True, figsize = (15, 25)) 그럼 subplots 인수의 값을 False로 하면 어떻게 될까?실행하기 전에 내 예상은 일단 y축 데이터에 대한 데이터를 특정하지 않았기 때문에 모든 코인에 대해서 한 그래프에 표시가 되지 않을까? 마치 다중 플롯(Multiple plots)처럼..실행해보니, figsize의 y축 높이를 길게 잡아서 그런지 좀 길죽하게 나오긴 했어도 예상했던 것처럼 다중 플롯의 형태로 세 개의 코인 정보가 하나의 그래프에 모두 표시됨을 확인할 수 있었다.(y = [...all args...]) == (~y and subplots == False) 산점도(SCATTERPLOT) 산점도 구상하기 연습의 일환으로 비트코인과 이더리움 사이의 산점도를 구상해보자. 1234# 비트코인과 이더리움의 날짜별 수익# 비트코인과 이더리움의 수익에 대한 정보가 점으로 표시# 각 점들은 다양한 날짜나 행들을 가르킨다.daily_return_df.plot.scatter('BTC', 'ETH', grid = True, figsize = (12, 7)) PI CHART 각기 다른 비율로 투자한 금액을 원 모양으로 각 각 다르게 할당해서 표현 123456# 각 각의 가상화폐를 각기 다른 비율로 투자my_dict = {'allocation %': [20, 55, 5, 17, 3]}crypto_df = pd.DataFrame(data = my_dict, index = ['BTC', 'ETH', 'LTC', 'XRP', 'ADA'])crypto_df.plot.pie(y = 'allocation %', figsize = (8, 8))plt.title('Crypto Portfolio Pie Chart') MINI CHALLENGE #3 투자 비율을 XRP(리플)을 60%, 나머지 코인에 대해서 각 각 10%씩 배당하여 원 그래프를 수정하자. 그리고 explode 메소드를 사용해서 XRP와 다른 코인들 간의 분리를 더 도드라지게 만들어보자.123456alloc = {'allocation %': [60, 10, 10, 10, 10]}explo = [0.10, 0, 0, 0, 0]crypto_df = pd.DataFrame(data = alloc, index = ['XRP', 'BTC', 'ETH', 'LTC','ADA'])crypto_df.plot.pie(y = 'allocation %', figsize = (8, 8), explode=explo)plt.title('Crypto Portfolio Pie Chart') 히스토그램(HISTOGRAMS) 히스토그램은 기본적으로 데이터를 대변해주는 역할을 한다. 다양한 높이들의 바(bars)가 사용되고, 각 각의 바 그룹 숫자들은 특정 범위 안에 들어간다. 표준편차 : 자료가 평균을 중심으로 얼마나 퍼져있는지를 나타낸다. 12345678910111213# 평균avg = daily_return_df['BTC'].mean()# 표준편차sigma = daily_return_df['BTC'].std()# plot의 figsize 정의plt.figure(figsize = (20, 15))#daily_return_df['BTC'].plot.hist(bins = 50) # bins에는 원하는 칸의 수를 입력해서 한 칸의 크기를 조절할 수 있다.# bins 속성과 함께 alpha 속성으로 0.5(최대값 1) 값을 넣어서 색을 흐리게도 할 수 있다.daily_return_df['BTC'].plot.hist(bins = 50, alpha = 0.5)plt.title('Histogram: mu=' + str(mu) + ', sigma=' + str(sigma)) 아래 히스토그램 그래프를 통해 비트코인은 위험도가 높은 투자라는 것을 보여준다.평균적이지는 않지만, 하루에 20~25%의 수익을 내는 경우도 있으며, 그 이상의 수익을 내는 경우도 있다.하지만 수익이 2% 이하로 떨어지는 경우가 많은 것을 알 수 있다. MINI CHALLENGE #4 ETH에 대한 히스토그램을 bins=30, 색상은 빨간색으로 해서 출력해보기12plt.figure(figsize = [20, 15])daily_return_df['ETH'].plot.hist(bins = 30, color='r') 이더리움과 비트코인은 연관성이 있다. 비트코인이 떨어졌을 때, 이더리움도 같이 가격이 떨어짐을 알 수 있었다.","link":"/2022/03/30/202203/220330-data-visualization_master_class/"},{"title":"220330 Hadoop과 친해지기 그 두 번째 이야기","text":"이번 포스팅에서는 Hadoop의 기본 원리에 대해서 정리를 해보려고 한다.하둡의 기본 원리에 대한 정리에 앞서, 현재 실습환경으로 구축한 HDP(Hortonworks Data Platform)가 Cloudera로 인수합병을 되었지만, Cloudera에 의해 개발된 CDP(Cloudera Data Platform)와 HDP 둘 다 내부적으로는 Apache 서비스 계열이며, 같은 방식으로 작동을 한다.배우는데 사용되는 플랫폼이 조금 다른 것일 뿐이지, 통용되는 내용은 모두 같기 때문에 각 기술들의 개념과 사용법 그리고 동작에 대해 집중해서 학습하도록 하자. 하둡의 개요 오픈소스 소프트웨어 플랫폼으로, 매우 큰 데이터셋을 컴퓨터 클러스터 상에서 분산 저장과 분할 처리한다. 클러스터상에 PC를 추가해주기만 하면 클러스터내의 컴퓨터들은 동일한 데이터를 서로 복제를 해서 가지고 있으며, 다수의 PC를 활용해서 빅데이터를 다룬다.이때문에 화재로 인해 클러스터 내 일부 PC가 손상이 되어도 클러스터 내 다른 PC를 통해서 복구가 가능하다. 하둡의 역사 하둡이라는 기술은 대용량 데이터를 분산처리하기 위한 첫 번째 솔루션이 아니었다.그 조상이 되는 솔루션은 바로 구글이 만든 GFS(Google File System)이라는 기술이다.관련된 논문은 2003-2004년에 출간되었으며, 이것이 Hadoop의 분산 저장 개념의 토대가 되었다. 하둡의 저장 시스템의 방향성을 제공했으며, MapReduce는 Hadoop이 고안해낸 기술적 개념이다. (1) GFS - 하둡의 분산저장(2) Map Reduce - 하둡의 분산처리 하둡이라는 기술은 원래 야후가 개발을 했으며, 구글에서 작성한 GFS라는 기술 논문에 영감을 받아서 개발이 되었다고 하는데, Doug Cutting과 Tom White 이 두 사람이 2006년에 주축이 되어 개발이 되었다고 한다.Doug Cutting의 아들이 가지고 놀던 장난감 코끼리 이름이 하둡(Hadoop)이었는데, 그래서 여타 기술의 이름들과 같이 별 다른 의미 없이 이렇게 이름이 붙여졌다고 한다. 그래서 하둡은 왜 쓰는 거지? 하둡을 쓰는 이유는 아래와 같다. (1) 요새 데이터의 크기가 무지막지하게 크기 때문이다. 매일매일 테라바이트 이상의 데이터가 생성된다. (DNA/센서/웹 로그/주식시정의 거래 정보) (2) 수직적 스케일링(Vertical scaling)의 한계 - Disk seek times (디스크 검색 시간이 길다) - Hardware failures (하드웨어에 문제가 생기는 경우, 복구시에 문제가 될 수 있다) - Processing times (긴 처리시간 문제) (3) 수평적 스케일링(Horizontal scaling)수평적으로 스케일링을 하기 때문에 더 많은 데이터를 다루거나 더 빨리 처리해야 된다면 클러스터에 컴퓨터를 더 추가하면 되고, 추가한 만큼 선형적으로 처리 속도가 빨라진다. (4) 하둡은 본래 batch processing을 위해 만들어졌지만, 이제는 하둡의 데이터를 노출시켜서 웹 어플리케이션 등과 매우 빠르게 데이터를 교류할 수 있는 시스템도 있으며, 하둡 위에 구축된 다른 어플리케이션을 사용하여 대화식 쿼리의 사용도 가능하다.","link":"/2022/03/30/202203/220330-hadoop_bigdata_class/"},{"title":"220331 데이터 시각화(Seaborn) TIL","text":"이번 포스팅에서는 Seaborn에 대해서 학습한 내용에 대해서 작성해보려고 한다. Seaborn Seaborn은 강력한 데이터 시각화 라이브러리로, matplotlib의 최상에 속하는 라이브러리이다. 그래서 Seaborn이 matplotlib에 비해 기능을 높이는 것을 볼 수 있다고 한다. 우선 판다스와 Seaborn을 활용해서 데이터를 시각화해보는 연습을 해보겠다.앞으로 많은 것을 학습하고, 까먹을 수 있기 때문에 기본적인 시각화하는 방법에 대해서 기록을 해두겠다. 시각화를 하는 방법에는 여러 방법이 있지만, 그것들은 방법이지 정말 중요한 것은 가장 기본이 되는 시각화를 통해서 얻고자하는 결과와 어떤 사회적 현상을 해결할 수 있는지에 대한 인식의 전환이 필요한 것 같다. 단순히 parameter값을 뭐를 넣고 빼고 함으로써 그래프를 뽑아내는 것이 중요한 것이 아닌, 실제 시각화를 통해서 머신러닝 모델링하기 전에 해당 데이터셋에 적합한 최적의 알고리즘을 선택하기 위한 직관을 얻기 위한 용도로써 데이터 시각화를 바라봐야 되는 것 같다. 아직은 잘 모르는 단계이지만, 일단 이정도의 이론상 개념을 알고 있는 상태에서 앞으로의 AI, ML에 대한 공부도 확장해가야겠다. 123456import pandas as pdimport numpy as npimport matplotlib.pyplotas pltimport seaborn as sns 123456789import pandas as pd # Import Pandas for data manipulation using dataframesimport numpy as np # Import Numpy for data statistical analysisimport matplotlib.pyplot as plt # Import matplotlib for data visualisationimport seaborn as sns # Statistical data visualizationsns.cancer_df = pd.read_csv('cancer.csv')cancer.df 아래의 데이터는 모두 유방암에 대한 데이터 세트이다.시각화를 할 때 단순히 수치가 나와있는 데이터 세트를 그래프로 시각화히기 위함이 아니다.아래의 데이터의 수치는 자세히 보면, 종양에 대한 평균 반지름, 평균 질감, 평균 둘레에 대한 정보를 포함하고 있다. 이 데이터 세트에서 마지막 칼럼(target)을 제외한 나머지 칼럼의 값은 주로 머신러닝을 위해 머신러닝 모델에서 입력값으로 쓰인다. 예를들어, AI 모델을 훈련시키기 위한 것으로 쓰이게 되는데, 데이터 세트를 사용하는 목표 또는 목적은 target(특정대상)을 예측하기 위함인데, 이 target column은 모두 0과 1로만 이뤄져 있다. 종양에 대한 평균 반지름 ~ 평균 둘레에 대한 정보의 숫자 정보들은 모두 target이라는 column으로 추려지는데, 0 아니면 1로 표시되고 이것이 악성인지 양성인지 판단되는 척도가 된다. (악성 또는 양성이라는 의미는 암이 자라고 있다는 뜻이거나 종양으로 볼 수 있다는 의미) (1) mean area vs mean smoothness 사이의 데이터를 산포도(Scatterplot)으로 그려서 출력을 해볼 것이다.1sns.scatterplot(x = 'mean area', y = 'mean smoothness', data = cancer_df) (2) mean area vs mean smoothness 데이터를 산포도(Scatterplot)으로 그려서 출력하되, 악성 종양인지 아닌지를 판별하는 마지막 칼럼(target)을 활용하여 암인 영역대와 암이 아닌 영역대를 색상으로 구분하여 출력될 수 있도록 산포도를 구성한다.1sns.scatterplot(x = 'mean area', y = 'mean smoothness', hue = 'target', data = cancer_df) 위의 산포도를 분석한다면, target이 0인 것이 target이 1인 것에 비해 평균 영역이 더 높거나 크게 위치해 있는 것을 알 수 있다. Seaborn으로 극대화 시킬 수 있는 CountPlot Seaborn으로 극대화해서 보여줄 수 있는 Plot 중에 CountPlot이라는 녀석이 있다.이 녀석을 기본적으로 unbalanced dataset이라고도 부른다.(다른 클래스에 비해서 한 클래스에 더 많은 데이터 샘플이 있는 데이터 세트인 경우를 말한다) 12# target 칼럼의 0과 1의 갯수를 카운트해서 그래프로 보여주는 CountPlotsns.countplot(cancer_df['target'], label = 'Count') MINI CHALLENGE #1 mean radius와 mean area 사이의 값을 ScatterPlot으로 그리고 Plot에 comment 달기1sns.scatterplot(x = 'mean radius', y = 'mean area', hue = 'target', data = cancer_df) comment on the plot : 산포도를 보면, 정상인 종양이 비 정상인 종양보다 더 넓은 반지름과 넓이를 가지고 있음을 알 수 있다. Seaborn PairPlot과 DisPlot(분포도)","link":"/2022/03/31/202203/220331-data-visualization_master_class_seaborn/"},{"title":"220330 Pandas TIL (작성예정...)","text":"이번 포스팅에서는 본격적으로 Pandas 학습한 내용을 나중에 복습하기 위한 목적에서 간단하게 정리해보려고 한다. Index 설정하고 재설정하기 판다스는 csv파일을 읽거나 데이터를 DataFrame의 형태로 저장하는데 사용된다. (numeric index는 기본적으로 설정된다) 12345import pandas as pdbank_df = pd.read_csv('bank_client_information.csv') # 자동으로 index를 숫자로 지정bank_df 특정 column을 index column으로 할당 1bank_df.set_index('First Name', inplace = True) # inplace = True로 원본 데이터 업데이트 다시 numeric index를 index값으로 되돌릴때에는 reset_index메소드를 사용한다. 1bank_df.reset_index(inplace = True) 열을 판다스 데이터 프레임의 인덱스로 사용하고 싶을 때 123bank_df = pd.read_csv('bank_client_information.csv', index_col = 'First Name')bank_df (CHALLENGE #4) 판다스 데이터 프레임 인덱스 설정하고 재 설정하기 123456# 방법1bank_df = pd.read_csv('bank_client_information.csv')bank_df.set_index('Last Name', inplace = True)# 방법2bank_df = pd.read_csv('bank_client_information.csv', index_col = 'Last Name') 데이터 프레임에서 열을 선택하기 123456789101112131415161718192021222324bank_df = pd.read_csv('bank_client_information.csv')bank_dfsample = bank_df['Email']sampletype(sample) # pandas.core.series.Seriesbank_df.Email # 이 방법에는 제한사항이 있기 때문에 대괄호 표기법을 사용하도록 한다. (예외: 이름에 공백이 있는 경우에는 작동을 하지 않는다.)bank_df['Net Worth']sample = bank_df[['First Name', 'Net Worth']] # 2개의 열 이상, 다차원일때에는 다음과 같이 내부에 괄호를 한 개 더 넣어 출력하고자 하는 칼럼의 이름을 넣어준다.type(sample) # pandas.core.frame.DataFrame# 이름과 순 자산만을 포함하는 데이터프레임my_selected_columns = ['First Name', 'Net Worth']sample = bank_df[my_selected_columns]samplebank_df[0:2] # 데이터프레임의 0, 1번째 행에 대한 정보만 추려서 출력을 해준다. (CHALLENGE #5) 순 자산(Net Worth), 거래 햇수(Years with Bank), 그리고 우편번호(Postal Code)에 해당하는 열을 데이터프레임에서 선택하시오. 12345my_selected_columns = ['Net Worth', 'Years with Bank', 'Postal Code']sample = bank_df[my_selected_columns]sample DataFrame에서 Columns을 추가하고 지우기 123456789101112131415bank_df = pd.read_csv('bank_client_information.csv')bank_df# Age column이 가장 마지막 열에 추가bank_df['Age'] = [25, 26, 28, 30, 36, 22, 48, 55, 70, 69]# 특정 index position에 새로운 칼럼 삽입# 0번째 인덱스에 Credit Score 칼럼 삽입bank_df.insert(0, column='Credit Score', value=[680,700, 750, 699, 550, 600, 750, 500, 520, 510])# 특정 칼럼을 데이터프레임에서 삭제하기del bank_df['Email']bank_df.drop(labels = ['Last Name', 'Net Worth'], axis = 1, inplace = True) # axis=1 열 방향 모두 제거하고, 메모리에서 삭제하겠다. 특정 칼럼 추출하기 12Years_with_bank = bank_df.pop('Years with Bank')Years_with_bank # 단일 판다 시리즈 데이터 (CHALLENGE #6) 고객의 대출 여부를 알려주는 열을 추가하시오. 그리고 대출금을 알려주는 열을 기존의 데이터프레임에 추가하시오. 12345678bank_df = pd.read_csv('bank_client_information.csv')# 고객 대출여부를 알려주는 열과 대출금을 알려주는 열을 넣기bank_df['Has Mortgage'] = [1, 1, 0, 0, 0, 0, 1, 0, 0, 0]#값이 1이라면 대출이 있다는 의미이고, 그렇다면 대출금을 달러로,bank_df['Mortgage Value'] = [200000, 130000, 0, 0, 0, 0, 400000, 0, 0, 0] DataFrame에서 LABEL-BASED로 요소선택하기 123456789101112131415161718192021222324252627import pandas as pdbank_df = pd.read_csv('bank_client_information.csv', index_col='Last Name')# alphabetical order로 dataframe 정렬 (LastName의 알파벳 순서로 정렬)bank_df.sort_index(inplace = True)# loc은 데이터 프레임에서 행과 열을 필터링하는데 사용된다.# index가 정수기반이라면 iloc을 사용하지만, 문자열 기반이라면, loc을 사용한다.bank_df.loc['Small'] # Type이 Series인 데이터bank_df.loc['Steve'] # Type이 DataFrame인 데이터bank_df.loc['Ahmed':'Patton'] # Ahmed ~ Patton 까지 모든 행이 출력 (Patton까지 모두 포함)# cf. iloc[x:y]에서는 y-1 정수 범위내에서 출력# Keller index까지 모두 출력bank_df.loc[:'Keller']# 출력을 원하는 index의 값을 배열로 넘겨준다.bank_df.loc[['Keller', 'Steve', 'Mo']]# 무작위로 5의 샘플을 출력한다.# axis=0는 행을 의미, axis=1는 열을 의미한다.bank_df.sample(n = 5, axis = 0)# 데이터프레임의 30%를 무작위로 출력bank_df.sample(frac=0.3, axis=0) (CHALLENGE #7) “first name” column을 index로 설정해서 csv 파일을 로드하고, 데이터프레임으로부터 랜덤으로 2개의 행 데이터를 선택한다. 12345import pandas as pdbank_df = pd.read_csv('bank_client_information.csv', index_col='First Name')bank_df.sample(n = 2, axis = 0) # axis = 0은 행이며, n = 2는 랜덤으로 뽑을 데이터의 표본 객체 수이다. DataFrame에서 INTEGER INDEX-BASED 요소 선택 1234567891011bank_df = pd.read_csv('bank_customer_information.csv')bank_df.iloc[9] # 10번째 행의 데이터에 접근bank_df.iloc[2:5] # 2~4까지의 행 데이터를 포함bank_df.iloc[:4] # 0~4까지, 4는 포함하지 않고 모든 행을 가져온다는 의미bank_df.iloc[[2, 4, 9]] # 2, 4, 9행의 데이터를 가져온다.bank_df.iloc[4, 0:3] # 4행으로 가서 0:3 (0,1,2)열만 가져온다. (CHALLENGE #8) 두 가지 다른 방법을 사용해서 마지막 두 행 데이터를 가져와서 출력하시오. 12bank_df.iloc[-2:] # 끝에서 2번째부터 끝까지 출력bank_df.iloc[8:] # 8행부터 끝까지 출력 브로드캐스팅 연산과 새로운 데이터 프레임값을 설정하는 방법 123456789101112131415161718bank_df = pd.read_csv('bank_client_information.csv')# 모든 고객의 순 자산이 증가 각 각 1000만원 증가bank_df['Net Worth'] = bank_df['Net Worth'] + 1000bank_df['Net Worth'] = bank_df['Net Worth'].add(1000)# 고객의 순 자산을 포함하는 열을 하나 추가 (캐나다 달러로 환산) 1USD = 1.3CADbank_df['Net Worth (CAD)'] = bank_df['Net Worth'].mul(1.3)bank_df# 특정 고객의 이메일 주소 업데이트bank_df.loc['Kate','Email'] = 'kate.noor@hotmail.com' # index가 숫자 기반이기 대문에 다음과 같이 label 기반의 .loc[]으로 선택을 할 수 없다.bank_df.iloc[4, 2] = 'kate.noor@gamil.com'bank_df.iloc[[0, 3],[4]] = [6000, 15000] # 두 고객의 networth 정보를 업데이트한다. 0행과 3행의 4열(Net Worth)정보를 주어진 정보에 따라 업데이트한다.","link":"/2022/03/30/202203/220330-pandas_master_class/"},{"title":"220721 ANSI SQL &amp; NON ANSI SQL","text":"이번 포스팅에서는 ANSI SQL과 NON ANSI SQL의 각기 다른 방식으로 JOIN 쿼리를 작성했을때의 차이점에 대해서 간단하게 포스팅하려고 한다. ANSI &amp; NON-ANSI SQL표준 ANSI 방식의 JOIN 쿼리에서는 JOIN 키워드와 ON 절을 사용하여 두 테이블을 합치며, 필터 조건은 WHERE 절에 작성을 해준다. 아래의 쿼리는 department name이 HR 부서인 employee의 이름과 부서 정보를 출력해주는 쿼리이다. 12345678910111213-- ANSI-- JOIN 키워드를 사용해서 ON clause에서 조인되는 조건을 명시했다면 표준 ANSI 방식SELECT e.emp_name, d.dept_nameFROM employee eINNER JOIN department d on d.dept_id = e.dept_idWHERE d.dept_name = 'HR';-- INNER JOIN에서는 ON 절에 AND 필터 조건을 붙여서 작성해도 결과는 같다.SELECT e.emp_name, d.dept_nameFROM employee eINNER JOIN department d ON d.dept_id = e.dept_id AND d.dept_name = 'HR';-- 하지만, OUTER JOIN에서는 위와같이 ON 절에 필터조건을 거는 경우 문제가 될 수 있다. 123456-- NON-ANSI-- JOIN 키워드 대신 ,(comma)를 사용해서 조인 조건을 명시했다면 NON-ANSI 방식SELECT e.emp_name, d.dept_nameFROM employee e,department d WHERE d.dept_id = e.dept_id AND d.dept_name = 'HR'; 아래의 쿼리는 모든 employee의 이름과 부서 정보를 출력해주는 쿼리이다.모든 employee의 정보를 출력해주기 위해서는 department 테이블에 명기된 부서 정보와 matching되지 않은 employee의 정보도 출력해야 되기 때문에 OUTER JOIN을 해서 모든 employee의 정보를 출력해줘야 한다. 123456-- ANSI-- JOIN 키워드를 사용해서 ON clause에서 조인되는 조건을 명시했다면 표준 ANSI 방식SELECT e.emp_name, d.dept_nameFROM employee eLEFT OUTER JOIN department d on d.dept_id = e.dept_idWHERE d.dept_name = 'HR'; ANSI 방식의 SQL 쿼리에서 LEFT OUTER JOIN을 할때에는 OUTER를 생략하여 LEFT JOIN으로 작성할 수도 있다. 왼쪽 테이블을 기준으로 OUTER JOIN을 하기 때문에 오른쪽 부서 정보 테이블이 NULL인 경우도 모두 포함하여 모든 employee의 정보를 출력한다. ANSI 방식의 JOIN에서는 간단하게 JOIN 키워드만 변경을 하면, FULL OUTER JOIN, LEFT OUTER JOIN, RIGHT OUTER JOIN 등을 손쉽게 할 수 있다. 하지만 NON-ANSI 방식의 쿼리문에서는 약간 까다로워지는 경우가 생긴다. NON-ANSI 방식으로 LEFT (OUTER) JOIN을 하기 위해서는 ORACLE DB에서 작성을 해야한다. ORACLE DB에서는 (+) symbol을 사용해서 OUTER JOIN 쿼리를 작성하게 되는데, (+) symbol은 조인하고자 하는 기준이 되는 열의 반대 열에 붙이도록 한다.PostgreSQL에서는 NON-ANSI 방식을 사용해서 내부 조인은 가능하지만, 외부 조인은 할 수 없고, MS-SQL, MYSQL도 마찬가지다. (단, ORACLE에서만 NON-ANSI 방식으로 OUTER JOIN이 가능하다) 12345678910111213-- NON-ANSI / LEFT OUTER JOIN-- 모든 employee를 출력하기 위해서 employee를 기준으로 OUTER JOIN을 해야되기 때문에 department의 조인 조건 절에 (+) symbol을 붙인다.SELECT e.emp_name, d.dept_nameFROM employee e, department dWHERE d.dept_id(+) = e.dept_id-- RIGHT OUTER JOIN-- 모든 department를 출력(Employee 정보가 NULL인 경우도 포함)SELECT e.emp_name, d.dept_nameFROM employee e, department dWHERE d.dept_id = e.dept_id(+) ANSI SQL이 NON-ANSI SQL 보다 좋은 점1. 쿼리문이 더 짧고 간결해지며, 가독성이 좋아지고 디버깅하기 쉬워진다.1234567891011121314151617-- ANSI -- FROM 절의 테이블과 JOIN 키워드 우측의 테이블을 각 각 왼쪽 오른쪽 테이블로 정의한다.SELECT e.emp_name, d.dept_name, m.manager_name, p.project_nameFROM employee eLEFT JOIN department d on d.dept_id = e.dept_idRIGHT JOIN manager m on m.manager_id = e.manager_idLEFT JOIN projects p on p.team_member_id = e.emp_id;-- NON-ANSI (ORACLE)SELECT e.emp_name, d.dept_name, m.manager_name, p.project_nameFROM employee e, department d , manager m, projects pWHERE d.dept_id (+) = e.dept_idAND m.manager_id = e.manager_id (+)AND p.team_member_id (+) = e.emp_id; 2. JOIN 조건과 FILTER 조건을 서로 분리할 수 있다.(ON 절 = JOIN 조건 / FILTER 조건 = WHERE 조건)12345678910-- 부서명이 HR인 경우만 emp_name을 출력SELECT e.emp_name, d.dept_nameFROM employee eLEFT JOIN department d ON e.dept_id = d.dept_idWHERE d.dept_name = 'HR';-- 부서명이 HR인 경우만 dept_name을 표기하고, 나머지는 null로 채워서 출력SELECT e.emp_name, d.dept_nameFROM employee eLEFT JOIN department d ON e.dept_id = d.dept_id AND d.dept_name = 'HR'; 3. 우연한 CROSS JOINS을 피할 수 있다.두 개의 테이블을 특정 조인 조건 없이 합치는 경우, CROSS JOIN이 발생할 수 있다. 123456789-- NON-ANSI (ORACLE)SELECT e.emp_name, d.dept_name, m.manager_name, p.project_nameFROM employee e, department d , manager m, projects pWHERE d.dept_id (+) = e.dept_id-- AND m.manager_id = e.manager_id (+)AND p.team_member_id (+) = e.emp_id; ANSI SQL로 작성을 하는 경우에는 ON 절에서 조인 조건을 빼먹고 쓰지 않은 경우에 별도의 에러 메시지를 통해 문제를 해결 할 수 있다. 4. ANSI는 모든 RDBMS와 시스템에서 범용적으로 사용 가능한다.","link":"/2022/07/21/202207/220720-sql-study/"},{"title":"220703 데이터 파이프라인 구축 오프라인 수업 &#x2F; 6주차","text":"이번 포스팅에서는 여섯 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. 이번 여섯 번째 수업을 마지막으로 데이터 파이프라인 구축과 관련한 데이터 엔지니어링 수업이 마무리되었다. 이번 수업을 통해 정말 많은 것들을 배울 수 있었다. 특히 이전에는 클라우드 플랫폼을 활용해서 데이터 파이프라인을 구축하는 것이 전부라고 생각했었지만, 이번 수업을 듣고나서 관리 및 운영의 관점에서 클라우드 플랫폼에서 제공하는 서비스들의 근간이 되는 오픈 소스 프로젝트의 세부 동작원리에 대해서 이해하는 것이 더 중요하다는 것을 배웠다. 그리고 클라우드 플랫폼을 활용해서 데이터 파이프라인을 처음 구축했을때 그 다음 스탭으로 어떤 식으로 공부를 이어나갈지 감을 잡지 못했었는데, 이번 총 6번의 수업동안 (6주간 진행)앞으로 어떻게 더 공부를 해야되는지, 그리고 새로운 기술스택이 나왔을때 어떤식으로 학습을 이어나가야 되는지에 대해서 알게 되었다.이번 마지막 수업을 마무리하며 강사님이 어느 데이터 파이프라인 구축에 있어, 어느 파이프라인 구성이 정답이고 그런 건 없다고 하셨다. 그리고 수업을 들으면서 느낀 것은 정말 하나의 파이프라인을 구성할때에도 많은 것들을 고려해야하며, 파이프라인의 각 구성 요소들의 특징들을 제대로 이해하고 있어야 비로소 효율적인 파이프라인을 구축할 수 있다는 것을 배웠다. 아무튼 이번 수업을 통해 좀 더 데이터 엔지니어의 업무 중 하나인 데이터 파이프라인 구축에 대해서 좀 더 심도있게 배울 수 있었던 것 같다. 매주 한 번 강남역에 가서 세 시간씩 하루도 빠지지 않고 수업에 참여하고, 배운 내용을 블로그에 하나도 빠뜨리지 않고 기록하였다. 이런 나에게 칭찬을 하며, 마지막 수업시간에 배운 내용을 정리해보려고 한다. ElasticSearch ES를 NoSQL DB와 같은 저장소로써 사용을 하면서 저장된 데이터를 검색하는 용도로 사용된다. 그리고 주로 모니터링이나 로깅과 같은 용도로 많이 사용된다. 메트릭 같은 것을 JSON에 같이 담아서 Kibana를 통해서 그래프로 그려주면, 프로메테우스와 같은 TS DB와 같은 성능은 내지 못하지만, 대시보드로 충분히 그려서 활용할 수 있다. 데이터가 적은 경우에는 앞에서 설명한 것과 같이 ElasticSearch에 저장된 데이터를 Kibana를 활용해서 시각화를 해줄 수 있지만, 이러한 메트릭 값들이 많아지면, TS(Time Series) 전용으로 담아두는 TS DB를 생성해서 관리한다. 이처럼 ES는 검색엔진이지만, 다양한 용도로 사용이 되고 있다. 메트릭?메트릭이란 타임스탬프와 보통 한 두 가지 숫자 값을 포함하는 이벤트이다. 이 메트릭은 모든 메트릭의 행이 타임스탬프로 정렬된 메트릭 파일에 순차적으로 추가된다.로그와는 달리 메트릭은 주기적으로 보내게 되며, 로그는 보통 무언가가 발생했을때 로그 파일에 추가되는 형식으로 동작한다.메트릭은 종종 리소스 사용 모니터링, 데이터베이스 실행 메트릭 모니터링 등 소프트웨어나 하드웨어의 상태 모니터링 맥락에서 사용되는 것이 일반적이다. Elastic은 솔루션의 모든 계층에서 메트릭 관리와 분석에 대한 사용자 경험을 증진하는 새로운 기능을 제공한다. Matricbeat는 5.0의 새로운 기능 중 하나이며, 사용자가 머신이나 어플리케이션에서 ElasticSearch로 메트릭 데이터를 전달할 수도 있고, Kibana에서 바로 사용 가능한 대시보드를 제공한다. Kibana는 메트릭 같은 숫자 데이터를 다룰 수 있게 설계한 타임라인온 플러그인도 코어에 연동해서 사용할 수 있다. TSDB(Time Series Database)?TSDB는 Time-Stamped Data라고도 불리며, 시간에 따라 저장된 데이터를 의미한다. 시계열 데이터는 동일한 소스로부터 시간이 지남에 따라 만들어진 데이터들로 구성이 되기 때문에 시간 경과에 따른 변화를 추적하는데 용이하다. ex) 어떤 집안의 온도들로부터 경제 지표, 환자의 심작 박동수나 회사의 주가, 서버로부터 기록되는 히스토리성 데이터, 센서 데이터 TSDB란 시계열 데이터를 처리하기 위해 최적화된 데이터베이스로, 빠르고 정확하게 실시간으로 쌓이는 대규모 데이터들을 처리할 수 있도록 고안되었다. TSDB는 데이터들과 시간이 함께 저장하는데, 이를 통해 시간의 흐름에 따라 데이터를 분석하기에 매우 용이하다. 오늘날 엄청나게 많은 데이터들이 수집이 되고 있고, 인공지능의 급격한 발달로 필요한 데이터들의 양이 급증하고 있다. 이러한 데이터들을 처리하기에는 관계형 데이터베이스와 NoSQL로는 한계가 있기 때문에 거의 끝없는 데이터들을 처리할 수 있는 TSDB, 시계열 데이터의 중요성이 대두되고 있다. ref) 자율주행과 같은 경우, 약 8시간 운전할 때마다 40TB의 데이터를 만들고 사용한다. Amazon QuickSight Amazon QuickSight는 Amazon에서 제공하는 BI툴로, AWS 계정이 별도로 요구되지 않기 때문에 손쉽게 다른 사람들과 구성한 Dashboard를 공유할 수 있다. BI의 영역이 Engineer와 Scientist 간의 중첩 영역(협업 영역)으로 데이터 엔지니어가 데이터를 잘 정제해서 서빙을 해주면, 서빙된 데이터를 DA와 DS들이 붙어서 시각화 및 분석을 하기 때문이다. 이러한 BI 툴은 규모가 점점 커지는 기업의 경우에는 FE개발자가 붙어서 전용 dashboard를 만들기도 한다고 한다. 이러한 FE 작업을 줄여주기 위해서 BI툴을 사용하기도 한다. 그리고 AWS QuickSight의 최고 장점은 다양한 AWS의 여러 데이터 소스를 기반으로 BI툴을 활용할 수 있다. (1) QuickSight demo : https://democentral.learnquicksight.online/ 비 IT 부서에 차트로 시각화해서 정보를 제공할때 사용이 되며, 생성된 dashboard에 접근하기 위한 개별 계정을 부여할 수 있다. (2) QuickSight workshop : https://catalog.us-east-1.prod.workshops.aws/workshops/ac8dc849-3d95-43e3-b380-aa38f0dc31e4/en-US/author-workshop/1-build-your-first-dashboard S3의 데이터를 조회할때 자체 성능을 위해서 Partitioning이 필요하지만 엄청난 양의 데이터를 쌓을 수 있다.그리고 Amazon QuickSight는 Third party platform, On-premise DB 등 다양한 데이터 리소스를 연결해서 BI툴을 활용할 수 있다. 이번 실습에서는 간단하게 CloudFormation에서 랜덤한 로그 데이터를 생성해주고, 적재된 데이터를 기준으로 QuickSight에서 시각화를 해주었는데, 실제 업무에서는 데이터가 실시간으로 계속 쌓이고 있기 때문에 신규 스트리밍 데이터의 경우에는 Athena에서 계속 조회를 해서 가져와야한다. 따라서 별도로 배치를 돌려서 일률적으로 데이터를 업데이트해줘야한다. 참고) 실제 hive, Presto에서도 MKCK repair update를 해서 메타 데이터를 업데이트해서 새로운 데이터를 가져오는데, QuickSight에서는 데이터 세트 메뉴에서 가져온 데이터를 조회하는 UI에서 &quot;지금 새로 고침&quot; 또는 &quot;새로 고침 예약&quot;을 통해 기존 데이터를 새로운 데이터로 업데이트해줄 수 있다. ES의 경우에는 새로운 데이터가 생기는 즉시 ES가 indexing을 통해 데이터를 업데이트해주기 때문에 실시간으로 Kibana를 통해 상시 업데이트된 시각화 정보를 보여줄 수 있다. 하지만 BI 툴의 경우, 비즈니스 용도로 월별, 연도별로 데이터를 분류하여 시각화를 해주기 때문에 이러한 데이터 새로고침이 ES에 비해 좀 떨어지는 편이다. QuickSight에서 업데이트된 데이터 세트를 refresh해주기 위해서 데이터 세트 메뉴에서 일정 생성을 통해서 scheduling하는 기능을 추가해줘야 한다. (일정 기간 단위 실행) 이러한 QuickSight의 dashboard를 활용하여 CS(Customer Satisfaction)부서 내에 근무하는 직원이 현재 고객응대 현황을 그래프로 확인을 할 수도 있고, 개발자가 현재 고객들이 어떠한 문제로 연락을 하고 있는지 현황을 파악하는 용도로도 활용될 수 있다. 그리고 QuickSight는 외부 서비스로써, 따로 조회만 가능한 계정을 발급해 줄 수도 있다. 실제 현업에서는 ES가 많이 사용되지만, QuickSight도 ES를 대체해서 사용할 수 있다. 그리고 대용량 데이터의 경우에는 Lambda로 데이터 변환작업을 하기 어려운 경우도 있기 때문에 Lambda를 Athena를 호출하는 용도로만 해서 작업을 할 수도 있고, Redshift에 데이터를 담아서 쿼리를 날리는 형태로 처리할 수도 있다. [추가 공부]추가적으로 단일 파이프라인이 아닌, job들이 복잡해지는 경우에는 데이터가 쌓인 후에 특정 파이프라인을 실행하고, 해당 파이프라인의 처리가 성공한 경우에는 A 파이프라인으로, 실패한 경우에는 B 파이프라인으로 처리할 수 있도록 dependencies를 처리해야되는 경우도 생긴다.이러한 복잡한 job에 대한 처리는 EventBridge로는 부족할 수 있기 때문에 Airflow나 아르고 플로우와 같은 스케쥴링 엔진을 적용해서 스케줄링 작업을 해줘야한다. (참고) SQS와 같은 큐 서비스를 사용해서 job을 분류해서 처리할 수도 있다. [실습] Amazon QuickSight우선 Amazon QuickSight에 sample csv 파일을 import하여 x-axis, y-axis의 기준 칼럼을 변경해가면서 데이터를 그래프로 시각화해보았다.Amazon QuickSight에서는 이상 감지(Anormaly Insight)를 지원하여, 그래프상에서 값이 갑자기 튀는 경우에 해당 부분에 대해 ML이 이를 분석하여 글로써 서술해준다. 그 외에 추가적으로 Add forecast를 사용하여 앞으로의 값 전망에 대해서 그래프상에서 확인할 수 있다. 마무리 통합 파이프라인 구축 이번 통합 파이프라인 구축 실습에서는 여지까지 실습했을때 활용되었던 Amazon CloudFormation, Kinesis data stream, Kinesis data firehose, S3, Lambda를 종합해서 실습하였다. Kinesis data stream 부분을 직접 Kafka를 구축하거나, AWS에서 제공해주는 MSK를 사용하면 데이터 스트림 부분을 대체해서 구성할 수 있다. Kinesis client library를 이용해서 변환하는 로직을 추가해서 처리하거나 plugin을 활용해서 데이터를 변환할때에는 logstash를 연결해서 구성을 할 수도 있다.그리고 Firehose를 거치지 않고, 바로 ES쪽으로 데이터를 바로 적재하는 것이 가능하다. 우선 전체적인 데이터 파이프라인의 flow는 우선 AWS의 CloudFormation을 통해 랜덤한 dummy data를 생성해주고, 생성된 데이터를 Kinesis data stream -&gt; Kinesis data firehose를 통해 S3 bucket에 데이터를 최종적으로 적재를 해준다. 적재된 데이터는 Athena를 통해서 쿼리를 사용해서 데이터를 필터하는 작업을 진행하도록 했다. (firehose에서 data transformation 옵션을 enable해서 생성해준 Lambda 함수를 넣어서 한 번 필터된 데이터를 S3에 저장할 수 있도록 구성하였다.(Lambda에서 제공해주는 blueprint template code활용 - 커스텀해서 작성해보기)) 참고 : Lambda 함수를 설정할때 timeout에러가 발생하는 이유는 Firehose와 Lambda 함수의 time interval sync가 맞지 않아서이다. Lambda function에서 time setup에 대한 configuration을 수정해줘야한다. S3에 적재된 데이터를 Athena를 통해서 테이블을 생성해서 쿼리로 분석을 하고, 분석 결과 데이터를 QuickSight를 통해서 시각화 할 수 있다. S3에 적재된 데이터를 direct로 QuickSight와 연동하는 작업도 해보기. Q&amp;A 관련 내용 정리 Q1. 데이터 엔지니어의 업무와 협업 방식 회사 규모가 커질 수록 데이터 엔지니어나 데이터 플랫폼 엔지니어로 직무를 구분하기도 한다.데이터 플랫폼 엔지니어의 경우에는 Presto나 AWS 사용하는 것을 좀 더 손쉽게 인프라를 구축해주는 작업을 하기도 하고, Presto나 Hive와 같은 인프라들을 관리해주는 플랫폼 개발자/엔지니어로 근무를 하기도 한다.그리고 데이터 엔지니어의 경우에는 SQL을 Presto나 Hive에서 돌려서 데이터를 뽑아내는 작업만을 하는 경우도 있다. Spark job을 말아서 Airflow를 통해 스케줄링 작업을 해주고, CI/CD 배포하듯이 구축을 해주는 업무도 있다. 협업의 경우, 데이터 플랫폼 개발자의 경우에는 데이터 엔지니어들이 쓸 수 있는 플랫폼/툴을 개발하는 업무를 주로 하기 때문에 SW engineer와 DE가 서로 협업을 하는 경우도 있다. 그리고 다른 경우에는 DE가 DA나 DS와 같이 협업을 하는 경우도 있는데, DA/DS 분들이 특정 데이터들이 필요하다고 했을때, 분산되어있는 요구된 여러 데이터들을 spark를 사용해서 join 작업을 해주기도 한다. 사이드 프로젝트 활용할 수 있는 데이터 : 공공 데이터, Uber 데이터 활용하기(미국 정부에서 관리해주는 사이트 참고) -&gt; 배웠던 것 복습한다는 의미로 여지까지 공부했던 파이프라인 구성요소들을 활용해서 직접 나만의 파이프라인을 만들어 보고, 구축한 후에는 데이터를 분석해보는 사이드 프로젝트 진행해보면 좋다.파이프라인을 구축했을때 왜 A라는 기술을 사용했는지와 왜 이런식으로 구성했는지에 대한 이유를 설명해주는 것도 좋다. 분석 단계의 전처리와 데이터 엔지니어링 단계에서의 전처리의 차이 분석 단계의 전처리에서는 데이터가 잘못 들어가 있는 경우나, 널 값이 들어가 있는 경우에 대한 데이터 전처리 작업을 한다. 데이터 엔지니어링 단계의 전처리의 경우에도 분석가가 특정 데이터 전처리 요구를 하는 경우에는 파이프라인 상에 로직을 심어서 처리를 추가해줄 수도 있다. (DA와의 협업)일반적으로 데이터 엔지니어링 단계에서의 전처리는 format이나 schema와 같은 변환작업을 주로 해주고, JSON, TEXT 데이터를 Parquet, ORC 포멧으로 변경하여 데이터 엔지니어링 관점에서 효율(속도/성능)을 위한 작업을 많이 해주게 된다. DM는 언제 구축해야되는지와 DW는 어느정도 규모일때 구축을 하는지 만약에 인사팀 데이터들은 타 부서 사람들이 조회하면 안되기 때문에 이러한 경우에 별도의 DM을 구축해서 관리를 하기도 한다.보통은 효율성을 위해서 빠르게 데이터를 조회하기 위해서 DW에 쌓인 데이터를 각 각의 개별 SCHEMA를 가진 DM으로 쌓아서 관리를 한다. (조직의 정책에 따라) 많은 경우에는 DW만 구축을 해주고, 새롭게 테이블만 구축해서 접근 권한만 다르게 부여해서 활용되는 경우도 많다. VPC 활용 및 나누는 방법 VPC는 네트워크나 보안 정책에 따라 구분된다. 이 부분은 데이터 엔지니어의 영역이 아닌, Cloud Architecture나 네트워크 엔지니어의 영역이다.글로벌 기업의 경우에는 해외 각국에 지사가 나뉘어져 있기 때문에 국가별로 VPC로 나눠서 각 VPC를 peering해주기도 한다. (회사 정책별로 상이) 기업사례 AWS를 활용하여 Lambda Architecture 구축 이 Lambda Architecture는 오프라인 수업 초반에 배웠듯이 Batch 데이터 처리와 Streaming 데이터 처리의 장/단점을 서로 보완해주기 위해 등장한 모델이다.위의 그림에서 보면, Batch 처리에서 S3에 데이터를 적재해주고, S3에 적재된 데이터를 metadata를 저장하고 있는 AWS Glue를 통해 Athena에서 데이터를 분석하고 있다. 또한 처음에 S3 bucket에 데이터가 적재되었을때, ETL 작업을 통해 또 다른 Batch View S3 Bucket에 데이터를 다시 저장하고 있다. Streaming data의 경우에는 Kinesis Stream을 통해서 데이터를 넘겨주고 있고, 넘겨진 데이터를 AWS Lambda를 통해 데이터 변환을 한 다음에 Kinesis Firehose를 통해 Kinesis Analytics로 이동해서 분석 환경을 구축하기도 하고 S3에 데이터를 적재해주기도 한다. 중단부를 보면, Batch 처리를 통해 적재된 Batch View S3 bucket과 Kinesis stream을 통해 바로 넘겨진 데이터를 Amazon EMR을 통해 데이터를 통합해서 통합된 데이터를 Merged View S3 bucket에 저장을 하고 있다. AWS GlueAthena를 사용해서 테이블을 만들게 되면, 필드를 일일이 지정을 해줘야 했지만, Glue Crawler를 이용하면 데이터를 자동으로 parsing해서 테이블로 만들어준다. 그리고 AWS Glue에는 AWS Glue Catalog 기능이라고 해서 내가 가지고 있는 데이터가 어떤 field나 schema를 가지고 있는지, 전체 메타 정보를 가지고 있다.AWS Glue Catalog에서 직접적으로 pySpark를 통해서 분석을 할 수도 있다. HDFS vs S3 사용S3는 모든 AWS의 각종 서비스들을 연결시켜주는 anchor point의 역할을 해주기도 한다. 그리고 데이터 governance를 S3에 걸 수 있기 때문에 데이터의 저장용도로 S3를 주로 활용한다. (S3는 AWS의 Killer App이다)REDSHIFT에 데이터를 쌓아도 되는데, S3에 쌓아놓고 REDSHIFT SPECTRUM에 저장을 할 수 있기 때문에 이러한 S3의 확장 가능성때문에 주로 사용이 된다. Uber data architecture[참고] : https://eng.uber.com/uber-big-data-platform/ 우버에서 데이터 아키텍처 구조의 변천사에 대해서 살펴볼 수 있다. 각 아키텍처의 구조를 변경했을때 한계점에 대해서도 위의 페이지에서 명기를하고 있으며, 2015-2016년에 들어서 하둡 에코 시스템을 도입한 내용도 유익하기 때문에 한 번 읽어보기를 권장한다. 2017-현재까지 Kafka를 사용해서 데이터를 처리함으로써 이전 하둡 시스템을 사용했을때와 비교했을때 데이터 처리의 민첩성을 향상시켰다.(이전 24h(end to end)에서 30min 미만 raw data, 1시간 미만으로 modeling되도록 개선)주로 Batch 처리를 하지 않고 Streaming 처리를 하는 이유는 latency를 개선하기 위해서이다. CDC (Change Data Capture)CDC를 활용해서 DB(RDBMS, Key-Val DB)상에서 변화가 감지되었을때 Kafka에 데이터를 먹일 수도 있다. [참고] https://www.youtube.com/watch?v=T6PAcWtoHTo Twitter[참고] : https://blog.twitter.com/engineering/en_us/topics/infrastructure/2021/processing-billions-of-events-in-real-time-at-twitter- 초기 old lambda architecture 도입부터 Twitter는 GCP를 많이 사용하기 때문에 GCP의 kinesis와 같은 서비스를 (Pub/Sub 구조) 활용해서 변경까지의 내용을 담고 있다.아키텍처 구성의 변화를 통해 Latency가 많이 개선되었음을 확인할 수 있다.(Kinesis로 데이터를 받아서 S3에 저장한 다음에 Athena로 데이터를 조회하는 구조가 Twitter에서 GCP를 사용해서 새롭게 도입한 데이터 아키텍처 구성으로 볼 수 있다.(유사)) 쏘카 국내에서 데이터 엔지니어링으로 유명한 기업 차량용 단말을 위한 IoT 파이프라인 구축 AWS IoT Core, Amazon MSK, Amazon S3 AWS IoT Core로의 전환쏘카에서는 기존에 Telemetrics Server가 차량 단말기로 HTTPS 호출을 통해 필요한 데이터들을 수집하는 구조로 되어있었지만, MQTT 프로토콜을 지원하는 브로커와 AWS IoT Core의 사용으로 대체를 하면서 좀 더 효율적인 데이터 파이프라인을 구성할 수 있었다. Kinesis 사용(아래 문제상황과 시도 그리고 해결과 관련된 내용은 쏘카 기술블로그에서 참고한 내용입니다)Kinesis는 좋고 편리한 서비스이지만 consumer가 많아질수로 기하급수적으로 서비스 사용하는 비용이 올라간다.또한 Kinesis stream을 사용하는 프로젝트가 늘어날수록(Kinesis stream에서 받아서 처리하는 consumer가 늘어날수록) 파이프라인이 복잡해지고,파이프라인을 관리하는 주체가 없는 상태에서 불 필요하게 많은 Consumer가 연결이 되면서 Kinesis stream에 많은 수의 Lambda 함수, 많은 Process들이 붙게 되고, 이로인해 결과적으로 Kinesis stream에 병목이 생기는 경우가 생겼다.(문제상황인지)(시도) 더 많은 처리량을 위해 샤드(샤드 당 1초에 최대 2MB의 데이터 처리)를 늘리기도 하고, 향상된 팬 아웃 기능을 사용하여 상황 극복이 가능하다. 하지만 이는 서비스 사용 비용 증가와 직결되어 근본적인 문제 해결을 위한 해결책이 되지 못했다.이러한 문제상황과 시도를 통해 MSK로 변경을 하여 좀 더 안정적인 새로운 파이프라인을 구성하였다.(해결)실제로 EC2에 Kafka를 구축하여 운영할 수도 있지만 운영 비용을 줄이면서 Kafka를 사용하고자 MSK를 사용한다. [다른 관점에서의 문제해결 - Q&amp;A]Topic을 분리해주는 것도 하나의 해결책이 될 수 있다. 예를들어 하나의 Topic으로 들어온 데이터 스트림에 3개의 consumer가 붙어있다면, 3개의 consumer가 각 각 나눠서 데이터를 가져가게 되는데, 들어가는 데이터가 배로 늘어나는 경우, consumer의 부담도 배로 늘어난다. (1초에 1개의 데이터 -&gt; 1초에 10개의 데이터 전송)이러한 경우에는 토픽을 분리해서 트래픽을 복사하거나 하는 로직들을 고려해봐야 한다. Kinesis Stream 병목 [참고] https://tech.socarcorp.kr/mobility/2022/01/06/socar-iot-pipeline-1.html 쏘카에서는 AWS IoT Core 서비스를 활용해서 데이터 아키텍처를 개선하였다. 쏘카 서비스 구조상 차량에서 서버쪽으로 계속 차량에서 수집한 정보를 넘겨주고, 서버쪽에서는 차량을 제어하기 위한 명령을 내려주는 구조로 되어있다. (단말과 서비스)MQTT 프로토콜에서 지원하는 브로커를 사용해서 단말과 서버와의 통신을 하도록 구성을 하였다. [배운점]쏘카의 Kinesis stream 병목에 대한 글을 통해 많은 것을 배울 수 있었다. 나는 AWS에서 제공해주는 서비스에는 streaming 데이터 처리에 있어 별다른 문제가 없을 것이라고 생각을 했는데, 실제 shard 수와 각 shard마다 감당할 수 있는 데이터의 양이 한정되어 있기 때문에 설정해준 shard 수와 실시간으로 전송되는 데이터의 수에 따라 병목현상이 일어날 수도 있다는 것을 배웠다.이러한 병목현상을 해결하기 위해 기존 Kinesis를 KMS(Kafka의 완전 관리형 서비스)로 전환을 하여 별도의 모니터링을 통한 shard 수 변경없이, Kafka의 완전 관리형 서비스인 Amazon MSK를 사용해서 문제 상황을 해결할 수 있다는 점도 배울 수 있었다.그리고 RaspberryPi를 활용해서 실제 센서의 데이터를 MQTT라는 메시지 프로토콜에서 지원하는 브로커(mosquitto)를 사용해서 AWS IoT Core 서비스(관리형 메시지 브로커 서비스)로 보내도록 구성을 해봤었는데, MQTT 프로토콜에서 지원해주는 브로커를 거치지 않고도 AWS IoT Core를 통해 직통으로 IoT 단말기로부터 데이터를 받아서 처리할 수 있다니, 한 번 방법을 찾아보고 변경된 구조로 데이터 파이프라인 구조를 재구성해봐야겠다.그리고 나는 IoT 기기로부터 센서 데이터를 받아서 처리하는 실습을 개인적으로 했기 때문에 안정성까지 고려하여 클러스터링을 지원하는 브로커인지 고려하지 않았지만, 찾아보니 내가 사용한 mosquitto 브로커의 경우에는 클러스터링을 지원하지 않고, HiveMQ와 같은 브로커만 클러스터링을 지원한다고 한다.또 HiveMQ의 경우에는 AWS나 Azure와 같은 Cloud provider에서 동작을 잘 하고, auto-discovery, distributed masterless architecture를 지원하기 때문에 다음에 좀 더 안정성을 요구하는 상황에서 데이터 파이프라인을 구축할때에는 HiveMQ를 활용해봐야겠다.(좀 더 찾아보기 - mosquitto가 clustering을 지원하는지에 대해 좀 더 찾아보자) 넷마블아키텍처보다 공부할만한 내용이 많기 때문에 읽어보면 도움이 된다. (데이터 파이프라인의 기본 원리와 원칙 및 분산환경에 대한 내용) 데이터 파이프라인 기본 원리와 원칙은 시간이 지나도 유효해야 한다. https://netmarble.engineering/data-pipeline-design-principles-a/?fbclid=IwAR3NakVT3JM5eEDKnaT6zCoFVN5fq9XwbG8XvzQqU8p9WoW7p460TSzec2Uhttps://netmarble.engineering/data-pipeline-design-principles-b/ 카카오 데이터 엔지니어링 ?https://tech.kakao.com/2020/11/30/kakao-data-engineering/ 대량의 스트림 데이터를 실시간으로 분류하기 (Elasticsearch percolator)https://tv.kakao.com/channel/3693125/cliplink/423590245 (자료)https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636524938152 분류할 데이터나 필터가 많은데 빠르게 분류하고 싶은 경우 데이터 기반 실시간 알람(문자열 매칭) 특정 조건에 만족하는 중고 제품이 올라오면 알람 Druid@Kakaohttps://tv.kakao.com/channel/3693125/cliplink/423590646 (자료)https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636526360840 Druid 도입 사례 및 Multi-Tenant 클러스터 소개 데이터 실시간 처리 관련 문제를 해결하고 싶은 경우 카카오 공용 하둡 운영 사례https://tv.kakao.com/channel/3693125/cliplink/423590637 (자료)https://t1.kakaocdn.net/service_if_kakao_prod/file/file-1636526054873 카카오의 전사 리소스 모니터링 시스템 카카오의 전사 리소스 모니터링 시스템은 크게 숫자 데이터를 모니터링하는 STAT(KEMI-STATS), 로그 문자 데이터를 모니터링하는 LOG(KEMI-LOG)로 분류하여 구성이 되어있으며, STAT METRIC 데이터의 경우에는 여러 데이터 리소스들을 카프카를 통해서 받아서 SAMZA(Metric Calculator)와 같은 실시간 처리 엔진을 붙여서 전처리를 해주고, OPEN TSDB라는 곳에 적재를 한 다음에 분석을 하고 있다.LOG 데이터의 경우에는 fluentd로 수집을 하고, fluentd로 다시 모아서 Lambda Architecture로 구성을 한 파이프라인에 로그 데이터를 흘려서 보내주고 있다. 실시간으로 카프카로 넘겨지는 데이터를 알람과 전처리를 위해서 스톰을 배치하고 있고, 최근에는 Flink로 대체해서 구성을 하고 있다고 한다. https://tech.kakao.com/2016/08/25/kemi/ 네이버네이버 플레이스(장소 검색 ex.맛집)에서는 플레이스 데이터 플랫폼을 구축하고 있다.(어뷰즈 검출 사례) 데이터 명세에 대한 내용 포함(format, schema) protobuf는 GRPC 바이트 인코딩해서 속도가 매우 빠르고, GRPC가 사용하고 있는 직렬화 방식을 사용하고 있기 때문에 참고해보기https://medium.com/naver-place-dev/%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-%EC%96%B4%EB%B7%B0%EC%A6%88-%EA%B2%80%EC%B6%9C-%EC%82%AC%EB%A1%80-e9caa31511dc CDC(Change Data Capture)툴이 DBMS에서 적재된 데이터에서 변경된 데이터만 캡처해서 Kafka와 같은 실시간 데이터 스트림 처리하는 곳으로 넘겨서 데이터를 추출해서 처리해주는 역할을 해준다. 토스 토스 데이터의 흐름과 활용https://toss.im/slash-21/sessions/2-1 (자료)https://static.toss.im/slash21/pdf/%5B%ED%86%A0%EC%8A%A4_SLASH%2021%5D%20%ED%86%A0%EC%8A%A4%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98%20%ED%9D%90%EB%A6%84%EA%B3%BC%20%ED%99%9C%EC%9A%A9_%EC%9C%A0%EA%B2%B0.pdf Sqoop(SQL to Hadoop): Hadoop과 RDB간 데이터를 전송할 수 있는 오픈소스로, RDB의 특정 테이블 또는 쿼리 결과를 HDFS로 쉽게 옮길 수 있다. 우아한 형제들 데이터 분석 플랫폼, AWS 이관기https://www.youtube.com/watch?v=rH_xJBoRuZE AWS EMR에서 온프레미스로 전환 AWS Athena, Trino 비교 및 Trino 사용기 Graviton2 (ARM 기반의 EC2 인스턴스) 사용기 Zeppelin 사용기 (대화형 분석/시각화 도구) 로그 데이터로 유저 이해하기https://techblog.woowahan.com/2536/ 실시간 인덱싱을 위한 Elasticsearch 구조를 찾아서https://techblog.woowahan.com/7425/ 쿠팡Big Data Platform: Evolving from start-up to big tech companyhttps://medium.com/coupang-engineering/evolving-the-coupang-data-platform-308e305a9c45 왓차단일 클라우드 플랫폼이 아닌 다양한 클라우드 플랫폼을 같이 사용해서 데이터 파이프라인을 구성https://medium.com/watcha/%EB%A9%80%ED%8B%B0%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A1%9C%EA%B7%B8-%EB%B6%84%EC%84%9D-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0-8c5f671df559 줌 인터넷검색 데이터 서빙 플랫폼 구축file:///Users/hyungilee/Desktop/Data%20engineering/Pipeline/Learning%20spoons/220703_Day6.pdf AWS IoT Core 관리형 서비스 MQTT, HTTPS, LoRaWAN등의 프로토콜을 지원 기기간 메시지를 AWS 서비스에 라우팅 IoT 전용 데이터 스트림 역할을 해준다. Q&amp;A Section Queue 서비스(MQ/SQS)와 스트리밍 서비스(Kinesis/Kafka)의 선택시 주요 포인트 -&gt; SQS를 사용하는 경우는 확장성을 고려해야한다. SQS는 단순 큐로써, 스케줄링을 하거나 작업을 순차적으로 처리해야되는 경우와 같이 간단한 경우에는 SQS를 사용해도 괜찮다.(초당 3천개 정도는 커버 가능) 하지만 빅데이터를 처리해줘야되는 경우에는 Kinesis를 고려해야하고, 더 확장성 있는 구조를 고려해야된다면, Kafka를 고려해줘야 될 수도 있다. Kafka + lambda 구조로 사용하는 경우는 괜찮지만, 더 많은 consumer가 붙어서 처리되는 경우에는 요금부하가 생기기 때문에 이 경우에는 lambda 대신에 Kafka connector, Kafka streams를 사용해서 DocumentDB로 넣어주는 것이 권장된다고 한다. 스트리밍 데이터를 여러 컨슈머 그룹으로 묶여서 처리를 해야되는 경우와 데이터가 일정기간동안 저장 가능해서 replayable하다는 장점을 가져와야 되는 경우에는 Queue 서비스 보다는 스트리밍 서비스를 선택해서 사용하는 것이 권장된다. 그리고 들어온 데이터가 순서를 보장해야되는 경우에는 Queue 서비스인 SQS를 사용해서 처리해주는 것이 좋다. (Kafka를 사용해서 단일 브로커로 구성해서 데이터 순서를 보장해서 처리를 해줄 수도 있다) 그리고 로그 데이터는 SQS를 사용할 필요는 없다고 한다. 초당 3000개 이상의 사용자의 API 호출 로그를 Kinesis로 보내고, Lambda를 이용해서 DocumentDB(NoSQL)로 넣는 구조를 하고 있는데, Kinesis의 write 성능 제한으로 peak hour에 reject 되는 경우가 있어서 On-failure destination SQS(실패한 경우에 SQS로 넣어주는 것을 고려)를 지정해서 사용하는 것을 고려중.","link":"/2022/07/03/202207/220703_datapipeline_study/"},{"title":"220721 SQL JOINS","text":"이번 포스팅에서는 SQL의 JOIN에 대해서 종합적으로 정리를 해보려고 한다. INNER JOINdepartment에 속하는 employee name을 출력 12345-- INNER JOIN에서 INNER 생략 가능 -- JOIN을 할때에는 ON 절에 작성하는 JOIN 조건의 column이름은 달라도 관계없다. Column의 값이 중요하다. SELECT e.emp_name, d.dept_name FROM employee e JOIN department d ON e.dept_id = d.dept_id OUTER JOINLEFT JOIN모든 employee 이름과 department 이름 출력 1234-- LEFT JOIN = INNER JOIN + ANY additional records from the LEFT TABLE.SELECT e.emp_name, d.dept_nameFROM employee eLEFT JOIN department d ON e.dept_id = d.dept_id; RIGHT JOIN1234-- RIGHT JOIN = INNER JOIN + ANY additional records from the RIGHT TABLE.SELECT e.emp_name, d.dept_nameFROM employee eRIGHT JOIN department d ON e.dept_id = d.dept_id; 모든 employee의 manager, department, project 정보 출력 123456-- PROJECT에 두 개 이상 참여하고 있는 employee의 경우에는 복수 emp_name으로 출력SELECT e.emp_name, d.dept_name, m.manager_name, p.project_nameFROM employee eLEFT JOIN department d ON e.dept_id = d.dept_idINNER JOIN manager m ON m.manager_id = e.manager_id LEFT JOIN project p ON p.team_member_id = e.emp_id; FULL (OUTER) JOIN12345678-- FULL JOIN = INNER JOIN + All remaining records from LEFT TABLE (returns null value for any columns fetch)-- + ALL remaining records from RIGHT TABLE (returns null value for any columns fetch)SELECT e.emp_name, d.dept_nameFROM employee eFULL OUTER JOIN department d ON d.dept_id = e.dept_id;-- INNER JOIN 결과가 출력되고 뒤이어 (department table을 기준으로 RIGHT OUTER JOIN한 결과 - INNER JOIN 결과) + (employee table을 기준으로 LEFT OUTER JOIN한 결과 - INNER JOIN 결과)가 붙어서 출력이 된다. CROSS JOIN특정 기업 정보(기업명, 위치 등)만 가지고 있는 테이블이 있다고 가정하고, 해당 정보를 모든 employee에 출력해야한다면 어떻게 해야될까?바로 CROSS JOIN을 사용하면 모든 employee 정보에 기업 정보들을 붙여서 출력할 수 있다. 12345--CROSS JOIN은 cartesian product를 반환한다.SELECT e.emp_name, d.dept_nameFROM employee e -- 6 recordsCROSS JOIN department d; -- 4 records-- 6 records * 4records = 24 records 1234SELECT e.emp_name, d.dept_name, c.company_name, c.locationFROM employee eINNER JOIN department d ON e.dept_id = d.dept_idCROSS JOIN company c; NATURAL JOINSELF JOIN은 INNER JOIN과 같다고 착각할 수 있지만, 그렇지 않다.NATURAL JOIN은 INNER JOIN과 같이 특정 조인 조건을 주지 않아도 같은 COLUMN명을 가진 COLUMN을 서로 JOIN함으로써 INNER JOIN과 같은 결과를 낼 수 있다. 12345678SELECT e.emp_name, d.dept_nameFROM employee eNATURAL JOIN department d;ALTER TABLE department RENAME COLUMN dept_id TO id;-- COLUMN이 수정하면, 서로 일치하는 COLUMN이 없기 때문에 NATURAL JOIN이 아닌 CROSS JOIN과 같은 효과를 낸다. -- NATURAL JOIN은 column name만을 비교해서 JOIN을 해주기 때문에 COLUMN명만 같고 값이 다른 경우에는 문제가 될 수 있기 때문에 권장되지 않는다. SELF JOIN부모 이름과 나이 정보를 그 자식의 이름과 나이 정보와 매칭해서 출력되도록 sql 쿼리를 작성하시오. 1234567891011SELECT * FROM family;SELECT child.name AS child_name, child.age AS child_age, parent.name AS parent_name, parent.age AS parent_ageFROM family AS childJOIN family AS parent ON child.parent_id = parent.member_id-- LEFT JOIN으로 수정하면, 부모 정보가 없는 child의 정보도 포함된 테이블 정보가 출력된다.","link":"/2022/07/21/202207/220721-sql-joins-study/"},{"title":"220728 Burrow - Kafka consumer Lag 모니터링","text":"이번 포스팅에서는 Kafka의 Consumer Lag를 모니터링할 때 필수적으로 사용되는 Burrow에 대해서 정리해보려고 한다. 이번 포트폴리오에 추가 할 사이드 프로젝트로 Kafka를 활용해서 Kafka cluster를 구성하고, Kafka-client 라이브러리를 활용하여 Python으로 Producer 및 Consumer를 구성하였다. Consumer에서는 ELK 스택의 docker 컨테이너를 연결하여, Producer로부터 유입된 로그 데이터를 Logstash를 통해 Elasticsearch에 저자을 하고, 저장된 로그 데이터를 Kibana를 통해서 시각화하였다. Kafka consumer는 kafka-client 라이브러리를 활용해서 Java, Python등의 언어로 개발을 할 수 있는데, 생성한 KafkaConsumer 객체를 통해 현재 lag 정보를 가져올 수 있다. 만약 lag을 실시간으로 모니터링하고 싶은 경우에는 Elasticsearch나 influxdb와 같은 곳으로 Consumer lag metric 정보를 보낸 뒤에 Grafana 대시모드를 통해서 실시간으로 시각화하여 확인 할 수 있다. 하지만 Consumer 단위에서 lag을 모니터링하는 것은 아주 위험하고, 운영요소가 많이 들어간다는 문제가 있다. 그 이유는 Consumer logic 단에서 lag을 수집하는 것은 Consumer 상태에 dependency가 걸리기 때문이다.만약에 Consumer가 비정상적으로 종료되게 된다면, 더 이상 Consumer는 lag 정보를 보낼 수 없기 때문에 더 이상 lag을 측정할 수 없는 문제가 발생한다. 그리고 차후에 추가적으로 consumer가 개발될 때 마다 해당 consumer에 lag 정보를 특정 저장소에 저장할 수 있도록 로직을 개발해야되기 때문에 공수가 많이 든다. 만약에 consumer lag을 수집할 수 없는 consumer라면, lag을 모니터링 할 수 없기 때문에 까다로워진다. 이러한 이유로 인해 linkedIn에서는 Apache kafka와 함께 Kafka consumer lag을 효과적으로 모니터링 할 수 있도록 Burrow를 개발하였다. Burrow는 오픈 소스 프로젝트로, Go 언어로 개발이 되었으며, 현재 깃허브에 올라가 있다. 이 Burrow는 Kafka와는 독립적인 consumer의 lag을 모니터링하기 위한 애플리케이션이다. Burrow의 특징Multi-Kafka cluster를 지원한다. Kafka를 사용하는 기업에서는 보통 2개 이상의 Kafka cluster를 구성해서 사용하는데, 여러 개의 Kafka 클러스터를 구성하더라도 한 개의 Burrow만으로 모든 카프카 클러스터에 붙은 consumer의 lag을 모두 모니터링 할 수 있다. Sliding window를 통한 Consumer의 상태 확인 Burrow에서는 sliding window를 통해서 consumer의 상태 정보를 ERROR, WARNING, OK로 나눠서 확인을 한다. 만약 일시적으로 데이터의 양이 많아져서 consumer offset이 증가하면 WARNING으로 표기하며, 데이터의 양이 많아졌는데, consumer가 데이터를 가져가지 않으면 ERROR로 정의한다.(consumer의 문제 확인) HTTP API 제공 위에서 정의한 정보들을 HTTP API를 통해서 확인할 수 있다. HTTP API를 통해 받은 response 데이터를 시계열 DB와 같은 곳에 저장하는 애플리케이션을 만들어서 활용할 수도 있다. 이러한 Burrow의 탄생 배경을 위해 LinkedIn Engineering블로그를 확인하도록 하자. Burrow 설치공식 문서 : https://github.com/linkedin/Burrow go 설치하기burrow는 go 언어로 개발된 애플리케이션이기 때문에 go를 우선적으로 설치해야 한다. 1$brew install go Burrow.exe 파일 생성하기go 언어를 설치했다면, burrow GitHub repository를 clone해야한다. 1$git clone https://github.com/linkedin/Burrow.git 이후에 clone 받은 directory로 이동해서 $go mod tidy 및 $go install 명령을 통해 /User/[사용자명]/go/bin 하위에 Burrow.exe 파일이 생성이 된다. 설정 파일 수정하기Borrow.exe 파일 실행 전에 우선 설정 파일을 수정해야 한다. (burrow.toml- clone한 repo의 config 폴더 확인) 123456789101112131415161718192021222324252627282930313233343536[general][logging]level=&quot;info&quot;[zookeeper]servers=[ &quot;localhost:2181&quot;][client-profile.local]client-id=&quot;burrow-local&quot;kafka-version=&quot;2.0.0&quot;[cluster.local]class-name=&quot;kafka&quot;servers=[ &quot;localhost:9092&quot; ]client-profile=&quot;local&quot;topic-refresh=120offset-refresh=30[consumer.local]class-name=&quot;kafka&quot;cluster=&quot;local&quot;servers=[ &quot;localhost:9092&quot; ]client-profile=&quot;local&quot;group-denylist=&quot;^(console-consumer-|python-kafka-consumer-|quick-).*$&quot;group-allowlist=&quot;&quot;[httpserver.default]address=&quot;:8000&quot;[storage.default]class-name=&quot;inmemory&quot;workers=20intervals=15expire-group=604800min-distance=1 Burrow.exe 파일 실행하기12# Burrow.exe 파일의 위치 경로를 지정한 후 수정한 burrow.toml 설정 파일을 실행하기 위해 아래의 명령을 실행한다.$/Users/[사용자명]/go/bin/Burrow --config-dir=/config Burrow 서비스 확인은 localhost:8000/burrow/admin URL을 통해서 확인하도록 한다. Burrow EndPoint 정리 문서는 아래의 링크를 확인하도록 하자. https://github.com/linkedin/Burrow/wiki/HTTP-Endpoint [추가] Docker 컨테이너에 build된 Kafka cluster 모니터링만약 Kafka cluster를 docker-compose.yml 파일에서 정의해서 구축했다면, 같은 파일 내에서 burrow에 대한 설정을 할 수 있습니다. 또한 burrow.toml 파일에서 server에 대한 정의도 docker-compose.yml에서 정의한 서비스의 hostname을 기반으로 작성할 수 있습니다. Burrow Dashboard 설치docker에서 Burrow dashboard를 구성하기 위해 필요한 이미지를 다운받습니다. 1$sudo docker pull joway/burrow-dashboard 아래의 method를 통해서 dashboard를 실행합니다. 1$sudo docker run --network host -e BURROW_BACKEND=http://localhost:8000 -p 80:80 joway/burrow-dashboard:latest Burrow 대시보드 만들기Burrow endpoint url을 통해 health check나 offset의 변화에 대해서 모니터링을 할 수는 있지만, json 형태로 출력이 되기 때문에 시계열 형태의 그래프로 값의 변화 추이에 대해서 모니터링이 필요하다. 텔레그래프 -&gt; ES -&gt; Grafana 순으로 데이터를 전달하여 시각화하도록 구성한다. 따라서 텔레그래프와 ES, Grafana의 설치가 필요하다. 1234$brew install telegraf$cd /usr/local/Cellar/telegraf/1.23.3/bin$telegraf config &gt; telegraf.conf telegraf.conf 파일에서 localhost 8086 포트에 대한 urls 정의 부분과 database 정의 부분에 대해 주석을 해제시켜준다.아래의 설정은 telegraf에서 수집된 정보를 influxdb로 내보내기 위한 설정이다. 12345678910111213141516171819################################################################################ OUTPUT PLUGINS ################################################################################[[inputs.burrow]] servers = [&quot;http://localhost:8000&quot;] topics_exclude = [ &quot;__consumer_offsets&quot; ] groups_exclude = [&quot;console-*&quot;] &lt;!-- prefix에 대한 설정도 주석 제거 --&gt;[[outputs.elasticsearch]] urls = [ &quot;http://localhost:9200&quot; ] timeout = &quot;5s&quot; enable_sniffer = false health_check_interval = &quot;10s&quot; index_name = &quot;burrow-%Y.%m.%d&quot; manage_template = false &lt;!-- elasticsearch username/password 설정에 대한 부분도 주석 제거 --&gt; 주석을 제거할 때 반드시 항목 이름에 대한 부분도 같이 제거를 해줘야 한다. telegraf.conf 파일 수정이 되었다면, 이제 telegraf를 아래 명령으로 실행시켜주고, ES에서 누적된 burrow 데이터를 확인한다. 1$telegraf --config telegraf.conf grafana를 설치한다. 1$brew install grafana grafana 시작 12$brew tap homebrew/services$brew services start grafana 이제 localhost:3000을 통해 Grafana 서비스 페이지로 접속할 수 있다.(admin/admin)접속한 후에 새로운 데이터 소스로 elasticsearch를 추가해준다. (index name 및 기타 정보 입력)","link":"/2022/07/28/202207/220728_datapipeline_study/"},{"title":"220804 Apache Airflow","text":"이번 포스팅에서는 Apache Airflow의 기본 개념과 사용에 대해서 정리해보려고 한다.이전에 AWS의 EventBridge라는 서비스를 사용해서 셍성한 Lambda 함수를 일정 주기의 시간동안 정기적으로 실행되도록 스케줄링해서 실습한 적이 있는데, Task를 좀 더 복잡한 구조로 스케줄링하기 위해서는 Apache Airflow를 활용하는 것이 좀 더 효율적이라고 오프라인 수업에서 배워서, 한 번 Apache Airflow를 사용해서 데이터 파이프라인을 구축해보고자 학습을 하게 되었다. 그리고 지원하고자 하는 기업에서 Apache Airflow를 업무에서 도입을 하여 사용하고 있기 때문에 좀 더 잘 이해하고 사용해보려고 한다. Apache Airflow를 사용하는 이유데이터 파이프라인 구축에 있어, 데이터를 추출하는 Extract, 데이터를 적재하는 Load, 데이터를 변환하는 Transform 과정을 거친다. 데이터 추출은 API를 통해서 하기도 하며, Load는 Snowflake와 같은 data warehousing, data lake와 같은 Single platform을 제공하는 SaaS를 활용하기도 한다. 그리고 Transform은 Dbt와 같은 분석을 위한 데이터 웨어하우스에 적재된 데이터를 간단한 SELECT 문 작성을 통해 변환을 할 수도 있다. 만약 Extract, Load, Transform 단계에서 예기치 못한 에러가 발생한다면, 그리고 데이터 파이프라인이 한 개가 아닌 100개 이상이라면 어떨까? 관리하기가 많이 어려워진다. 이러한 이유로 인해 Airflow를 사용해서 종합적인 파이프라인의 관리가 필요하다. DAG(Directed Acyclic Graph)DAG는 방향성 비순환 그래프로, Airfow의 주요 컨셉이며, 복수 개의 Task를 모아서 어떻게 실행이 되어야 하는지에 대한 종속성과 관계에 따라 구조화 시키는 것을 말한다. OperatorOperator는 Task이며, 실행이 되면 Task instance가 생성이 되며, Operator의 예로는 Action operator, Transfer operator, Sensor operator가 있다. Action operator의 예로는 Python operator, Bash operator가 있는데, Python operator는 Python function을 실행시키며, Bash operator는 Bash command를 실행시키는 역할을 한다. 그리고 Transfer operator의 예로는 MySQL의 데이터를 RedShift로 이전하는 작업이 있으며, Sensor operator의 예로는 특정 이벤트가 발생하면 다음 Step으로 넘어가도록 하는 작업이 있다. Apache Airflow의 요소Apache Airflow에는 Webserver, Meta store, Scheduler, Executor, Queue, Worker가 있다. Executor는 직접적으로 Task를 실행하지 않으며, K8S 클러스터는 K8S Executor를 사용하고, Celery 클러스터는 Celery Executor를 사용한다. 여기서 Celery는 multiple machine에서의 multiple tasks를 실행하기 위한 Python 프레임워크이다. Queue는 주어진 Task를 보장된 순서로 실행시키기 위해 존재한다. Worker는 Task가 효과적으로 실행될 수 있도록 하는 역할을 하며, Worker가 없다면, sub processes 혹은 K8S를 사용하는 경우, Path가 주어진다. Apache Airflow의 ArchitectureApache Airflow의 Architecture로는 단일 노드로 구성된 One Node Architecture, 그리고 복수 개의 노드들로 구성된 Multi Node Architecture가 있다. 우선 첫 번째로 단일 노드 아키텍처를 살펴보면, 하나의 노드에 Web server와 Meta store, Scheduler, Executor, Queue가 존재한다. 여기서 Meta store는 Airflow에 존재하는 서로 다른 component 사이에서 데이터를 교환할 수 있도록 한다. 또 다른 섹션에서 전체적인 흐름을 정리하겠지만, Scheduler는 사용자가 작성한 Dag 파일을 정기적으로 정해진 시간 간격으로 파싱해서 새로운 Dag Run Object를 Meta store에 생성되고, Task의 수에 따라 Task Instance가 이어서 생성이 된다. 그리고 Dag Run Object와 Task Instance는 상태 정보와 함께 관리된다. 앞서 언급한 객체와 인스턴스의 상태 정보들이 Meta store에 업데이트가 되면서 Apach Airflow의 UI가 업데이트되고, 이로써 사용자가 UI를 통해 Task의 상태 정보를 Tracking할 수 있게 되는 것이다. 두 번째로 multi-node Architecture를 살펴보면, 위에서 살펴보았던 단일 노드 아키텍처와 달리 Meta store와 Queue를 별도의 노드로 분리하고, Queue에 쌓인 Task를 실행할 Worker node들이 복수 개 존재하고 있음을 알 수 있다. 이러한 복수 개의 노드로 구성하는 이유는 고가용성(HA)을 위함인데, Single point failure 이슈를 사전에 예방하고자 구성한다. 만약에 Load balancer를 사용해서 요청을 분산해서 처리하고자 한다면, 최소 두 개의 Web Server와 Scheduler가 필요하다. Apache Airflow의 전체 흐름Apache Airflow의 Task와 데이터 파이프라인을 debug하기 위해서는 Task가 어떻게 실행되는지에 대한 전반적인 이해가 필요하다. 우선 단일 노드의 관점에서 살펴보면, 아래 그림과 같이 Web server, Meta store, Scheduler, Executor, Folder Dags가 존재한다. (1) 개발자가 dag.py파일을 작성해서 Folder Dags에 추가한다. (2) Scheduler는 정해진 시간동안 정기적으로 새로운 Dags 객체 생성을 위해 Folder Dags를 파싱한다. (3) Dag가 Trigger될 준비가 되었다면, Scheduler는 Dag Run Object를 Meta store에 생성한다. (4) Task의 갯수에 따라 Task Instance가 상태 정보와 함께 생성이 된다. (5) Task가 Trigger될 준비가 되었다면, Task Instance Object를 Executor로 보내게 되고, Task Instance의 상태는 queued상태로 변경된다. (6) Executor가 실행을 위해 Task instance를 Sub process나 Worker에 Task를 위치시키면, (7) Task Instance의 상태가 queued에서 running으로 변경된다. (8) Task가 완료되면, executor가 Task Instance Object의 상태를 databaser에 업데이트한다. (9) Scheduler가 Dag Run Object에 작업이 완료되었는지 확인을 하고, 작업이 완료되었다면, 상태가 success로 업데이트된다. (10) Web server는 Meta store를 참고해서 Apache Airflow의 UI를 업데이트하게 되고, Apache Airflow의 UI를 통해서 Task의 상태를 Tracking할 수 있게 된다. Apache Airflow 설치(꼭 알아야 할 중요한 내용) Apache(1) Airflow is an orchestrator, not a processing framework, process your gigabytes of data outside of Airflow (i.e. You have a Spark cluster, you use an operator to execute a Spark job, the data is processed in Spark). (2) A DAG is a data pipeline, an Operator is a task. (3) An Executor defines how your tasks are execute whereas a worker is a process executing your task (4) The scheduler schedules your tasks, the web server serves the UI, the database stores the metadata of Airflow. (5) airflow db init is the first command to execute to initialise Airflow (6) If a task fails, check the logs by clicking on the task from the UI and “Logs” (7) The Gantt view is super useful to sport bottlenecks and tasks are too long to execute Postgres Database와 연결하기Operator가 Postgres, MySQL, AWS, dbt 등의 외부 툴과 함께 상호작용을 해야 될 때에는 Apache Airflow의 [Admin]-[Connections]에서 새로운 Connection을 생성해줘야 한다. Task 생성하기(1) Import the operator(2) Define the task id Task ID는 같은 DAG 내에서 반드시 유일해야한다. 123456789101112131415161718192021from airflow.providers.postgres.operators.postgres import PostgresOperatorwith DAG('user_processing', start_date=datetime(2022, 1, 1), schedule_interval='@daily', catchup=False) as dag: # variable name과 task_id를 일치시켜놓는 것이 좋다. # task id는 같은 DAG 내에서 반드시 유일해야 한다. # database와 상호작용을 하기 위해서 데이터베이스와 연결을 해야하며, create_table = PostgresOperator( task_id='create_table', postgres_conn_id='postgres', sql=''' CREATE TABLE IF NOT EXISTS users ( firstname TEXT NOT NULL, lastname TEXT NOT NULL, country TEXT NOT NULL, username TEXT NOT NULL, password TEXT NOT NULL, email TEXT NOT NULL ) ''' ) DAG에 Task를 추가한 후 확인하기Airflow의 DAG에 Task를 추가한 후에는 반드시 확인해야 한다. 12345# Airflow의 scheduler Name 확인$docker-compose ps $docker exec -it materials_airflow-scheduler_1 /bin/bash# airflow task 확인 $airflow tasks test user_processing create_table 2022-08-06 Sensor operatorSensor는 Airflow의 Context로, 내가 원하는 무언가를 위해 기다려준다. 예를들어, 특정 위치에 파일이 생성되거나 SQL 테이블의 속성에 파일 정보가 기입되는 등의 작업들이 그 예시가 되며, Airflow에서는 수 많은 Sensor operator를 제공해준다. 기억해야 될 것은 두 가지 parameter, poke_interval(default: 60s)과 timeout(default: 7days)이 있다. poke_interval이 default 값으로 설정이 되어있으면, sensor는 다음 Task를 실행하기 전에 조건이 참이지 거짓인지 매 60초마다 확인을 한다. timeout은 무기한으로 poking하는 것을 허용하지 않기 위해 default 값으로 설정하는 경우, 최대 7일이라는 poking을 멈추고 종료하는 최대 시간을 갖습니다. (sensor operator는 FAILED나 SKIPPED로 표기됩니다.) 주어진 API가 유효한지에 대해 검사를 하기 위해 HttpSensor operator를 사용하며, 이외에 파일의 생성에 대해 감지하고 싶다면, FileSensor를, S3 bucket에 대한 감지가 필요하다면, S3KeySensor를 사용하면 된다. HookAirflow에서는 다양한 외부 툴과 손쉽게 상호작용 할 수 있도록 Hook이라는 개념이 존재한다. 예를들어 PostgresSQL 데이터베이스에 sql 쿼리를 처리할때, PostgresHook이라는 것이 PostgresOperator와 PostgresSQL 데이터베이스 사이에 존재하기 때문에 툴이나 서비스와 상호작용하기 위한 복잡한 부분이 추상화되어 개발자가 손쉽게 작업을 할 수 있도록 도와준다. PostgresHookPostgresSQL 데이터베이스의 테이블에 데이터를 저장하기 위해서는 PostgresHook이 필요하다. Dependency 추가각 각의 개별 Task를 생성한 후에는 개별 Task에 대해 파일의 마지막에 dependencies를 정의해서 연결해줘야 한다. 아래 코드를 정의한 다음에 Apache Airflow의 DAG의 Graph 메뉴를 보면, 이전에 개별로 표시된 Task들이 화살표로 엮여서 표시된 것을 확인할 수 있다. 1create_table &gt;&gt; is_api_available &gt;&gt; extract_user &gt;&gt; process_user &gt;&gt; store_user DAG 실행DAG 파일의 작성이 끝나면, DAG를 실행해서 모든 Task들이 정상적으로 실행이 되고, 완료되는지 확인하고, 아래 명령으로 Airflow worker container에 접속해서 테스트로 작성한 DAG에서 외부 API 호출을 통해 데이터를 긁어와서 CSV파일로 추출한 데이터 파일을 확인한다. 12$docker exec -it materials_airflow-worker_1 /bin/bash$ls /tmp/ 이제 PostgresSQL 데이터베이스에 csv 데이터가 잘 적재되었는지 아래의 명령을 통해 확인한다. 123$docker exec -it materials_postgres_1 /bin/bash$psql -Uairflowairflow=#SELECT * FROM users; DAG 스케줄링우리가 DAG 객체를 정의할때, start_date와 schedule_interval을 정의해주는데, 아래의 규칙으로 DAG가 실행을 한다. (end_date는 생략 가능) DAG는 start_date/last_run + the schedule_interval 이후에 트리거된다. 주의해야 될 것은 start_date에는 아무 것도 발생하지 않는다는 것이다. 예를들어, start_date가 10:00AM 이고, schedule interval이 매 10분이라고 정의가 되었다면, 최초 10:00AM에는 아무 것도 발생하지 않고, 최초 10분 이후인 10:10AM에 DAG가 실행되어, data_interval_end가 10:10AM이 된다. 이후에는 data_interval_start이 10:10AM이 되고, data_interval_end가 10:20AM이 된다.","link":"/2022/08/04/202208/220804_airflow_on_docker/"},{"title":"220805 Python 동시성 &amp; 병렬성 프로그래밍","text":"venv 명령어 123$python -m venv venv # venv이름으로 가상환경 생성$source venv/bin/activate # 가상환경 활성화 시키기$deactivate # 가상환경 나가기 pip 명령어 pip는 Python의 패키지 매니저로, 외부 패키지나 라이브러리, 프레임워크를 설치하고 관리할 수 있도록 도와준다. 123456$pip install pip --upgrade # pip upgrade$pip install &quot;package~=3.0.0&quot; #3.0.0 version의 패키지를 설치$pip install [package] # package 설치$pip uninstall [package] # package 삭제$pip --version # 설치된 pip version을 확인할 수 있다.$pip freeze # 설치된 패키지를 확인할 수 있다. 설치된 페키지를 text로 보내고 설치하기(협업) 12$pip freeze &gt; requirements.txt # requirements.txt 파일에 설치된 패키지 리스트를 파일로 뽑아내기$pip install -r requirements.txt # requirements.txt파일에 기록된 패키지를 설치 123456789101112131415# python version: 3.8.1autopep8==1.6.0click==8.1.3flake8==5.0.4importlib-metadata==4.12.0itsdangerous==2.1.2Jinja2==3.1.2MarkupSafe==2.1.1mccabe==0.7.0pycodestyle==2.9.1pyflakes==2.5.0toml==0.10.2Werkzeug==2.2.1zipp==3.8.1 CPU 바운드, I/O 바운드, Blocking바운드바운드란 장애물에 막혀서 실행이 되지 않는 상태를 말한다. CPU 바운드프로그램이 실행될 때 실행속도가 CPU 속도에 의해 제한되는 것을 말하며, 복잡한 수학 수식을 계산하는 경우, CPU의 연산 작업에 의해 프로그램이 실행될때 실행속도가 느려지거나 멈춰있는 되는 현상이 발생하게 되는데, 이를 CPU 바운드라고 한다. I/O 바운드프로그램이 실행될 때 실행속도가 I/O에 의해 제한되는 것을 말하며, 프로그램에서 사용자의 입력을 기다리기 위해 프로그램이 멈춰있는 경우가 발생하는데, 이를 I/0 바운드라고 한다. Network I/O 바운드사용자로부터 입력을 기다리기 위해 프로그램이 멈추는 것이 아닌, 외부 서버에 요청을 하여 응답을 기다리는 경우에도 프롤그램이 멈춰있는 현상이 발생하는데, 이를 Network I/O 바운드라고 한다. Blocking바운드에 의해 코드가 멈추게 되는 현상이 일어나는 것을 블로킹이라고 한다. 동기 및 비동기동기(Sync)코드가 동기적으로 동작한다는 의미는, 코드가 작성된 순서대로 실행된다는 것을 의미한다. 비동기(Async)코드가 비동기적으로 동작한다는 의미는, 코드가 반드시 작성된 순서 그대로 실행되지 않는 것을 의미한다. 파이썬 코루틴과 비동기 함수루틴루틴이란 일련의 명령으로, 코드의 흐름을 말한다. 메인 루틴메인 루틴이란 프로그램의 메인 코드의 흐름을 말한다. 서브 루틴서브 루틴은 하나의 진입점과 하나의 탈출점이 있는 루틴을 말한다. 메인 루틴을 보조하는 역할로, 함수나 메소드가 대표적이며, 별도의 스코프에 모여있다가 호출이 되었을 경우에 해당 스코프로 이동을 한 후에 return을 통해 원 호출 시점인 메인 루틴으로 돌아오게 된다. 코루틴코루틴은 다양한 진입점과 다양한 탈출점이 있는 루틴을 말한다. 코루틴은 서브루틴과는 다르게 해당 로직들이 진행되는 중간에 멈춰서 특정위치로 돌아 갔다가 다시 코루틴 함수에서 진행되었던 원 위치로 돌아와서 나머지 로직을 수행한다. 아래의 비동기 처리코드를 보면, meeting 함수에서 진입점이 두 개(함수 인자, await 구문), 탈출점이 두 개(await 구문, return 구문)이 있음을 확인할 수 있다. 동기 코드에서는 코드가 순차적으로 실행되어야 되기 때문에 각 각의 meeting 함수의 처리가 모두 완료된 후에 순차적으로 함수가 실행되지만, 비동기 코드에서는 meeting 함수의 await 구문 실행에서 기다리지 않고, 바로 다음 함수 실행을 함으로써 실행시간이 단축된다. 1초 후 A 실행, A 실행 후 1초 후에 B 실행, B 실행 후 1 초후에 C 실행 12345678910111213141516171819202122import asyncioasync def meeting(name, time): print(f&quot;Hi! {name}&quot;) await asyncio.sleep(time) print(f&quot;See you next time, {name}, ({time}초 만남)&quot;) print(f&quot;{name} 만남 완료&quot;) return timeasync def main(): result = await asyncio.gather( meeting(&quot;A&quot;, 1), meeting(&quot;B&quot;, 2), meeting(&quot;C&quot;, 3), ) print(result) #[1, 2, 3]if __name__ == &quot;__main__&quot;: start = time.time() asyncio.run(main()) end = time.time() print(end - start) 활용코루틴의 사용에 있어, asyncio 라이브러리를 사용할 수 있는데, async 키워드로 작성된 코루틴 함수를 아래와 같이 main 함수에서 asyncio.run(x) 메소드를 통해 실행할 수 있다. 12345678910import asyncioasync def hello_world(): print(&quot;hello world&quot;) return 123if __name__ == &quot;__main__&quot;: # await 키워드의 경우, async 함수 내에서만 사용할 수 있기 때문에 main함수에서는 asyncio.run() 메소드를 통해 코루틴 함수를 실행한다. # await hello_world() asyncio.run(hello_world()) Fetcher 작성requests 라이브러리만을 사용하게 되면, URL에 요청을 보내고 요청에 대한 결과를 받은 후에 연결이 끊기기 때문에 지속적으로 연결상태를 유지하면서 response로부터 원하는 데이터를 얻기 위해 Session을 사용한다. Session을 연결한 후에는 반드시 session 연결을 끊어주는 처리를 해줘야 하는데, with 구문내에서 처리를 함으로써 session 연결을 끊어주는 별도의 처리를 하지 않아도 된다. 1234567891011121314151617181920# 일반 fetcher 코드 import requestsimport timedef fetcher(session, url): with session.get(url) as response: return response.textdef main(): urls = [&quot;https://naver.com&quot;, &quot;https://google.com&quot;, &quot;https://instagram.com&quot;]*10 with requests.Session() as session: result = [fetcher(session, url) for url in urls] print(result)if __name__ == &quot;__main__&quot;: start = time.time() main() end = time.time() print(end - start) # 12 코루틴으로 작성한 fetcher에서는 requests.Session()으로 작성된 session 호출을 aiohttp.ClientSession()으로 대체한다. 123456789101112131415161718192021# 코루틴 fetcher 코드 import aiohttpimport timeimport asyncioasync def fetcher(session, url): async with session.get(url) as response: return await response.text()async def main(): urls = [&quot;https://naver.com&quot;, &quot;https://google.com&quot;, &quot;https://instagram.com&quot;]*10 async with aiohttp.ClientSession() as session: result = await asyncio.gather(*[fetcher(session, url) for url in urls]) print(result)if __name__ == &quot;__main__&quot;: start = time.time() asyncio.run(main()) end = time.time() print(end - start) # 4.8 컴퓨터 구조와 OS컴퓨터의 구성 요소컴퓨터의 구성요소로는 명령어를 해석하여 실행하는 장치로 CPU가 있으며, 작업에 필요한 프로그램과 데이터를 저장하는 장소로써의 주메모리와 데이터를 일시적으로 혹은 영구적으로 저장하는 보조 메모리가 있다.이 외에 키보드와 마우스와 같은 입출력장치가 있으며, CPU, 메모리, 입출력장치 사이를 연결하고 데이터를 주고 받는 역할을 해주는 시스템 버스가 있다. 프로세싱프로그램이 저장(HDD, SSD 저장장치(보조 메모리))되고, 사용자는 프로그램을 실행시키기 위해서 아이콘을 클릭해서 실행을 하게 되는데, 이 프로그램이 실행된다는 의미는 해당 프로그램의 작성 코드들이 주메모리로 올라와서 작업이 진행되는 것을 의미한다.프로세스가 생성되면, CPU는 프로세스가 해야할 작업을 수행한다. 다시 정리하면, 프로세스란 실행을 위해 주메모리에 올라온 동적인 상태를 의미하며, 프로그램이란 저장장치에 저장된 정적인 상태를 의미한다. 스레드CPU가 처리하는 작업의 단위가 스레드인데, 스레드란 프로세스 내에서 실행되는 여러 작업의 단위를 말한다. 스레드가 한 개로 동작하면 싱글 스레드, 여러 개의 스레드가 동작하면 멀티 스레딩이라고 하며, 복수 개의 스레드를 사용하는 멀티 스레딩에서 스레드는 다수의 스레드끼리 메모리 공유와 통신이 가능하다. 이는 자원의 낭비를 막고 효율성을 향상시키기 위함이며,한 스레드에 문제가 생기면 전체 프로세스에 영향을 미친다. 스레드의 종류로는 사용자 수준 스레드와 커널 수준의 스레드로 나뉘는데, 파이썬에서는 사용자 수준 스레드 선에서 스레드를 다룬다. 파이썬 멀티 스레딩과 멀티 프로세싱동시성 vs 병렬성소프트웨어 공학에서 동시성(병행성)이란 Concurrency에 대한 번역이다. 그리고 병렬성이란 Parallelism의 의미를 가진다.앞서 살펴본 코루틴으로 작성한 코드는 동시성(병행성)을 구현한 것이며, 그 차이에 대해서 살펴보자. 동시성(Concurrency)?동시성이란 한 번에 여러 작업을 동시에 다루는 것(switching을 하면서 작업을 다루는 것)을 의미한다. 동시성은 논리적 개념인데, 멀티 스레딩에서 사용이 되기도 하고, 싱글 스레드에서 사용이 되기도 한다. 또한 싱글 코어 뿐 아니라 멀티 코어에서도 각각의 코어가 동시성을 사용할 수 있다. 싱글 스레드에서 사용한 동시성이 바로 asyncio를 사용한 프로그래밍이다.그리고 코루틴 함수를 사용하지 않고, 스레드 자체가 함수들을 맡게 된다면, 멀티 스레딩에서 동시성을 지킬 수 있게 된다. 병렬성(Parallelism)?한 번에 여러 작업을 병렬적으로 처리하는 것을 의미한다. (at the same time)그리고 이는 멀티 프로세싱과 멀티 스레딩을 가능하게 한다. 각 각의 작업이 분리된 CPU 코어에서 각 각 작업을 하게 되면, 멀티 코어 환경에서 병렬성 프로그래밍이 가능한 것이고, CPU 단위가 아닌, 하나의 프로세스 단위에서 생각해보면, 스레드가 여러 개 있어야지만 병렬성 프로그래밍이 가능한 것이다.따라서 병렬성은 물리적 개념으로, 복수 개의 작업이 병렬로 수행되는 것을 의미한다. 또한 병렬성과 동시성은 동시에 공존할 수 있는데, 예를들어 100개의 요청이 있고 CPU 코어가 3개 있다고 가정하면, CPU 3개에서 병렬적으로 처리하는 병렬성을 유지하면서 다른 요청에 대한 처리를 switching하면서 처리하는, 동시성 또한 가질 수 있는 것이다. 하지만, 파이썬에서는 스레드로 병렬성을 구현할 수 없다. GIL Global Interpreter Lock이라는 개념이 있는데, 이로인해 구현될 수 없는 것이다. 따라서 파이썬에서는 멀티 스레드가 동시성으로 수행되어야 한다. 파이썬 멀티 스레딩우선 Single thread에서의 작업처리에 대해 살펴보면, 아래의 urls의 각 각의 작업에 대해서 같은 thread에서 처리되고 있음을 확인할 수 있다. 12345678910111213141516171819202122232425import requestsimport timeimport osimport threadingdef fetcher(session, url): print(f&quot;{os.getpid()} process | {threading.get_ident()} url : {url}&quot;) with session.get(url) as response: return response.textdef main(): urls = [&quot;https://google.com&quot;, &quot;https://apple.com&quot;] * 50 with requests.Session() as session: result = [fetcher(session, url) for url in urls] # print(result) if __name__ == &quot;__main__&quot;: start = time.time() main() end = time.time() print(end - start) # 19s 코루틴으로 작성한 fetcher 코드를 실행시켜보면, 같은 스레드에서 처리되고 있지만, 5초 정도 처리되는 시간이 단축됨을 확인할 수 있습니다. 1234567891011121314151617181920212223242526import aiohttpimport timeimport asyncioimport osimport threadingasync def fetcher(session, url): print(f&quot;{os.getpgid} process | {threading.get_ident()} url : {url}&quot;) async with session.get(url) as response: return await response.text()async def main(): urls = [&quot;https://google.com&quot;, &quot;https://apple.com&quot;] * 50 async with aiohttp.ClientSession() as session: result = await asyncio.gather(*[fetcher(session, url) for url in urls]) print(result)if __name__ == &quot;__main__&quot;: start = time.time() asyncio.run(main()) end = time.time() print(end - start) # 14.65s 만약에 aiohttp에서 제공하는 코루틴 함수가 없고, 이 상황에서 동기적 코드를 사용해서 동시성 프로그래밍을 하려면 어떻게 해야될까? 바로 이 상황에서는 멀티 스레딩을 사용하면 된다. 12345678910111213141516171819202122232425262728293031import requestsimport timeimport osimport threadingfrom concurrent.futures import ThreadPoolExecutordef fetcher(params): session = params[0] url = params[1] print(f&quot;{os.getpid()} process | {threading.get_ident()} url : {url}&quot;) with session.get(url) as response: return response.textdef main(): urls = [&quot;https://google.com&quot;, &quot;https://apple.com&quot;] * 50 # max_workers: 실행할 스레드의 수 executor = ThreadPoolExecutor(max_workers=10) with requests.Session() as session: # result = [fetcher(session, url) for url in urls] # print(result) params = [(session, url) for url in urls] results = list(executor.map(fetcher, params))if __name__ == &quot;__main__&quot;: start = time.time() main() end = time.time() print(end - start) # 3.4s 스레드를 사용하는 것보다는 코루틴을 사용하는 것을 권장한다. 그 이유는 스레드를 늘려서 작업을 처리하는 것에는 많은 연산과정이 추가되기 때문에 메모리 점유율이 많이 들어가기 때문이다. 파이썬 멀티 프로세싱, GIL다른 프로그래밍 언어에서는 멀티 스레딩을 사용해서 병렬성 프로그래밍이 가능하고, 멀티 스레딩의 장점이자 단점은 메모리를 공유한다는 것이다. 그 이유는 멀티 스레딩에서는 스레드를 하나의 프로세스에서 여러 개로 만들어서 진행을 하게 되는데, 메모리를 공유하기 때문에 하나의 스레드에서 에러가 발생하면 다른 스레드에서도 에러가 발생하기 때문이다.반면에 멀티 프로세싱에서는 각 각의 프로세스를 자식 프로세스로써 복제해서 진행을 하게 된다. 위의 이유로 인해 파이썬을 만든 개발자가 파이썬을 만들었을 때 GIL(Global Interpreter Lock)을 도입하게 되었는데, 이는 한 번에 1개의 스레드만 유지하는 락을 의미한다. GIL은 본질적으로 한 스레드가 다른 스레드를 차단해서 제어를 얻는 것을 막아준다.이는 앞서 언급한 멀티 스레딩의 위험으로부터 보호하기 위함이다. 이러한 이유로 파이썬에서는 스레드로 병렬성 연산을 수행하지 못한다.하지만, 파이썬의 멀티 스레딩은 동시성(Concurrency)를 사용해서 Network I/O bound 코드에서 유용하게 사용할 수 있지만, CPU bound에서는 GIL에 의해서 원하는 결과를 얻을 수 없다. 이 경우에 사용되는 것이 멀티 프로세싱인데, 프로세스를 여러 개 복제하고, 각 각의 프로세스들이 메모리 공유를 하지 않기 때문에 서로 소통을 하기 위해서 직렬화와 역직렬화 작업이 필요한데, 이런 비용이 멀티 스레딩을 사용했을때보다 크다.만약에 이러한 단점을 감수해서라도 속도를 높이고 싶다면, 멀티 프로세싱을 사용한다. CPU 연산 작업에 있어서 파이썬으로 멀티 스레딩으로 처리를 한다면, 이는 일반적으로 처리한 코드와 걸리는 시간은 별 차이가 없다. (동시성 프로그래밍이 불필요한 케이스)이 경우에는 함수 하나 하나를 별도의 프로세싱으로 분리해서 병렬로 처리하는 것이 시간단축에 도움이 된다. 이는 기존의 ThreadPoolExecutor를 ProcessPoolExecutor로 수정해서 실행하면 되는데, 많은 시간을 단축할 수 있다.","link":"/2022/08/05/202208/220805-python-study/"},{"title":"220807 Apache Airflow","text":"BackfillingDAG를 처음 실행하게 되면, scheduler는 자동으로 non-triggered DagRuns을 시작 날짜(start_date)와 현재(now) 사이 시점에서 실행하게 된다.catch up mechanism은 자동으로 non-triggered DagRun을 마지막으로 실행된 날짜와 현재 시간 사이에서 실행할 수 있도록 허용한다. 예를들어, 만약에 DAG를 2일동안 중지시키고나서 DAG를 다시 시작했다면, 이 기간 동안 트리거되지 않은 DAG 실행에 해당하는 일부 DAG 실행이 발생합니다.Backfilling mechanism은 historical DagRuns를 실행하도록 하는데, 예를들어 start date 이전의 기간에 DagRun을 실행할 수 있다.방법은 Airflow DAG Backfill 명령을 실행하는 명령을 사용하면 된다. (예를들어 01/03(start_date)부터 01/07(now)까지 DAG RUN을 실행했고, start_date 이전인 01/01부터 01/02 기간동안 DAG RUN을 실행하고자 한다면, Backfilling mechanism을 위한 명령을 사용하면 된다) 1with DAG('my_dag', start_date=datetime(2022, 1, 1), schedule_interval='@daily', catchup=False) as dag: 이렇게 catchup=False로 설정값을 바꿔주면, non-triggered DAG RUN이 실행되게 된다. 이 mechanism은 과거에 non-triggered DAG RUN을 자동으로 재실행할때 사용된다. Executor이전 포스팅에서도 다뤘던 내용이지만, Executor는 이름 자체는 Task를 실행할 것 같지만, Task를 실행하지 않는다. 단지 tasks를 시스템에서 어떻게 실행할 것인가에 대해 정의한다. Executor에는 다양한 종류가 있는데, local executors와 remote executors가 있다. local executor는 여러 개의 task를 single machine에서 실행을 하고, sequential executor는 single machine에서 한 번에 하나의 task를 실행할때 사용된다. remote executor에는 Celery executor가 있는데, tasks를 multiple machine, 그리고 salary cluster에서 실행한다. K8s executor는 multiple machine에서 K8s cluster의 multiple pods에서 multiple tasks를 실행한다. Executor의 변경은 Airflow의 환경설정 파일에서 executor parameter를 변경함으로써 적용할 수 있다. (사용되는 executor에 따라 변경해야 되는 별도의 환경설정 요소가 있다) Executor configuration 확인 Airflow의 Executor의 configuration 파일을 확인하기 위해 docker container(scheduler)의 cfg 파일을 현 위치에 복사하도록 한다. 1$docker cp materials_airflow-scheduler_1:/opt/airflow/airflow.cfg . 현재 Airflow를 docker를 사용해서 띄워서 사용하고 있고, Docker-compose 파일의 환경변수 정의를 보면, AIRFLOW__CORE__EXECUTOR: CeleryExecutor를 통해 CeleryExector가 사용되고 있음을 확인할 수 있다. 하지만, scheduler container에서 복사한 airflow.cfg 파일의 정의를 보면, SequentialExecutor가 정의되고 있음을 알 수 있다. 이는 configuration 파일에 정의된 내용을 docker-compose.yml 파일에서 정의된 환경변수가 덮어쓰고 있다는 의미이다. 따라서 환경설정에 관한 내용은 docker-compose.yml 파일에서 정의할 수 있도록 해야한다. Sequential Executorsequential executor는 Airflow를 수동으로 설치했을때 설정되는 default executor이다. Sequential Executor는 Web Server, Scheduler, SQLite로 구성되며, DAG가 실행될때, Scheduler는 한 번에 하나의 Task를 실행한다. (동시에 여러 개의 Task 실행 불가능) 따라서 매우 간단한 실험을 하거나, 디버깅을 하는 경우에만 사용이 되며, executor 설정에 대한 환경변수 부분만 수정해주면 사용할 수 있다. Local ExecutorLocal Executor는 Sequential Executor에서 한 단계 더 나아가서 단일 기계에서 여러 작업을 동시에 할 수 있도록 해준다. 그 외에도 데이터베이스가 Sequential Executor에서는 SQLite인것에 반해 Loal Executor에서는 Postgres 데이터베이스를 사용한다. SequentialExecutor와 달리 Local Executor에서는 두 개이상의 Task에 대해 동시 실행이 가능하다. 12executor=LocalExecutorsql_alchemy_conn=postgresql+psycopg2://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;/&lt;db&gt; 단, 단일 머신에 한정해서 Task가 실행되기 때문에 확장성이 좋지 않으며, 리소스가 단일 기계로 제한된다는 단점이 있다. Celery ExecutorCelery Executor는 동시에 여러 컴퓨터에서 작업을 실행하기 위해 Celery 클러스터를 사용한다. 우선 간단한 구성을 살펴보면, Web server, Meta database(Postgres), Scheduler 그리고 추가적으로 Worker가 있는데, 이는 실제로 Task를 실행하는 것을 담당하는 기계인 Airflow의 Worker이다. 만약에 3명의 Airflow Worker가 있다면, 작업을 실행하는 3대의 machine이 있다는 것을 의미하고, 더 많은 작업을 실행하기 위해 더 많은 리소스가 필요한 경우에는 추가적으로 Airflow Worker를 추가하기만 하면 된다. Celery Queue는 Result Backend와 Broker, 두 가지 구성요소로 구성되어 있는데, Result Backend에는 Airflow worker가 Task의 상태정보를 저장하는데 사용이 된다. 그리고 Broker는 Scheduler가 실행을 하기 위한 task를 보내면, worker가 task 실행을 위해 pull해서 사용하기 위한 queue의 역할을 한다. Scheduler가 Celery Queue의 Broker에 DAG의 Task를 전달하면, Worker 중 하나에서 해당 Task를 pull하게 된다. 그리고 Worker에서 작업이 완료되면, 작업 상태가 Result Backend에 저장이 된다. Result Backend는 Airflow의 Database와 같은 역할을 하며, 원하는 경우, 다른 데이터베이스를 사용할 수 있다. Celery Executor를 사용할때에는 Celery Queue를 설치해야 되는데, 이는 Redis나 RabbitMQ가 될 수 있다. 1234executor=CeleryExecutorsql_alchemy_conn=postgresql+psycopg2://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;/&lt;db&gt;celery_result_backend=postgresql+psycopg2://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;/&lt;db&gt;celery_broker_url=redis://:@redis:6379/0","link":"/2022/08/07/202208/220807_airflow_on_docker/"},{"title":"220805 Apache Airflow","text":"Apache Airflow UI 구성Apache Airflow UI에서 DAGs 리스트를 보면, 현재 Apache Airflow에 포함되어있는 DAG의 목록이 출력된다. 리스트 중 하나의 아이템을 살펴보면, DAG의 이름의 좌측에 해당 DAG를 Pause/Unpause 할 수 있는 토글 버튼이 있고, 이름의 아래에는 data pipeline이나 팀 또는 기능별로 묶어서 관리할 수 있도록 태그가 표시되어있다. (단, tag에 따라 permission을 부여할 수 없다) 그 외에 Owner에 대한 정보와 Runs 칼럼에서는 queued, success, running, failed 상태별로 현재 DAG이 실행 상태를 확인할 수 있다. Last Run 및 Next Run 항목을 통해서는 DAG가 언제 마지막으로 실행이 되었고, 그 다음 실행은 언제 되는지에 대해 확인할 수 있다. Recent Tasks에서는 총 15개의 상태 정보로 나뉘어 활성화된 DAG의 Task들의 실행 상태에 대해서 확인을 할 수 있다. (none, removed, scheduled, queued, running, success, shutdown, ...) Actions에서는 강제로 Trigger 시키거나 DAG를 삭제할 수 있는데, DAG를 삭제한다는 의미가 DAG 자체를 삭제하는 것이 아닌, Meta store에서 DAG RUN Instance를 삭제한다는 것을 의미한다. Apache Airflow DAG item 상세보기DAG의 이름을 클릭하면, Grid를 통해 실행한 DAG들의 상태 정보에 대해서 모니터링 할 수 있으며, Graph View를 통해서는 DAG의 각 Tasks가 어떤 Tasks를 dependencies로 가지고 있는지에 대해서 구조적으로 확인을 할 수 있다. Landing Times에서는 DAG에서의 모든 작업들이 scheduled 상태에서 completion으로 완료되는데 얼마나 걸렸는지에 대한 정보를 시각화된 그래프로 확인할 수 있다. 만약 시간이 오래걸린다면, 걸리는 시간을 줄일 수 있도록 별도의 대응이 필요하다. Calendar view에서는 각 각의 네모칸이 특정 DAG의 상태 다이어그램 정보를 종합해서 보여준다. 특정 DAG가 특정 날짜에 문제가 생긴다면, 빨간색으로 칸이 표시되며, 성공적으로 DAG가 시행이 되었다면 해당 일자에 초록색으로 포시된다. 점으로 표시된 칸은 얼마나 많은 다이어그램이 해당 날에 계획되어있는지에 대한 정보를 제공한다. 따라서 이러한 모니터링 정보를 기반으로 어떤 날에 데이터 파이프라인을 수정해서 문제를 해결해야되는지 알 수 있다. Gantt view에서는 DAG의 특정 Task가 완료되는데에 얼마나 시간이 걸렸는지와 pipeline에서 bottleneck이 발생했는지에 대한 전반적인 overview를 제공한다. 상대적으로 긴 직사각형은 그만큼 Task를 실행하는데 시간이 걸렸다는 의미이고, 오래걸린 작업에 대해서는 어떻게 하면 작업이 완료되는데 걸리는 시간을 단축시킬 수 있는지에 대한 고민이 필요하다. 직사각형이 overlapping되었다는 것은 복수 개의 Task를 동시에 실행할 수 있다는 것을 의미한다. (DAG parallelism) Code view에서는 데이터 파이프라인의 코드에 접근할 수 있는데, 이를 통해서 적용한 수정사항이 DAG에 제대로 적용이 되었는지 확인을 할 수 있다. 첫 번째 데이터 파이프라인 작성 in Airflow(1) Create table with Postgres separator 이 단계에서는 어떻게 SQL request를 Airflow에서 실행하는지 알 수 있다. (2) API 사용가능 유무 확인 이 단계에서는 API를 사용할 수 있는지에 대해 확인할 수 있으며, 특별한 종류의 operator를 사용하여, 다음 Task로 넘어가기 전에 특정 이벤트를 기다릴 수 있도록 설정할 수 있다. (3) User 정보 추출 -&gt; process_user -&gt; store_user 추출된 User 정보를 Airflow에서 가장 보편적인 operator를 사용해서 처리할 수 있다. (User 정보를 Password 데이터 베이스에 저장) DAG?DAG는 방향성 비순환 그래프로, Node는 Task를 나타내고, Edge는 각 Task간의 Dependency 나타낸다. DAG Skeleton (1) Import the DAG object (2) Instantiate a the DAG object (3) Define a unique dag id (4) Define a start date (5) Define a scheduled interval (6) Define the catchup parameter 12345678910111213# 이 파일이 DAG정보를 담고 있는 파일인지 알 수 있다.from airflow import DAGfrom datetime import datetime# 첫 번째 parameter는 DAG ID로, unique identifier로 DAG의 이름이 된다. 이 ID는 Airflow instance의 모든 태그들에 대해 유일해야 한다.# 두 번째 parameter는 start_date로, DAG가 언제 schedule이 시작되느지에 대한 날짜 정보를 넣는다.# 세 번째 parameter는 schedule interval로, DAG가 trigger되는 빈도 수에 대해서 정의한다. (매일, 매 15분, 주에 1번 등...) @daily는 매일 자정을 의미한다.# 네 번째 parameter는 catchup으로, false로 setting 하는 것이 좋다. catchup은 default로 true로 설정이 되어있는데, 이 의미는 현재와 start date 사이에# DAG가 triggered되어있지 않으면, Airflow UI로부터 데이터 파이프라인의 scheduling을 시작하면, 해당 기간내(현재~마지막으로 DAG가 실행된 날짜 or 시작 날짜)의# 모든 non-triggered DAG를 실행한다. 따라서 false로 설정함으로써 직접 세세하게 실행할 DAG를 컨트롤 할 수 있으며, 한 번에 많은 양의 DAG를 동시에 실행시키지 않아도 된다.with DAG('user_processing', start_date=datetime(2022, 1, 1), schedule_interval='@daily', catchup=False) as dag: None Operator? Operator는 데이터 파이프라인에서 하나의 Task를 정의한다. 따라서 하나의 Operator에는 두 개 이상의 Task가 위치하면 안된다. 그 이유는 예를들어 Python Operator가 존재한다고 가정하고, Operator 내에 Cleaning Data와 Processing Data가 같이 존재한다면, Processing Data가 실패했을때 같은 Operator 내에 존재하는 두 개의 Task에 대해 모두 재시작을 해야한다. (Processing Data에 대한 작업만 재시작해야 한다) 따라서 하나의 Operator에는 하나의 Task가 배치되도록 구성해야 한다. PythonOperator(Cleaning Data) -&gt; PythonOperator(Processing Data) Providers? Apache Airflow는 module화 되어 만들어졌다. 처음 Apache Airflow를 시작했을때, 아래의 명령을 통해서 Airflow Core를 설치했다. 12# Airflow Core$ pip install apache-airflow 만약에 AWS, Snowflake, Dbt, Databricks와 같이 사용되어야 한다면, 추가적으로 provider를 설치해야 한다. 12345678# AWS$pip install apache-airflow-providers-amazon# Snowflake$pip install apache-airflow-providers-snowflake# Dbt$pip install apache-airflow-providers-dbt-cloud# Databricks$pip install apache-airflow-providers-databricks","link":"/2022/08/05/202208/220805_airflow_on_docker/"},{"title":"220808 Apache Airflow","text":"Configuration 살펴보기더 많은 Airflow Worker가 필요하다면, 추가 machine에서 celery worker를 명령한다. FlowerAirflow workers를 dashboard를 통해서 모니터링하기 위한 툴이다. Celery Executor를 사용하면, Flower에 접속해서 Celery Executor의 Administrator와 Airflow worker를 대시보드를 통해 모니터링 할 수 있다. localhost:5555/dashboard 아래의 Worker dashboard를 살펴보면, Max concurrency 값이 16인 것으로 보아, 최대 16개의 Task를 동시에 실행하는 것이 가능하다. 이는 사용하는 PC의 리소스에 따라 줄일 수도 늘릴 수도 있다. 다음으로 Queues 메뉴는 유용하게 사용될 수 있는데, 특정 Task를 특정 worker로 라우팅되도록 할 수도 있다.예를들어 높은 리소스를 소비하는 Task가 존재하고, 현재 하나의 높은 리소스 Worker를 가진다고 가정하면, queue를 생성해서 queue를 Worker에 붙이고, 높은 리소스 소비 Task를 생성한 queue로 보내서 해당 Worker만 해당 작업을 실행할 수 있도록 queue를 지정해서 사용할 수 있다. DAG 예시 제거하기Airflow instance를 깨끗하게 유지시키기 위해서 아래의 방법으로 DAG를 UI로부터 제거한다. (1) docker-compose.yml 파일에서 AIRFLOW__CORE__LOAD_EXAMPLES 환경변수를 true에서 false로 변경 후 파일을 저장한다. (2) 아래 명령을 통해서 Airflow를 재시작한다. 1$docker-compose down &amp;&amp; docker-compose up -d Flower 모니터링DAG를 Trigger한 후에 Flower를 통해 모니터링을 해보면, Active column을 통해 현재 DAG의 Task가 Celery worker에서 실행되고 있음을 확인할 수 있으며, status가 Processed를 거쳐 Succeeded로 완료가 되는 것을 확인할 수 있다. Flower에서 Task를 모니터링할때 주의해서 봐야되는 부분은 failure와 retried부분으로, 이 부분에 대한 카운트가 있는지에 대해서 확인해야 한다. 이후에 Tasks 메뉴를 통해서 Task의 개별 UUID를 클릭해서 실행된 Task의 상세정보에 대해서 확인할 수 있다. 상세정보로는 Task가 현재 어떤 상태이고, 어느 Worker에서 실행이 되었는지에 대해서도 확인할 수 있다. Task가 실행된 Worker는 queue에 따라 다른 Worker에서 실행할 수 있다. 복수 개의 Queue가 필요한 이유Airflow에서 queue(FIFO)는 trigger되기를 기다리는 Task들의 대기열로 볼 수 있다. Celery queue는 Broker와 Result Backend로 구성이 되어있고, Worker는 복수 개로 구성이 되어있다. 복수 개의 Worker는 각 각 5 CPUs, GPU, 1GPU등의 스펙을 가질 수 있는데, 이는 처리해야되는 Task에 따라서 Worker를 지정해서 처리해야한다. 만약 복잡한 연산처리가 필요한 경우에는 5CPUs Worker를 통해 Task가 실행되도록 해야하며, 머신러닝과 같은 처리를 위해서는 GPU의 성능이 좋은 Worker에서 처리를 해야한다. 그리고 그 이외의 경우에 일반적인 Task의 처리에서는 1CPU worker에서 처리되도록 분류해야한다. 이 경우에는 어떻게 분류를 해서 처리를 해야할까? 바로 Celery Queue에 세 개의 queue(high_cpu, ml_model, default)로 분류해서 high_cpu 큐는 5CPUs Worker에서 처리되는 Task를 쌓아서 처리하도록 하고, ml_model 큐는 GPU 성능이 좋은 Worker에서 처리가 되도록 Task를 쌓아둔다. 그리고 마지막으로 일반적인 Task를 쌓아두는 큐의 역할을 default queue가 담당하게 된다. Celery worker 생성하기Queue를 생성하기 전에, 우선 복수 개의 Celery Worker를 Apache Airflow 인스턴스에 생성해야 한다.docker-compose.yml 파일에서 airflow-worker 서비스 항목을 복사해서 하나 더 생성을 한 다음에 아래 명령으로 서비스를 다시 시작한다.서비스를 다시 시작하면, localhost:5555 Flower web UI에서 두 개의 Worker가 생성이 된 것을 확인할 수 있다. 실제로 다수의 machine이 있을때, machine에서 아래의 명령을 실행함으로써 Celery worker로써 동작하도록 할 수 있다. 1$celery worker queue 생성하기Airflow에서 queue를 생성하는 것은 간단하다. airflow-worker 서비스의 command 항목에서 celery worker를 해주는 경우, default queue를 통해 worker로 Task가 분배가 되지만, 아래와같이 worker 생성시에 command에 -q 옵션으로 Worker에 queue를 붙여주면, 특정 Task를 지정한 airflow worker에서 실행되도록 할 수 있다. 1234567891011121314151617181920airflow-worker-1:&lt;&lt;: *airflow-commoncommand: celery worker -q high_cpuhealthcheck: test: - &quot;CMD-SHELL&quot; - 'celery --app airflow.executors.celery_executor.app inspect ping -d &quot;celery@$${HOSTNAME}&quot;' interval: 10s timeout: 10s retries: 5environment: &lt;&lt;: *airflow-common-env # Required to handle warm shutdown of the celery workers properly # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation DUMB_INIT_SETSID: &quot;0&quot;restart: alwaysdepends_on: &lt;&lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully Task를 지정한 queue에 보내기DAG에서 Operator로 Task를 작성할때, Operator의 인자로 아래와 같이 queue를 지정해주면, 지정한 queue로 해당 Task가 전송된다. 12345transform = BashOperator( task_id='transform', queue='high_cpu', bash_command='sleep 10') Airflow 동시성을 위한 parameters병렬로 동시에 처리할 tasks와 DAG Runs를 정의하기 위해서는 아래의 parameters를 configuration settings에서 설정해줘야 한다. parallelism / AIRFLOW__CORE__PARALELISM이 parameter는 Airflow의 Scheduler당 실행할 수 있는 task instances의 갯수에 대해서 정의한다. default로는 Scheduler당 최대 32개까지 동시에 실행이 가능하며, Scheduler의 수가 늘면 그 수가 배가 된다. max_active_tasks_per_dag / AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG이 parameter는 각 각의 DAG에서 동시(concurrency)에 실행할 수 있는 최대 task instance의 수를 정의한다. default로는 최대 16개의 tasks를 지정된 DAG에서 동시에 처리할 수 있다. (모든 DAG 실행에서) max_active_runs_per_dag / AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG이 parameter는 DAG당 최대 활성시킬 수 있는 DAG의 수를 정의한다. default로는 최대 16개의 DAG를 동시에 실행할 수 있다. Sequential executor에서의 SQLite의 특징Sequential executor에서 사용되는 SQLite는 무제한으로 reader를 허용한다. 하지만 writer는 한 번에 한 명 밖에 허용하지 않는다. 이러한 이유로 SQLite는 복수의 Worker에서 병렬로 Tasks를 실행할 수 있는 Local executor와 Celery executor에서는 사용될 수 없다. Repetitive Patterns만약 파일을 다운로드하는 Task 세 개가 주어지고, 세 개의 Task가 일괄적으로 Checking Files이라는 하나의 Task를 통해 파일 검사를 시행한다. 그 후에 세 개의 Task로 분리하여 다운로드 받은 파일들을 처리한다. 이런 구조로 되어있는 Task sequence에서는 다운로드하는 세 개의 Task를 그룹화해서 Downloading Files인 하나의 Task로 만들고, 다운로드 받은 파일을 처리하는 Processing Files도 하나의 Task로써 작성한다. 이는 DAG의 Task 구조를 쉽게 읽고 유지보수하기 위함이다. 이는 SubDAGs나 TaskGroup을 사용해서 구현할 수 있다. SubDag를 사용해서 구현했을때에는 SubDag로 묶인 Task를 클릭해서 별도의 DAG 페이지에서 하위 Task 구성만 별도로 확인을 할 수 있었는데, TaskGroup을 사용해서 구현을 하면, 현재 DAG 구조에서 가시적으로 Group이 표시됨을 확인할 수 있었다.","link":"/2022/08/08/202208/220808_airflow_on_docker/"},{"title":"220602 종합 프로젝트 전 빅데이터 개념 종합정리","text":"이번 포스팅에서는 빅데이터 구축의 각 단계를 6V관점에서 살펴보고, 중요 개념과 사용되는 상세 기술에 대한 내용을 정리하려고 한다. 사이드 프로젝트를 진행하면서 중간에 개념을 한 번 정리하는 이유는 데이터 파이프라인 구축에 있어, 요즘 AWS EMR이라는 관리형 서비스를 사용해서 손쉽게 하둡의 에코 시스템을 사용(단 7분이면 하둡의 에코 시스템을 사용)하고, 직접 전반적인 서비스들을 통합해서 직접 프로젝트를 진행하지 않았기 때문에 뭔가 개념적으로 정리가 되지 않은 것 같다.그래서 이번 기회에 빅데이터의 전반적인 개념에서부터 전체적인 하둡 에코 시스템을 구성하고 있는 서비스들을 활용해서 사이드 프로젝트를 진행해보고, 좀 더 체계적으로 개념과 응용적인 부분에 대해서 정리해보려고 한다. 빅데이터요즘에는 데이터가 대량으로 쏟아지고 있기 때문에 빅데이터라는 개념은 우리 일상속에 쉽게 찾아 볼 수 있는 개념이다. 2020년도의 전세계적인 데이터 성장률을 살펴보면, 80%이상의 데이터 성장은 비정형 데이터(Machine data, Social media, VoIP, Enterprise data)로 구성이 되어있다. 과거에는 이러한 대량의 데이터들을 적재하고 분석하는 부분에 대한 기술이 부족했기 때문에 그냥 버리는 경우가 많았는데, 요즘에는 과거로부터 현 시점까지 쌓인 데이터를 분석해서 현재의 문제점을 이해하고, 데이터로부터 해결하는데 사용되는 통찰력을 얻기도 한다. 현재의 문제점을 해결하기 위한 통찰력 이외에도 데이터로부터 다양한 패턴들을 해석해서 미래를 예측하기 시작했으며, 조직의 중요한 의사결정에 빅데이터가 많이 사용되고 있다. 3V + 2V = 1V이전 포스팅에서는 Big Data 4V라는 개념에 대해서 정리를 했었는데, 기본 3V(Volume, Variety, Velocity) + 2V(Veracity(진실성), Visualization(시각화))이 있으며, 그 결과로 새로운 Value(가치)가 창출된다고 하여, 6V로 정의된다고 한다.2V(Veracity, Visualization)은 IBM에서 추가된 개념으로, 이 6V는 빅데이터의 기획, 설계, 분석, 운영에서 사용되는 매우 중요한 개념이다. [ DL &amp; DW ] Volume : 방대한 양의 데이터(TB, PB 이상) Variety : 정형(DBMS, 전문) + 비정형(SNS, 동영상, 사진, 음성, 텍스트 등) Velocity : 실시간으로 생산이 되며, 빠른 속도로 데이터를 처리/분석 [ DM ] Veracity : 주요 의사결정을 위해 데이터의 품질과 신뢰성 확보(상품/서비스, 고객/마케팅, 리스크 관리 이 세가지 활용영역으로 DM가 만들어진다.) [ Insight - 데이터 분석가들의 영역 ] Visualization : 복잡한 대규모 데이터를 시각적으로 표현 [ Business ] Value : 비즈니스 효익을 실현하기 위해 궁극적인 가치를 창출 전 세계적으로 방대한 크기의 다양한 데이터들이 빠른 속도로 발생하고 있으며, 주요 의사결정에 사용되는 데이터의 품질과 신뢰성을 확보하기 위해 데이터의 진실성은 중요하다.이러한 데이터를 분석하고, 시각화함으로써 새로운 효익을 가져다 줄 가치를 창출한다. 빅데이터의 목적사람이 기술과 대량의 데이터를 활용해서 빅데이터 시스템을 통해 과거의 현상에 대한 원인을 발견하고 그 원인을 통해 현재의 상황을 이해하고, 더 나아가 미래의 결과를 예측하는 한다.이를 통해 의사결정을 위한 인사이트를 발견하고 적용함으로써, &quot;비용 절감&quot;, &quot;수익 창출&quot;, &quot;문제 해결&quot;이라는 효과를 낼 수 있다. 빅데이터 인사이트의 이해빅데이터의 인사이트는 현상에 대한 이해, 발견, 그리고 예측이라는 인사이트로 구분이 된다.이해 인사이트는 시계열별 회원 가입 추이와 같은 대규모 데이터의 통계를 통한 이해를 말하며, 매출이 증가/감소한 원인과 같은 발생한 현상에 대해 이해하는 것을 발견 인사이트, 부도 위험이 있는 고객과 같은 발생하지 않은 현상에 대해 예측을 하는예측 인사이트이 있다.마지막의 예측 인사이트는 ML/DL과 같이 예측 모델을 만들고 현상을 예측하는 등과 같은 높은 기술 수준이 요구된다. 빅데이터 시스템 vs AI 시스템AI 시스템은 빅데이터 시스템과 분리된 형태로 존재한다.빅데이터가 AI에 대한 학습 데이터에 대해서 부담을 줄여주고 있다. (보완관계) 빅데이터 시스템 = (DL -&gt; DW -&gt; AI 관련 DM) -&gt; AI 시스템 (AI 개발/학습 -&gt; 머신러닝 및 딥러닝) 빅데이터 관련 업무빅데이터 관련 업무는 크게 세 가지로 구분이 되는데, 플랫폼 구축형 프로젝트, 빅데이터 분석 프로젝트, 빅데이터 운영 프로젝트로 구분된다. 실제 회사에서 업무를 하게 되면, 이 세 가지 중에서 한 가지를 받게 된다. (1) 플랫폼 구축형 프로젝트는 규모에 따라서 차이는 있지만 보통 3~6개월 정도의 기간동안 진행된다. 전형적인 빅데이터 SI 구축형 사업과 관련이 있으며, 빅데이터의 하드웨어와 소프트웨어를 설치하고 구성하는 것을 메인으로 한다.(수집 -&gt; 적재 -&gt; 처리 -&gt; 탐색 -&gt; 분석의 기능을 구현한다.)수 년간 쌓인 데이터를 마이그레이션하고, 간단한 지표를 알아내거나 고급 분석을 한다. 세부 파트 구성은 플랫폼 Part(설치 및 구성), 전처리 Part(수집 및 적재), 후처리 Part(처리, 탐색, 분석)으로 크게 구성하며, 규모에 따라 다르지만, 각 파트당 최소 3명이상 투입한다. PM은 기술형 PM을 포니셔닝하며, 이유는 기술적 이슈가 발생했을때 대응해야되기 때문이다. (2) 빅데이터 분석 프로젝트는 1~3개월 일정으로 진행된다. 단기간에 성과를 내려면 안되기 때문에 장시간에 거쳐 시행착오를 많이 겪어야 완성도가 높은 데이터 결과를 뽑아낼 수 있다. 위의 플랫폼 구축이 완료된 후에 수행하며, 빅데이터 탐색으로 데이터의 이해가 높아질 때 시간한다.분석 주제의 영역은 크게 세 가지로, 고객 분석을 통해 맞춤형 전략을 수립하는 “마케팅/고객, 신규 제품을 개발할때, 삼품의 가격 및 출시일을 결정할때 활용된다. 그리고 내/외부 리스크를 사전에 예측(경쟁사, 평판)할때 사용된다. (3) 빅데이터 운영 프로젝트는 이미 구축이 완료된 플랫폼을 중/장기적으로 유지 관리한다. 대규모 하드웨어 및 소프트웨어로 운영되기 때문에 비용이 높으며, 각 빅데이터 분야별로 전문가 그룹이 확보되어야 한다.빅데이터란 특정 업무 부서가 아닌 전사 시스템이기 때문에 여러 부서/업무 담당자들이 빅데이터 시스템에서 분석을 통해 서로 공유를 해야한다.필요한 프로세스 및 역할에 대해 정의가 필요하며, 추가적으로 데이터에 대한 표준화가 필요하다. 이러한 전반적인 작업이 빅데이터 거버넌스 체계를 수립하는 과정이다. 빅데이터 업무 분담기업마다 다르지만, PM 아래에 비즈니스, 데이터 분석, 데이터 엔지니어링 이렇게 세 개의 조직으로 분리를 할 수도 있으며, 비즈니스 + 데이터 분석, 데이터 분석 + 데이터 엔지니어링으로 조직을 분리 할 수 있다. 빅데이터 조직 구성(선진화된 빅데이터 시스템 도입)빅데이터 관련 부서의 배치를 빅데이터 플랫폼팀을 IT부서의 하위에 배치하고, 빅데이터 분석을 담당하는 부서를 CEO 직할의 빅데이터 센터의 하위에 배치함으로써 빅데이터 관련 거버넌스에 필요한 정책 및 표준을 하위의 다른 부서들에 내려주고 역할아래에 활용하도록 구성한다. (빅데이터 분석팀을 하위의 부서들에 공통으로 배치하게 되면, 각 부서별로 충돌이 발생할 가능성이 있다) 하둡(Hadoop)하둡은 2005년 첫 등장을 하였고, 빅데이터 생태계에서 절대적인 영향력을 행사하고 있다. Cloudera, Hortonworks, MapR 등의 업체들이 Hadoop S/W를 중심으로 빅데이터 소프트웨어를 개발하고 공개하고 있다. 빅데이터 구축 단계빅데이터 구축 단계는 총 4개 단계로 구분할 수 있으며, 수집 -&gt; 적재 -&gt; 처리/탐색 -&gt; 분석/응용이다. [ 수집 ] 실시간 데이터 수집 : CEP(Complex Event Processing) 기술 적용ref. CEP란 전통적인 RDB로는 실시간 데이터 처리 및 분석이 불가능하다는 문제를 해결하기 위한 솔루션 대용량 데이터 수집 : ESP(Event Stream Processing) 기술 적용 조직 내/외부로부터 데이터를 수집하는 단계로, 정형/비정형 데이터를 크롤링 및 NLP 처리 기술을 통해 취득한다. (블로그, 포털 등 뉴스 데이터는 비정형 처리 기술을 선택한다.) Volume(상) : 대용량 데이터(TB 이상) 수집 Variety(상) : 정형/반정형/비정형 데이터 수집 (Log, RSS, XML, 파일, DB, HTML, 음성, 사진, 동영상) Velocity(상) : 실시간 스트림 데이터를 수집한다.(요건의 주요도에 따라서 최적화된 아키텍처를 구축한다.) [대표 기술] Flume : 클라우데라에서 처음 개발된 기술로, 다양한 수집 요구 사항들을 해결하기 위해 다양한 기능으로 구성된 소프트웨어이다. 로그 데이터를 깔끔하게 수집하는데 가장 최적화되어있다. 많은 기업에서 실제 서비스 로그 데이터 관리를 위해 사용중이다. 스톰 : 실시간 데이터를 병렬 프로세스로 처리하기 위한 소프트웨어, 데이터를 적재하기 전에 발생과 동시에 이벤트를 감지해서 처리하는 방식을 채택한다. 에스퍼 : 스트리밍 데이터가 복잡한 이벤트 처리가 필요할 경우 사용되는 룰 엔진이다. 스톰은 실시간 데이터로부터 패턴을 찾고 패턴에 따라 이벤트를 처리하는데는 기능이 부족하다. 그런데 에스퍼는 실시간 발생 데이터간의 관계를 복합적으로 판단하고 처리하는 CEP(Complex Event Processing) 기능을 제공한다. Logstash [ 적재 ] 수집한 데이터를 필터링(정제)하여 데이터의 품질을 향상 시킨 후 빅데이터에 저장하는 과정이다. (분산 스토리지에 영구/임시로 적재) 데이터 적재시 데이터의 종류에 따라 구분 처리 [대용량 파일] (1) 대용량 파일을 영구 저장하기 위해서는 HDFS에 적재 [대규모 메시징 데이터] (1) 영구 저장시에는 NoSQL인 HBase, MongoDB, Cassandra에 적재 (2) 일부만 임시 저장할 때에는 In-Memory 캐시인 Redis, RAMCache, 인피니스팬(Infinispan)에 적재 (3) 전체를 버퍼링하기 위해서는 MOM(Message Oriented Middleware)인 Kafka, RabbitMQ, ActiveMQ등을 활용 위와같이 구분해서 저장하는 이유는 예를들어 실시간 및 대량으로 발생하는 데이터를 HDFS에 저장하게 되면, 파일 수가 기하급수적으로 증가하면서 관리 노드와 병렬 처리의 효율성이 낮아진다. 따라서 반드시 데이터의 성격별로 구분해서 저장해야한다. 대량의 데이터가 적재될 때는 추가적인 전처리 작업이 발생한다. 전처리 작업은 때로는 데이터의 크기와 요건에 따라 HDFS에 적재한 후에 처리하는 후처리를 하기도 한다. ex) 비정형 =&gt; 정형 데이터로 가공, 개인정보로 의심되는 데이터는 비식별 처리를 선행한다. Volume(상) : 대용량 데이터(TB이상)를 적재, 대규모 메시지(1,000TPS 이상) 적재 Variety(중) : 정형/반정형/비정형 데이터를 적재 Velocity(상) : 실시간 스트림 데이터 적재 Veracity(상) : 데이터의 품질과 신뢰성을 확보해서 적재 [ 처리/탐색 ] 하둡 에코 시스템의 20여개 오픈소스 서비스를 사용한다. Adhoc 쿼리로 데이터를 탐색/서택/변환/통합/축소 작업한다. 내/외부의 정형/비정형 데이터를 결합함으로써 기존 기술적 한계로 만들지 못한 새로운 데이터셋을 생성한다. 정기적으로 발생하는 처리/탐색 과정은 workflow로 프로세스화/자동화 한다. workflow 작업이 끝나면, 데이터셋은 특화된 저장소(DM)로 이동하고, 데이터셋이 측정가능한 구조로 만들어지게 되면서 빅데이터 분석이 빠르고 편하게 된다. 대용량 저장소에 적재된 데이터를 분석에 활용하기 위해서 데이터를 정형화, 정규화한다. 탐색적 분석 : 데이터를 이해하기 위해 지속적인 관찰을 하며, 탐색결과를 정기적으로 구조화하는 과정을 수행한다. [대표 기술] 대표적인 기술로는 Hue/Hive/Spark/SQL 등을 사용한다. 후처리 작업으로써 자동화하는 workflow 작업을 하게 되는데, 이때는 Oozie를 사용한다. Volume(상) : 대용량 데이터(TB이상)에 대한 후처리 및 탐색 Veracity(상) : 데이터의 품질과 신뢰성을 확보하기 위한 후처리 및 탐색 Visualization(상) : 후처리된 데이터셋을 시각화해서 탐색 Value(중) [ 분석/응용 ] 과거의 데이터로부터 문제의 원인을 찾아, 현재의 현상을 이해하고 개선한다. 인간의 힘으로는 찾기 힘든 패턴들을 빅데이터 분석 기술로 찾아서 알고리즘화하고, 미래 예측 분석 모델을 생성해서 활용 영역에 따라 통계, 데이터 마이닝, 텍스트 마이닝, 소셜 미디어 분석, 머신러닝/딥러닝 등 다양하게 분류를 수행한다. 대규모 데이터로부터 새로운 패턴을 찾고, 패턴을 해석함으로써 새로운 통찰력(insight)를 얻게 되는 과정이다. 분석과정은 선형적 확장이 가능하며, 대규모 분산 환경을 낮은 비용으로 구축할 수 있다. 분산 환경 위에서 ML 구현(기존 분석 기술의 한계를 극복) 파일 기반의 Batch 분석에서 수십배 빠른 In-Memory 기반의 분석기술이 빅데이터 생태계에서 빠르게 발전하고 있고, 그 활용범위도 점점 넓어져가고 있다. 군집/분류/회귀/추천 등 고급 분석 영역까지 확장되고 있다. [대표 기술] Zeppelin, R, Tensorflow 외부 RDBMS에 데이터를 제공하는 과정이 진행 (Scoop) (분석/응용 단계에서는 모든 요소가 중요하다) Volume(상) : 대용량 데이터(TB이상) 분석 Variety(상) : 정형/반정형/비정형 등의 다양한 데이터 분석 Velocity(상) : 인메모리 기반으로 실시간 데이터 분석 Veracity(상) : 신뢰성 높은 분석 결과를 비즈니스에 적용 Visualization(상) : 분석 결과 및 창출된 가치를 시각화 Value(상) : 분석된 결과를 비즈니스에 적용해서 가치 창출 빅데이터 보안빅데이터에서의 보안은 일반 시스템에서도 적용되는 보안 요소들을 포함하고 있으며, 분야에 따라 다양한 보안요소들이 고려된다. 그 중 데이터 보안과 접근제어 보안에 빅데이터 만의 특징을 가지고 있다. 데이터 보안과 빅데이터의 발전 데이터 보안과 빅데이터의 발전은 서로 trade-off 관계에 있다.데이터 보안에서는 개인식별이 가능한 어떠한 정보도 수집하지 않는 것을 원칙으로 하나, 이러한 개인식별 정보들을 수집하지 못하면 의미가 없다. 따라서 개인에서 기업의 정보보호를 위한 정책 및 기술들에 따라서 개인정보를 이름에서는 성을 제외한 이름을 *, 거주지의 상세정보를 OO, 전화번호의 일부를 XXXX로 마스킹하거나 통계처리, 삭제, 범주화하는 등의 비식별 처리 기술을 활용한다. 위와같이 비식별 처리 기술을 활용해서 데이터를 처리하게 되면, 아래와 같은 문제점이 생긴다. (1) 개인정보 재식별화 : 데이터의 식별력이 높아지게 된다. (비식별 처리된 데이터에 새로운 카테고리의 데이터가 추가되면 식별력은 더욱 높아진다) 따라서 소속/유관기관과 국가별 법령에 따라 다르기 때문에 사내 법률팀과 확인이 필요하다. (2) 비식별화 + 대체키 활용 : 개인화 서비스 및 마케팅 분야에서 데이터 활용이 어려워진다. (1:1 마케팅 및 신용정보 점수, 구매 이력에 따른 실시간 추천) 빅데이터 시스템에서는 수시로 변경되는 마케팅 등의 현황 관리가 어렵다. 이 문제에 대해서는 관련 시스템으로 짧은 주기로 데이터를 체크하고 수집 및 업데이트함으로써 해결할 수 있다. 특정 개인을 타겟팅하기 위한 경우에는 고유값이 비식별화 처리되어 타겟 분석을 이용한 서비스 개발이 어렵다. 이 경우에는 고객 관리 시스템에서 고유하게 발급하는 대체키를 활용해서, 빅데이터에 적재된 다른 데이터와 이 대체키를 결합해서 사용하도록 한다. 엄격한 보안 정책에 따라 대체키와 매핑된 개인정보를 조회/활용함으로써 마케팅팀과 영업팀, 리스크 관리팀에서는 데이터를 활용할 수 있게 된다. 접근제어 보안빅데이터 저장소에 저장된 개인정보, 거래정보, 행동이력을 보관하고 있는 경우, ID/PASSWORD를 관리하는 인증 관리자와 인증된 계정에 역할과 권한을 부여하는 권한 관리자 이 두가지가 취약하다. 따라서 안전하게 데이터를 제공하기 위해서는 Third party 접근제어 기술 관련 제품을 활용하여 이러한 취약점을 해결할 수 있다. [아파치 녹스] 아파치 녹스는 DMZ 영역에 설치되며, 클라이언트와 하둡 에코 시스템의 직접적인 통신을 막는다. 클라이언트와 하둡 에코 시스템이 통신을 할 때에는 LDAP, KDC로부터 개인/권한 정보를 받아서 클라이언트는 하둡 에코 시스템과 통신한다. [아파치 센트리 서버] 아파치 센트리 서버는 임팔라, 하이브 서버, 하둡 네임노드 등에 각 각 센트리 에이전트를 설치하고, 계정과 권한 정보를 통합관리하고 있는 Policy MetaStore로부터 접근제어의 계정 정보를 받아서 센트리 에이전트에서 사용해서 접근하도록 한다. [아파치 레인지 서버] 아파치 센트리 서버와 유사한 구조로 되어있으며, 다른 점은 레인저 플러그인이 하이브 서버, HDFS, 스톰, HBase, 녹스에 각 각 설치되어있으며, Policy MetaStore로부터 접근제어의 계정 정보를 받아서 레인저 플러그인에서 사용해서 접근하도록 한다. [커베로스] 커베로스는 매우 유명한 Third party 접근제어 보안 서비스이며, 클라이언트가 Kerberos KDC(Key Distribution Center)에 인증을 요청하고, 티켓을 획득하면, 해당 티켓으로 하둡 파일 시스템에 접근을 허용받아 접근하는 방식으로 동작한다.","link":"/2022/06/02/202206/220602_side_project/"},{"title":"220603 AWS Certified Solutions Architect Associate Certificates (SAA-C02) (작성중...)","text":"이번 포스팅에서는 본격적으로 AWS SAA(Solutions Architect Associate) level에 맞는 파트에 대해서 학습을 시작할 것이다. 그 첫 시작으로 Private, Public, Elastic IP의 비교에 대한 내용을 학습할 것이다. 이전에 EC2 인스턴스를 생성한 뒤에 속성으로 간략하게 위의 3가지 종류의 IP에 대해서 살펴보았는데, EC2인스턴스를 중지 후 재 실행하게 되면, Public IP 주소가 새롭게 생성되기 때문에 고정 IP로 사용하려면 Elastic IP를 사용하면 되고, Elastic IP의 경우에는 실제 사용이 될 때가 아닌, 사용이 되지 않을때 요금이 부가된다는 내용이 기억에 남는다. 그럼 좀 더 세부적으로 이론을 배워보고 실습해봐야겠다. Private &amp; Public &amp; Elastic IP Elastic IPs는 인스턴스의 고정된 Public IP를 위해 필요하다. Elastic IP Address는 가지고 있는데, 사용하지 않으면 과금이 된다. 빠르게 계정 내 다른 인스턴스로 주소를 매핑함으로써 인스턴스나 소프트웨어의 실패를 마스킹 할 수 있게 도와준다. 계정당 5개의 Elastic IP를 가질 수 있다. (필요에 따라 AWS 요청하여 갯수 확장 가능) 종합적으로 살펴보면, Elastic IP 주소를 사용하는 것은 권장되지 않는다.-&gt; Elastic IP 주소를 사용하는 Architecture는 안좋은 구조적 결점으로써 언급되기도 한다.(안좋은 Architecture) 대신 random public IP를 사용하고, 해당 IP에 DNS 이름을 지정해서 사용하도록 한다. (DNS -&gt; Route 53 - 훨씬 더 많은 제어가 가능하고, 확장 가능성도 크다) 또는 Load Balancer를 사용하고, public IP를 사용하지 않도록 하는 방법도 있다. (AWS에서 취할 수 있는 최상의 Pattern이다) Private IP and Public IP in AWS EC2EC2인스턴스에 SSH 연결을 하는 경우, 같은 네트워크에 있는 것이 아니기 때문에 Private IP를 사용할 수 없다. (단 VPN을 사용하는 경우 가능)Private IP는 AWS의 내부 네트워크 통신을 위해 사용되며, 인터넷 연결을 위해서는 Public IP가 사용된다.오직 Public IP를 사용해서 EC2 인스턴스에 SSH 연결을 할 수 있다. EC2 배치 그룹Ec2 인스턴스의 배치그룹은 인스턴스가 AWS 인프라에 배치ㅐ되는 방식을 제어하고자 할 때 사용된다. EC2 인스턴스 배치 전략을 통해 다양한 case에 따라 EC2 인스턴스를 사용할 수 있다. EC2 instance Placement group은 AWS H/W와 직접적인 상호작용을 하지 않는다. 배치 그룹 전략에는 Cluster, Spread, Partition이 있다. Cluster : 하나의 AZ에서 low-latency group으로써 EC2 인스턴스를 배치한다. (EC2 인스턴스의 네트워킹, 컴퓨팅 성능이 많이 요구되는 경우 사용되는 배치 전략) 단, 같은 AZ와 Rack(H/W) 내에 구성되어있기 때문에 실패를 하게 되면, 모든 인스턴스가 동시에 실패하게 된다. Spread : AZ 당 최대 7개의 인스턴스 배치가 가능하며, 각기 다른 분산된 AZ에 EC2 인스턴스를 할당한다. Partition : AZ내의 각기 다른 렉(rack)에 의존하며, 그룹당 수백개의 EC2인스턴스를 통해 확장할 수도 있고, 이를 통해 Hadoop, Cassandra, Kafka와 같은 어플리케이션을 실행할 수 있다. (Partition을 구분할 수 있는 어플리케이션)Partition은 AWS의 각 각의 Rack을 나타내며, 물리적으로 각 각의 instance들이 격리되어 Rack의 실패로 인해 모든 instance가 실패하는 경우를 예방할 수 있다. 그리고 각 파티션 정보는 메타 데이터를 통해 접근/확인이 가능하다. ENI(Elastic Network Interface)EC2 Hibernate modeEC2 고급 개념(Nitro, vCPU, 용량 예약)첫 번째 EC2 SAA Level Test","link":"/2022/06/03/202206/220603-aws-saa-study/"},{"title":"220601 Docker 스터디 2일차","text":"이번 포스팅부터는 본격적으로 Docker를 설치해보고 실행해보는 부분에 대해서 다뤄보려고 한다.Docker는 다양한 데이터베이스를 손쉽게 생성 및 삭제할 수 있기 때문에 개발할때 많이 사용된다. Docker 설치docker 공식 홈페이지에서 docker를 설치하면 된다.기본적으로 도커는 리눅스를 지원하기 때문에 Mac의 경우에는 가상머신에 설치가 된다.(Mac의 경우에는 xhyve를 사용하고, Windows는 Hyper-V를 사용한다) 1$docker version client와 server가 출력되는 이유는 현재 Mac의 localhost에 docker server가 설치되고, 그 위에 client가 동작하고 있기 때문이다. 그래서 터미널에서 작성한 명령어는 client server로 전달이 되고, 반환된 결과는 client로 전달해서 출력을 하게 되는 것이다.(docker CLI는 docker host에 명령을 전달하고, 결과를 받아서 client에 출력한다.) Docker 기본 명령어 (1) run : 컨테이너 실행 명령 다양한 옵션 -d : detached mode(터미널 상에서 계속 실행된 상태로 표시되지 않게 Background로 실행) -p : host와 container의 Port를 정의해서 연결한다.(host port:container port) -v : host와 container의 directory를 서로 mount -e : container 내에서 사용할 환경변수를 설정 --name : container의 이름 설정 --rm : 프로세스 종료시 컨테이너를 자동으로 제거 -it : -i와 -t를 동시에 사용한 것으로, 터미널 입력을 위해 사용 --network : 네트워크를 연결하기 위해 사용 1234567891011# [-v option 예시]$docker stop mysql$docker rm mysql# docker를 삭제하고, 다시 새로운 컨테이너를 띄우게 되면, 내부의 데이터도 같이 삭제된다. 따라서 컨테이너의 데이터 저장 directory를 localhost의 directory로 mount시켜서 데이터를 영구적으로 저장해서 사용할 수 있도록 해야한다.$docker run -d -p 3306:3306 \\ -e MYSQL_ALLOW_EMPTY_PASSWORD=true \\ --network=app-network \\ --name mysql \\ -v /Users/hyungi/Workspace/~:/var/lib/mysql \\ mysql:5.7 (2) exec : run 명령어와는 달리 실행중인 도커 컨테이너에 접속할 때 사용된다. 보안상의 문제로 컨테이너 안에 ssh server를 설치하지 않고, edxec 명령어로 접속을 한다. (3) ps : 현재 실행중인 docker 컨테이너 확인 다양한 옵션 -a : 중지 상태인 docker 컨테이너 리스트 확인 (중지된 컨테이너의 경우에는 삭제되지 않고 남아있는 경우가 있기 때문에 반드시확인 후 rm 명령으로 삭제) (4) stop [한개 or 복수 개(space로 구분)] : 현재 실행중인 docker 컨테이너 중지 (5) rm : 종료된 docker 컨테이너를 완전히 제거 (6) log : 현재 실행중인 docker 컨테이너의 로그 확인(컨테이너가 정상적으로 동작하는지 확인하는 좋은 방법) 다양한 옵션 -f : watch 상태로 새롭게 올라오는 로그 기록 follow (7) images : 도커가 다운로드한 이미지 목록을 출력 (8) pull : 이미지를 다운로드 (9) rmi : images 명령에서 출력된 컨테이너 리스트에서 이미지 ID를 사용해서 docker 컨테이너 이미지를 삭제(단, 실행중인 컨테이너가 사용하고 있는 이미지는 삭제되지 않음) (10) network create : docker container끼리 이름으로 통신할 수 있는 가상 네트워크 생성1234567891011121314151617# 생성한 network에 서로 연결할 container들을 연결시킨다. (bridge 역할)$docker network create [network name]# mysql, wordpress 연결# 기존에 생성된 mysql에 위에서 생성한 네트워크를 추가한다.$docker network connect app-network mysql# wordpress 실행시에 위에서 생성한 네트워크를 추가한다.# 같은 네트워크 상에 mysql이 떠있기 때문에 바로 연결할 수 있다.# 앞서 --name 옵션으로 mysql이라고이름을 지정해주었기 때문에 이름으로 접근을 할 수 있다.$ docker run -d -p 8080:80 \\ --network=app-network -e WORDPRESS_DB_HOST=mysql \\ -e WORDPRESS_DB_NAME=wp \\ -e WORDPRESS_DB_USER=wp \\ -e WORDPRESS_DB_PASSWORD=wp \\ wordpress docker-compose 명령위의 docker의 기본 명령어를 입력할때에 띄어쓰기 등의 입력이 편하지는 않기 때문에 이러한 불편함을 개선하고자 docker-compose 명령이 생겨났다. 기본적으로 Mac에서 docker를 설치하면, docker-compose는 설치가 된다. 기존의 terminal에서 입력하였던 명령들을 docker-compose.yml 파일(띄어쓰기로 데이터를 표현하는 방식)에 작성하여 관리할 수 있다. 12345678910111213141516171819202122version: '2'services: db: image: mysql:5.7 volume: - ./mysql:/var/lib/mysql restart: always environment: MYSQL_ROO_PASSWORD: wordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: image: wordpress:latest volumes: - ./wp:/var/www/html ports: - '8000:80' restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_PASSWORD: wordpress restart는 container가 죽게 되면, docker가 자동으로 다시 띄워주도록 주는 옵션이다. 위와같이 작성된 docker-compose.yml파일은 아래의 명령어들을 사용해서 손쉽게 실행시킬 수 있다. docker-compose 기본 명령어 (1) up : docker compose를 이용하여 정의된 컨테이너 실행 1$docker-compose up -d (2) down : docker compose를 이용하여 실행중인 컨테이너 종료하고 삭제 1$docker-compose down (3) start : docker compose를 이용하여 중지상태인 컨테이너 시작 12$docker-compose start #멈춘 모든 컨테이너를 재시작$docker-compose start wordpress #지정한 특정 컨테이너만 시작 (4) restart : docker compose를 이용하여 실행중인 컨테이너 재시작 (5) stop : docker compose를 이용하여 실행중인 컨테이너 중지 (6) logs : docker compose를 이용하여 실행중인 컨테이너의 로그 출력-f 옵션으로 로그 follow 가능 (7) ps : docker compose를 이용하여 실행중인 컨테이너의 목록 출력 (8) exec : docker compose를 이용하여 실행중인 컨테이너에서 명령어 실행 123$docker-compose exec {container name} {command}$docker-compose exec wordpress bash (9) build : docker 컨테이너에 정의된 build 부분의 내용만 빌드(단, image 대신 build로 선언된 컨테이너만 빌드된다) 123$docker-compose build # build로 정의된 container(s) build$docker-compose build wordpress #wordpress container만 build docker compose 기본 문법 version : docker compose에 정의된 버전에 따라 지원하는 docker engine의 버전도 달라진다. services : 실행할 컨테이너를 정의한다. ($docker run –name [name] 명령과 동일) image : 컨테이너에 사용할 이미지 이름과 태그를 명기하고, 태그를 생략하면 latest, 이미지가 없으면 자동으로 pull(설치)한다. ports : 컨테이너와 연결할 localhost의 포트를 정의한다.{host port:container port}(-p 옵션과 동일) environment : 컨테이너에서 사용할 환경변수들을 정의한다. {환경변수 이름}:{값} volumes : mount하려는 디렉토리(들) {호스트 디렉토리}:{컨테이너 디렉토리} restart : 컨테이너를 재시작한다. restart의 옵션으로는 no, always, on-failure, unless-stopped가 있다. build : image 속성 대신에 사용하며, 이미지를 자체 빌드한 후에 사용한다. build 부분에서 정의할 dockerfile이 별도로 필요하다.image 정의대신에 build로 선언을 한 경우, $docker-compose up --build명령을 사용해서 도커 이미지를 다시 빌드하고 container를 실행할 수 있다. 1234django: build: context: . dockerfile: ./compose/django/Dockerfile-dev (실습1) /bin/sh 실행하기123# ubuntu:2.04 container를 실행함과 동시에 /bin/sh 쉘 프롬프트를 띄운다. (terminal상에서 command 입력을 위해서 -it옵션 추가)# container가 종료되면 자동으로 제거되도록 한다.$docker run --rm -it ubuntu:2.04 /bin/sh (실습2) 워드 프레스 블로그 실행하기wordpress를 실행하기 전에 MySQL을 우선 실행한다. 1234567891011$docker run -d -p 3306:3306 \\ -e MYSQL_ALLOW_EMPTY_PASSWORD=true\\ --name mysql \\ mysql:5.7# 실행된 mysql container에 mysql 명령을 실행한다.$docker exec -it mysql mysql$create database wp CHARACTER SET utf8;$grant all privileges on wp.* to wp@'%' identified by 'wp';$flush privileges;$quit 1234567# MySQL을 실행한 상태에서 워드 프레스 컨테이너를 실행한다.$docker run -d -p 8080:80 \\ -e WORDPRESS_DB_HOST=host.docker.internal \\ -e WORDPRESS_DB_NAME=wp \\ -e WORDPRESS_DB_USER=wp \\ -e WORDPRESS_DB_PASSWORD=wp \\ wordpress (실습3) hashicorp 서버 실습pc의 5678 포트가 hashicorp/http-echo 서버 1$docker run --rm -p 5678:5678 hashicorp/http-echo -text=&quot;hello world&quot; curl : 브라우저처럼 http에 접속을 해서 결과를 받아오는 명령어 1$ curl localhost:5678 #요청을 보내서 결과 출력 (실습4) Redis 실행 실습메모리 기반 데이터 베이스인 Redis를 docker 명령으로 간단하게 올릴 수 있다. (port만 다르게 해서 복수 개의 Redis를 동일 서버에 올릴 수 있다) 12345678910$docker run --rm -p 1234:6379 redis$telnet localhost 1234$set hello world$get hello#$5#world$quit","link":"/2022/06/01/202206/220601_docker_study/"},{"title":"220603 Docker 스터디 3일차","text":"이번 포스팅에서는 docker 실습한 내용과 docker image를 만들고, 생성한 이미지를 docker hub에서 관리하고 배포하는 부분에 대해서 정리해보려고 한다. 실습1) nginx 컨테이너 만들기12345# local에서 생성한 index.html파일을 생성할 container 내의 index.html 파일로 -v 옵션으로 생성한다.$ docker run -d --rm \\ -p 5000:80 -v $(pwd)/index.html:/usr/share/nginx/html/index.html \\ nginx -v 에서 $(pwd)를 통해 현재 위치 정보를 입력할 수 있다는 것을 배웠고, local에서 생성한 파일을 -v 옵션을 사용해서 container 내의 파일을 mapping 시킬 수 있다는 것도 다시 한 번 정리할 수 있었다. 실습2) php 컨테이너 실행하기1234$ docker run --rm \\ -v $(pwd)/hello.php:/app/hello.php \\ php:7 \\ php /app/hello.php php 컨테이너에서 /app 하위에 hello.php 파일을 mapping 시키고, 실행시키고자 하는 service의 버전을 지정한 뒤에 바로 php 명령을 사용해서 mapping시킨 컨테이너 내의 php 파일을 실행시킬 수 있다. Docker 이미지 만들기도커는 레이어드 파일 시스템을 기반으로 한다. 이미지의 구성 방식은 AUFS, BTRFS, Overlayfs 등의 파일시스템을 사용해서 구성된다.(layered된 형태로 이미지 생성) ref.(1) AUFS(Advanced multi layered Unification FileSystem) : 리눅스 파일 시스템의 union mount를 구현하기 위해서 시작한 프로젝트 (2) BTRFS : B-tree 파일 시스템 또는 배더 파일 시스템 (3) Overlayfs : Union mount를 지원하는 파일 시스템 중 하나로, AUFS가 그 중 하나이다. AUFS는 리눅스 커널의 Mainline에 포함되어 있지 않지만, Overlayfs는 2010년에 개발된 Union mount fs으로, 2014년에 리눅스 커널 Mainline에 통합되었다.(AUFS는 소스코드가 읽기 힘들고 주석이 없다는 이유로 리눅스 커널에 통합되지 않았다고 한다.) 도커의 이미지는 프로세스가 실행되면, 파일이 하나 하나 생성이 되는데, 이런 파일들이 이미지에 하나씩 차곡 차곡 쌓인다. 이렇게 차곡 차곡 쌓인 파일들이 새로운 이미지를 생성한다. 이전까지는 누군가 만든 이미지를 가져다가 사용했는데, 직접 이미지를 만들어서 사용할 수 있다. Docker 이미지이미지에는 읽기 전용과 쓰기 전용이 있다. 이전에 사용했던 다른 사람들이 만든 이미지는 Base Image(rootfs + Base Image)라는 수정이 불가능한 영역이 존재했고, 사용자는 새로운 기능을 추가/수정/삭제함으로써 또 다른 형태의 이미지를 생성 할 수 있다. (1) Base Image를 실행 -&gt; (2) 실행된 Container 내에 응용프로그램 설치 -&gt; (3) docker commit 명령을 통해 Custom Image를 생성 1$docker commit git ubuntu:git 이름이 git인 이미지를 ubuntu:git(태그)으로 새로운 이미지로 commit한다. 새로운 docker image 생성하기123#docker build -t {image name : image tag} {build context}$docker build -t hyungi(이름)/ubuntu(이미지 이름):git01(태그) .(빌드 컨텍스트) 한 번에 성공하는 빌드는 없기 때문에 빌드 성공이 될 때까지 많은 시행착오를 경험하게 된다. 빌드가 성공해도 리팩토링을 통해 더욱 최적화된 이미지를 생성해야 한다. Build context인 Dockerfile의 작성 규칙 FROM : 기본 이미지 RUN : 쉘 명령어 실행 CMD : 컨테이너 기본 실행 명령어 (Entrhypoint의 인자로 사용) EXPOSE : 오픈되는 포트 지정 ENV : 환경변수 설정 ADD : 파일 또는 디렉토리 추가. URL/ZIP 사용가능 COPY : 파일 또는 디렉토리를 복사해서 추가 ENTRYPOINT : 컨테이너 기본 실행 명령어 VOLUME : 외부 마운트 포인트 생성 USER : RUN, CMD, ENTRYPOINT를 실행하는 사용자 WORKDIR : 작업 디렉토리 설정 (cd 명령과 유사) ARGS : 빌드타임 환경변수 설정 LABEL : key - value 데이터 ONBUILD : 다른 빌드의 베이스로 사용될때 사용하는 명령어 123456789101112131415# node image 사용FROM node:12-alpine # 이미 만들어진 base 이미지를 사용하고, *-alpine은 only node 패키지만 설치된 이미지이므로 용량이 95MB로 작다.(full package: 900MB 이상)# 패키지 우선 복사 (변경된 파일이 포함되어있으면, cache가 되지 않기 때문에 변화가 없는 /package 파일을 우선적으로 url/src/app에 복사해서 cache를 사용하도록 한다)COPY ./package* /usr/src/app/WORKDIR /usr/src/app# cache된 package* file을 읽어서 npm package 설치RUN npm install# 변경된 소스코드를 포함한 소스를 복사한다. (cache 되지 않은 영역)COPY . /usr/src/app# WEB 서버를 실행한다. (Listen port setup)EXPOSE 3000CMD node app.js docker build 옵션 -f : 다른 위치의 Dockerfile 빌드 가능 -t 명령어로 Docker image 이름을 지정 가능 {namespace}/{image name}:{tag} 포멧으로 이름 작성 빌드 컨텍스트가 위치한 디렉토리를 {빌드 컨텍스트}에 작성 .dockerignore .gitignore과 비슷한 역할을 한다. Dockerfile이 빌드될때 .dockerignore에 작성된 패턴의 파일은 무시한다. Dockerfile / Build context 123FROM ubuntu:latestRUN apt-get updateRUN apt-get install -y git Dockerfile build 후 새롭게 생성된 git-dockerfile 이름의 이미지를 확인 12345$docker build -t ubuntu:git-dockerfile .$docker images | grep ubuntu# 생성한 ubuntu:git-dockerfile 이미지를 git3 이름의 컨테이너로 실행하고, 동시에 git 명령어를 실행한다.(-it option)$docker run -it -name git3 ubuntu:git-dockerfile git Docker 이미지 저장소여지까지 생성한 Docker 이미지 파일은 모두 내 PC에 있기 때문에 다른 사람들이 쓰게 하기 위해서는 dockerhub라는 곳에 저장해야 한다. 123$docker login$docker push {ID}/example # docker hub로 이미지 올리기docker pull {ID}/example # docker hub로부터 이미지 가져오기 Docker hub는 무료 계정의 경우, 한 개까지만 비공개이고, 그 이상은 전체공개가 되기때문에 주의해서 사용하도록 한다. (유료계정은 모두 비공개) 배포하기컨테이너 실행이 곧 배포이다. (이미지 PULL + 컨테이너 START) 1$docker run -d -p 3000:3000 hyungi/app","link":"/2022/06/03/202206/220603_docker_study/"},{"title":"220604 데이터 파이프라인 구축 오프라인 수업 &#x2F; 2주차 오프라인 수업 전 준비","text":"이번 포스팅에서는 내일 있을 데이터 파이프라인 구축 관련 수업에서 실습할 내용들에 대해 미리 실습을 해보고, 개념적인 부분을 좀 다져보려고 한다.미리 실습을 해보면서 혼자서 해보는 과정에서 생기는 의문이나 질문거리를 좀 정리해서 내일 수업시간에 적극 질문해봐야겠다. 이전 수업 실습내용우선 이전 실습에서는 ubuntu로 EC2 인스턴스를 하나 생성하고, EC2 인스턴스 내에 logstash와 filebeat를 설치하였다. (logstash가 Java 기반이기 때문에 JDK도 설치를 해주었다) 그리고 샘플로 받은 logstash *.conf 파일을 받아서 간단한 로그 파일을 생성해보았다. 딱 여기까지 실습을 하고 마무리 하였는데, 추가적으로 하면 좋을 부분이 있었다. 버전이 바뀌더라도 일관되게 logstash 명령을 사용하기 위해서 ln -s로 압축을 해제한 폴더와 logstash 명령을 linking해주는 추가적인 작업을 하면 좋을 것 같았고, 어느 곳에서나 logstash 명령만으로 사용할 수 있도록 .profile 파일에 logstash의 bin 폴더의 경로와 관련된 환경변수를 선언해주는 것이 좋을 것 같다고 생각했다. 1234567$ln -s logstash-7.4.0 logstash$vi ~/.profile# export LS_HOME=/home/ubuntu/logstash# PATH=$PATH:$LS_HOME/bin$source ~/.bash_profile 위의 부분에 대해서도 미리 실습을 하면서 적용을 해 볼 것이다. 다음 시간 실습내용2주차 수업에서는 이미 생성된 로그파일을 input의 file 플러그인을 사용해서 읽어서 output의 file 플러그인으로 새로운 파일로 추출해낼 것이다. 그리고 filter 플러그인을 사용해서 읽어들인 로그 데이터를 정제(filter 플러그인)해서 내보내는 방법에 대해서도 실습을 하고, 최종적으로 filebeat와 logstash를 연동해보는 실습까지 해 볼 예정이다. file 플러그인에 대해서 구체적으로 알아보기 start_positionfile 플러그인에서 start_position은 “beginning”, “end”로 값을 설정하는데, “beginning”은 말 그대로 데이터를 처음부터 읽어들이는 방식이고, “end”는 가장 최근부터 읽어 들이는 것을 말한다. 데이터 유실이 없이 데이터를 읽기 위해서는 “beginning”으로 설정해서 데이터를 읽어야 한다. (default value : end) sincedblogstash의 input file 플러그인을 사용하게 되면, sincedb 파일에 어디까지 데이터를 읽었는지 정보를 기록하게 된다. sincedb 파일들은 총 네 개의 칼럼으로 구성이 되어있는데, inode number, The major device number, The minor device number, The current byte offset(파일 내) 별도로 sincedb_path에 대한 설정을 하지 않았을 경우, user home 디렉토리를 확인해보거나 logstash 폴더/data/plugins/inputs/file/ 안을 확인해보면 된다.input파일을 여러개 등록해두면, sincedb가 여러개 생성이 된다.참고 소스코드(filewatch) : logstash 폴더/vendor/bundle/jruby/1.9/gems/filewatch-0.9.0/lib/filewatch input의 file 플러그인으로 읽고 있는 log 파일 자체의 변경이 발생하면, 이미 생성되어있는 sincedb에 새로운 inode 값과 offset 정보가 새로운 행 정보로써 추가된다. (예시) 파일 삭제 후 재 생성시) 12#pick SINCEDB_DIR if available, otherwise use HOMEsincedb_dir = ENV[&quot;SINCEDB_DIR&quot;] || ENV[&quot;HOME&quot;] filter 플러그인 테스트 filter 플러그인 중에 grok, prune, mutate가 있다. grokgrok은 (심적으로)이해하다, 공감하다라는 의미를 가졌다. 이런 의미와는 전혀 관계없이 grok이라는 필터 플러그인의 기능은 문자열 포멧 개념으로 이해 할 수 있다. input을 통해 들어온 로그 데이터 정보가 KEY:HI VALUE:100라는 문자열을 가지고, filter의 grok 플러그인은 아래와 같이 정의된다고 하자. 123456789101112131415input { .....}filter { grok { match =&gt; { &quot;message&quot; =&gt; &quot;KEY\\:%{WORD:key_field} VALUE\\:%{WORD:value_field}&quot; } }}output { stdout { codec =&gt; rubydebug }} grok의 패턴은 파이썬과 달리 아래와 같은 필터 규칙을 따른다. 123# ${정규 패턴명:사용자 정의 필드}`# KEY 이후에 오는 단어 하나를 key_field라는 사용자 정의 필드에 저장한다.KEY\\:%{WORD:key_field} VALUE\\:%{WORD:value_field} 패턴을 사용해서 로그를 분석할 수도 있지만, 패턴으로 표현하기에 복잡한 로그들의 경우에는 패턴이 아닌 정규 표현식을 사용한다. 12345678# ?&lt;필드명&gt;정규표현식# KEY에 적용된 정규표현식 w+은 값이 단어로, 한 단어 이상이라는 의미를 가지며,# VALUE에 적용된 정규표현식 d+는 숫자이면서 정수 전체라는 의미를 가진다.grok { match =&gt; { &quot;message&quot; =&gt; &quot;KEY\\:(?&lt;key_field&gt;\\w+) VALUE\\:(?&lt;value_field&gt;\\d+)&quot; }} grok 패턴을 이해하기 위해서는 정규 표현식을 정확히 알아야 어떤 패턴의 데이터인지 구분할 수 있기 때문에 패턴 사용보다는 정규 표현식으로만 해보는 것이 좋다고 한다. grok의 정규표현식 예시 12345678910111213# \\w : 한 글자# \\w+ : 한 단어# \\d : 정수 한 자리# \\d+ : 정수 전체# \\s : 공백 한 칸# \\s+ : 공백 전체# \\특수문자 : 특수문자 구분grok {match =&gt; { &quot;message&quot; =&gt; &quot;\\[(?&lt;year&gt;\\d+)\\-(?&lt;month&gt;\\d+)\\-(?&lt;day&gt;\\d+)T(?&lt;hour&gt;\\d+)\\:(?&lt;minute&gt;\\d+)\\:(?&lt;second&gt;\\d+)\\,(?&lt;millisecond&gt;\\d+)\\]\\[(?&lt;log_level&gt;\\w+\\s+)\\]\\[(?&lt;log_kind&gt;\\w+\\.\\w+\\s+)\\]\\[(?&lt;log_class&gt;\\w+)\\] %{GREEDYDATA:log_info}&quot; }} prune(구체적인 활용방법 찾아서 정리해보기)prune은 가지치기하다, 부분을 없애서 축소하다는 의미를 가졌다. interpolate(참견하다, 덧붙이다)와 whitelist_names 라는 옵션을 가진다. mutate(구체적인 활용방법 찾아서 정리해보기)mutate는 “새로운 형태로 변형되다”라는 의미를 가졌다. remove_field라는 옵션을 가지며, remove_field 옵션의 값으로 제거 할 필드의 이름을 배열로 선언한다. filebeat-logstash 연동 실습여러 타입들이 있는데, 그 중 파일(input.file)을 읽기 위해 filestream을 type의 값으로 넣어준다. path(기본경로)로 지정한 위치의 input.file을 읽어서 해당 파일의 내용을 logstash로 전달하도록 output.logstash의 hosts 정보에 5044 포트번호를 listen하도록 한다. (command의 실행 순서는 logstash 다음에 filebeat를 실행해야 한다. 우선 logstash가 filebeat를 listen하고 있는 상태를 유지한 상태에서 filebeat가 5044번 포트로 로그 데이터를 쏴준다) [logstash-config-filebeat.conf]우선 logstash는 5044번 포트를 listen하고 있는 상태이다. 1234567891011input { beats { port =&gt; 5044 }}output { stdout { codec =&gt; line { format =&gt; &quot;From filebeat : %{message}&quot;} }} [filebeat.yml]filebeat를 run하기 위해서는 *.yml 파일이 필요하다.chmod go-w ﬁlebeat.yml는 그룹 및 기타 사용자(group/others)에 대한 쓰기 권한 거부 (오직 소유자만 수정 할 수 있도록 설정)filebeat는 logstash가 listen하고 있는 5044 포트로 읽은 log data를 넘겨준다. 12345678filebeat.inputs: - type: filestream enabled: true paths: - /home/ubuntu/input.fileoutput.logstash: hosts: ['localhost:5044'] 과정 : input.file -&gt; beats -&gt; logstash -&gt; output 실습 과정 중 알게 된 내용들filebeat의 filebeat.inputs의 path에 input.file을 넣어주게 되면, 임의로 &quot;echo &quot;1&quot; &gt;&gt; input.file&quot;명령으로 생성해준 로그파일의 데이터를 읽어서 넘겨준다. output의 file 플러그인에서 path 속성에 경로와 함께 파일명을 지정해주면, 아래 codec에 line 내에 지정한 format 텍스트가 새로 지정한 로그 파일 안의 내용으로 들어간다.(단, 동 시간에 복수의 로그파일을 생성할 경우, 동일한 로그파일에 중복되서 데이터가 누적될 수 있기 때문에 삭제를 하고 다시 로그파일을 생성하는 것이 좋다)","link":"/2022/06/04/202206/220604_datapipeline_study/"},{"title":"220604 Docker 스터디 4일차","text":"실습1) Nginx를 이용한 정적 페이지 서버 만들기 Docker 이미지 생성하기(Dockerfile)123FROM nginxCOPY ./index.html /user/share/nginx/html/index.htmlEXPOSE 80 생성한 이미지를 활용해서 배포하기1234$docker build -t hyungi/test1 .$docker run -d --rm \\ -p 50000:80 \\ hyungi/test1 실습2) NodeJS 어플리케이션 이미지 빌드 및 배포 (1) 우선 디렉토리 내에 NodeJS 프로젝트를 구성한다. (server.js 및 package.json) (2) 이미지 생성을 위해 Dockerfile 작성한다. 1234567FROM node:12-alpineCOPY ./package* /usr/src/app/WORKDIR /usr/src/appRUN npm installCOPY . /usr/src/appEXPOSE 8080CMD [&quot;node&quot;, &quot;/app/server.js&quot;] (3) .dockerignore 파일을 작성해서 이미지 빌드시에 node_modules 폴더가 빌드되지 않도록 한다..dockerignore 1node_modules/ (4) 작성한 Dockerfile로 이미지를 빌드한다. 1234$docker build -t hellonode .$docker run --rm -d \\ -p 60000:8080 \\ hellonode Docker 추가 학습여지까지 가장 기본적이면서 핵심적인 도커에 관한 내용에 대해서 학습하였다. 그런데 가장 기초적인 부분을 배운 것이기 때문에 아직 익숙하지 않은 부분도 많아서 실습이 필요할 듯 하다.기초적인 부분이 실습이 끝나면, 아래의 내용에 대해서도 추가적으로 공부하도록 하다. 이미지를 만들기 위한 다양한 쉘 스크립트와 환경변수를 사용 CI/CD 자동빌드와 자동배포, BLUE &amp; GREEN 배포 / 무중단 배포 (컨테이너는 무조건 죽였다가 띄워야하는데, 무중단으로 배포도 가능하다) 모니터링, 로그 가상 네트워크 보안 쿠버네티스(Kubernetes) 이스티오(istio) 서비스매시","link":"/2022/06/04/202206/220604_docker_study/"},{"title":"220605 데이터 파이프라인 구축 오프라인 수업 &#x2F; 2주차","text":"이번 포스팅에서는 두 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. 2주차 첫 번째 실습) logstash의 input/output의 file 플러그인 활용가장 일반적으로 많이 쓰이는 방식으로, 실제 VM에 서버를 띄우고 해당 로그를 취득하거나 도커나 쿠버네티스에 컨테이너에서 서버를 띄우고 로그를 취득하는 경우에도 logstash의 input의 file 플러그인을 사용해서 로그 데이터를 땡겨온다. VM이나 컨테이너의 특정 경로에 로그 데이터를 적재해주고, 시스템 자체로는 Log rotate라고 하는 방식으로 오늘 날짜 기준으로 로그를 쌓다가 날짜가 바뀌면, 다른이름으로 압축을 하거나 삭제를 해서 새로운 로그를 다시 쌓는 방식으로 한다. 이러한 작업에 logstash와 같은 agent를 통해서 밖으로 빼내는 작업을 하는데, 이때 input의 file 플러그인을 가장 많이 사용한다고 한다. 이전에 logstash를 사용해서 단순히 twitter의 로그 데이터만 뽑아서 kafka에 넘겨주는 작업을 했을때에는 logstash에 다양한 플러그인에 대해서 잘 몰랐고, logstash의 단편적인 기능만 알았었는데, 이번 실습을 통해서 logstash라는 agent의 input/output의 플러그인은 종류가 엄청 다양하고, 실무에서는 어떤식으로 이 logstash를 사용해서 서버로부터 로그 데이터를 땡겨서 적재하는지에 대해서 개괄적으로 알게 된 것 같다. 이번 실습에서는 외부에서 로그 데이터가 쌓이는 과정을 임의의 &quot;echo &quot;1&quot; &gt;&gt; input.file명령을 통해서 새로운 input.file 로그 파일을 생성하고, 임의의 명령을 통해 로그 파일(input.file)에 데이터가 적재될때마다 logstash의 output의 file 플러그인의 path에 지정된 파일로 새로운 형태의 데이터가 적재되는 과정에 대해서 실습해보았다. 1234567891011121314151617181920212223242526$logstash -f logstash/config/logstash-config-file.conf# logstash가 input의 file 플러그인을 사용해서 input.file 로그 파일을 listen한다.input { file { path =&gt; &quot;/home/ubuntu/input.file&quot; start_position =&gt; &quot;beginning&quot; }}# 다양한 플러그인을 사용해서 로그 데이터를 S3에 직접 저장을 하거나 할 수 있다.output { file { path =&gt; &quot;/home/ubuntu/logstash-output-%{+YYYY_MM_dd_HH}.log&quot; # 위에 지정한 새로운 로그파일에 &quot;output: 1&quot; 형태로 데이터가 전처리되어 쌓인다. codec =&gt; line { format =&gt; &quot;output: %{message}&quot;} }}# echo 명령을 통해서 input.file에 임의의 로그 파일을 쌓으면, 자동으로 output의 file플러그인에 지정한 파일에 정제된 데이터가 저장된다.#input.file 내의 데이터가 업데이트되면, logstash에서는 output 파일에 지속적으로 업데이트를 한다.#실제로는 어플리케이션으로부터 로그 데이터가 넘겨져서 떨궈지면, 해당 로그 데이터를 읽어서 처리하는데, 현재 실습에서는 임의로 데이터를 쌓기 위한 작업이 추가되었다.$echo &quot;1&quot; &gt;&gt; input.file$echo &quot;2&quot; &gt;&gt; input.file$echo &quot;3&quot; &gt;&gt; input.file 두 번째 실습) 실제 Apache sample log파일을 활용한 실습week-1의 apache-sample-log 파일의 샘플 로그 데이터로 실습을 진행하였다. grok 플러그인은 들어온 로그데이터를 parsing하는 formatter역할을 한다. 문자열 한 줄로 출력되던 로그 데이터를 json 형식으로 parsing 되는 것을 확인할 수 있었고, Apache log의 경우에는 보편적으로 사용되기 때문에 grok으로 filter하면 자동으로 json 형식으로 parsing된다. 123456789101112131415161718192021222324252627282930313233input { file { path =&gt; &quot;/home/ubuntu/[log file]&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;/dev/null&quot; }}filter {// 결과로 문자열로 출력된 데이터를 json 형태의 데이터로 변환해준다. grok { match =&gt; {&quot;message&quot; =&gt; &quot;%{COMBINEDAPACHELOG}&quot;} } // grok으로 parsing된 json 형식의 데이터를 prune을 활용해서 특정 필드의 데이터만 추출한다. // 용량이 큰 경우나 분석가가 원하는 데이터만 추려서 제한된 데이터로 만들때 사용된다. // 반드시 위의 grok을 한 뒤에 prune으로 처리해야한다. prune { interpolate =&gt; true // clientip와 timestamp field data만 추려서 로그 파일 출력 whitelist_names =&gt; [&quot;clientip&quot;, &quot;@timestamp&quot;] }# mutate {# remove_field =&gt; [&quot;referrer&quot;, &quot;path&quot;, &quot;agent&quot;, &quot;message&quot;]# }}output { file { path =&gt; &quot;/home/ubuntu/logstash-output-%{+YYYY_MM_dd_HH}.log&quot; codec =&gt; rubydebug }} 아래 메뉴얼에서 filter plugins을 확인하고 개인적으로 실습을 해보자. https://www.elastic.co/guide/en/logstash/current/filter-plugins.html 유용한 플러그인으로는 IP주소를 CIDR 표기법(ip/(서브넷 bit 수))로 변환해 줄 수 있는 cidr플러그인도있으며, csv플러그인을 사용해서 comma-separated value 데이터를 개별 필드로 분류해서 뽑아낼 수도 있고, kv 필터 플러그인을 사용해서 key-value 페어로 로그 데이터를 parsing 할 수도 있다. (이 부분에 대해서는 차후에 개인 프로젝트에 적용해서 실습을 해보도록 하자) 세 번째 실습) Beats - Logstash 연동 실습Beats는 Logstash의 앞 단에 경량화된 데이터 수집기를 심기 위한 목적에서 사용한다. (input.file 로그 파일을 Beats가 읽어서 Logstash로 넘겨준다.) 현재 실습으로 제공된 파일에서 절대경로로 filebeat가 실행이 되지 않기 때문에 우선적으로 filebreat.yml파일을 ec2 instance의 local의 filebeat 압축해제 폴더에 복사를 한 뒤에 상대경로로 해서 filebeat를 실행해서 실습하도록 한다. 12345#다른 ec2 instance에서는 logstash를 실행한다.$logstash -f ...../logstash-config-filebeat.conf#우선 git repository의 week-1 폴더에 있는 filebeat.yml 파일을 local의 filebeat 폴더에 복사를 해둔다.$./filebeat run -e filebeat.yml logstash 로그에서 확인을 해보면, filebeat로부터 넘겨받은 로그데이터 목록을 순차적으로 받는 것을 확인할 수 있다.(filebeat가 listen하고 있는 input.file 로그 파일에 echo 명령을 사용해서 새로운 로그 데이터를 적재해서 변화를 줘야 filebeat에서 이를 감지하고, 변화된 로그 데이터를 읽어서 logstash로 넘겨준다) filebeat - logstash - [output plugin 사용해서 S3에 저장을 할 수도 있다] (응용해서 실습해보기) filebeat - logstash 구조로 실행을 할 때, background에서 계속 실행이 될 수 있도록 할 수도 있고, 개별 node에 설치를 할 수도 있다.logstash 전용 노드를 설치를 해서 사용할 수 있는 부분에 대해서도 직접 실습을 해보자.(추가 자료 참고) 이 부분부터는 2주차 본 수업에서 다룬 데이터 파이프라인의 이론적인 부분에 대한 정리이다. 용어에 대한 설명 Data Lake최근들어 클라우드의 도입으로, 대용량 분산 스토리지 개념이 많이 발달하였다. 따라서 정형과 비정형 데이터(사진, 동영상, json 파일(반정형)를 구분하지 않고 우선적으로 적재를 한 뒤에 필요에 따라 데이터를 추출해서 일부 데이터를 추출하고, 정제한다.(정형화나 정규화가 되어있지 않은 데이터 스토리지, 분산 스토리지(S3, HDFS 등을 사용))DL는 테이블이 아니며, 별도의 스키마가 없고 schema on read(스키마를 읽을 때 스키마가 결정)의 형태를 갖는다. 그리고 ELT(Extract Load Transform)의 중간 저장소 역할을 한다.ETL에서는 DB에 정제된 데이터를 DB에 로드할 때 각 각의 데이터의 스키마가 정해져야 문제없이 데이터가 적제될 수 있기 때문에 우선적으로 데이터가 정제되고, DB에 데이터를 쌓는다. Data Warehouse실제 의사결정을 위해서 소스 DB(사용자 정보, 거래정보)를 분석하고, 구조화된 형식으로 저장하는 것이 Data Warehouse이다. (다양한 소스로부터 필요한 데이터를 수정/정제해서 집계하는 저장공간)ETL 과정을 통해서 분석가능하고 구조화된 데이터로 저장하는 저장소대량의 데이터를 장기 보관(3개월 ~ 2년) (부서 및 기업별로 분류하여 데이터를 체계적으로 정리한다)원천 데이터를 가져와서 데이터를 테이블화하는 것을 말하며, 테이블화를 할 수 있다는 의미는 key:value 형태로 데이터가 정형화되어있다는 의미이다.잦은 읽기 쓰기의 RDB를 쓰는 것이 아닌, 웨어하우스 솔루션이 많이 쓰인다. (일반적인 RDB와는 다른 구조를 갖지만, 때때로 작은 규모의 데이터에서는 RDB로 대체해서 사용하기도 한다)보통 데이터 웨어하우스 솔루션은 같은 칼럼의 데이터만 묶어서 관리한다. (대량의 배치작업을 위한 처리)보통은 각 각의 칼럼 데이터가 하나의 행으로써 관리가 되는데, 웨어하우스 솔루션에서는 칼럼 데이터로 묶여서 관리된다.ref. columnar : 데이터 베이스의 각 칼럼 데이터를 별도로 분리해서 관리한다. 그 이유는 대량의 데이터 쓰기 작업이 발생하기 때문 Data Mart데이터 웨어하우스가 중앙 집중식으로 여러 분야의 데이터를 가지고 있다면, 데이터마트는 특정 분야의 데이터만 따로 분류해서 구성한다.(Domain specific data는 DW에 비해 소규모화된 데이터) 각 부서에서 필요로하는 도메인이 확실한 데이터이고, 데이터 웨어하우스보다는 용량이 작기 때문에 (특정 도메인의 데이터를 가져와서 사용하기 때문에) RDS의 사용이 가능하다.그리고 Tableau와 같은 시각화 도구를 직접 DM에 붙여서 사용하기도 하며, OLAP(Online Analysis Processing) 작업을 위해서 BI 툴을 붙여서 사용한다. ETL 과정을 통해 데이터 웨어하우스의 데이터를 도메인 세분화된 데이터 마트의 데이터로 분류를 한다. OLAP(Online Analytical Processing)온라인으로 분석한다는 의미로 데이터 마트에서 사용된다. 반대되는 개념으로 OLTP(RDB)이다. 사용자가 대화형 쿼리를 통해서 다차원의 데이터(OLAP CUBE)들을 분석하고, 여러 테이블을 조인해서 분석하는 과정을 통해 의사결정에 데이터를 활용한다.주로 Columnar (Column Oriented DB)를 사용한다. Row oriented DB &amp; Column oriented DB(Columnar)Row Oriented DB와 Column Oriented DB의 개념은 데이터베이스의 내부 엔진의 동작방식상의 차이이다. 실제 사용자에게 있어서는 사용에 있어 차이가 없다. 특정 행을 검색/삭제/업데이트하는 과정에서 사용되는 DB를 Row Oriented DB하고 하며, 트랜젝션과 데이터의 무결성이 중요하다.반면에 데이터 웨어하우스에 많이 사용되는 데이터베이스는 Columnar(Column Oriented DB라고 한다)(Row Oriented DB - OLTP - RDB) 데이터 엔지니어 분야에서는 Column oriented DB(columnar)를 주로 사용한다. Columnar는 열별로 데이터를 저장하고, Row Oriented DB와는 달리 특정 행의 데이터를 삭제하는 것은 가능하나, 데이터를 업데이트하는데 비효율적이거나 불가능하다.Columar DB는 특정 칼럼의 데이터만 가져오거나 대량의 데이터에 대한 쿼리를 하는데 유용하다. (같은 자료형의 데이터 처리에 유용)그리고 대량의 데이터를 읽고 쓰는데 종종 쓰인다.(Column Oriented DB - OLAP - DW(Data Warehouse) 업무에서 배치처리가 70%이상 많은 비중을 차지하고 있다. Batch processing데이터의 일괄처리에 주로 사용되며, 데이터를 일정 기간동안 수집을 하고 나서 one slot으로 데이터를 처리하는 것을 말한다. (대용량의 데이터 처리에 사용)주로 연산량이 많은 ML(Machine Learning)/DL(Deep Learning)과 같이 CPU 집약적인 작업에 많이 사용되며, latency보다는 throughput(일정 시간내에 처리해야 되는 처리량)에 더 중요성이 부여된 작업의 경우 사용된다. 단점 불규칙한 데이터 사이즈(데이터 사이즈의 변화가 심하기 때문에 각 데이터 처리 section에 대해 작업 주기를 할당하기가 어렵다. (컴퓨팅 리소스와 배치 작업 주기의 산정의 어려움)) 비즈니스나 DS의 특정 요구사항에 따라 연산 처리 속도와 배치 주기를 다르게 하기 위해 별도의 리소스가 필요 데이터 처리 중에 문제가 발생해서 재처리하는 경우, 기존의 리소스를 재사용할지 아니면 재처리용 리소스를 별도로 할당할지에 대해 기존의 작업에 영향이 없는 선에서 결정해야 한다. 연속된 태스크를 스케줄링하는 작업이 태스크 스케줄링(워크플로 스케줄링)이다.(A Task -&gt; B Task if A Task is failed, C Task is executed와 같은 데이터 스케줄링 과정) 실시간 데이터의 처리가 증가하고 있지만, 여전히 데이터 처리에 있어서는 Batch 처리의 비중이 크다. Hadoop echo system하둡은 빅데이터 프레임워크로, 주로 배치 처리를 담당한다. HDFS, MapReduce, Spark 등의 Hadoop echo system을 구성하는 오픈소스 기술들을 사용한다.(AWS EMR을 사용하여 Hadoop의 역할을 대체할 수 있고, Hive의 역할은 Amazon Athena이다.(Query로 데이터를 분석)) 이 부분에 대해서는 기본 개념을 잡고, 실제 업무를 하면서 깊게 파고드는 것이 좋다. ETL &amp; ELT ETL(Extract Transform Load)주로 OLTP(Online Transaction Processing) DB를 source로 하는 ETL 파이프라인을 만들고 DW와 DM으로 저장한다. 그리고 주로 일괄 처리 방식에서 사용되며, 주로 정형 데이터를 처리한다. ref. Transform(=ﬁlter, sort, aggregate, join, dedup(중복제거), validate, missing value, etc) ETL의 단점 추출해야되는 데이터 스토어가 BE, DE의 영역으로 분류되어 DS나 BI 담당자가 원하는 데이터를 얻기 어렵다. DE가 ETL 작업을 해줘야 원하는 dataset을 얻을 수 있다. (민첩성 결여) 기존 DB 구조에서 크게 벗어나지 못하기 때문에 스키마 종속적이고 제한적인 데이터 타입을 가지며, 쿼리 등의 성능 때문에 DB 모델링의 중요성이 크다. ETL보다 ELT인가?아니다. 개인정보, 보안, 암호화 등의 요구사항에 따라 데이터의 비식별 처리를 위해서는 ETL 방식이 필요하다.(전체 사용자 대상의 추천 시스템 구축) 그리고 Data outpost가 필요한 경우에는 ETL로 데이터를 뽑아서 DW, DM을 구축한다.따라서 ETL과 ELT를 시스템 및 비즈니스의 요구사항에 맞춰서 구축하는 것이 현명하다. Storage 구분Storage는 Block, File, Object Storage로 나뉜다. File Storage는 File system이 셋팅된 환경에 데이터를 저장하는 것을 말하며, 물리적 저장소에 Block 단위로 저장하는 것을 Block Storage, Object Storage는 구글 드라이브와 같은 저장소에 파일(Object) 단위로 데이터를 저장하는 것을 말한다. Amazon S3Amazon S3는 DL의 역할을 한다. 개발자들이 코드를 올려서 배포도 가능하며, Data Lake 역할 이외에도 다른 서비스와 연결되는 anchor point 역할을 해주기도 한다.다양한 스토리지 클래스를 나뉘고, 스토리지 내의 데이터 로그 정보, Compliance(규정 준수 및 감사 관련) 관련 정보도 지원한다. Amazon S3의 특징 Object storage 정형/비정형 데이터를 모두 저장할 수 있는 Data Lake 역할 무한대로 확장할 수 있다. 99.999999999% 데이터 내구성 중간(파일 캐싱, 코드 저장소 등 다양한 쓰임)/최종 저장소로 사용된다. AWS의 많은 서비스들을 엮어주는 허브의 역할 객체를 버저닝하고, 비용 효율을 위해서 특정 조건에 따라 스토리지 클래스를 변경하는 것이 자유롭니다.(예시. 30일 후에 Glacier로 저장하는 작업, 1년이 지난 후에 Glacier의 데이터를 업데이트) Amazon S3의 스토리지 클래스 Standard : 자주 읽고 쓰는 경우 사용 Intelligent-Tiering : 엑세스 빈도에 따라 비용 효율적인 티어로 자동 관리 Standard-IA : 자주 액세스하진 않지만,필요할 때 빠르게 액세스해야되는 데이터 One Zone-IA : 3개가 아닌 1개의 AZ에만 저장(Standard보다 20% 저렴) Glacier : 데이터 아카이브 (instant retrieval, flexible retrieval, deep archive 등) Amazon AthenaAmazon Athena는 S3의 데이터를 표준 SQL을 사용해서 분석할 수 있는 서버리스(별도의 infrastructure나 Capacity에 대한 설정없이 서비스만 이용) 대화식 쿼리 서비스(Interactive하게 데이터를 조회하면서 결과를 바로 학인 가능)이다. 비용 청구비용에 대한 청구는 정상적으로 SQL로 처리한 데이터의 양에 따라 청구된다. CTAS 쿼리 사용CTAS 쿼리를 사용해서 쿼리를 통해 나온 결과 데이터를 별도의 파일로 추출 할 수 있다. (이외에도 데이터 변환, 포멧 변환이 가능)포멧 변환은 CSV, JSON, ORC, Avro, Parquet 등 다양한 표준 데이터 형식과 호환된다. S3 &amp; Amazon Athena 실습 (1) 분석할 target log 데이터를 담을 디렉토리와 log정보를 담을 디렉토리를 위한 각 각의 디렉토리는 S3에 생성한다.파이프라인이 구축되면 지정된 디렉토리에 정기적으로 로그 데이터가 쌓인다. (2) Amazon Athena(Query editor) - [설정] - log를 떨궈줄 S3의 디렉토리 경로 위치정보를 업데이트해줘야 한다.이 설정을 안해주면 테이블 생성시에 에러가 발생한다. (3) Amazon Athena에 새로운 테이블 생성테이블과 DB 생성 - Amazon Athena 페이지에서 Tables and views의 우측 [Create] 클릭 - S3 bucket data 클릭 - Table명 입력, Database 선택, Dataset을 분석할 dataset이 위치한 S3 bucket을 선택한다.Data format은 JSON-Hive SerDe를 선택하고, Column details에서 각 칼럼의 이름과 타입을 분석할 타겟 로그 데이터를 참고하여 입력해준다.최종적으로 Preview table query를 통해 쿼리가 생성된 것을 확인 할 수 있으며, 최종적으로 Create table 클릭을 통해 테이블을 생성해야 한다. (4) CTAS 쿼리를 통해서 쿼리한 결과 데이터에 해당하는 칼럼만 가지는 새로운 테이블을 생성한다.1234567CREATE TABLE new_table1 WITH (external_location='s3://..../ctas/',format='PARQUET',parquet_compression='SNAPPY')AS SELECT time, remote_ip FROM “class&quot;.&quot;hg-data-table&quot;CREATE TABLE new_table2 WITH (external_location=‘s3://..../ctas-bucketing/',format='PARQUET',parquet_compression='SNAPPY',bucketed_by=ARRAY['time'], bucket_count=1)AS SELECT time, remote_ip FROM &quot;class&quot;.&quot;hg-data-table&quot; 디렉토리 내에 있는 전체 파일에 대해서 분석이 되기 때문에 요금은 디렉토리 내의 전체 파일에 대해서 계산을 해야한다. Q&amp;A section logstash와 filebeat의 실행 순서는 logstash가 listen된 상태에서 filebeat를 띄워야 순차적으로 처리가 되기 때문에 logstash를 우선적으로 실행한 상태에서 filebeat를 실행하도록 한다. JDBC와 같은 툴을 이용해서 다이렉트로 Athena에 로그 데이터를 날려서 처리를 할 수도 있다. (Athena와 JDBC와 연동을 해서 사용하는 방법 찾아보기) CTAS 쿼리를 통해서 나온 결과 데이터를 S3에 새로운 파일의 형태로 (parquet) 생성을 해주는데, 파일이 2개 생성되는 이유는 Athena 내부적으로 여러 engine들이 돌고 있기 때문이다. (테이블의 갯수와는 상관이 없음)","link":"/2022/06/05/202206/220605_datapipeline_study/"},{"title":"220606 AWS Certified Solutions Architect Associate Certificates (SAA-C02)","text":"이번 포스팅에서는 EC2 인스턴스의 전반적인 스토리지 옵션들에 대해서 알아본다. 스토리지 옵션에는 EBS, EC2 Instance store가 있으며, 각 각의 세부적인 특징에 대해서 알아본다. EBS(Elastic Block Storage)EBS는 EC2 인스턴스가 실행중인 동안 연결이 가능한 네트워크 드라이브이다. EBS 볼륨을 사용하면, 인스턴스가 종료된 후에도 데이터를 지속적으로 유지할 수 있다. 인스턴스를 재생성하고, 이전에 사용한 EBS 볼륨을 마운트하면, 데이터를 다시 받을 수 있다. 이전에 CCP자격시험을 준비할때에는 하나의 EBS는 하나의 EC2 인스턴스에만 마운트 할 수 있다고 배웠다. 그리고 하나의 인스턴스에는 다중 EBS 볼륨의 연결이 가능하다. EBS는 특정 가용 영역에 제한되서 사용할 수 있다. 따라서 EC2 인스턴스가 생성된 AZ과 EBS의 AZ를 매칭시켜줘야 한다. 단, EBS를 snapshot해주게 되면, 다른 AZ 영역의 EC2 인스턴스에 연결해줄 수 있다.(Free tier에서는 매달 30GB의 EBS 스토리지 를 범용 SSD나 마그네틱 유형으로 제공) EC2 인스턴스를 생성할때, Root EBS의 경우에만 default로 인스턴스 종료시에 삭제되도록 설정이 되어있다.추가적으로 생성한 EBS의 경우에는 Delete on Termination에 체크가 되어있지 않으며, 인스턴스가 종료되어도 해당 EBS는 삭제되지 않는다. (***시험에 출제될 가능성이 있는 시나리오) EBS 볼륨의 성능은 GB 및 IOPS(I/O Per Second/초당 전송 수)를 기준으로 하며, 이는 미리 정의를 해줘야 하며, 프로비전 용량에 따라 요금이 다르게 청구된다. 필요에 따라 차후에 용량을 늘릴 수 있다. EBS SnapshotEBS Snapshot을 사용하는 경우에는 두 가지 이유가 있다.첫 번째는 특정 시점에 EBS에 대한 백업본을 만들때 사용하며, 스냅샷을 생성하기 위해 EBS를 인스턴스로부터 detach할 필요는 없지만, detach하는 것이 권장된다.이렇게 EBS의 Snapshot을 생성하게 되면 AZ 또는 Region을 변경하여 복제할 수 있다. [실습] EBS SnapshotEC2 콘솔의 EBS Volume Interface에서 [Actions]-Create Snapshots를 선택해서 EBS 스냅샷을 생성한다. 생성된 스냅샷은 특정 AZ(가용영역)에 연결되어있지 않고, Region 내에서 사용할 수 있다.생성된 스냅샷을 Copy함으로써 해당 스냅1샷을 복사할 수 있다. 스냅샷을 복사하면, 새로운 스냅샷이 생성되는데, 이 스냅샷을 사용해서 어느 Region에든 위치시킬 수 있다. 스냅샷을 다른 Region으로 복사하고, 어플리케이션을 복원하거나 재해복구를 위해 데이터를 다른 곳에 보관해놓을 수 있다.스냅샷에 대해 볼륨을 생성할 수 있는데, ELASTIC BLOCK STORE의 하위에 Snapshots 메뉴에서 [Actions]-Create Volume을 선택해서 선택한 스냅샷에 대해 특정 region을 선택해서 볼륨을 생성할 수 있다.(COPY -&gt; CREATE VOLUME) AMI(Amazon Machine Image)AMI는 EC2 Instance의 기반이 된다. AMI는 AWS에서 생성한 AMI를 사용하거나, 사용자 지정 AMI를 만들 수 있다. 각자의 소프트웨어 구성에 대해 운영 체제를 정의 및 설정하여 자체적으로 AMI를 생성하면, 부팅과 구성에 시간이 단축된다. (AMI에 사전에 패키징)AMI를 구축하면, 별도의 EBS 스냅샷 또한 생성된다.Custom AMI의 경우에는 필요한 소프트웨어를 EC2 인스턴스에 설치하고, 1234# User data에 As text option으로 초기에 설치할 소프트웨어에 대한 명령어를 작성해준다.#!/bin/bash실행 명령어 작성 인스턴스 리스트에서 해당 인스턴스를 우측 클릭한 후, &quot;Image and Templates&quot;를 선택하후에 &quot;Create image&quot;를 선택한다.생성된 AMI는 새로운 EC2인스턴스를 생성할 때 My AMI를 선택해서 확인 할 수 있다. EC2 인스턴스 스토어앞서 살펴본 EBS와 같은 네트워크 드라이브는 성능 자체는 뛰어나지만, 이보다 더 높은 성능을 요구하는 경우에는 성능이 제한된다. 따라서 이러한 경우에는 EC2 인스턴스에 연결된 하드웨어 디스크 성능을 향상시켜야 한다.EC2 인스턴스는 가상머신이면서 실제 하드웨어 서버에 연결되어있다. (서버에 물리적으로 연결된 디스크 공간이 있음)따라서 특정 유형의 EC2 인스턴스는 EC2 인스턴스 스토어라고 불리고, 해당 서버에 연결된 하드웨어 드라이브를 가르킨다.EC2 인스턴스 스토어는 주로 I/O 성능을 향상시키기 위해 활용될 수 있고, 훌륭한 처리량을 갖고 있기 때문에 향상된 디스크 성능이 요구될 때 활용할 수 있도록 미리 확보할 필요가 있다. 주의할 점은 EC2 인스턴스(인스턴스 스토어)를 중지나 종료하면, 해당 스토리지 또한 손실이 된다. 이러한 이유로 EC2 인스턴스 스토어를 임시 스토리지라고 불린다.따라서 버퍼나 캐시, 스크래치 데이터 또는 임시 컨텐츠를 사용하는 경우에 유용하게 사용된다. 장기 저장소로는 EBS가 가장 적합하며, EC2 인스턴스의 기본 서버에 장애가 발생하게 되면, 해당 EC2 인스턴스가 연결된 하드웨어에도 장애가 발생하기 때문에 데이터 손실에 대한 위험이 존재한다.따라서 EC2 인스턴스 스토어를 사용할 때에는 필요에 따라 반드시 백업을 해두거나 복제를 해둬야한다.각 각의 인스턴스는 타입에 따라서 Read IOPS(330만)와 Write IOPS(140만)를 통해 초당 수행 가능한 I/O 작업의 수를 확인할 수 있다. 32,000 IOPS 정도인 BP2 유형의 EBS 볼륨과 비교하면, EC2 인스턴스 스토어가 훨씬 더 훌륭하다는 알 수 있다. 이러한 성능의 차이에 대한 내용은 시험 대비를 위해 숙지해두어야 하며, EC2 인스턴스에 성능이 아주 뛰어난 하드웨어가 연결된 것이 보일 때, 로컬 EC2 인스턴스 스토어를 생각해내는 센스가 있어야 한다. 32,000 IOPS이상을 요할때는 io1/io2 볼륨의 EC2 Nitro가 필요하다. EBS 볼륨 유형모든 볼륨의 세부 내용에 대해서 시험에 출제는 되지 않지만, 대략적으로 모든 볼륨의 차이를 이해하고, 범용 SSD와 프로비저닝 IOPS SSD의 차이나 DB가 필요할 때와 대용량, 저비용을 요할 때, st1을 사용하는 경우 그 차이를 이해하는 정도만 알면된다. EBS 볼륨의 유형은 총 여섯 개의 유형이 있으며, 여러 범주로 나눠서 살펴볼 수 있다.첫 번째로 gp2(general purpose 2), gp3(general purpose 3) 두 가지 타입이 있다. EBS는 물리적인 드라이브가 아닌, 네트워크 드라이브이며, 인스턴스와 EBS 볼륨이 서로 통신하기 위해서는 네트워크를 필요로 한다. 이는 범용 SSD 볼륨으로, 다양한 워크로드에 대해 가격과성능의 절충안이 된다.(latency 존재) 두 번째로 io1, io2가 있는데, 최고 성능을 제공하는 SSD 볼륨으로, 지연시간도 낮고 대용량의 워크로드에 사용된다. 세 번째로 st1볼륨이 있는데, 이는 저비용의 HDD 볼륨으로 잦은 접근과 처리량이 많은 워크로드에 사용이 된다. 처리량 최적화 HDD로, 빅데이터나 데이터 웨어 하우징 로그 처리에 적합하다. (처리량은 초당 500MB, 최대 IOPS는 500) 네 번재로 sc1볼륨이 있는데, 가장 비용이 적게 드는 HDD 볼륨으로 접근 빈도가 낮은 워크로드를 위해 설계가 되었다. 그리고 콜드 HDD가 있는데, 이는 아카이브 데이터용으로 접근 빈도가 낮은 데이터에 적합하다. 최대 처리량은 초당 250MB이고, 최대 IOPS도 250이다.(최저 비용으로 데이터를 저장할 때 적합) 이렇게 다양한 종류의 EBS 볼륨 유형이 있는데, EBS 볼륨 유형을 정의하기 위해서는 어떤 부분이 고려되야 할까? 바로, 크기, 처리량, IOPS가 있다. IOPS는 말 그대로 초당 I/O 작업 수를 의미한다. 앞서 첫 번째와 두 번째로 살펴본 gp2, gp3, io1, io2, 이 네 가지 EBS 볼륨 유형만이 부팅 볼륨으로 사용되고, 루트 OS가 실행될 위치에 위치하게 된다. 단, 시험에 있어서는 범용 gp2와 IOPS 프로비저닝이 가장 중요한 내용으로 출제된다.gp2는 짧은 지연 시간과 효율적인 비용의 스토리지이며, 시스템 부팅 볼륨에서 가상 데스크톱, 개발, 테스트 환경에서 사용할 수 있다. (크기는 1GB - 16TB 가 있다.) gp2 gp3 볼륨 gp ３보다 조금 오래된 버전 최신 세대의 볼륨 IOPS 3,000(최대) 3,000(기본)/16,000(최대) 처리량 - 1,000MB/s(최대) (IOPS가 증가 = 볼륨의 GB 수를 늘릴 때) gp3는 IOPS와 volumes을 독자적으로 설정할 수 있지만, gp2는 IOPS와 volumes이 서로 연결되어있다.작은 용량의 gp2의 용량은 IOPS를 3,000까지 올릴 수 있다. 시험 출제 내용시험에서 중요하 내용을 gp2와 gp3가 비용 효과적인 스토리지이며, gp3에서는 IOPS와 처리량을 독자적으로 성장할 수 있지만, gp2에서는 IOPS와 처리량이 서로 연결되어있다. 가장 중요한 내용은프로비저닝을 마친 IOPS로, 이는 성능을 유지할 필요가 있는 주요 비즈니스나 주요 비즈니스 어플리케이션이나 16,000 IOPS 이상을 요하는 요구하는 어플레케이션에 적합ㄴ하다. 스토리지의 활용스토리지를 이용하는 경우는 일반적으로 데이터베이스 워크로드를 사용할때이다.이는 스토리지 성능과 일관성에 민감한데, 이러한 경우에는 gp2/gp3 volume type에서 io1/io2 볼륨에서 최신 세대를 고르는 것이 해결책이 될 수 있다. io1과 io2를 비교하면, io2가 io1보다 내구성과 기기 당 IOPS 수가 크다.(io2를 사용하는 것이 합리적 선택)io2는 4GB - 64TB의 볼륨을 가지며, 지연시간이 밀리초 미만이고, IOPS:GB의 비율이 1,000:1일때, 최대 256,000 IOPS를 가진다. 그리고 Nitro EC2 인스턴스에서는 최대 64,000IOPS 까지 가능하다. Nitro EC2 인스턴스Nitro EC2 인스턴스의 경우에는 더 높은 IOPS까지 이용할 수 있다. Nitro EC2 인스턴스가 아닌 경우에는 최대 32,000 IOPS까지 지원된다. 그리고 io1/io2를 이용하면, gp3 볼륨처럼 프로비저닝된 IOPS를 스토리지 크기와 독자적으로 증가 시킬 수 있다. EBS 다중 연결(Multi-Attach)우선 EBS의 다중연결은 이전에 살펴보았던 EBS 볼륨 유형 중 io1/io2 제품군에 해당하는 내용이다.이전에는 EBS 보륨은 단일 EC2 인스턴스에만 연결할 수 있다고 했는데, 이는 EBS다중 연결을 제외한 경우의 이야기이다. 다중 연결은 하나의 EBS 볼륨을 동일 가용 영역 내의 여러 EC2 인스턴스에 연결하여 사용할 수 있다.io2 타입의 EBS 볼륨은 한 번에 총 세 개의 EC2 인스턴스에 연결할 수 있다. 이때 각 각의 EC2 인스턴스는 볼륨에 대한 전체 읽기 및 쓰기의 권한을 갖는다.(Teradata와 같이 클러스터 된 리눅스 어플리케이션에서 어플리케이션의 가용성을 높여야 하는 경우가 있을 시에는 특정 어플리케이션 내에서 동일한 볼륨에서의 동시 쓰기 작업을 관리할 수 있어야 한다.(특정 워크 로드에만 해당)) 이를 위해 반드시 클러스터 인식 파일 시스템을 사용해야 한다. XFS나 EX4 등의 파일시스템에서는 사용할 수 없다. 이외의 특별한 파일 시스템을 필요로 한다. 여기서 핵심은 EBS는 io1이나 io2 제품군일 때만 여러 EC2 인스턴스에 연결이 가능하다.(시험 출제 가능성 있음) EBS 암호화EBS 볼륨을 암호화하게 되면, 볼륨 내에서 암호화된 저장 데이터를 가져온다. 인스턴스와 볼륨 간에 전송되는 몯은 데이터는 암호화된다. 모든 스냅샷이 암호화되고, 스냅샷에서 생성된 모든 볼륨은 암호화된다.사용자가 별도의 작업 없이 모든 작업을 뒷단의 EC2와 EBS에서 처리되며, 암호화는 지연 시간에 미치는 영향도 거의 없고, KMS (AES-256의 키 사용)를 활용한다. 여기서 핵심은 암호화되지 않은 스냅샷을 복사할 때 암호화를 활성화해야 한다는 것이다. 암호화되지 않은 EBS 볼륨은 우선 EBS 스냅샷을 생성한 다음에 복사 함수로 EBS 스냅샷을 암호화한다. 그런 다음에 암호화된 스냅샷에서 새로운 EBS 볼륨을 생성하면, 생성된 볼륨도 같이 암호화된다.이후에 필요에 따라 암호화된 볼륨을 원본 인스턴스에 연결할 수 있다. (암호화되지 않은)EBS 볼륨 -&gt; 볼륨 스냅샷 생성 -&gt; 스냅샷 COPY와 동시에 암호화 -&gt; 암호화된 스냅샷을 활용해서 암호화된 EBS 볼륨 생성 암호화되지 않은 볼륨 스냅샷은 암호화되지 않는다. 따라서 볼륨의 스냅샷을 생성한 다음에 해당 스냅샷으로 가서 Actions의 Copy를 클릭해서 해당 스냅샷을 복사하면서 암호화를 동시에 할 수 있다.(KMS 암호화 EBS 볼륨 생성) 이제 암호화된 스냅샷이 생성이 되었으니, 이 암호화된 스냅샷을 사용해서 EBS 볼륨을 생성하면 암호화된 EBS 볼륨을 생성할 수 있다. 이제 최종적으로 EBS 볼륨의 Encryption을 보면, KMS로 암호화된 것을 확인할 수 있다. 더 간단하게 암호화된 EBS 볼륨을 생성하는 방법은 암호화되지 않은 스냅샷을 사용해서 새로운 볼륨을 생성을 클릭하고, 생성할 때 볼륨을 암호화하고, KMS 키를 선택하면 앞서 했던 방법과 동일한 방식으로 암호화된 EBS 볼륨이 생성된다.","link":"/2022/06/06/202206/220606-aws-saa-study/"},{"title":"220606 Docker 스터디 5일차","text":"실습1) ghost 블로그 docker 컨테이너 생성12345678910111213141516# Ghost는 무료 오픈소스 블로깅 플랫폼으로, 자바스크립트로 작성되었다고 한다.# *.yml 파일은 docker-compose 명령을 사용해서 up, down을 할 수 있도록 도와준다.# 기존의 콘솔에서 명령어 입력할 때 띄어쓰기등의 입력의 불편한 점을 개선하고자 *.yml파일은 유용하게 사용된다.version: '3' # docker compose에 정의된 버전에 따라 지원하는 docker의 engine도 달라진다.services: # 실행할 컨테이너를 정의한다. ($docker run -name [name] [service]) ghost: image: ghost:latest ports: - '60000:2368' volumes: - ./ghost/content:/var/lib/ghost/content environment: url: http://localhost:60000 실습2) FrontEnd - BackEnd - DB로 이루어진 웹 서비스를 배포1234567891011121314151617version: '3'services: frontend: image: subicura/guestbook-frontend:latest environment: PORT: 8000 GUESTBOOK_API_ADDR: backend:8000 ports: - '62000:8000' backend: image: subicura/guestbook-backend:latest environment: PORT: 8000 GUESTBOOK_DB_ADDR: mongodb:27017 restart: always mongodb: image: mongo:4 실습3) 투표 앱 생성clone한 프로젝트 폴더 내의 각 디렉토리 안에 있는 Dockerfile로 각 각의 도커 이미지를 생성한다. 123456789101112131415161718version: '3'services: vote: image: voting-vote ports: - '60001:80' redis: image: redis:alpine worker: image: voting-worker db: image: postgres:9.4 environment: POSTGRES_HOST_AUTH_METHOD: trust result: image: voting-result ports: - '60002:80' 실습4) 실시간 채팅 앱clone한 프로젝트 폴더 내에 있는 Dockerfile을 build해서 도커 이미지를 생성한다. 12345678910111213141516171819202122232425262728version: '3.6'services: chatapp: image: chatapp ports: - '60003:8000' postgres: image: postgres restart: always volumes: - db_data:/var/lib/postgresql/data environment: POSTGRES_HOST_AUTH_METHOD: trust graphql-engine: image: hasura/graphql-engine:latest.cli-migrations ports: - '60004:8080' depends_on: - 'postgres' volumes: - ./hasura/migrations:/hasura-migrations restart: always environment: HASURA_GRAPHQL_DATABASE_URL: postgres://postgres:@postgres:5432/postgres HASURA_GRAPHQL_ENABLE_CONSOLE: 'true' # set to &quot;false&quot; to disable console HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-logvolumes: db_data:","link":"/2022/06/06/202206/220606_docker_study/"},{"title":"220606 Kubernetes 스터디 1일차","text":"이번 포스팅부터는 쿠버네티스에 대해서 학습한 내용을 정리해보려고 한다. 빠르게 기본적인 내용에 대해서 정리를 하고, 앞으로 계속 실습을 해가면서 익숙해지는 시 간에 많이 투자해야겠다. Kubernetes ?큰 기업은 대규모의 서비스를 운영하고 있기 때문에 최대한 자원을 효율적으로 써야 비용적으로 유리하다. 따라서 서버자원을 효율적으로 쓰기 위해서는 가상화 기술에 대해서 이해해야 한다. 과거에는 리눅스의 자원격리 기술(1991)이 있었다. 이후에 VMware, Xen, openstack VM(Virtual Machine)(2010.7)가상화를 위해서 띄우고자 하는 가벼운 서비스를 사용하기 위해 더 무거운 OS를 올려야되는 상황이 생겼다. 어려운 자원격리기술을 dotCloud라는 회사에서 쉽게 사용할 수 있도록 container 서비스를 만들었고, 회사를 docker로 변경하면서 만든 서비스를 오픈소스로 공개하였다. (2014.06)컨테이너 가상화 기술은 서비스간에 자원격리를 하는데 별도의 OS를 안띄워도 되기 때문에 OS 기동시간이 필요없게 되고, 자동화시 엄청 빠르고 자원의 효율도 매우 높아지게 되었다. 이러한 이유로 도커는 유명해졌다. 하지만 도커 자체는 하나의 서비스를 컨테이너로 가상화시켜서 배포를 하는 것이기 때문에 많은 서비스들을 운영할때 일일이 배포하고 운영하는 역할을 해주지 않기 때문에 불편함이 있었다. 이러한 불편함을 해소하는데 도움을 준 것이 바로 컨테이너 Orchestrator 개념인 쿠버네티스이다. 여러 컨테이너를 관리해주는 서비스인데, docker가 오픈소스이기 때문에 많은 기업에서 다양한 Orchestrator를 출시하게 된다.대표적으로 AWS의 EKS(Elastic Kubernetes Service), RANCHER, HashiCorp의 Vault, Google의 Kubernetes 등이 있다.이 중에서 Kubernetes의 프로젝트에 구글의 주도 하에 여러 기업(RedHat, MS, IBM, docker, SALTSTACK 등)이 참여하였고, 타 기업의 서비스보다 만족도가 높았다고 한다.이러한 Kubernetes를 활용해서 Google cloud, AWS, Azure, IBM Cloud, Oracle Cloud에서 서비스들을 만들어서 운영하고 있다. 쿠버네티스 공부 순서쿠버네티스를 운영하는 운영자, 쿠버네티스를 기능을 활용해서 서비스를 배포하는 사용자로 나뉜다.(아무것도 모르는 상태에서 운영에 대한 내용을 먼저 학습하면 너무 어려워서 질려버릴 수 있다) 초반에는 사용자 기능을 중점으로 학습하고, 운영에 대한 내용은 차후에 학습을 하는 것을 권장한다고 한다. 쿠버네티스의 자동화 기능서버 운영에 있어 충분한 서버 자원이 뒷받침 되어야 한다. 서버의 자원을 효율적으로 사용하기 위해 쿠버네티스는 서버 자원에 대해 Auto Scaling을 해주게 되는데, 시간대별로 트래픽에 따른 자원의 사용량이 다른 각기 다른 서비스간에 평균 자원 사용량을 계산해서 Kubernetes는 최대한 낭비하지 않는 선에서 서버 자원을 사용할 수 있도록 도와준다.서버 자원에 대해 백업 및 장애대응을 위해 여분의 서버를 서비스당 한 대씩 준비하고 있으며, 쿠버네티스로 관리되는 서버는 Auto Healing 기능을 제공하기 때문에 여분의 서버로 서비스를 이전시켜서 서비스를 유지시켜준다.서비스의 중단이 허용되면 모든 서버를 내렸다가 업데이트 작업 후에 다시 전체 서버를 올리는 경우도 있으며, 무중단 서비스를 해야되는 경우, 한 개의 서비스씩 내렸다가 업데이트 작업후에 서비스를 올리는 방식으로 진행된다.쿠버네티스도 Deployment라는 객체를 통해서 업데이트 방식에 대해 자동적으로 처리되도록 하고 있다. (RollingUpdate, Recreate) VM과 Container 비교VM은 Host OS 위에 Hypervisor를 올리고 그 위에 각 각의 Guest OS를 올리는 형태로 관리가 되며, Container는 Host OS 위에서 각 각의 서비스들이 자원격리 기술을 통해 각 각의 Container로 분리되서 관리가 된다. (namespace: 커널에 관련된 영역을 분리, cgroups: 서버의 자원에 대한 분리)컨테이너에서 Pod은 하나의 배포 단위를 말하며, 각 각의 micro service단위로 쪼개진 컨테이너를 배포 단위에 따라 각 각의 Pod으로 묶어서 관리를 하게 된다. Kubernetes 첫 실습 그래도 이전에 도커에 대해서 어느정도 기본적인 학습을 해서 위의 구조도에 대한 전반적인 이해는 된다. 실습에 있어서, 우선 간단한 nodejs 프로젝트를 생성해주고, 커스텀 이미지를 생성할때 프로젝트의 코드를 COPY해서 새로운 도커 이미지를 빌드할 것이기 때문에 프로젝트 폴더 내에 프로젝트에 필요한 기본 구성을 한 다음에 .dockerignore파일과 Dockerfile을 작성해준다. Dockerfile 1234FROM node:slimEXPOSE 8000COPY ./hello.js .CMD node hello.js 파일이 준비된 다음에는 docker build -t [dockerhub username]/[image name] . 명령으로 도커 커스텀 이미지를 생성해준다. 컨테이너가 구동된 상태에서 접속을 하기위해서는 docker exec -it [container id] /bin/bash로 내부 컨테이너로 접속할 수 있다. 이제 이미지가 생성이 되었으니, 위 환경 구성에서와 같이 8000번 포트를 local의 8100번 포트와 mapping 해준다.(8100:8000) local에서 8100번을 통해 컨테이너가 잘 구동되었는지 페이지를 확인한다. 이제 커스텀해서 생성한 도커 이미지를 Docker Hub에 올린다. 1234$docker login# 사용자 이름, 비밀번호 입력해서 로그인# 이미지를 생성할 때에는 꼭 dockerhub의 username과 tag이름을 matching 시켜줘야 한다. (그렇지 않으면 resource is denied 에러 발생)$docker push mikehyungilme/hello docker Hub에 업로드한 이미지가 확인이 되었으면, 이제 Kubernetes에서 사용할 컨테이너의 이미지가 준비된 것이다. 쿠버네티스에서는 dashboard를 제공하기 때문에 실습할때 활용하도록 한다. 실제 운영환경에서는 dashboard를 사용하지 않도록 권고한다.(보안적인 부분이 직관적으로 볼 수 있기 때문에 해킹의 우려가 있다) Kubernetes의 Pod를 통해서 컨테이너 구동시키기이제 본격적으로 앞서 생성한 컨테이너 이미지를 활용해서 Pod를 통해 컨테이너를 구동시킬 것이다. 우선 쿠버네티스 실습을 위해서 Kubernetes 클러스터를 위한 환경구성을 해야한다. Pod를 띄워주기 위한 *.yml 파일아래 스크립트를 복사해서 dashboard의 +생성을 클릭해서 쿠버네티스의 objects을 생성할 수 있다. Pod를 생성한 후에는 파드(Pod)메뉴를 통해서 현재 생성된 파드의 상태정보를 확인할 수 있다.그 다음에 EXEC 버튼을 클릭해서 파드에 생성된 컨테이너에 직접 내부에 접근할 수 있다. 123456789101112apiVersion: v1kind: Podmetadata: name: hello-pod labels: app: hellospec: containers: - name: hello-container image: mikehyungilme/hello-image ports: - containerPort:8000 이렇게 생성된 Pod를 외부에서 접근할 수 있도록 서비스를 만들어야 한다. Service도 Pod와 동일한 방법으로 object를 생성해준다.생성이 된 Service를 확인해보면, 외부 엔드포인트 정보가 있는데, 엔드포인트를 통해서 외부에서 파드에 접근할 수 있다. Service를 띄워주기 위한 *.yml 파일 12345678910111213apiVersion: v1kind: Servicemetadata: name: hello-svcspec: # selector의 app:hello 부분과 pod의 labels의 app:hello 부분과 mapping되서 service가 해당 파드로 연결이 된다. selector: app: hello ports: - port: 8200 targetPort: 8000 externalIPs: - 192.168.0.30 쿠버네티스의 기능 전반에 대해 살펴보기서버 한 대는 Master로 사용하고, 다른 서버들은 Node로써 Master에 연결해서 사용한다. 이렇게 쿠버네티스 클러스터가 구성이 된다.Master는 쿠버네티스 전반을 컨트롤하는 역할을 하고, 나머지 노드들은 자원을 제공해주는 역할을 한다. 만약 클러스터 자원을 늘리고 싶다면, 노드들을 늘려주면 된다. 위의 쿠버네티스의 Controller의 세부 구조를 보면, 아래와 같다. 이제 막 쿠버네티스를 배우는 단계라 아직 이해가 되지 않는 부분이 많지만, 일단 Kubernetes 클러스터의 전체적인 구조에 대해서 머릿속으로 그려가면서 부분 지식을 습득해야겠다.","link":"/2022/06/06/202206/220606_kubernetes_study/"},{"title":"220607 프로그래머스 코딩 테스트 커뮤러닝 참여 중 배운 유익한 내용 (1주차) (업데이트 중...)","text":"이번 포스팅에는 프로그래머스 코딩 테스트 커뮤러닝에 참여하면서 1주차 문제풀이를 통해 다른 사람의 코드와 강사님의 문제풀이를 보면서 배웠던 내용에 대해서 정리해보려고 한다. 나중에 코딩 테스트 보기 전에 한 번 블로그에 정리한 포스팅 내용을 한 번 읽어보면서 정리하기 위한 목적에서 정리를 해본다. collections의 Counter 활용여지까지 리스트내의 값을 key:value(등장 횟수) 형태인 dictionary 자료형으로 만들때 일일이 for문으로 순회를 하면서 value 값을 증가 시켜주었는데, 이번 커뮤러닝을 통해 참여하신 분 중 한 분이 내 코드에 리뷰를 해주셨는데, collections의 Counter를 활용하면 더 간결하게 코드를 작성할 수 있다고 해서 코드를 다시 작성해보았는데, 리뷰해주신 내용대로 코드가 훨씬 간결해졌다. 12345678910111213141516171819202122232425# (수정 전) - 시간 복잡도 O(N)def solution(participant, completion): p_set = set(participant) p_hash = {name: 0 for name in p_set} for n in participant: p_hash[n] = p_hash.get(n) + 1 for n in completion: p_hash[n] = p_hash.get(n) - 1 for k, v in p_hash.items(): if v != 0: return k# (수정 후) - 시간 복잡도from collections import Counterdef solution(participant, completion): p = Counter(participant) # Counter({'marina': 2, 'josipa': 1, 'nikola': 1, 'vinko': 1, 'filipa': 1}) for c in completion: p[c] -= 1 # 리스트 p에 최대값을 구하고, 최대값의 key값을 반환하는 구문이다. return max(p, key=p.get) 문자열 숫자의 정렬 문자열 숫자 리스트의 요소를 조합하여 최대값 만들기 - (Stack 자료구조 활용)","link":"/2022/06/07/202206/220607_coding_test_community_learning/"},{"title":"220609 Kubernetes 스터디 2일차","text":"이번 포스팅에는 쿠버네티스 클러스터를 구축하고 실습한 내용에 대해서 정리하려고 한다. 쿠버네티스 클러스터 실습 구성도 Vagrant 명령(손쉽게 VirtualBox의 VM관리)host os의 cmd창에서 실행 1234$vagrant up: 가상머신 기동$vagrant halt: 가상머신 shutdown (클러스터 실습이 끝나고 VM 내리기)$vagrant ssh: 가상머신 접속 (vagrant ssh k8s-master)$vagrant destroy: 설치한 가상머신 삭제 네트워크 구성 이해Vagrant 스크립트 안에 VirtualBox를 이용해서 어떤 내부 네트워크를 생성하라는 부분이 있고, VirtualBox는 VirtualBox Host-Only와 nat를 만들어주는데, nat의 역할은 각 각의 VM들에 특정 ip를 똑같이 할당해줘서 똑같이 할당된 ip가 인터넷과 연결된 nat와 연결이 되서, 외부 인터넷과 연결이 된다.위의 과정이 있기 때문에 스크립트가 실행되는 동안 필요한 설치파일을 다운받아서 설치할 수 있는 것이다.VirtualBox Host-Only는 각 각의 VM에 개별 IP를 할당해서 VM간의 통신을 하는 역할과 HOST OS에서 쉘을 통해서 개별 IP를 통해 VM에 접근할 수 있도록 돕는다. 클러스터 설치가 완료되면,Service Network CIDR: 10.96.0.0/12(서비스의 IP대역대는 10.X.X.X)Pod Network(CNI: Calico)CIDR: 20.96.0.0/12(파드의 IP대역대는 20.X.X.X) Maser node에서 kubectl 명령으로 설치 확인12$kubectl get pod -A #모든 파드들의 상태정보 확인 (Running Status 확인)$kubectl get nodes #설치된 노드의 상태정보 확인 이후에 최종적으로 브라우저를 통해 K8s cluster의 Dashboard 확인한다. Maser node 재기동 후에 Dashboard 접근시에 아래의 명령으로 Proxy를 오픈1$nohup kubectl proxy --port=8001 --address=192.168.56.30 --accept-hosts='^*$' &gt;/dev/null 2&gt;&amp;1 &amp; Container &amp; Label &amp; Node Schedule 이미지 트레이닝하기 [Container]클러스터 내의 단일 Pod 내에는 복수 개의 컨테이너가 위치 할 수 있다. Pod 내의 컨테이너들에는 내부에서 컨테이너들간에 통신을 위해 각 각 복수 개의 ip를 할당할 수 있는데, Pod 내에서는 반드시 고유한 IP여야한다. (IP 충돌)각 각의 Pod에는 하나의 고유 IP 주소가 할당되는데, 클러스터 내에서 이 IP를 통해 해당 Pod에 접근을 할 수 있다. 하지만, 클러스터의 외부에서는 이 Pod IP를 통해 접근이 불가능하다. 이 고유 IP의 경우에는 Pod에 문제가 생겨서 시스템이 자동 감지하여 재생성을 해주게 되면, IP도 재할당된다. 1234567891011121314apiVersion: v1kind: Podmetadata: name: pod-1spec: containers: - name: container1 image: tmkube/p8000 ports: - containerPort: 8000 - name: container2 image: tmkube/p8000 ports: - containerPort: 8080 [Label]Label은 사용 목적에 따라 등록해서 사용하는데, 모든 Object에 Label을 다는 것이 가능하지만, 주로 Pod에서 사용된다.Label은 key:value 형태의 구조를 가지며, Service를 특정 label별로 그룹화하거나 Pod를 특정 label별로 그룹화할 때 사용된다.Pod들에 type과 lo라는 Key값이 할당되어있다고 가정했을 때, 아래의 예시 yml 파일과 같이 Service는 spec &gt; selector &gt; type에 값을 정의해서 Service를 Pod별로 묶어서 정의할 수 있다. Pod는 metadata &gt; labels &gt; type, lo에 값을 정의해서 개별 Pod를 그룹화해서 묶을 수 있다. 이렇게 Pod들을 묶어서 관리해서 사용하는 예시로는, 상황1) 웹 개발자가 웹 화면만 확인하고 싶을 때, type이 web인 port들만 service에 연결시켜서 웹 개발자들에게 알려줄 수 있다. 상황2) 서비스 운영자가 각자가 원하는 Pod에 접근하고자 할 때, lo: production인 Pod들을 묶어서 새로운 pod의 이름을 할당해서 알려줄 수 있다. Service.yml 123456789apiVersion: v1kind: Servicemetadata: name: svc-1spec: selector: type: web ports: - port: 8080 Pod.yml 1234567891011apiVersion: v1kind: Podmetadata: name: pod-2 labels: type: web lo: devspec: containers: - name: container image: tmkube/init [Node Schedule]각 각의 Pod들은 여러 노드들 중에 하나의 노드에 올라가야한다. 노드에 올리는 방법으로는 직접 선택과 k8s가 자동으로 선택해주는 기능이 있다. [직접 선택]우선 직접 Pod가 올라갈 Node를 지정해주는 방법으로는 아래와 같이 Pod 생성시에 spec &gt; nodeSelector &gt; hostname: node1와 같이 해당 노드 이름을 Pod의 yml파일에서 할당해줄 수 있다. 각 각의 Node에도 label을 달 수 있는데, 위의 hostname의 값으로 연결하고자 하는 Node의 hostname을 넣어주면 된다. 12345678910apiVersion: v1kind: Podmetadata: name: pod-3spec: nodeSelector: hostname: node1 containers: - name: container image: tmkube/init [자동 할당]Pod를 Node에 자동으로 할당하는 방법은 스케줄러가 판단해서 할당해주는 방법이 있는데, Scheduler가 현재 각 각의 node들이 가지고 있는 여유 메모리 공간을 확인하여, 메모리상 여유가 있는 Node에 자동할당하고자 하는 pod를 할당해주게 된다.pod의 yml파일에서 spec &gt; containers &gt; resources &gt; requests &gt; memory, limits에서 request의 memory에 요구되는 메모리의 양을 명기하고, requests의 limits에 최대 허용 메모리를 명기해줌으로써 HDD의 App에서 부하가 생겼을때 무한정으로 노드의 자원을 사용하는 것을 막는다.만약 설정을 안하게 되면, 노드 내의 자원을 무한정으로 사용하게 되고, 이로인해 노드내의 자원이 부족해지게 되면, 노드 내의 다른 pod들은 자원이 없어서 다 같이 죽게 되는 상황이 된다. memory가 초과되면 클러스터 내의 pod를 종료시키지만, cpu가 초과되면 request의 memory 용량의 수치를 낮추고, memory가 초과되었을 때와 같이 pod가 종료되지는 않는다. 이렇게 Memory와 Cpu가 다르게 동작하는 이유는 각 자원의 특성이 다르기 때문인데, 예를들어 파일을 복사할때 또 다른 파일을 또 다시 복사하게 되면, 기존 작업들이 중단되지 않고, 단지 처음 복사한 파일의 작업의 속도가 느려지게 된다. 이러한 예처럼 클러스터 내에서 cpu가 초과하게 되면, pod가 종료되지 않고, 단지 requests의 memory의 사이즈가 줄어들게 되는 것이다. pod.yml 12345678910111213apiVersion: v1kind: Podmetadata: name: pod-4spec: containers: - name: container image: tmkube/init resources: requests: memory: 2Gi limits: memory: 3Gi 실습내용 Pod 생성Pod내에 각기 다른 port가 할당되어있는 container생성 Pod 내에 동일한 ip를 가진 컨테이너 생성pod내에서 각 각의 컨테이너가 동일한 ip를 가질 경우, 충돌이 일어나기 때문에 제대로 Pod가 생성되지 않는다. ReplicationControllerReplicationController로 pod를 생성하고, 다시 ReplicationController를 삭제하게 되면, controller가 삭제가 진행됨과 동시에 새로운 ReplicationController를 사용해서 pod를 다시 재생성한다.(pod의 ip가 재할당) 근데 영구적으로 삭제는 어떻게 하는거지?? 1234567# k8s-master node 에서$kubectl get pods # 현재 cluster내에 있는 pods 리스트 출력# replication controller관련 파트들을 삭제하기 위해서$kubectl delete rc rep-pod # NAME이 rep-pod-* 인 모든 replication controller를 삭제한다. 문자-pod 까지만 입력(첫 - 다음 문자까지만 입력)# namespace가 default로 설정된 모든 pod들을 삭제하도록 한다. (이렇게 삭제하면 rc 파드가 다시 생성된다. 따라서 kubectl delete rc [삭제하고자 하는 pod name]의 패턴으로 rc pod를 삭제해줘야 한다)$kubectl delete --all pods --namespace=default 복수의 pods를 label별로 묶어서 service 생성yml 파일에서 spec &gt; selector &gt; type: web을 통해서 type이 web인 pod들을 묶어서 service를 생성할 수 있다.그리고 production 전용의 pod들을 묶기 위해서 lo: production인 pods를 모아서 service를 생성할 수도 있다. pod의 현재 CPU 및 메모리 사용량 확인현재 dashboard에서 CPU, MEMORY 사용량에 대한 확인이 안되는데, 방법 찾아서 해결하고, console에서 명령으로 확인할 수 있는 방법 찾아보기","link":"/2022/06/09/202206/220609_kubernetes_study/"},{"title":"220611 Kubernetes 스터디 3일차","text":"이번 포스팅에는 쿠버네티스 클러스터에서 서비스와 관련된 이론내용을 정리하고, 실습한 내용에 대해서 정리하려고 한다.저번시간에는 실습을 위한 쿠버네티스 클러스터 환경을 구축하고, Container, Label, NodeSchedule에 대한 부분을 학습하고 실습을 해보았다.컨테이너는 쿼버네티스 클러스터 내의 namespace 상에 존재하는 pod의 내부에 복수로 존재하며, 각 각의 컨테이너들은 복수 개의 IP 할당이 가능하지만, pod 내에서는 IP 충돌로 인해 동일한 IP의 할당이 불가하다는 것을 이론과 실습을 통해 이해했다.Label은 각 각의 용도에 따라 Pod에 명기된 label로 Pod들을 묶어서 하나의 서비스를 생성하거나, 또 다른 pod 집합으로 생성하기 위해 사용된다는 것을 이론과 실습을 통해 이해하였다.마지막으로 NodeSchedule 파트에서는 각 각의 Pod들은 Cluster에 연결된 노드들에 할당이 되어야 하는데, 할당하는 방법에 대해서는 직접 Pod를 Node에 할당하는 방법과 스케줄러에 의해 각 각의 노드가 가지고 있는 여유 메모리 공간에 대한 확인으로, 자동 할당되는 방법에 대해서도 이론과 실습을 통해 이해하였다. 아직 개괄적으로 쿠버네티스에 있는 각 각의 요소들의 개념과 원리에 대해서 이해하는 과정이기 때문에 전체적인 그림 안에서 각 각의 요소가 어떤 기능을 하는지에 대해서 이해하면서 학습을 하고 있다.엇그제 Label을 통해서 용도에 따라 pod(들)을 묶어서 하나의 Service로 만들어서 다른 운영자에게 IP와 Port를 제공한다고 배웠다. (웹 개발자가 웹 화면만 확인하고자 할때 type이 web인 pod들만 service에 연결시켜서 웹 개발자들에게 제공 / 외부에서 원하는 service의 pod들에만 접그하고자 할 때 label이 production으로 지정된 pod들을 묶어서 하나의 service로 생성하고 제공)이 Service에 대해서 이론적으로 학습을 하고 직접 실습을 해 볼 것이다. Service in k8s 쿠버네티스의 Service의 종류로는 크게 기본적으로 ClusterIP, NodePort, Load Balancer가 있으며, 순서대로 ClusterIP Service의 성격을 NodePort Service가 상속하고, NodePort Service의 성격을 Load Balancer가 상속을 한다. [ClusterIP Service]ClusterIP는 가장 기본적인 Service의 형태로, 실제 서비스를 생성할 때 spec &gt; type의 값은 option인데, default가 ClusterIP이다. service_sample.yml 1234567891011apiVersion: v1kind: Servicemetadata: name: svc-1spec: selector: # 이 부분이 Pod와 Service를 연결시키기 위한 부분이다. app: pod # 연결하고자 하는 Pod의 이름 ports: - port: 9000 # 9000번 포트로 들어오면, targetPort: 8080 # 8080번 포트로 연결 type: ClusterIP # 이 부분은 optional한 부분이며, 기본값은 ClusterIP이다. pod_sample.yml 123456789101112apiVersion: v1kind: Podmetadata: name: pod-1 labels: # 이 부분이 Service와 연동시키기 위한 부분이다. app: pod # 이 부분이 Service와 연동시키기 위한 부분이다.spec: containers: - name: container image: tmkube/app ports: - containerPort: 8080 ClusterIP의 구성을 한 번 이미지 트레이닝 해보자. 우선 클러스터 내에 IP와 Port가 할당된 서비스 하나와 Pod이 두 개 있다고 가정한다.Service에는 기본적으로 자신의 클러스터 IP를 가지고 있으며, 외부로부터 접근이 불가하다.(서비스와 Pod에 할당된 IP와 Port는 클러스터 내부에서 접근할 때만 사용)각 각의 Pod들에도 IP와 Port가 할당되어 있기 때문에 굳이 Service를 연결하지 않고, Pod의 IP와 Port로 접근하면 되지 않나? 라고 생각이 들겠지만, pod의 특징을 이해하면 왜 Pode들을 Service와 연동시켜서 Service를 통해서 Pod들에 접근을 하는지 이해할 수 있다.그 이유는 Pod는 성능상에 문제가 생기면, Pod를 재성성하고 Pod에 할당되었던 IP를 재할당하게 된다. 따라서 Pod에 할당된 IP는 클러스터 내에서 접근이 가능한 수단이 될 수는 있지만, 신뢰성이 매우 낮다.따라서 사용자가 지우지 않으면 사라지지 않는 Service라는 친구를 두고, 그 하위에 관리하고자 하는 Pod들을 연결시켜서 관리하게 된다. 서비스는 트래픽을 분산해서 pod에 전달하는 역할을 하며, 종류도 다양한데, Pod에 접근을 도와주는 방식이 다 다르다. [ClusterIP Service의 사용 케이스]앞서 이미 살펴봤듯이 ClusterIP는 Service에 할당된 IP로, 외부에서 접근이 불가능하다. 따라서 ClusterIP Service를 사용하는 경우는 외부의 접근이 불가능하고, 오직 내부에서만 클러스터에 접근하는 경우에 사용 가능하다.예를들면, 클러스터 내부에 접근이 가능한 운영자와 같은 인가된 사람이 클러스터의 dashboard를 관리하거나, 각 pod들의 서비스 상태를 직접 디버깅하는 업무를 하는 경우에 ClusterIP Servicer를 생성해서 사용할 수 있다. [NodePort Service]NodePort Service는 용어에서 알 수 있듯이 Node의 Port와 관련있는 Service이다. 앞서 NodePort Service가 ClusterIP Service의 성격을 상속한다고 했는데, 클러스터 내의 구조를 살펴보면, Service에 복수 개의 Pod들이 물려있는 구조로 되어있다. 이전 시간에 NodeSchedule에 대해서 학습을 했을 때 각 각의 Pod는 Node에 할당이 되어야 한다. 따라서 각 각의 Pod들은 특정 Node에 종속이 되어있는데, 이름에서 알 수 있듯이 모든 Node들에 동일한 Port가 할당이 되어있다. 따라서 외부로부터 특정 노드의 IP와 동일하게 할당된 Port로 접근을 하게 되면, 어느 노드에서 접근을 했던 간에 각 각의 Pod들이 물려있는 Service로 연결이 가능하며, Service에 연결된 Pod에 트래픽이 전달된다.단, 만약에 특정 노드의 IP와 Port를 통해 서비스에 연결했을 때, 연결할 때 접근했던 노드를 제외한 다른 노드에 포함된 Pod들에는 Service에서 트래픽을 보낼 수 없게 제한을 할 수 있는데, 아래와 같이 yml파일에서 externalTrafficPolicy: Local을 추가해주면 된다. nodeport.yml 123456789101112apiVersion: v1kind: Servicemetadata: name: svc-2spec: selector: app: pod ports: - port: 9000 targetPort: 8080 nodePort: 30000 # 30000~32767 범위내의 Port type: NodePort # 옵션! 생략이 가능 [NodePort Service의 사용 케이스]NodePort는 물리적인 호스트(Node)의 IP를 통해서 Pod에 접근이 가능하다.대부분의 호스트 IP는 보안적으로 내부망에서만 접근이 가능하도록 네트워크가 구성이 되어있으며, 종종 내부 시스템을 개발하고 외부에서 간단하게 데모를 할 때 네트워크 중계기에 포트 포워딩을 해서 외부에서 내부 시스템으로 연동해서 일시적으로 외부 연동을 해서 사용할 때 사용된다. [Load Balancer Service]Load Balancer Service는 NodePort Service의 성격을 상속받고, Node Balancer의 구조를 그대로 가진 상태에서 Load Balancer가 추가된 형태로 되어있다. Load Balancer는 각 각의 노드에 트래픽을 분산시켜주는 역할을 한다.단, 주의해야 될 점은 Load Balancer에 할당된 IP(외부에서 접근하기 위한 IP)는 개별적으로 k8s를 설치했을 때 기본적으로 생성되지 않는다.Load Balancer의 IP를 생성하기 위해서는 Load Balancer 외부 IP지원 플러그인을 설치해야 한다. 예를들어 AWS의 k8s 플랫폼 관련 서비스를 사용하게 되면, 해당 서비스 내에는 자체적으로 Load Balancer 외부 IP 지원 플러그인이 설치가 되어있기 때문에 Load Balancer 서비스 생성시에 자동으로 외부에서 서비스로 접근이 가능한 IP를 할당해준다. load_balancer.yml 1234567891011apiVersion: v1kind: Servicemetadata: name: svc-3spec: selector: app: pod ports: - port: 9000 targetPort: 8080 type: LoadBalancer # 이렇게 타입만 추가해주면, LoadBalancer service가 생성이 된다. [LoadBalancer Service의 사용 케이스]Load Balancer 서비스는 실제로 외부에 서비스를 노출시킬 때 사용된다. Load Balancer를 사용하게 되면, 내부 IP가 노출되지 않고, 외부 IP를 사용해서 안정적으로 외부로 서비스를 노출 시킬 수 있다.","link":"/2022/06/11/202206/220611_kubernetes_study/"},{"title":"220612 데이터 파이프라인 구축 오프라인 수업 &#x2F; 3주차","text":"이번 포스팅에서는 세 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. 참고로 첫 번째와 두 번째 수업때도 너무 유익한 내용들이 많았는데, 이번 시간이 정말 너무 유익하고 좋았다.아마도 이전에 인터넷 강의로 수강을 했을 때 아쉬웠던 부분이 많았는데, 이번에 개별적으로 현직자 분께 오프라인으로 직접 수업을 들으니, 궁금했던 부분이 많이 해소되기도 했고, 강사님이 수업에 필요한 여러 자료나 실제 회사에서 업무했을 때 필요한 부분에 대해 설명을 잘 해주셔서 그런 것 같다. 이번 3주차 수업에서는 수업 한 시간 전에 미리 강의장에 도착해서 어떤 식으로 데이터 엔지니어 포트폴리오의 프로젝트를 구성해야되는지 과거에는 K사에서 근무하셨고, 현재는 N사에서 데이터 엔지니어로 근무하시는 강사님께 조언을 구했는데, 너무 감사하게도 세세하게 답변을 해주셔서 이 부분에 대해서도 한 번 정리를 해보려고 한다. 포트폴리오 프로젝트 준비이번 3주차 수업 시작전에 강사님께 미리 데이터 엔지니어 포트폴리오는 어떤 식으로 준비를 해야되는지 여쭤보았다. 내가 이전에 생각했던 것은 FE/BE는 하나의 프로젝트당 하나의 코드로 깃허브 Repository에 정리를 하면 되는데, 데이터 엔지니어는 AWS와 같은 클라우드 서비스를 이용해서 데이터 파이프라인을 구축하고, 내부적으로 일부 코드를 작성하기 때문에 문서 형태로 정리를 하면 되는 것인가? 라는 생각을 했었다.그런데 강사님이 &quot;데이터 엔지니어도 FE/BE 프로젝트와 다를 것이 없고, 단지 데이터 엔지니어를 위한 프로젝트는 FE/BE처럼 하나의 코드로는 나오지 않을 것이다.&quot;라는 답변을 해주셨다.덫붙여서 1주차 강의때 더 알면 좋은 내용들 중에 IaC(Infrastructure as Code)로 인프라구축을 코드로 작성해서, 프로젝트를 구성하고 있는 인프라를 손 쉽게 구축할 수 있게 하는 부분을 해보고, 해당 코드를 프로젝트에 넣으면 좋을 것 같다는 말씀도 해주셨다.아마 FE/BE 사이드 프로젝트와 별반 다르지 않다는 말씀이 이런 코드들을 GitHub 레포지토리에 정리해서 올리고, 데이터를 정제할때나 Event Driven Architecture로 구성했을때 작성했던 코드들도 GitHub에 프로젝트 단위로 묶어서 올리면 되기 때문에 그렇게 말씀해주신 것 같다.(Markdown으로 문서화는 필수) IaC로 인프라 구축을 코드로 작성했을 때, Terraform에 대해 언급을 해주셨는데, 이전에 Vagrant로 스크립트를 작성해서 VM의 구성을 손쉽게 하는 것을 해보았는데, 이것과 같은 맥락인 것 같았다. (찾아보니, Vagrant와 Terraform은 둘 다 HashiCorp에서 파생된 프로젝트라고 한다. Vagrant는 개발 환경을 관리하는데 주안점을 둔 툴이고, Terraform은 인프라를 구축하는데 주안점을 둔 툴이라는 차이가 있다.) AWS로 구축한 인프라는 HCL(Haship Configuration Language, *.tf)으로 작성해서 GitHub에 코드를 올리도록 하자. 실제 업무를 할때나 사이드 프로젝트를 할때에도 우아하게 파이프라인을 구축할 필요 없이 Lambda와 triggering하는 요소들을 잘 조합해도 효과적으로 공수를 덜 들이고 인프라를 구축할 수 있다.간단한 처리는 Lambda와 EventBridge만을 사용해서 처리를 할 수 있다. Athena의 사용Athena 엔진(Version2)은 Presto(0.217)라는 오픈소스를 기반으로 만들어졌다. 관련된 함수, 연산자, 표현식에 대한 자세한 내용은 Presto documentation에서 확인할 수 있다.(Kakao ) Athena는 Presto 및 Trino의 함수와 기능의 전부는 아니지만, 일부 지원을 한다. 최근에는 Presto에서 Trino로 바뀌었다. Athena를 사용할때 직접 Athena에 접속을 해서 브라우저에서 쿼리를 날리거나 하는 작업을 하기도 하지만, 현업에서는 파이프라인을 구축할때, Athena를 원격지의 데이터베이스 개념으로 다뤄서 쿼리를 보내는 형태로 작업을 한다.(한번 감싼 형태로 Athena를 활용) 이전에 BI툴인 Tableau에서 Amazon Athena의 데이터와 연결해서 실습해본적이 있는데, 이렇게 사용한다는 의미인 것 같다. (Tableau에서 Athena JDBC 드라이버를 설치해서 사용, 자동화시에 사용) Presto와 Trino는 파일들을 Parsing해서 테이블의 스키마를 생성해서 데이터를 조회한다거나 하는 작업을 가능하게 해주는 엔진이다. Output이 여러개 파일로 나오는 것을 하나의 파일로 통합이전 실습을 할때 CTAS query의 결과로 S3에 저장된 데이터가 여러개로 생성(Athena가 Presto 엔진을 기반으로, 분산된 되서 처리를 해주기 때문에 여러개의 파일로 생성)이 되었었는데, 아래와 같이 bucketed_by=ARRAY['time'], bucket_count=1 time 필드를 기준으로 한 번 만 실행되도록 설정해서 CTAS 쿼리를 날려주면, 한 개의 파일로 추출이 된다. 1234CREATE TABLE new_table2 WITH (external_location=‘s3://hg-data-bucket/ctas-bucketing/', format='PARQUET',parquet_compression='SNAPPY',bucketed_by=ARRAY['time'], bucket_count=1) AS SELECT time, remote_ip FROM &quot;class&quot;.&quot;hg-data-table&quot; PARQUET 데이터 포맷으로 하고, 압축방식은 SNAPPY로 한다.(SNAPPY 압축방식은 네트워크 통신비용을 줄이기 위해 많이 사용된다) 그리고 결과로 S3에 적재된 파일의 데이터가 제대로 들어갔는지 확인하려면 생성된 파일을 체크하고, [작업] - &quot;S3 Select&quot;를 사용한 쿼리를 선택해서 data type을 parquet로 변경하고 쿼리를 날려서 결과 데이터를 확인해볼 수 있다. (parquet 타입의 데이터의 경우에는 다운받아서 확인하려면 별도의 프로그램을 다운받아야 한다) 이렇게 하면 별도로 Athena로 연결해서 데이터를 확인하지 않아도 된다. Hadoop, Spark를 Athena로 대체ETL과정에서 Transform 과정에서 Hadoop이나 Spark를 사용하지 않아도 Athena를 사용해서 대체 가능하다. (time format의 포멧을 변경하거나, 파일의 포맷을 바꾸는 쿼리 옵션도 있기 때문에 Athena에서 쿼리를 작성하면 별도로 Hadoop이나 Spark를 사용하지 않고 데이터 전처리가 가능) 데이터 저장 포맷(CSV, TSV, JSON, Parquet, ORC)CSV는 Comma Separatored Values로 comma로 구분된 데이터 포멧을 말한다. 그리고 TSV는 Tab Separated Values로, Tab으로 각 칼럼 데이터가 구분된 데이터 포맷을 말한다.CSV, TSV, JSON은 사람이 읽기에는 용이하나, 압축률이나 데이터를 읽어들이는 속도가 느리기 때문에 빅데이터를 보관 및 저장을 할때는 Parquet나 ORC를 주로 사용한다.Parquet와 ORC는 column oriented data(columnar)로, 칼럼 기반 저장 포멧이다. 따라서 압축률이 높고, spark, hadoop에서 많이 사용(Parquet)되며, Hive에 특화(ORC)되어있다. S3 객체의 스토리지 클래스 변경S3에 보관된 객체의 스토리지 클래스를 변경할 수 있다. 이는 데이터의 사용빈도에 따라서 다르게 해서 적용해야 자원을 좀 더 효율적으로 사용할 수 있다. 그 예시로, Glacier Deep Archive가 있는데, 일년에 한 번 엑세스하는 오래된 아카이브 데이터가 있으며, 매우 저렴하게 이용할 수 있다. 저렴하지만, 해당 스토리지 클래스 객체를 검색하게 되면, 몇 분 내지 몇 시간의 검색 시간이 소요된다는 단점이 있다.Standard는 자주 엑세스하는 데이터로, 한 달에 한 번 이상 접근하는 데이터의 경우에 적용되는 스토리지 클래스이다. 가격이 다른 스토리지 클래스에 비해 비싸다는 단점이 있다. S3의 수명 주기와 복제 규칙앞에서는 저장된 데이터 객체의 특성에 따라서 직접 객체의 스토리지 클래스를 변경하였는데, 또 다른 방법으로는 S3 bucket에 수명 주기 규칙이나 복제 규칙등을 지정해주는 것이다.(AWS S3 버킷 선택해서 내부 버킷으로 들어와서 [관리]탭 선택 - 수명 주기 규칙 및 복제 규칙 생성 및 관리)이렇게 규칙을 생성해주면, S3 버킷 내의 특정 디렉토리에 계속해서 데이터가 쌓이게 된다고 가정했을때 필터의 접두사로 long-으로 설정해주고, 해당 디렉토리에 쌓인 데이터의 객체의 현재 버전 전환으로 특정 클래스를 지정하고 경과 시간을 입력해주면, log-으로 시작하는 디렉토리에 쌓인 데이터 객체가 지정한 기간 이후에 지정된 스토리지 클래스로 전환이 된다. 이외에도 특정 기간이 지나면 해당 데이터가 삭제되도록 할 수도 있다. 복제 규칙의 경우에는 일정 기간이 지나면 암호화를 하거나 다른 버킷으로 옮기거나 다른 계정의 버킷으로 옮기는 것이 가능하도록 해준다.본사에 한 달 동안 데이터 분석 처리를 하고, 계열사나 고객사로 데이터를 넘기는 경우, 복제 규칙을 활용할 수 있다. 위와같이 자동화 설정이 가능하다. AWS LambdaFaS(Function as Service)로 Serverless computing이라고 한다. CPU나 메모리는 신경쓰지 않고, 오직 코드만 작성하는데 집중할 수 있다.(Infra의 provisioning 없이 내가 선택한 언어로 코드만 업로드해서 사용) 코드는 직접 브라우저에서 작성을 하거나 S3에 업로드된 코드를 import해서 사용할 수 있는 방식으로 되어있다. 초당 수십만개의 데이터를 요청 처리하는 것이 가능하기 때문에 경우에 따라서 이벤트가 올때마다 서버(Spring boot와 같은)에서 처리를 하도록 처리하지 않고, 간단한 코드처리의 경우에는 Lambda에 올려서 처리하도록 할 수 있다. 특정 API 요청이 오거나 Action, Event 가 왔을때 넘길 수 있거나 회원가입이나 간단한 조회의 경우 람다함수를 사용할 수 있다.비용 청구는 코드가 실행되는 시간(ms)단위로 기준으로 청구된다. (길게 연산이 요구되는 처리에는 적합하지 않다) 다양한 언어를 지원해주며, 연산을 하거나 데이터 처리를 하고 결과만 넘기는 처리(stateless)를 주로 하기도 하지만, 요즘에는 storage를 직접 붙이거나해서 로컬에 데이터를 저장할 수 있도록 변하고 있다. 용도와 가격에 맞게 EC2로 서버를 올릴 것인지 ECS(Elastic Container Service)를 사용해서 서버를 올릴 것인지 선택을 해서, Filebeat를 올릴 것인지 고려를 해서 AWS를 활용하는 것이 좋다.(이 부분은 프로젝트 구성할때 한 번 고려를 해봐야겠다.) Lambda를 호출(트리거)할 수 있는 것은 API Gateway, S3, SQS, EventBridge가 있다. 시나리오1) S3에 데이터가 들어오면 람다가 돌면서 데이터를 처리한 다음에 다른 서비스로 넘길 수 있다. 시나리오2) SQS(Simple Queue Service)에 이벤트가 쌓이면 Lambda에서 처리를 할 수 있게 할 수 있다.예를들어, 마케팅을 위해서 여러 다수의 고객들에게 이메일을 보낼때, 사용자 데이터를 뿌려서 큐에 집어넣고, Lambda로 호출해서 큐의 정보를 뽑아서 처리하는 경우에도 활용될 수 있다. [Lambda 함수 생성]람다 함수를 생성할때에는 새로 code editor를 사용해서 생성을 하거나 블루프린트(자주 사용되는 template을 AWS에서 제공)로 생성을 할 수 있다. 함수 이름과 사용할 언어의 런타임을 선택해주고, 내부 코드 소스에서 editor에 직접 코드를 입력하거나 S3에 업로드된 code를 담은 zip 파일 패키지를 업로드할 수 있다.생성된 코드는 별도로 테스트를 할 수 있도록 AWS Lambda에서 테스트 전용 탭을 제공한다. 그리고 작성한 코드에서 아래와 같이 환경변수를 호출해서 사용하는 경우도 있는데, 이 환경변수는 생성한 Lambda 함수의 “구성”탭에서 하위의 “환경변수”를 선택하면 환경변수를 추가할 수 있는데, 여기에 추가된 환경변수를 참조한다. 1234567891011# 예시import os#boto3는 python에서 AWS 서비스를 사용할때 많이 사용되는 라이브러리이다.import boto3SOURCE_DATABASE = os.getenv('SOURCE_DATABASE')# AWS Athena 서비스 접근boto3.client('athena')# AWS S3 서비스에 접근Bucket = boto3.resource('s3').Bucket('hg-project-data')Bucket.put_object(Key=file_name, Body=log) 추가적으로 Trigger해주는 서비스들 중에서 CodeCommit이 있는데, 이는 깃에서 코드를 commit했을때 자동으로 CI/CD가 돌도록 구성하는 부분과 같이 CodeCommit의 서비스가 구동이 되면 생성한 Lambda함수가 실행되도록 설정할 수 있다. Amazon EventBridge 서버리스 이벤트 버스이다. 여기서 이벤트 버스란, 수많은 이벤트들을 받아서 라우팅하거나 필터링, 트리거링 할 수 있도록 도와주는 서비스이다. EDA(Event-Driven Architecture) 구축을 간편하게 처리할 수 있도록 해준다. AWS 서비스에서 생성되는 다양한 이벤트들을 가져올 수 있다. 특정 규칙을 지정하고 규칙에 맞을때, AWS의 다른 서비스를 호출할 수 있도록 해준다. 리눅스의 cron 기능과 같이 일정 시간마다 AWS 서비스를 호출시켜주는 스케줄러의 기능을 제공한다. 주문이 왔을때 Evnet를 EventBridge로 발생시키고, 발생시킨 EventBridge가 하위의 각 각의 서비스(청구서, 지불 등의 서비스들)을 호출한다. 이전에는 API 호출을 해서 일일이 트리거 시켜줘야했는데 EDA에서는 EventBridge를 사용해서 Loose coupling을 할 수 있고, copepipe라인에 들어온 이벤트를 이벤트 브릿지가 처리해서 람다함수를 호출하게도 할 수 있다. Apach Airflow를 사용해서 dag를 생성해서 일정 시간에 실행되도록 만들 수 있다. 실습 1) Lambda + EventBridge 통합 실습 log-generator Lambda function을 생성하는데, s3 버킷에 지정한 디렉토리를 생성하고, timestamp, device_name, metric 값을 생성해서 해당 log값을 내용으로 하는 파일을 버킷에 저장되도록 코드를 구성한다.반환되는 값은 statusCode와 body의 내용으로 한다. Access denied 에러가 발생하는 이유는 IAM 권한이 없어서다.log-generator 코드상에서 결과 파일을 S3에 저장하는 부분이 있는데, 이 부분때문에 에러가 발생된다.AWS에서는 whitelist 정책에 의해 허가된 서비스 간에서 서로 통신이 가능하다. 따라서 현재 실습에서는 Lambda에서 S3에 접근을 할 수 있도록 해야한다. (최소권한의 원칙 - Lambda함수에서 업로드를 하는 기능만 하기 때문에 S3서비스에 대해 put에 대한 권한만 부여해야 한다.)권한에 대한 부여는 생성된 람다 함수의 &quot;구성&quot;탭에서 하위메뉴의 &quot;권한&quot;을 선택하고, 실행역할에 역할 이름 링크를 타고 들어가서 추가 권한을 부여해주면 된다. (&quot;권한추가&quot; - &quot;정책연결&quot; - AmazonS3FullAccess 권한 추가) 그리고 구성 부분에서 timeout 및 메모리의 사용량에 대해서도 편집할 수 있다.Lambda 함수를 작성하고 반드시 Deploy를 해주도록 하자.Deploy후에 테스트를 해주면 결과적으로 S3에 코드에서 생성한 임의의 디렉토리의 하위에 파일이 생성된 것을 확인할 수 있다. EventBridge Trigger 추가는 우선 EventBridge에 대한 개별설정을 한 다음에 연결되도록 하는 것이 좋다. (EvnetBridge에 대한 개념 설명은 별도의 세션에 정리를 하였다(참고))EvnetBridge에서 &quot;규칙 생성&quot; - 이름 입력 및 규칙 유형을 &quot;일정&quot;으로 선택하고 일정한 빈도로 실행(1분에 한 개씩)할 것이기 때문에 빈도 값과 단위를 선택해서 EventBridge의 규칙을 생성해주면 된다. 그 다음에는 AWS 서비스를 선택하고, Lambda function을 선택해서 이전에 생성한 log-generator lambda function을 선택한다. 이렇게 되면, EventBridge에 의해 Lambda 함수가 실행되어, S3에 파일이 하나씩 떨어지는 것을 확인할 수 있다. (IoT 기기에서 센서 로그를 찍어내는 것과 같은 상황 구현) log-generator에 의해서 생성되는 JSON 포맷의 데이터에 맞게 Athena table의 schema를 생성해줘야 한다. [Create] - S3 Bucket data을 선택하고, 테이블 명과 데이터 베이스를 선택해준 다음에 Dataset을 현재 1분에 한 개씩 로그 데이터가 쌓이고 있는 location을 지정해준다. 그리고 Data format section에서는 현재 CSV 파일의 형태로 S3 Bucket에 데이터를 쌓고 있기 때문에 JSON을 선택해준다.열 정보에서는 현재 쌓고 있는 JSON 타입의 데이터의 각 Key값을 column으로 타입과 함께 명시를 해준다.(최종적으로 테이블을 생성한 다음은 테이블 미리보기를 통해서 테이블을 통해 값을 미리 확인해본다) 5분에 한 번씩 compaction해서 처리하는 Batch 작업을 하기 위해 log-compactor Lambda 함수를 생성한다.log-compactor에서는 5분에 한 번씩 Athena에 쿼리를 날려서 압축 및 변환을 하는 query를 실행을 시키도록 하는 부분이다. (실제 실행은 Athena에서 처리)실제 log-compactor Lambda 함수의 파이썬 스크립트를 살펴보면, 실제 Athena로 전송하는 CTAS 쿼리문의 조건 WHERE 절을 살펴보면, timestamp 칼럼의 start_time, end_time을 통해 특정 기간내의 데이터 파일만 모아서 하나의 파일로 압축하는 형태로 쿼리를 처리하고 있다.log-compactor lambda 함수의 환경변수는 S3의 Bucket(1분에 한 개씩 센서 데이터를 누적하고 있는 버킷)과 Athena에서 생성한 테이블 정보와 테이블을 포함하고 있는 DB, 생성될 파일의 prefix(NEW_TABLE_NAME), 참조할 DB와 TABLE 정보를 입력해주면 된다. 새로 생성한 log-compactor Lambda function에는 이전 log-generator Lambda function과 동일하게 Athena와 S3에 접근해서 처리할 수 있도록 권한을 부여해야 한다. (AmazonS3FullAccess, AmazonAthenaFullAccess) 이제 생성한 log-compactor Lambda 함수와 mapping되는 EventBridge 이벤트를 생성해줘야 한다. 이 EventBridge는 log-generator와 동일하게 하되, 5분 간격으로 복수 개의 파일에 대해 compaction을 할 것이기 때문에 5분 단위로 lambda함수가 실행되도록 해야한다. S3의 압축된 파일도 압축된 original file의 데이터 타입(JSON)을 통해 “Query with S3 Select”를 해서 제대로 데이터가 찍혀있는지 확인을 할 수 있다. 기타 주의사항 Athena 테이블에서 ‘-‘ 특수기호를 넣어서 DB나 Table이름을 지정하게 되면, 쿼리에서 테이블을 호출 할때 에러가 발생한다. (중요)Apache KafkaApache Kafka는 데이터 엔진지어 분야 외에도 백엔드에서도 많이 사용된다.MS에서 인수한 Linked에서 개발되었다가 2011년쯤에 오픈소스화된 프로젝트로, 스칼라 언어로 개발되었다. 주로 데이터 스트림을 위한 미들웨어, 메시지 큐, 메시지 브로커(이벤트 브로커)등의 말로 불리고 있다.어플리케이션이나 소프트웨어 간의 메시지 중계자 역할을 해주고 있다. [Kafka의 특징] pub/sub 모델이다. 데이터의 영속성을 가진다.(Data Persistency) 일정기간동안 데이터를 보관하기 때문에 데이터의 재처리가 가능하다. 손쉽게 scale in/out이 가능하며, 고가용성이다. (Highly scalable and available) At least once를 지원(최근에는 exactly once를 지원하지만 성능이 낮아진다) 비동기 처리를 위한 Pub(발행)/Sub(구독) Model마치 전화가 아닌 이메일과 같은 연락 방식과 유사하다고 생각할 수 있다.송/수신 관계에 있는 양 극단의 Publisher와 Subscriber가 Direct로 연결하는 방식이 아닌, 중간에 중개자 역할을 해주는 Broker를 위치시킨다.(A----(Broker)----B) A는 B 서버로 보내기 위해 B의 서버 주소나 DNS에 관한 정보를 알 필요 없이 바로 Broker로 데이터를 보내서 적재하면 되며, 만약에 B 서버에 문제가 생겨서 다운되더라도 A에서 송신한 데이터는 Broker에 누적이 되고 있기 때문에 B 서버가 복구된 후에 Broker로부터 쌓인 데이터를 이어서 수신해오면 된다. 그리고 Subscriber 역할을 하는 B는 역량만큼만 Broker로부터 데이터를 취득해서 처리하며, 기존 API호출의 경우에는 DDos 공격과 같이 request를 무한정 보내서 문제가 될 수 있지만, 중간에 Broker역할을 해주는 Message Queue를 위치시켜주면, 구조상 Loose coupling이 가능해서 DDos와 같은 문제상황을 사전에 예방하고, 확장가능한 설계를 할 수 있다. (Direct로 A와 B 서버를 연결하게 되면, 이러한 확장 가능한 구조가 되지 않는다) MSA(MicroService Architecture) - 확장 가능한 설계 Kafka에서는 Producer(Publisher), Consumer(Subscriber)라고 한다. 원래 RabbitMQ나 일부 메시지 큐의 경우에는 메시지 큐에서 받은 데이터를 가져가면 브로커 입장에서는 메시지가 사라지게 된다.(일정기간 in-memory에 가지고 있다가 최종 전달자에게 전달하면, 사라지는 형태로 동작) 하지만, Kafka의 경우에는 publisher가 데이터를 넣으면 스토리지에 저장한다. (데이터의 영속성(Persistency)(Data retention 기간(데이터를 스토리지에 저장하는 기간)에 대한 설정을 할 수 있다. (해당 retention 기간 내에는 다른 consumer들이 요청을하면 같은 메시지를 처리할 수 있다 - 데이터 재처리 가능)) Kafka의 구조Kafka는 Broker라고 불리우는 서버가 여러대로 클러스터링 되어 구성되어있으며, Apache Zookeeper와 함께 쓰인다. Apache Zookeeper는 누가 리더인지 정해주는 코디네이터 역할을 해준다. Apache 3.0부터는 Zookeeper없이 Zookeeper의 기능을 Broker가 내장되서 기능이 개선되고 있다. 아직은 Production 단계이며, 올해 말쯤에는 도입될 새로운 기능이다.이렇게 Kafka cluster + Apache Zookeeper에 producer와 consumer가 붙어서 데이터를 주고받는 형태를 갖는다. [참고] KIP-500 IssueKafka open source project 500번 이슈 -&gt; Kafka without Zookeeper(kafka 2.8) Zookeeper의 기능Zookeeper는 분산된 카프카 브로커를 클러스터링해주는 중요한 컴포넌트이다. 클러스터링 되어있는 Broker들 사이에서도 리더가 있어서 어떤 친구가 데이터를 처리할 것인지, 쓸 것인지 구분해야 되기 때문에 Zookeeper가 이러한 리더의 선출을 담당한다. 그리고 파티션 수와 같은 Topic(Topic은 메시지를 넘겨 줄때 사용되는 채널이며, 해당 토픽에 producer가 메시지를 push하고, consumer가 메시지를 pull해서 소비하는 구조의 형태로 되어있다)의 메타데이터를 관리하며, 정보를 공유한다.또한 새로운 브로커를 추가하거나 브로커의 장애를 감지하는 기능을 한다. Topic / Partition Topic은 메시지를 분류해주는 채널이며, partition은 Topic을 여러 애들이 분산처리하기 위해서 파티션으로 나눈다.(데이터 병렬 처리) 그리고 나뉜 파티션들은 각 각 브로커들이 담당한다. 단일 파티션 생성이 가능하지만, 내결함성이 떨어진다.(만약에 토픽에 브로커가 세 대있는데, 단일 파티션으로 하나의 브로커만 이용하도록 하면, 장애가 발생해서 데이터가 유실되었을 했을 때 대처하기 어렵기 때문에 세 개의 브로커를 사용하는 것이 좋다)단, 파티션은 늘릴 수는 있어도 줄일 수는 없다. 나뉜 각 각의 파티션은 데이터양이 많을 경우, 각 각의 파티션에 consumer를 물려서 확장성있게 대량의 데이터를 처리할 수 있다. (확장성과 병렬처리 - 파티션) 모든 메시지는 offset 값(bigint 기준)으로 어디까지 메시지가 읽혔는지 기록이 된다. (토픽이 새롭게 생성되지 않는 이상 계속 증가되는 형태이다) Replication Replication이란, Fault-tolerance(장애)에 대비하여 메시지를 설정한 replication-factor만큼 복제해서 각 각의 브로커들에 분산시키는 작업을 말한다.데이터는 파티션 단위로 복제를 하며, 복제를 통해 일부 브로커가 불능 상태가 되어도 전체 클러스터는 정상 작동하도록 도와준다.안전을 위해서 권장은 되지만, overhead가 존재하기 때문에 토픽의 특성과 리소스 사용량을 고려해서 replication-factor를 정해야한다. ISR(In-Sync Replicas)와 Producedr acks Replication을 위해서는 파티션별로 Zookeeper가 Leader와 Follower 역할을 할당한다. Leader와 Follower는 ISR이라는 그룹으로 묶는다. 각 각의 파티션의 Leader들은 Follower들이 offset을 잘 맞춰서 따라오고 있는지 모니터링하고, 잘 못 따라오는 Follower가 있다면, ISR 그룹내에서 추방한다. Follower들은 Leader가 가진 데이터와 자신의 데이터를 일치시키기 위해서 지속적으로 리더로부터 데이터를 땡겨온다. Leader가 불능상태가 되면, ISR 그룹 내의 Follower 중 하나를 Leader로 선출한다. Consumer &amp; consmer group(메시지를 가져오는 에이전트들의 집합) Consumer Group으로 나누는 이유는 그룹내의 Consumer들끼리 서로 중복된 데이터를 땡겨오지 않게 하기 위해서이다. Consumer는 특정 Consumer Group에 반드시 속한다. 각 consumer들은 토픽의 파티션에 1:1로 매핑되어 메시지를 땡겨온다. Consumer Group은 고유 group-id를 가지고 있고, 이를 통해 그룹을 구분하고, offset등의 정보를 관리한다. Consumer를 띄울때 Consumer Group을 지정할 수 있다. consumer group의 group id를 group1으로 띄우게 되면, 이 group1에 속한 Consumer들이 모든 데이터를 땡겨오고 나서 다음 offset부터 데이터를 땡겨올 수 있다. (group-id: group1로 지정한 경우)하지만 데이터의 처음 offset부터 데이터를 다시 땡겨오고 싶은 경우에는 어떻게 해야될까? 방법은 group-id를 group1이 아닌 다른 이름의 consumer group(다른 이름)으로 해서 별도의 Consumer를 그룹화하면 된다. 그러면 새로 생성한 group-id: x는 offset을 처음부터해서 데이터를 읽어온다.따라서 지정한 group-id별로 offset이 관리가 된다는 것을 이해할 수 있었다. 각 consumer group에 속한 consumer들은 메시지를 가져오고, 그 메시지에 해당하는 offset을 commit하게 되는데, 메시지를 어디까지 가져왔는지 Broker에 표시하기 위한 용도로 사용한다. offset 값을 통해서 데이터를 생산해내는 쪽(producer)와 소비하는 쪽(consumer)간의 속도차이도 파악할 수 있다. 만약 producer에서 100의 데이터를 보내고 consumer에서 50만큼 소비했다면, 이 속도차이 50을 Lag(데이터의 생산과 소비 속도차이)-offset 번호 차이이라고 한다.이를 통해 producer와 consumer의 갯수를 조정한다. Consumer 장애 발생시 구동 방식 Consumer group내에서 특정 Consumer에 장애가 발생하면, re-balancing이 일어나서 consumer 그룹 내의 다른 consumer가 장애 발생한 consumer의 몫까지 데이터를 소비한다. 이후에 장애가 발생한 consumer가 복구가 다 되면, 다시 Partition과 연결되어 데이터를 이어받는다. 데이터가 많아져서 파티션을 늘린 경우에는 다수의 Partition을 하나의 Consumer에 물릴 수 있다. 단, 하나의 파티션에 여러개의 Consumer가 붙을 수는 없다. ProducerKafka broker에 메시지를 send/publish/produce하는 인스턴스(에이전트)이다. Producer관점에서는 Acks 옵션 [0, 1, -1(all)]이 매우 중요하다. 이전에 ISR 그룹 내에서의 작동원리에 대해서 배웠는데, Producer의 입장에서는 아래와 같이 acks의 옵션으로 다르게 할 수 있다. acks=0으로 설정을 하면, 안정성은 낮은 대신에 속도가 빠르다. 그 이유는 프로듀서가 리더 파티션에 메시지를 전송하고 나서 리더 파티션으로부터 별도의 acks(확인)을 하지 않는 방식이다. 재전송이나 offset을 신경쓰지 않기 때문에 별로 중요하지 않은 로그 데이터를 보낼때 사용되는 옵션이다.acks=1은 안정성과 속도가 중간으로, 프로듀서가 리더 파티션에 메시지를 전송하고, 리더로부터 ack를 기다린다. 하지만 팔로워들에게까지 잘 전송되었는지에 대해서는 신경쓰지 않는다. 리더들에게는 ack를 받았지만, 팔로워들에게 데이터를 복제하는 도중에 리더가 죽으면 fail이 발생한다.acks=-1(all)은 안정성이 높은 대신에 속도가 느리다는 특징을 가진다. 프로듀서가 전송한 메시지가 리더와 팔로워 모두에게 잘 저장이 되었는지 확인을 전부하기 때문에 속도가 느릴 수는 있지만 안정적이다. ISR에 포함된 모든 파티션에 전달이 되었는지 확인하는 것이 아닌 min.insync.replicas에 값을 별도로 지정해서 몇 번째 팔로워까지 복제본 저장이 되었는지 확인을 해야하는지 지정할 수 있다.(min.insync.replicas 값이 2이면 리더(1), 팔로워(1)까지 확인을 하고, 값이 3이면, 리더(1), 팔로워(2)까지 확인을 한다.) Amazon MSK(Managed Streaming for Kafka) 이전에 완전 관리형 Kafka 서비스라고 들어본적이 있는데, 마침 오프라인 수업에서 다뤄주셔서 너무 좋았다. Amazon MSK는 Zookeeper의 설치 없이 손쉽게 Kafka cluster를 관리하고 Connector 연동하는 것이 가능하다. Kafka는 Connector가 중요한데, 앞/뒤로 데이터를 넣고 빼는 부분을 MKS에서는 플러그인을 설치해서 연결시켜줄 수 있다. 아직 한국에서는 서비스를 하고 있지 않지만, Amazon MSK Serverless라고 하는 별도의 용량관리 없이 자동으로 프로비저닝해주고, 순수 Kafka의 기능 API만 사용하도록 지원하고 있다. (Amazon Kinesis가 Amazon MSK Serverless와 같이 Kafka의 기능을 별도의 프로비저닝 없이 기능만 사용하고 사용한 만큼 요금만 납부하는 형태로 이용할 수 있도록 해주는 서비스이다)","link":"/2022/06/12/202206/220612_datapipeline_study/"},{"title":"220614 Terraform study","text":"이번 포스팅에서는 Terraform에 대해 공부한 내용을 정리해두려고 한다. 지금 Docker나 Kubernetes도 데이터 파이프라인 구축시 별도의 서버를 컨테이너로 띄우고 관리하기 위해서 공부를 하고 있지만, 결국에는 반복을 통해서 익숙해지는 것이 최고이기 때문에 Terraform도 빠르게 개념을 정리하고 어떻게 AWS 인프라를 코드로 작성해서 간편하게 관리할 수 있는지에 주안점을 두고 학습을 해보려고 한다.Terraform을 학습하게 된 계기는 우선 AWS의 서비스로 데이터 파이프라인을 구축할때 웹 페이지나 AWS CLI 상에서 계속 반복적인 작업을 통해 각 컴포넌트를 생성하는데 있어, 비효율적인 것 같다는 생각에서 비롯되었다.그러던 중 현재 오프라인으로 DE 현직자에게 AWS 클라우드 환경에서 데이터 파이프라인 구축하는 방법에 대해서 수업을 듣고 있는데, Terraform과 같은 IaC(Infrastructure as Code)로 관리를 하게 되면, 좀 더 수월하게 데이터 파이프라인의 AWS 인프라를 빠르게 구축할 수 있다고 해서 개인적으로 공부를 시작하게 되었다. Terraform ? Terraform은 IaC로, 인프라를 코드로 정의함으로써, 손쉽게 인프라 전반을 관리할 수 있도록 해주는 툴이다. Homebrew를 사용해서 Terraform 설치 package management tool인 homebrew를 사용해서 Terraform을 설치할 수 있다. 12$brew install terraform$terraform -v # Terraform v1.2.2 Terraform SETUP in VSCode Terraform을 만든 HashiCorp의 VSCode - Terraform extension을 설치한다. 자동완성, 자동 포맷팅 등의 다양한 기능들을 지원한다. Terraform 실습 Terraform 파일은 HashiCorp configuration language로 작성이 되며, 파일 확장자는 *.tf이다. Terraform은 다양한 provider를 제공한다. 따라서 terraform과 연동하고자하는 provider의 plug-in을 다운받아서 설치해야한다. 플러그인 다운은 Terraform configuration 파일에서 선언해서 필요한 provider의 플러그인을 다운받도록 할 수 있다. 아래 링크는 내가 데이터 파이프라인을 구축할때 AWS 클라우드 플랫폼을 사용할 것이기 때문에 AWS Provider에 대한 Terraform 공식 사이트 documentation이다. https://registry.terraform.io/providers/hashicorp/aws/latest/docs 나는 현재 Terraform v1.2.2를 사용하고 있기 때문에 Terraform 0.13 이상 버전과 호환되는 공식문서의 내용을 참고한다. main.tf 1234567891011121314151617181920212223242526terraform { required_providers { aws = { source = &quot;hashicorp/aws&quot; version = &quot;~&gt; 3.0&quot; } }}# Configure the AWS Provider# Seoul - (ap-northeast-2)provider &quot;aws&quot; { region = &quot;ap-northeast-2&quot;}# 권장되지 않는 방식provider &quot;aws&quot; { region = &quot;ap-northeast-2&quot; access_key = &quot;[access_key]&quot; secret_key = &quot;[secret_key]&quot;}# 예시) Create a VPCresource &quot;aws_vpc&quot; &quot;example&quot; { cidr_block = &quot;10.0.0.0/16&quot;} Authentication &amp; Configuration공식 사이트에 따르면, Provider configuratiopn에서 Hard-coded Credentials 방식은 Terraform configuration에서 권장되지 않는다고 한다.(만약 AWS의 access_key나 secret_key에 대한 정보를 provider &quot;aws&quot;에 함께 기입을 했을 경우에는 반드시 VCS(Version Control System)에 업로드되지 않도록 주의해햐 한다.)CodeBuild나 ECS를 사용하고 있고, IAM Task Role에 대해 정의를 하고 있다면, Terraform에서 해당 컨테이너의 Task Role을 활용할 수 있다고 한다. 또는 Users/사용자명/.aws 하위에 conf와 credential 파일이 있는데, 해당 path를 적용해서 하는 방법도 있다.그런데 적용을 해봤는데, 적절하지 않는 argument라고 나와서 이 부분도 다시 document를 보고 해봐야겠다. 실습1) Terraform으로 AWS EC2 인스턴스 생성하기 EC2 인스턴스를 생성하기 위해서 resource 뒤에 생성할 resource 이름을 적게 되는데, 항상 위에 첨부한 공식 사이트에서 검색을 통해서 참고하도록 한다. ec2_instance.tf 1234567891011121314151617181920212223242526272829terraform {required_providers { aws = { source = &quot;hashicorp/aws&quot; version = &quot;~&gt; 3.0&quot; } }} # Configure the AWS Provider# Seoul - ap-northeast-2provider &quot;aws&quot; { region = &quot;ap-northeast-2&quot; access_key = &quot;[access_key]&quot; secret_key = &quot;[secret_key]&quot;}# [참고] provider의 resource 생성하기resource &quot;&lt;provider&gt;_&lt;resource_type&gt;&quot; &quot;name&quot; { config options...... key = value&quot; key2 = &quot;another value&quot;}resource &quot;aws_instance&quot; &quot;my-first-terraform-server&quot; { # ami의 id는 EC2 인스턴스를 생성할때 ami-* 로 시작하는 부분을 복사해서 넣어주면 된다. ami = &quot;ami-058165de3b7202099&quot; instance_type = &quot;t3.micro&quot;} 우선 위에서 작성한 *.tf 파일을 실행하기 전에 가장 먼저 $terraform init을 해줘야 한다. terraform init 명령을 통해 현재 위치한 디렉토리 내의 *.tf파일의 설정을 스캔하고, *.tf 파일의 내용에서 정의한 provider에 대한 내용 또한 스캔한다. 이를 통해 정의한 aws과 관련되서 필요한 플러그인을 aws api와의 상호작용을 통해 다운로드받는다. 12345$terraform init# Initializing provider plugins...# - Finding hashicorp/aws versions matching &quot;~&gt; 3.0&quot;...# - Installing hashicorp/aws v3.75.2...# - Installed hashicorp/aws v3.75.2 (signed by HashiCorp) 그 다음으로 살펴 볼 명령은 $terraform plan인데, 이 명령은 선택사항인데, 내가 code로 작성한 infrastructure를 실제로 적용하기 전에 변경사항을 체크해볼 수 있는 명령이다. terraform plan 명령을 하기 전에 terraform init 명령이 선행되어야 한다. 초록색으로 표기(+)되는 리소스는 새로 추가되는 리소스이며, 빨간색으로 표기(-)되는 리소스는 제거된 리소스, 주황색으로 표기(~)되는 리소스는 수정된 리소스로 표기된다. 1$terraform plan 이제 최종적으로 진행될 작업을 확인한 뒤에는 $terraform apply명령을 통해 실행을 시킨다. 1$terraform apply --auto-approve 최종적으로 AWS EC2 Instance Dashboard를 통해서 인스턴스가 생성된 것을 확인할 수 있었다. 인스턴스 세부 옵션을 넣는 부분도 확인해봐야겠다. 만약 작성한 *.tf파일을 수정없이 다시 terraform apply를 하게 되면, 실제 AWS Infrastructure와 적용하고자 하는 *.tf파일의 내용을 비교해서 변화가 없으면 실행을 하지 않는다. 이렇게 간단하게 코드로 작성해서 EC2 인스턴스를 생성하니, AWS 웹 페이지에서 번거롭게 마우스로 클릭하는 일을 하지 않아도 되서 좋은 것 같다.","link":"/2022/06/14/202206/220614-terraform-study/"},{"title":"220616 Kubernetes 스터디 4일차","text":"ClusterIP Service 실습ClusterIP Service를 생성하고, 지정한 selector가 app:pod를 가지는 pod와 연결을 하였다. 그런 다음에 연결된 app:pod label을 가지는 Pod를 삭제하고, 다시 재생성을 한다. 이렇게 되면, 실제 pod에 할당되어있던 IP는 동적으로 할당이 되어 변경이 되지만, ClusterIP Service는 재생성이 되지 않아, 동일한 Service IP로 Pod에 접근이 가능하다. NodePort Servce 실습실제 Dashboard의 디스커버리 및 로드 밸런싱 하위의 서비스 항목을 가보면, 내부 엔드 포인트가 두 개 생성된 것을 확인할 수 있는데, 상위에 있는 9000번 포트가 클러스터 내에서 접근할때 사용되는 포트이고, 30000번 포트가 각 각의 노드들에 동일하게 할당된 포트 번호이다. 이 포트를 통해 외부에서 클러스터 내부의 Service를 통해 하위 Pod들에 트래픽을 보낼 수 있다.실습에서는 외부 터미널에서 curl 명령을 통해 서비스에 접근하여 각 pod에 대한 hostname이 출력되는 것을 확인하였다. LoadBalancer Servce 실습LoadBalancer service의 경우에는 이전에 이론에서 배웠듯이 외부로 연결되는 IP가 생성되기 위해서는 플러그인이 설치가 되어있어야 한다. 1$kubectl get service svc-2 위 명령을 통해 확인해보면, EXTERNAL-IP가 으로 표기된 것을 확인할 수 있다. Object - Volume 이전에 Kubernetes의 전체 구조를 살펴볼때 pod에 문제가 생겨서 재생성되면, 내부 데이터가 다 날라가기 때문에 pod의 데이터를 별도의 volume에 mount시켜서 저장을 하고 관리해야 한다는 부분에 대해서 배웠다. 이번 세션에서는 이 Volume에 대해서 좀 더 구체적으로 이론적인 부분을 정리해보려고 한다. Volume의 종류로는 emptyDir, hostPath, PVC/PV가 있다. (1) emptyDiremptyDir은 Pod 생성시 Pod의 내부에 만들어지고, Pod가 삭제되면 사라진다. 그래서 일시적 사용 목적으로 사용이 되며, 복수의 컨테이너 간에 파일을 공유할 때 사용이 된다.만약 Web Server를 서비스하고 있는 Container1과 Backend server를 서비스하는 Conatiner2가 있다고 가정하고, 두 서버간에 파일을 공유해야되는 경우, 두 컨테이너를 하나의 볼륨에 연결시켜놓으면 손쉽게 파일을 공유할 수 있다. 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: pod-volume-1spec: containers: - name: container1 image: tmkube/init volumeMounts: - name: empty-dir # container가 mountPath 경로로 마운트하겠다는 의미이다. mountPath: /mount1 - name: container2 image: tmkube/init volumeMounts: - name: empty-dir # 만약 경로가 잘못되어도 name에서 empty-dir을 가르키고 있기 때문에 # empty-dir volume으로 mount가 된다. mountPath: /mount2 volumes: - name: empty-dir emptyDir: {} (2) hostPathhostPath는 Pod의 데이터를 저장하기 위한 용도가 아닌, Node에 있는 데이터를 Pod에서 쓰기 위한 용도이다.hostPath에서 host란 Pad가 올라가있는 Node를 의미한다. 전체 구조는 Node내부에 Pod가 있고, Pod가 Node에 존재하는 Volume에 연결되어있는 구조이다. 그리고 Node 내의 Volume은 Pod의 유/무와 관계없이 사라지지 않는다. 문제상황Node1에 Pod1, Pod2가 있고, Volume에 두 Pod가 연결되어있는 상황에서 Pod2에 문제가 생겨서 재생성되었을때, 스케줄러에 이해서 Node2에 재생성이 되거나 Node1에 장애가 발생을 해서 통으로 다른 Node로 이전을 해야되는 상황이 생겼을때, 기존 Node1에 있는 Volume에 연결되지 못한다.이러한 상황에서 해결방법은 Node가 추가될때마다 항상 동일한 경로에 마운트 시켜주는 것이다. (Node1의 Volume(/node-v1), Node2의 Volume(/node-v1)) 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: pod-volume-2spec: containers: - name: container image: tmkube/init volumeMounts: - name: host-path mountPath: /mount1 volumes: - name: host-path hostPath: # 사전에 해당 경로에 생성을 해둬야 한다. path: /node-v type: Directory 사용사례각각의 노드는 자기자신을 위해서 사용되는 파일이 있다. (시스템 파일, 기타 설정 파일들)Pod는 자신이 속해있는 host(Node)의 데이터를 읽거나 써야될때 hostPath 유형의 Volume을 사용한다. (3) PVC(Persistence Volume Claim) / PV(Persistence Volume)PVC/PV의 목적은 Pod에 영속성있는 Volume을 제공하기 위함이다. 우선 PVC/PV의 흐름을 살펴보면, 우선 (1)admin이 PV 정의에 대해서 생성을 하면, (2)사용자가 PVC를 생성한다. 그런 다음에 (3)K8S가 PVC의 내용에 맞는 적절한 Volume에 연결을 시켜준다.Pod를 생성할때 사용자가 생성한 PVC를 사용하면 된다. 흐름도는 Pod -&gt; PVC -&gt; PV (복수) -&gt; Volume인데, Pod가 PV에 직접 연결이 되지 않고, PVC를 통해서 연결되는 이유는 k8s는 volume영역에 대해 사용자 영역과 관리자 영역으로 나뉘기 때문이다.여기서 사용자란 Pod에 서비스를 만들고 관리/배포를 담당하는 업무를 하며, 관리자는 PV를 작성한다.Volume에는 Local과 Remote 원격 Volume이 있는데, 원격 Volume에는 AWS, Git, NFS(Network File System), Volume을 직접 만들고 서비스도 가능한 Sorage OS가 있다. (Pod ~ PVC 영역이 사용자 영역이며, PV ~ Volume이 관리자 영역이다) pod.yml 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-volume-3spec: containers: - name: container image: tmkube/init volumeMounts: - name: pvc-pv mountPath: /volume volumes: - name: pvc-pv persistentVolumeClaim: # 이 claimName이 PVC 정의 파일의 metadata name과 연결된다. claimName: pvc-01 pvc.yml 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc-01spec: accessModes: - ReadWriteOnce # 읽기/쓰기 모드가 되고, resources: requests: storage: 1G # 용량이 1G인 Volume이 필요(용량 할당) # 현재 생성되어 있는 PV들 중 선택이 된다. &quot;&quot;(필수) storageClassName: &quot;&quot; pv.yml 123456789101112131415161718192021222324252627apiVersion: v1kind: PersistentVolumemetadata: name: pv-01 # 앞서 k8s가 PVC 내용에 맞는 적절한 Volume에 연결을 해준다고 했는데, # 바로 아래 spec의 capacity와 accessModes의 내용이 근거가 된다!! # 설정이 되면, PVC에서 작성한 요청에 맞는 Volume에 연결이 된다. spec: capacity: storage: 1G accessModes: - ReadWriteOnce # 읽기/쓰기 모드가 되고, # 실제로는 많이 사용되지 않는다. local: path: /node-v # 아래의 내용은 PV에 연결되는 Pod들은 node들은 node1(label) # 노드 위에서만 무조건적으로 생성된다는 의미이다. nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - {key: node, operator: In, values: [node1]} resources: requests: storage: 1G # 용량이 1G인 Volume이 필요(용량 할당) # 현재 생성되어 있는 PV들 중 선택이 된다. &quot;&quot;(필수) storageClassName: &quot;&quot; PV를 전문적으로 관리하는 관리자가 필요한 이유는 아래와같이 세부적으로 PV에 대한 작성을 해야되며, PV를 만들어두면, User는 사용을 위해서 PVC를 생성하는 패턴으로 작업을 하기 때문이다. 아래 내용은 remote volume 지정시, spec의 하위에 정의한다. 1234567891011121314nfs: service: path:iscsi: targetPortal: iqn: lun: fsType: readOnly: chapAuthSession:gitRepo: repository: revision: directory:","link":"/2022/06/16/202206/220616_kubernetes_study/"},{"title":"220618 Terraform study","text":"이번 포스팅에서는 Terraform에서 생성한 AWS Resource를 수정하는 부분에 대한 내용부터 시작해보려고 한다.Terraform은 손쉽게 AWS Infrastructure를 구성할 수 있도록 도와줌과 동시에 코드를 통해 전체적인 AWS Infrastructure의 구조를 파악할 수 있다. Terraform의 동작 방식 만약 실행하고자 하는 *.tf 파일이 이전에 실행을 한 다음에 별도의 수정을 하지 않았다면, $terraform apply명령을 실행하여도 아무런 변화가 일어나지 않는다. 그 이유는 terraform은 기본적으로 $terraform plan명령을 통해 확인할 수 있는 변화된 내용이 있어야 $terraform apply명령을 통해 새롭게 인프라를 구축할 수 있다. 만약 기존 리소스에서 tag에 대한 정보만 추가한 다음에 다시 실행하면, 기존에 생성되었던 리소스는 유지된 상태에서 resource name (Name)이 추가가 된다. Delete resources 기존에 *.tf파일을 통해 생성하였던 리소스들을 삭제하려면, $terraform destroy 명령을 치면 된다. 1$terraform destroy 만약 기졵의 .tf 코드에서 일부 리소스를 주석처리하게 되면, apply를 했을때 주석처리된 리소스를 제거한다. (일부 리소스 제거) Reference resources 우선 실습을 위해 VPC를 구성하고, VPC 내에 별도의 서브넷을 구성하도록 한다. VPC는 Virtual Private Cloud의 약어로 격리된 Private Network 영역이다. AWS 사용자가 정의한 가상 네트워크로써 cidr 블록 방식으로 IP대역대를 설정한다. VPC의 대표 구성요소는 아래와 같다. ref. *.tf 코드에서 선언 순서는 문제가 되지 않는다. 그 이유는 Terraform이 Subnet이 VPC에 속해있다는 것을 파악하고 코드를 실행하기 때문에 순서가 뒤바뀌어도 VPC를 우선 생성하고, Subnet을 생성한다. VPC 구성 Internet Gateway(IGW)vpc 내부의 public subnet 상의 인스턴스들과 외부 인터넷 간의 통신을 위해 vpc에 연결하는 게이트웨이이다. NAT Gateway(NGW)nat는 네트워크 주소를 변환하는 장치로, ngw는 vpc 내부의 private subnet의 인스턴스들과 인터넷/AWS 서비스에 연결하는 게이트웨이이다. 실제 구성은 public subnet에 위치하며, Elastic IP를 할당한 상태로 구성된다.고정 할당된 Elastic IP를 route tableㅇ르 통해 연결시킨다.여기서 말하는 Routing Table이란 네트워크 트래픽을 전달할 위치를 결정하는데 사용되는 라우팅 규칙들의 집합이다.(subnet - subnet, subnet - gateway간의 통신을 결정한다) Security GruopNetwork ACL과 함께 VPC의 보안 장치 중 하나이며, Security Group은 인스턴스 단위의 보안계층이다. 인스턴스에 대한 인바운드/아웃바운드 트래픽을 제어하는 가상 방화벽 역할을 한다.Network ACL과 달리 stateful하다. Network ACLsecurity group과 함께 VPC의 보안 장치 중의 하나로, 1개 이상의 subnet과 외부 트래픽을 제어할 수 있다. 즉, subnet 단위의 보안 계층이며, stateless라는 특징을 가진다. [aws_vpc] 구성https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc 12345678910# VPC 내에 Subnet을 구성한다.# VPC(Virtual Private Cloud) - Private Isolated Network# first-vpc는 지정한 리소스의 이름으로, 해당 리소스를 참조할때 사용된다. (terraform내에서 참조)resource &quot;aws_vpc&quot; &quot;first-vpc&quot; { cidr_block = &quot;10.0.0.0/16&quot; tags = { Name = &quot;production&quot; }} [aws_subnet]https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/subnet 12345678910# Subnet 생성resource &quot;aws_subnet&quot; &quot;subnet-1&quot; { # reference the vpc created vpc_id = aws_vpc.first-vpc.id cidr_block = &quot;10.0.1.0/24&quot; tags = { Name = &quot;prod-subnet&quot; }} 최종적으로 AWS에서 VPC - VPC dashboard를 살펴보면, default VPC와 production VPC 가 생성된 것을 확인할 수 있다.그리고 Virtual private cloud의 하위에 Subnets 메뉴를 통해서 VPC와 함께 새로 생성된 prod-subnet도 생성된 것을 확인할 수 있다. Terraform project files .terraform/plugins/… .terraform/plugins/… 하위에는 terraform init명령을 했을때 작성한 *.tf 파일을 읽고, 설치가 필요한 platform의 plugin을 다운받는다. 다운받음과 동시에 해당 configuration 정보가 하위 폴더에 저장한다. terraform.tfstate(매우 중요한 파일) 모든 Terraform의 상태정보를 담고 있는 파일이다. 우리가 *.tf 파일에 작성했던 AWS의 리소스의 정보 일부를 수정하게 되면, Terraform이 이를 감지해서 apply시 변경사항을 반영해주는데, 이러한 상태정보를 저장해서 담고 있는 파일이 바로 terraform.tfstate 파일이다.*.tf 파일의 내용을 수정하고, tfstate 파일의 내용을 보면, 수정된 내용이 반영된 것을 확인할 수 있다. Practice Project 여지까지 배운 내용을 기반으로 간단한 연습 프로젝트를 진행해본다. 내용은 EC2 인스턴스를 Custom Subnet 영역에 Deploy하고, Public IP를 할당해서 SSH 연결뿐만 아니라 웹 서버를 구동시킬 수 있도록 구성해본다. [STEP1] VPC 생성하기이름이 prod-vpc인 VPC를 생성한다. 123456resource &quot;aws_vpc&quot; &quot;prod-vpc&quot; { cidr_block = &quot;10.0.0.0/16&quot; tags = { Name = &quot;production&quot; }} CIDR(Classless Inter-Domain Routing)은 클래스가 없는 도메인간 라우팅하는 기법이다.클래스가 없다는 뜻은 네트워크 구분을 별도의 클래스로 하지 않는 다는 의미로, Class는 사이더가 나오기전 사용했던 네트워크 구분 체계이다. 사이더가 나오면서 Class 체계보다 더 유연하게 IP 주소를 여러 네트워크 영역으로 나눌 수 있게 되었다. CIDR는 위의 Infra-Domain과 같이 각 네트워크 대역을 구분 짓고, Inter-Domain과 같이 구분된 네트워크간 통신을 위한 주소 체계라고 이해하면 된다. [STEP2] Internal Gateway 생성하기traffic을 인터넷 밖(outbound)으로 보내고, public ip를 서버에 할당하기 위함이다. ref.https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/internet_gateway 12345# IGW(Internet Gateway) 생성# VPCresource &quot;aws_internet_gateway&quot; &quot;gw&quot; { vpc_id = aws_vpc.prod-vpc.id} [STEP3] Custom Route Table 생성하기VPC 내부에서 외부로 Routing 시켜줄때 참조할 Route table을 생성한다. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table 12345678910111213141516171819202122232425# Route table 생성resource &quot;aws_route_table&quot; &quot;prod-route-table&quot; { # Route table이 적용되는 vpc영역에 대한 지정 vpc_id = aws_vpc.prod-vpc.id # IPv4에 대한 Route table setup route { # cidr_block address를 IGW로 보내고, cidr_block = &quot;0.0.0.0/0&quot; # 0.0.0.0/0 send all traffic before ip will route to IGW. # 모든 트래픽이 aws IGW로 전송되기 때문에 gaqteway_id에 대한 설정이 필요하다. gateway_id = aws_internet_gateway.gw.id } # IPv6에 대한 Route table setup route { # 실제로는 IPv6를 사용하지 않지만 All traffic이 동일한 IGW로 routing되도록 설정한다. ipv6_cidr_block = &quot;::/0&quot; gateway_id = aws_internet_gateway.gw.id } tags = { Name = &quot;Prod&quot; }} [STEP4] Subnet 생성하기Subnet을 생성할때 Subnet이 속한 vpc에 대한 id 지정을 해줘야 한다. 그리고 cidr_block으로 네트워크 영역을 구분하고, AZ에 대한 설정과 tag 정보를 넣어준다. 1234567891011# Subnet 생성# AZ(Data centre에 대한 setup)# 생성한 Subnet을 상단에서 설정한 Route table에 넣어준다.resource &quot;aws_subnet&quot; &quot;subnet-1&quot; { vpc_id = aws_vpc.prod-vpc.id cidr_block = &quot;10.0.1.0/24&quot; availability_zone = &quot;ap-northeast-2a&quot; tags = { Name = &quot;prod-subnet&quot; }} [STEP5] Route Table에 Subnet 연관시키기https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table_association 12345# Associate subnet with route tableresource &quot;aws_route_table_association&quot; &quot;a&quot; {subnet_id = aws_subnet.subnet-1.idroute_table_id = aws_route_table.prod-route-table.id} [STEP6] Security group 생성(22, 80, 443 포트 허용)하기일반적으로 네트워크 트래픽은 Ingress와 egress으로 구분된다. Ingress는 외부로부터 서버 내부로 유입되는 네트워크 트래픽(Inbound), egress는 서버 내부에서 외부로 나가는 트래픽(Outbound)를 말한다.여기서 Ingress와 egress는 컨테이너 환경에서 과거 네트워크에서 사용되었던 정의가 포함이 되면서 데이터 경로와 컨테이너 환경에서 Ingress/egress라는 용어가 사용되고 있다. Ingress : 클러스터 내 서비스에 대한 외부에서의 접근(일반적으로 HTTP)를 관리하는 API 객체로, 로드 밸런싱, SSL 종료, 이름 기반 가상 호스팅 을 제공할 수 있다. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group 123456789101112131415161718192021222324252627282930313233343536373839404142# create security group resource &quot;aws_security_group&quot; &quot;allow_web&quot; { name = &quot;allow_web_traffic&quot; description = &quot;Allow Web traffic&quot; vpc_id = aws_vpc.prod-vpc.id # Port의 range setup ingress { description = &quot;HTTPS&quot; from_port = 443 to_port = 443 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } ingress { description = &quot;HTTP&quot; from_port = 80 to_port = 80 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } ingress { description = &quot;SSH&quot; from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } # egrass policy # protocol: -1 (any protocol) egress { from_port = 0 to_port = 0 protocol = &quot;-1&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } tags = { Name = &quot;allow_web&quot; }} [STEP7] STEP4에서 생성한 Subnet의 IP 주소로 Network Interface 생성하기Network Interface의 정의에서 attachement section은 나중에 EC2 Instance를 provisioning할때 대체해서 처리할 것이다. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/network_interface 1234567891011121314# Internet interface# subnet 내의 ip를 private_ips의 ip로 지정한다.(any)# 이제 sresource &quot;aws_network_interface&quot; &quot;web-server-nic&quot; { subnet_id = aws_subnet.subnet-1.id private_ips = [&quot;10.0.1.50&quot;] security_groups = [aws_security_group.allow_web.id]# EC2 Instance의 provisioning section에서 대체해서 처리한다.# attachment {# instance = aws_instance.test.id# device_index = 1# }} [STEP8] STEP7에서 생성된 Network Interface에 Elastic IP 할당하기https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eip 123456# 위에서 생성한 NI에 EIP적용를 적용한다.resource &quot;aws_eip&quot; &quot;one&quot; { vpc = true network_interface = aws_network_interface.web-server-nic.id associate_with_private_ip = &quot;10.0.1.50&quot;} AWS EIP는 IGW의 deployment에 의존적이다. 따라서 실제 EIP를 deploy하기 위해서는 IGW가 우선적으로 deploy되어야한다. (선 IGW deploy, 후 EIP deploy) 공식 문서를 보면, EIP는 IGW가 associate하기 전에 존재해야된다고 명기하고 있다. 이를 위해 EIP의 depends_on 속성을 사용하여, 명시적으로 IGW에 대한 의존성을 명시해줘야 한다. (전체 객체에 대한 참조이므로 의존성에 대해 명시할때 특정 id가 아닌 객체로써 명시해준다) [STEP9] Ubuntu server 생성 및 Apache2 설치 및 Enable하기생성되는 EC2 Instance의 AZ와 Instance가 속해있는 Subnet의 AZ는 반드시 일치시켜줘야 한다. AZ가 같다는 것은 같은 Data Centre라는 것을 의미하기 때문에 반드시 AZ은 일치시켜서 관리하도록 한다. 모든 EC2 인스턴스를 배포하기 위해서는 NIC에 대한 정의가 필요하다. 앞서 NIC에서 attachement section에 대해 생략한 부분을 Instance를 정의할때 network_interface에 대한 block 부분에 정의하도록 한다. 12345678910111213141516171819202122# Server# device에 접근하기 위해서 key-pair에 대한 이름을 정의해줘야 한다.resource &quot;aws_instance&quot; &quot;web-server-instance&quot; { ami = &quot;ami-058165de3b7202099&quot; instance_type = &quot;t2.micro&quot; availability_zone = &quot;ap-northeast-2a&quot; key_name = &quot;class-hglee-seoul&quot; network_interface { device_index = 0 network_interface_id = aws_network_interface.web-server-nic.id } user_data = &lt;&lt;-EOF #!/bin/bash sudo apt update -y sudo apt install apache2 -y sudo systemctl start apache2 sudo bash -c 'echo your very first web server &gt; /var/www/html/index.html' EOF tags = { Name = &quot;web-server&quot; }}","link":"/2022/06/18/202206/220618-terraform-study/"},{"title":"220618 데이터 파이프라인 구축 오프라인 수업 &#x2F; 4주차 오프라인 수업 전 준비","text":"이번 포스팅에서는 내일 있을 데이터 파이프라인 구축 관련 수업에서 실습할 내용들에 대해 미리 실습을 해보고, 개념적인 부분을 좀 다져보려고 한다.내일 수업에서는 Apache Kafka를 실습하는데, Kafka cluster의 각 Broker와 Zookeeper, Producer와 Consumer를 전부 Docker container로 띄우고 관리한다. 이미 Docker에 대해 공부했고, Kubernetes에 대해서 추가적으로 공부를 하는 중이라 이번 수업을 통해 Kafka의 각 구성요소들을 Docker container에 띄워서 관리하면 많은 도움이 될 것 같다.그리고 여지까지 했던 실습들과 비교했을때 상대적으로 구조가 복잡하기 때문에 미리 Kafka의 개념적인 내용을 머릿속에 그리면서 실습하게 될 전체적인 구조에 대해서 파악하고 가야 할 필요성을 느꼈다. Apache Kafka 우선 실습은 EC2 인스턴스 (Ubuntu)를 하나 띄워서 인스턴스 내부에서 진행하게 된다. (1) Kafka cluster의 각 Broker, Zookeeper, 그리고 Producer와 Consumer를 Docker로 띄워서 관리를 할 것이기 때문에 우선적으로 docker와 docker-compose를 설치한다. (2) consumer 구성에 대한 docker-compose.yml 파일 dockedr-compose.yml파일은 multi consumer와 single consumer로 나뉘어져있는데, 이름 그대로 multi consumer의 docker-compose.yml에는 producer_host가 하나, consumer_host가 1, 2로 두 개 설정되어있다. 그리고 single consumer의 docker-compose.yml에는 producer_host 하나와 consumer_host 하나가 설정되어있다. 이 차이점에 유의해서 실습하도록 하자. 앞서 두 파일의 차이점에 대해서 살펴보았으니, 이제 두 파일의 공통점에 대해서 살펴보면, 두 파일은 공통적으로 zookeeper에 대한 container service 3개, kafka에 대한 container service 3개, kafka_manager에 대한 container service 1개, 그런데 zookeeper가 Kafka cluster에 대해 관리를 해준다고 알고 있었는데, kafka manager는 뭐지? 찾아보니, Kafka manager는 CMAK로, 브라우저상에서 Kafka 클러스터와 Topic(Partitions, Replication Factor)을 손쉽게 추가하고 관리할 수 있게 해주는 것 같다. Zookeeperzookeeper는 항상 재시작 되도록 설정이 되어있으며, port는 2181, 2182, 2183번으로 설정되어있고, 환경변수로는 SERVER_ID, CLIENT_PORT, TICK_TIME, INIT_LIMIT, SYNC_LIMT, ZOOKEEPER_SERVERS(zk1, zk2, zk3:2888:3888)에 대한 정보를 담고 있다.위의 각 각의 환경변수의 의미에 대해서 한 번 찾아보고 정리를 해보자. https://docs.docker.com/config/containers/start-containers-automatically/#:~:text=Docker%20provides%20restart%20policies%20to,process%20managers%20to%20start%20containers. restart:always설정은 컨테이너가 항상 실행되도록 한다. 컨테이너가 중지되면 즉시 다시 시작되며, 직접적인 중단(docker stop)된 경우에는 컨테이너를 다시 시작하지 않고, Docker daemon이 재시작되거나 컨테이너 자체가 수동으로 재시작되면 다시 시작된다. (오류로 인한 중단인 경우에 컨테이너가 다시 시작)반면에 producer_host와 consumer_host의 경우에는 restart: on-failure로 설정이 되어있다.이 옵션은 error(non-zero exit code)때문에 exit이 되거나 docker daemon이 컨테이너를 재시작하는 시도의 횟수에 대해 :maz-retries옵션으로 제한한 경우 재시작되도록 할때 사용된다. restart:always와 restart: on-failure의 차이는 전자는 컨테이너가 오류로 인해 중지되는 경우에 한해 컨테이너가 재시작되고, 후자는 컨테이너가 오류로 인해 exit되는 경우와 Docker daemon의 컨테이너 재시작 시도 횟수에 대한 제한이 걸려있는 경우에 컨테이너가 재시작되도록 설정할때 사용된다. (docker 데몬은 docker를 이용하기 위해서 반드시 실행중이어야 된다. 하지만 모종의 이유로 인해 데몬이 꺼지거나 서버 재부팅시 데몬이 실행되도록 해놓지 않았다면, 이용하던 컨테이너들이 제공하던 서비스를 전부 이용할 수 없게 되는 경우가 생긴다.) Docker 데몬 실행1234567# dockerd이 foreground 형태로 실행이 된다.$dockerd# dockerd이 background 형태로 실행이 된다.$dockerd &amp;# service혹은 systemctl 명령을 통해 docker 데몬 실행 가능$service docker start (stop|restart)$systemctl start docker 일반적으로 production workload에 always, on-failure, unless-stopped 이 세가지 옵션 중에 하나가 사용되며, Docker container는 장기간 실행되는 백그라운드 서비스에 자주 사용되기 때문에 일반적으로 문제가 발생하면 다시 시작되기를 원한다. producer_host와 consumer_host의 tty:true 옵션에 대해서 찾아보기tty는 terminal로, linux환경에서 docker container에 연결해서 command를 실행할때 필요한 옵션이다.이러한 이유로 producer_host와 consumer_host에만 tty:true option이 추가되있다.","link":"/2022/06/18/202206/220618_datapipeline_study/"},{"title":"220619 데이터 파이프라인 구축 오프라인 수업 &#x2F; 4주차","text":"이번 포스팅에서는 네 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. Kafka 실습환경 구축 및 Single consumer 실습 이전에 Kafka를 실습했을때는 생성한 EC2 인스턴스에 apache kafka 압축파일을 다운받고, 압축을 풀고, zookeeper와 kafka server를 시작하고, Topic을 partitions와 replication-factor 옵션의 값을 1로 생성하고 bootstrap-server 옵션은 localhost의 9092번 포트로해서 설정하였다. (kafka server: 9092, zookeeper: 2181) 이때 실습을 했을때는 세부 옵션에서 partitions이 뭔지 replication-factor가 뭔지 구체적으로 알지 못했었는데, 이제는 ISR 그룹이 뭔지 partition과 replication-factor이 뭔지 이해를 한 상태이기 때문에 좀 더 체계적으로 실습을 할 수 있는 것 같다. 그리고 이번 실습에서는 zookeeper와 kafka server를 하나의 터미널에서 명령어로 일일이 실행시키지 않고, zookeeper와 kafka host server, producer, consumer를 각각의 container로 구동을 시키기 때문에 구조적으로 좀 더 확장성 있는 것 같다. (docker image를 이용해서 컨테이너 서비스로 구동시키기 때문에 좀 더 빠르게 환경 구성을 할 수 있는 것 같다. 이래서 컨테이너로 서버를 띄우고 관리하는 것 같다.) [실습전 Setting]이번 docker-compose.yml 파일 안에 zk1zk3, kafka1kafka3, producer와 consumer에 대한 service를 미리 지정해서 docker image로부터 지정한 환경변수를 setting해서 구성하였다.(Kafka와 Zookeeper는 각 각 3대이상 설치가 되어있어야 한다.) Q1. Producer와 Consumer 서비스 부분은 restart: on-failure, cluster / zookeeper 부분은 restart: always로 설정되어있는데 그 이유는?docker-compose.yml 파일을 보던 중 이 restart policy에 대해서 의문이 생겼었다. 이전에 docker에 대해서 학습을 했었기 때문에 어떤 역할을 하는지는 알았지만, 왜 두 개를 다르게 설정을 했지? 라는 궁금증이 생겨서 오프라인 수업때 강사님께 여쭤보았다. 어떻게 보면, 좀 이상할 수 있는 질문이긴 했지만, 모르니깐 배우는 거고, 질문하는 거라고 생각한다. 비싼 돈을 내고 듣는 수업이니 많이 배워야한다. A(Q1)producer와 consumer는 ssh로 연결해서 사용할 PC의 개념이기 때문에 restart:always로 설정하게 되면, 인스턴스를 껐을때 항시 컨테이너가 재시작이 되버린다. 그래서 restart: on-failure로 해서 에러로 인한 exit이 발생하였을 경우에만 재시작되도록 설정한 것이다. 그리고 Kafka cluster는 항시 구동이 된 상태에서 에러로 인해 중지가 되면, 다시 자동으로 구동시켜야되기 때문에 restart: always정책으로 restart를 설정해줘야 한다.이렇게 설명을 듣고 보니, 깔끔하게 이해가 되었다. [Single consumer 실습]실습을 위한 환경 구성에 대해 준비가 끝나고 이제 Single consumer에 대한 실습을 하였다. 현재 실습 환경에 대해서 간단하게 살펴보면, ZK 3대, Kafka 3대, Producer와 Consumer host가 각 각 1대로 구성이 되어있다. 1234567# producer_host에 bash shell로 ssh 연결$docker exec -it producer_host /bin/bash# docker container로 kafka 서비스를 setup할때, confluentinc/cp-kafka 이미지를 사용하였는데, Apache kafka를 위한 docker이미지이다.# kafka-console-producer는 Apache Kafka 내 명령이다.$kafka-console-producer --bootstrap-server kafka1:9091 --topic ft# --bootstrap-server 옵션은 실제 명령으로 Kafka에 붙을때는 bootstrap server라고 해서, Kafka에서 노출하고 있는 host의 port로 붙어서 데이터를 쏴주게 된다.# kafka host로 producer를 붙여줌과 동시에 topic을 생성해줄 수 있다.(--topic 옵션) 위에서는 producer에서 생성된 데이터를 ft topic에 전송하고, kafka1:9091에 전달해서 처리를 하고 있지만, kafka1 host는 클러스터 내의 host들 중에서 하나로 클러스터링 되어있기 때문에 다른 Kafka host들에 데이터를 분산하여 처리한다.위의 명령을 하면, &gt; prompt로 Kafka cluster에 있는 Kafka host들로 데이터를 쏴줄 수 있는 환경이 Setting된 것이다. 1234567 # 생성된 kafka topic의 리스트를 확인 (모든 kafka host에서 확인 가능) $kafka-topics --bootstrap-server kafka1:9091 --list # $kafka-topics --bootstrap-server kafka1:9091 --topic ft --describe # Topic: ft PartitionCount: 1 ReplicationFactor: 1 Configs:# Topic: ft Partition: 0 Leader: 1 Replicas: 1 Isr: 1 위에서 Kafka cluster의 구성을 자세하게 살펴보기 위해 명령으로 확인을 할 수 있는데, 내용을 자세히 살펴보면 별도로 kafka-topic에 대해 partition이나 replication-factor을 설정하지 않았기 때문에 default로, PartitionCount: 1로, partition은 하나(partition 0), ReplicationFactor도 하나(리더만), replicas값이 1인 것은 리더만 1번째 브로커에 할당이 되었기 때문이다. (producer관점에서 acks=1과 동일한 결과(프로듀서가 리더 파티션에 메시지를 전송하고, 리더로부터 ack를 기다린다. 단, 리더로부터 데이터를 복제해서 갖고 있는 팔로워들에게 까지의 전달은 확인하지 않는다))그리고 마지막으로 Isr은 1개로, partition이 하나이기 때문에 ISR 그룹도 하나 존재한다.파티션의 갯수는 브로커의 갯수만큼 나눠줘야 안정적으로 처리가 가능하다. 그 다음으로 producer에서 topic을 통해 kafka cluster의 kafka host(broker)로 데이터를 전달을 하고, 전달한 데이터를 consumer에서 뽑아내기 위한 setting이 필요하다. 1234# Apache kafka 내의 kafka-console-consumer 명령으로, consumer를 kafka host에 붙인다.# 여기서 producer에서 데이터를 쏠때 지정했던 kafka host와 다른 host를 지정해도 데이터를 정상적으로 받아올 수 있다. 그 이유는 kafka host1, 2, 3이 클러스터링 되어있기 때문이다.# topic은 producer에서 생성한 topic을 바라보게 만든다.$kafka-console-consumer --bootstrap-server kafka2:9092 --topic ft 위의 구성이 끝나면 producer에서 전송한 텍스트 메시지를 consumer에서 받아서 화면에 출력하게 된다. 지금은 간단하게 consumer console 명령으로 붙여서 실습을 했지만, Logstash나 Flink와 같은 오픈 소스 툴들을 양단에 붙여서 데이터 스트리밍 실시간 처리를 할 수 있다. [실무 팁]실제 업무시에는 Kafka의 앞단에 Load balancer를 붙여서 Kafka host를 하나로 묶어주기도 한다. [Multi consumer 실습]이번 실습에서는 위의 Single consumer 실습과 전반적인 구성은 같지만, consumer_host가 2개 존재한다. 실습은 consumer_host를 각기 다른 consumer group으로 지정해서 데이터가 어떤식으로 받아오는지에 대해 자세히 살펴본다.우선 topic을 커스텀해서 생성할 것이기 때문에 아래와 같이 --replication-factor와 --partitions 옵션을 지정해서 커스텀해준다. partition이 하나인데, consumer가 2개이면, consumer 한 대는 놀기 때문에 파티션을 2개로 주는 것이 좋다. 1234567891011121314151617# 이번 multi consumer 실습에서는 topic을 자동생성하지 않을 것이기 때문에 우선적으로 topic을 생성해준다.# topic은 --replication-factor와 --partitions 옵션으로 각 각 2를 준다.# replication-factor를 2를 주었기 때문에 리더 하나와 팔로우 하나로 ISR 그룹이 구성이 된다.# 그리고 partition은 2개로 구성되도록 설정하고 생성한다.$kafka-topics --bootstrap-server kafka1:9091 --replication-factor 2 --partitions 2 --topic st --create$kafka-topics --bootstrap-server kafka1:9091 --list # 생성된 topic 리스트 확인$kafka-topics --bootstrap-server kafka1:9091 --topic st --describe# topic의 파티션은 위에서 지정해준 것 같이 총 2개 (partition 2, 3) 생성되었다.# 그리고 t-0, t-1 각 각의 파티션 Leader는 2번째와 3번째 브로커에 각 각 할당이 되었고,# 리더가 2번 브로커에 할당된 친구(t-0)는 Replicas 값이 2(리더가 2번 브로커), 3(팔로워가 3번 브로커) # 리더가 3번 브로커에 할당된 친구(t-1)는 Replicas 값이 3(리더가 3번 브로커), 1(팔로워가 1번 브로커)에 배치었다.# Isr 그룹은 리더와 팔로워가 배치된 브로커의 위치를 (리더, 팔로워)순으로 표기한 것이다. Topic: st PartitionCount: 2 ReplicationFactor: 2 Configs:Topic: st Partition: 0 Leader: 2 Replicas: 2,3 Isr: 2,3Topic: st Partition: 1 Leader: 3 Replicas: 3,1 Isr: 3,1 위와같이 하나의 파티션은 replication-factor의 수 만큼 복제되어 분산되어있는 것을 확인할 수 있다. Consumer group 구성하기 Consumer group은 group-1 이름으로 두 개의 consumer_host를 묶어서 구성하고, group별로 offset을 달리해서 데이터를 뽑아내는 것을 확인하기 위해 별도의 consumer_host를 하나 더 생성해서 별도의 group-2 customer group을 생성하도록 한다. 12345678$docker exec -it consumer_host1 /bin/bash$kafka-console-consumer --bootstrap-server kafka1:9091 --topic st --group group-1$docker exec -it consumer_host2 /bin/bash$kafka-console-consumer --bootstrap-server kafka1:9091 --topic st --group group-1$docker exec -it consumer_host3 /bin/bash$kafka-console-consumer --bootstrap-server kafka1:9091 --topic st --group group-2 기존 컨테이너 서비스가 유지된 상태에서 새로운 컨테이너 서비스 추가하기원래 이전 docker-compose.yml에는 consumer_host3가 없었는데, 추가하려고 한다. 현재 docker container 서비스들이 실행되는 것은 유지된 상태에서 변경된 docker container를 추가해서 적용하려고 하는데, 어떻게 해야할까? 123$docker-compose up --build --force-recreate -d# --build : 컨테이너를 시작하기 전에 이미지를 빌드한다. # --force-recreate : 컨테이널르 재생성한다. docker-compose up을 하면, 변경된 사항을 적용하여 컨테이너를 재생성하지만, up을 했을때에도 변경이 적용이 안되는 경우, 이 옵션을 적용시킨다. 우선 docker-compose.yml 파일을 수정하고, 위의 명령을 하면, 아래와 같이 새롭게 추가된 consumer_host3가 새롭게 생성되는 것을 확인할 수 있다. 새롭게 consumer_host3을 추가해서 group-2의 consumer_host로 배정했다. 실습한 결과, 같은 group 내의 consumer_host는 producer로부터 전송되는 메시지 데이터를 분산해서 처리하고 있음을 알 수 있었고, 새롭게 추가한 group-2의 consumer_host는 단일 호스트이기 때문에 producer로부터 전송되는 데이터가 전부 단일 호스트로 들어오고 있음을 확인할 수 있었다. [Q&amp;A(1)]partition이 2개인 상태에서 consumer를 한 개 더 늘려서 총 3대가 되면, consumer에서 나눠서 처리하는가에 대한 질문이 있었는데, partition이 2개이기 때문에 consumer host 2대에서 나눠갖고 한 대는 놀게 되는 구조가 된다.따라서 partition의 수가 충분하다면 consumer를 늘려서 나눠갖는 것이 좋다. [Q&amp;A(2)]순서를 보장해야되는 특별한 경우에는 단일 broker 내에서는 순서가 보장이 되기 때문에 partition을 하나로 사용하는 경우도 있다. 하지만 데이터의 처리상의 안전성을 위해서는 기본적으로 partition 2개 이상을 사용하는 것이 좋다. (실무 팁)Broker를 두면 좋은 이유는 A부서가 기존에 Broker로부터 데이터를 받아서 전처리를 하고 있었는데, 신규 사업때문에 B부서가 생겨난 경우, 새롭게 데이터를 받아올 필요가 있을때 이 Broker에 Consumer_host를 붙여서 새로운 데이터셋을 받아오면 되기 때문에 매우 유용하다.그리고 A부서에서 broker에 물리고 있는 consumer group의 consumer_host가 producer로부터 오는 데이터를 감당할 수 없다면, partition과 consumer_host를 늘려서 빠르게 병렬처리해서 처리하도록 하면 된다. 단, Kafka에서는 partition을 늘릴 수는 있어도 줄일 수는 없다. 따라서 partition을 늘리는 경우에는 신중하게 생각하고 늘리도록 해야한다. Partition을 늘리는 경우는 브로커 한 대에서 얼마나 처리할 수 있는가에 대한 고려가 필요하다.MSK를 사용하면 통계된 데이터가 나오는데, 통계된 데이터를 기반으로 의사결정을 하면 된다.만약에 데이터를 기존 consumer에서 처리를 못하는 경우에 우선 partition을 늘리고, consumer를 늘리도록 하는 것이 맞다. 그리고 좀 드문 경우인데 producer가 생성하는 데이터의 양 만큼 Partition을 늘리는 경우이다. (현재 producer로부터 오는 데이터를 Partition(Broker)에서 견디지 못하는 경우) Amazon Kinesis Amazon에서 만든 Kafka와 비슷한 서비스이다. Amazon kinesis 서비스는 내부적으로 Amazon Kinesis Data Analytics, Amazon Kinesis Data Streams, Amazon Kinesis Firehose, video stream이 있다. 카프카를 대체할 수 있으며, 저지연 스트리밍을 위한 서비스이다. EC2, Client, Agent, 사용자가 개발한 코드에서 생산된 데이터를 받고, 이를 다른 서비스에서 소비할 수 있도록 처리해준다. Kafka의 Partition과 같은 것이 Kinesis의 shard이다. 그리고 Data retention은 24시간에서 1년까지 가능하고, 데이터 보존기간이 길어질수록 가격이 비싸진다. Kafka는 partition을 늘리면 다시 줄일 수 없지만, Kinesis는 reshard가 가능하다.(split &amp; merge) Data Stream: 데이터 스트림을 캡처, 처리 및 저장(Kafka와 대응) Data Firehose: 데이터 스트림을 AWS 데이터 스토어로 저장(S3, Redshift에 저장, Fluentd, Logstash와 대체가능) Data Analytics: SQL 또는 Apache Flink로 데이터 스트림 분석 Video Stream: 비디오 스트림을 캡처, 처리 및 저장 실시간으로 비디오 및 데이터 스트림을 손쉽게 수집, 처리 및 분석할 수 있는 완전 관리형 서비스이다. (별도로 spec에 대한 설정이 필요가 없다) 구조(STEP1) Input (Producer와 동일 레벨) Kinesis producer SDK, Kinesis producer Library(KPL), Kinesis agent(STEP2) Amazon Kinesis Data Streams(STEP3) Amazon Kinesis Data Analytics, Spark on EMR, Amazon EC2, AWS LambdaOutput - (Consumer와 동일 레벨) Kinesis consumer SDK, Kinesis Client Library(KCL), Kinesis Connector Library, Third party(Spark, Flume, Kafka connect, Flink), Kinesis Firehose, AWS Lambda 등으로 전달을 할 수 있으며, Kinesis Data Stream으로부터 받은 데이터를 AWS Lambda에서 특정 시간 주기로 함수를 실행시켜서 데이터가 처리될 수 있게 할 수 있다.(STEP4) 최종적으로 BI툴을 활용해서 consumer로부터 받은 데이터를 시각화시켜서 처리할 수 있다. Amazon Kinesis Data FirehoseData Firehose를 사용해서 데이터를 transformation하는 것이 가능하다. (Format conversion(Parquet, ORC), Encryption)만약에 데이터를 custom하고자 할때는 Lambda를 붙여서 DataStream으로부터 넘어온 데이터를 Lambda로 밀어넣고 처리된 데이터를 Data Firehose로 넘겨주도록 처리하면 된다. [연결 서비스]Data Firehose의 뒷단에는 Amazon S3, Amazon Redshift, Amazon Elasticsearch, Splunk 등의 서비스에 연결시켜서 처리된 데이터를 BI 툴에 연결시켜서 분석 및 시각화를 할 수 있다. Amazon Kinesis Data Analyticsinput과 output 사이에 붙여서 SQL과 Flink를 붙일 수 있도록 해주는 Amazon의 서비스이다.(Input -&gt; Amazon Kinesis Data Streams -&gt; Amazon Kinesis Data Analytics -&gt; Amazon Lambda -&gt; Amazon DynamoDB -&gt; Output의 구성과 같이 Data Stream으로부터 들어온 데이터를 Data Analytics로 붙여서 실시간으로 들어온 데이터를 분석할 수 있다)실시간으로 데이터를 처리하기 때문에 대용량의 로그 데이터를 처리(분석)하는데 매우 빠른 시간내에 처리할 수 있으며, 문제를 실시간으로 발견 및 대응을 할 수 있기 때문에 고가용성은 물론 우수한 고객 경험을 줄 수 있다. (Netflix 사용 사례) Amazon Kinesis 실습 실습 구조STEP1. Amazon Kinesis Data Generator (랜덤 로그 데이터 생성)STEP2. Kinesis Data StreamSTEP3. Kinesis FirehoseSTEP4. S3 CloudFormationCloudFormation을 활용하면 Data Generator환경을 생성할 수 있다.그리고 IaC(Infrastructure as Code)의 일환이다.CloudFormation으로 랜덤한 로그 데이터를 생성하는 producer를 구성하기 위해서는 Template이 필요하다. Amazon CongnitoAmazon Cognito는 웹 및 모바일 앱을 위한 Authentication, User management의 기능을 제공한다. 사용자는 thrid party 웹 페이지를 통하거나 직접적으로 사용자 이름과 비밀번호를 입력해서 로그인을 할 수 있다.cognito는 user pools과 identity pools 두 가지 메인 컴포넌트가 있다. user pools는 사용자 디렉토리로, 앱에서의 회원가입과 로그인에 대한 옵션을 사용자에게 제공한다.identity pools는 다른 AWS 서비스들에 접근하는 권한을 사용자들에게 부여할 수 있도록 허용을 해주며, user pools와 identity pools를 같이 혹은 나눠서 사용할 수 있다. 실습을 위해 주어진 congnito-setup.json의 내용을 보니, 해당 json파일은 인증된 사용자에게 cognito의 identity pool에서의 역할을 할당하고, 사용자가 Kinesis Data Generator tool을 사용할 수 있도록 해주는 설정들이 담겨져있는 파일이다. (+Lambda 함수와 Cognito를 붙여주는 작업)해당 파일을 template으로써 지정해서 stack을 생성하면 된다. 생성을 해주게 되면, Output 탭에서 링크가 생성된 것을 볼 수 있는데, template으로부터 생성된 Kinesis Data Generator tools를 사용할 수 있는 링크이다. 이제 username과 password를 입력해서 들어가면, UI상으로 Region, Stream/Delivery stream을 선택해서 생성된 랜덤한 로그 데이터를 쏴줄 수 있다. Kinesis stream 서비스 생성이제 Kinesis Data Generator를 통해 생성된 랜덤 sensor 데이털르 쏴 줄 target인 Kinesis data streams와 Kinesis data firehose 생성을 한다. Kinesis streams샤드 계산기를 통해서 적정 샤드의 수를 계산해서 프로비저닝된 샤드의 갯수를 설정하고 Kinesis streams를 생성해주면 된다. (기본 데이터 retention 시간은 24시간이다) Kinesis delivery streamsource는 생성한 data streams으로 하고, destination은 S3를 선택하고 생성해주면 된다. 추가적으로 firehose에서는 생성한 lambda function을 붙여서 데이터를 transformation할 수 있다. 이외에 옵션으로 버퍼 사이즈나 버퍼 인터벌 시간을 조절 할 수 있는데, 버퍼 인터벌 사이즈를 줄여서 반응시간을 빠르게 할 수 있다. Kappa Architecture : The concentration in stream processing(스트림 프로세싱 심화) Kappa Architecture에서는 배치와 스트림 프로세스를 모두 실시간으로 처리된다. (Lambda Architecture에서 배치 레이어가 없어진 형태 - 파이프라인의 구조 단순화) 현대에 와서는 컴퓨터 리소스와 컴퓨팅 기술, 스트림처리 엔진에 대한 기술의 발달로 모든 처리(배치, 스트림 프로세스)를 실시간 스트림으로 처리하는 것이 가능해졌다. 이에 따라 개발/운영 이중화에 대한 부하가 줄어들게 되고, 이는 Kafka를 개발한 Jay Kreps에 의해 제안되었다. 디즈니(Disney)디즈니라고 하면 애니메이션만 만드는 회사라고만 알고 있었는데, 데이터를 많이 사용해서 앞서나가는 아키텍처를 구축하고 있는 기업이라고 한다. 수업에서는 Lambda Architecture에 대한 Disney의 생각이라는 section으로 설명을 해주셨는데, 아래와 같이 디즈니에서는 Lambda Architecture에 대한 생각을 가지고 있다고 한다. Duplicate CodeDeplicated code, Lambda Architecture는 구조상 보면, 스트림 처리와 배치 처리하는 구간이 나눠져 있기 때문에 서로 다른 두 코드에 대한 관리를 위한 개발팀과 유닛 테스트가 필요하다. 따라서 처리상 하나만 바뀌어도 모두 전파해야하고, 릴리즈 또한 연동이 되어야한다. Data QualityBatch와 Speed layer 간의 알고리즘이 일치하는지에 대한 확인과 입증이 필요하다. Added Complexity추가적인 복잡도에 대한 고려가 필요하다. 두 파이프라인에서의 데이터 중 어떤 데이터를 언제 읽을지에 대해 고려해야되며, 배치 잡의 딜레이되는 경우에 대해서도 고려해야한다. Two Distributed System하나의 데이터 처리에 대해서 두 개의 파이프라인으로 나뉘어서 같은 데이터에 대해 처리하므로, 두 배의 인프라를 구성해야 하고, 모니터링과 로깅도 각 각 나눠서 해야되기 때문에 2배의 리소스가 든다. Single pipeline for streaming and batch consumer유입되는 Data resource가 단일 Real-Time Layer을 통해 좀 더 민첩하게 처리해야 할 데이터는 ms 단위로 처리하고, 덜 민첩하게 처리해도 되는 데이터에 대해서는 Real-Time Layer를 통해 들어온 데이터를 담고 있는 Storage를 통해 min/hr 단위로 배치처리를 하게 된다. 스트림 프로세싱의 심화여지까지 Kafka와 Kinesis를 이용해서 데이터를 전달하고 적재하는 부분까지는 했고, Lambda나 Kinesis Firehose에서 정형화된 데이터의 transformation을 경험했다. 이번 시간에는 스트림 프로세싱에서 처리의 관점에서 데이터를 어떻게 transformation할 것인지에 대해 고도화시킬 수 있는 방법에 대해서 심도있게 다룬다. 오픈소스나 클라우드 서비스에서 제공되는 기능 이외에 커스텀해서 구성할 수 있는 방법들이 많기 때문에 이 부분에 대해서도 배우게 된다. Apache Flink (stream engine) 분산 스트림 처리 프레임워크이며, Spark streaming와 대응된다. Spark streaming은 Flink와 비교했을때 성능면에서 많이 차이가 나지만, 국내에서 Spark 사용자들이 많기 때문에 Spark로 배치처리를 하다가 Streaming이 필요한 경우에 기존 기술셋을 사용해서 기존에 사용하던 언어를 사용하기 위해 Spark streaming을 도입한느 경우가 많다. 하지만 Flink가 기능이나 생태계를 보면, Spark보다 Flink가 더 스티리밍 데이터(데이터의 제한이 없는 무한한 데이터) 처리에 최적화가 되어있다. Flink를 도입해서 사용하고 있는 기업은 Alibaba, Tencent, Uber, AWS, Lyft, SKT, Kakao, NAVER, Toss, Coupang, 하이퍼커넥트 등이 있다.Flink는 Native streaming 방식(건 by 건으로 처리하는 방식-Performance 좋음)을 채택하고 있으며, micro batch 방식(작은 단위(5초, 1분 단위)로 배치처리를 함으로써 마치 실시간 스트리밍 처리를 하는 것과 같은 효과를 주는 방식)을 사용하고 있는 Spark와 대비된다.그리고 Flink는 High Performance, low latency, exactly-once라는 특징을 가지고 있지만, 처리 속도는 빠르다는 장점을 가지고 있다. 그리고 Flink는 Java(86%)와 Scala(10.1%), Python(2.4%)로 만들어진 프로젝트이다.그리고 Flink에서 사용할 수 있는 언어로는 Java, Python, Scala로 개발을 할 수 있다. [Apache Flink의 특징] Flink를 사용해서 스트림 및 배치 처리를 둘 다 할 수 있다. (통합 데이터 처리 엔진) High Performance(Native Stream, Low Latency, High Throughput)-&gt; Spark streaming처럼 micro/mini batch 구조가 아닌, 완전 스트림 최적화 방식을 사용하고 있다. Fault tolerance -&gt; Flink의 핵심인 Checkpoint를 통해 Exactly-once를 지원한다. Checkpoint는 분산 체크포인팅 및 분사 스냅샷 기술의 일종으로, 작업 중간 중간에 스냅샷을 찍어서 장애 발생시 스냅샷을 한 상태로 되돌릴 수 있다. 내부적으로는 Chandy-Lamport라는 알고리즘을 개선한 알고리즘이 적용되어있다고 한다.(나중에 찾아서 읽어보기) https://en.wikipedia.org/wiki/Chandy%E2%80%93Lamport_algorithm Message guarantees로 Exactly-once를 지원한다. DataStream API, DataSet API(legacy), Table API를 지원하기 때문에 All rounder로 볼 수 있다. Batch 처리를 위한 별도의 Batch runtime mode가 제공되기 때문에 배치 처리도 가능 env.setRuntimeMode(RuntimeExecutionMode.BATCH);[참고]: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution_mode/ [Flink의 구조] Flink Architecture는 크게 Job Manager는 Master 노드의 역할로, Task Manage(Worker)의 작업 스케줄링을 하는 역할과 Streaming 데이터의 각 구간에 Checkpointing을 하는 역할과 복구를 하는 역할을 한다.Task Manager는 실제 일을 하는 Worker로, 분산된 Operator들은 TM로 분산 처리된다.위의 구성도를 보면, Job Manager에서 Dataflow Graph를 통해 각 각의 Operator들의 순서도를 관리한다. Dataflow Graph의 각 각의 STEP의 작업을 TaskManager(Worker)에 분산을 시키게 된다. [Apache Flink의 활용]Apache Flink는 단순 수집과 전달에서부터 합계, 평균 계산과 같은 집계와 패턴에 기반한 예측 분석 및 데이터 형식을 변환하고 다른 데이터 소스와 결합(join)등의 작업이 가능하다. 그리고 Flink는 streaming 엔진이지만 배치 처리가 가능하다는 장점을 가지고 있다. 또한 케이블 기능도 하기 때문에 데이터 베이스에서 하나의 테이블을 가져와서 스트리밍 데이터와 조인하는 기능도 제공한다. [경쟁 프로젝트]앞서 살펴본 Spark streaming, Storm(현재는 잘 사용되지 않음), Kafka streams, Samza(현재는 잘 사용되지 않음) [배포]Flink는 주로 Standalone, Kubernetes(YARN Resource에 배포하지 않고 Kubernetes에 배포해서 백엔드 개발자와 같은 인프라를 사용하는 경우도 많다), Hadoop YARN에 배포가 되어 사용이 된다. 그리고 Kubernetes는 Spark에서보다 더 잘 지원하고 있다. [내부 동작방식]Flink를 사용해서 HDFS와 같은 FileSytem으로부터 데이터를 뒷단으로 넘겨서 처리를 할 수 있으며, Kafka로부터 스트림 데이터를 읽어서 처리를 하거나 주기적으로 JDBC와 같은 DB에서 데이터를 읽어서 처리를 할 수 있다.그 외에도 Socket에서 실시간으로 데이터를 읽어와서 처리를 할 수 있다. (Source)이렇게 Source로 받아온 데이터를 File의 형태로 출력, Elasticsearch로 데이터를 올리거나, HBase, 다시 또 다른 Kafka의 Topic으로 받아서 처리한 데이터를 쏴 줄 수 있다. [사용 예시]신규 기술이 나오면, 기업에서는 바로 도입을 하지 않고, 다른 큰 기업에서 도입해서 안정성과 효율성에 대한 인정을 받았을때 도입을 한다.이 Flink라는 기술도 우버(Uber)에서 스트리밍 분석 플랫폼인 AthenaX에 Flink를 사용함으로써 알려졌고, 알리바바(Alibaba)는 Flink를 기반으로 한 Blink를 개발하여, 자체 실시간 검색 숝위를 최적화하였다고 한다. 그리고 데이터가 폭증하면서 배치 처리와 스트림 처리 모두 Flink를 사용하고 있다고 한다.AWS의 스트림 프로세싱을 위한 완전 관리형 클라우드 서비스인 Kinesis Data Analytics도 내부적으로 Flink를 사용하고 있다고 한다. 앞에서 이미 정리를 했지만, Flink는 단순 데이터 전달 및 저장만 하는 것이 아닌, 자체적으로 컴퓨팅을 통해 연산 및 분석 결과만 다른 곳으로 전달하는 역할도 가지고 있다.(연산/분석을 통해 실시간 검색어 순위에 대한 정보도 출력할 수 있으며, ML 라이브러리를 통해 처리를 할 수도 있다) [Flink에서의 Time] Flink에서는 세 가지 유형의 시간개념으로 구분을 한다. Event Time : 데이터가 실제로 생성되는 시간 (Event producer)으로, 데이터가 실제로 생성되었을때를 말한다. (센서로부터 센서 데이터가 생성되었을때) Ingestion Time : 데이터가 Flink job으로 유입된 시간 (Flink Data)을 말한다. Processing Time : 데이터가 특정 operator에서 처리된 시간이렇게 구분하는 이유는 Operator마다 시간기준으로 정확한 연산을 해야 할 필요성이 있기 때문이다. [Flink의 Task &amp; Operator chain] Flink에서 로직을 복잡하게 작성을 하다보면, 효율성을 위해서 연관된 각 처리과정을 Chaining하는 경우도 많다. [Flink의 Sink Parallelism]Task slot: 6, Sink parallelism: 1 기본적으로 Flink는 다른 작업의 하위 작업인 경우에도 하위 작업이 슬롯을 공유할 수 있도록 허용한다. 결과적으로 하나의 slot은 전체 파이프라인의 job에 대해 저장을 할 수 있다.Task slot은 정적인 개념으로, Task Manager 참조하여 동시 실행 기능을 가지고 있다.Task Manager들의 내부의 Task slot이 각기다른 Thread로 구성이 되며, Task Manager들은 Job Manager에 의해 관리가 된다. [Windows]https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/Flink에는 다양한 방식으로 데이터를 나눠서 처리하는 방법이 있는데, Windows란 개념으로 데이터를 잘라서 처리를 한다.Windows를 사용해서 데이터를 분석하는 예로 10초에 한 번 평균을 내야되는 연산을 하는 경우나 사용자가 접속한 순간부터 나간 순간까지의 이벤트를 모아서 한 번에 처리하는 경우가 있다. 여기서 윈도우란 Spark streaming의 micro batch의 개념과 다르다. 5초 단위로 들어오는 데이터들을 처리해야될 경우에 Window size를 5초 단위로 나눠서 처리를 할 수 있으며(Tumbling Windows), 각 각의 Window의 구간이 겹쳐서(데이터가 중복) 처리하는 것은 Sliding Windows, 정확히 시간을 기준으로 구간을 나누기 애매한 경우(사용자의 접속부터 종료 시점까지의 이벤트 일괄처리)에는 각 session별로 데이터 구간을 나눠서 처리하는 Session Windows의 개념으로 데이터를 잘라서 처리한다.만약에 특정 시간 구분 없이 전체 데이터를 처리하는 경우는 Global Windows개념으로 데이터를 일괄 처리하도록 한다. Flink에서는 Tumbling Windows, Sliding Windows, Session Windows 이 세가지를 지원한다. ex) 실제 사용자가 들어와서 세션내에서 어떤 작업을 했는지 취합하고 검색을 할때 주로 Session Window를 사용한다. Dataflow Programming 스트림 프레임워크 엔진을 개발할때 중요하게 보는 것이 Dataflow Programming이다. DAG(Directed Acyclic Graphs):유향 비순환 그래프(방향성은 있는데 순환하지 않는 형태), Airflow에서도 많이 접하게 되는 개념 각 각의 데이터 플로우 그래프를 만들어서 서로 연결시켜서 하나의 그래프로 만들어주는 것이 Dataflow Programming이다. [Dataflow Programming의 세 가지 Operator] Source Operator : 그래프 상에서 제일 처음인 부분, Kafka와 같이 외부에서 데이터를 받아오는 부분 (입력이 없는 연산자 - root node (a operator)) Custom source Apache Kafka AWS Kinesis Streams RabbitMQ Twitter Stream API Google PubSub Collections Files Sockets Transformation Operator : 변환/연산 등의 연산자 (b, d operator) Map : 사용자가 정의한 변환 코드를 통과시켜서 하나의 이벤트를 출력 FlatMap : map과 유사하지만 각 요소에 대해 0개 이상의 출력을 생성하는 것이 가능하다. (문자열을 split해서 처리 여러 요소로 나눠서 처리) Filter : 특정 조건에 따라 pass or drop하는 필터 기능 KeyBy : 특정 옵션(키)에 따라 스트림을 파티션별로 분리 Union : 두 개 이상의 동일한 타입의 스트림을 병합 Reduce : 키별로 나뉘어진 데이터 스트림을 합쳐주는 역할 Sink Operator : 그래프상에서 제일 마지막 출력부 (leaf node - c, f operator) Custom sink Elasticsearch Kafka producer Cassandra AWS Kinesis Streams File Socket Standard output Redis 실제 프로그래밍을 할때에는 Source operator 부분을 개발한다. 예를들어 Kafka를 활용해서 데이터를 입력받는 부분을 우선적으로 만들고(프레임워크에서 제공), Kafka에서 들어온 데이터를 transform해주는 Transformation operator를 만든다. 그리고 최종적으로 변환된 데이터를 다른 저장소나 목적지로 쏴주는 Sink operator를 만들어서 연결시켜준다.이와 같은 일련의 과정을 DAG라고 하며, Flink 어플리케이션에서는 이러한 일련의 과정을 만드는 것이다. ETL과 비슷한 과정 Flink는 분산 스트리밍 엔진이기 때문에 source -&gt; map -&gt; key By()/Window()/Apply() 구조의 각 STEP을 여러 노드로 분산해서 처리를 할 수 있도록 할 수 있다. 123456789101112// Input SourceDataStream&lt;String&gt; lines = env.addSource(new FlinkKafkaConsumer&lt;&gt;());// TransformationDataStream&lt;Log&gt; logs = lines.map((line) -&gt; parse(line));// TransformationDataStream&lt;Statistics&gt; stats = logs.keyBy(&quot;id&quot;) .timeWindow(Time.second(10)) .apply(new MyWindowAggregationFunc());// Output Sinkstats.addSink(new CustomSink(path)) 예를들어 Kafka에서 1, 2, 3, 4라는 숫자 데이터를 받아오면, Flink 클러스터의 노드가 2개 있다고 가정을 했을때, 받아온 데이터 중에서 1, 2를 A라는 노드에서, 3, 4를 B라는 노드에서 처리하도록 분배할 수 있다. 이렇게 분배된 데이터들은, map()을 통해 제곱된 수로 변환이 되고, 홀/짝수로 분류를 하는 일종의 MapReduce와 같은 과정을 거쳐서 홀수로 분류된 데이터는 또 다시 Sink를 통해 또 다른 Kafka의 odd topic으로 날리고, 짝수로 분류된 데이터는 even topic으로 날리도록 처리를 할 수 있다. Flink와 Logstash를 비교해보면, Logstash는 각 각의 Logstash에서의 작업을 하지만 다른 Logstash 간에는 소통을 할 수 없다. 하지만 Flink는 하나의 클러스터 이기 때문에 각 각의 처리 노드들끼리 서로 소통을 할 수 있다. (실무팁) Kafka의 Partition이 3개 있고, consumer가 4개 있는 상황에서는 consumer 하나가 잉여가 된다. 만약에 Flink로 어플리케이션을 만든 다음에 복잡한 데이터를 처리할때에는 source node가 10대 이상이 되어야 되기 때문에 이 경우에 consumer를 10대 이상으로 늘려주게 되면, 실제로는 Partition이 3개이기때문에 consumer 3개에서만 데이터를 받아서 처리를 해주게 된다. 이 경우에는 Flink가 클러스터링 되어있기 때문에 데이터를 받지 못하는 나머지 consumer에게도 실제 데이터를 받아오는 consumer로부터 분배받을 수 있도록 구성할 수 있다. (Kafka 자체에서는 데이터를 나눠줄 수 없는 consumer이지만, Flink 내부의 operator가 consumer들에게 데이터를 분배할 수 있도록 해줄 수 있다) (수업내용 외 질문)Q1. VPC와 Subnet 구성에 대한 질문 내가 수업시간에 한 질문은 다른 AWS service topology를 살펴보면, VPC와 Subnet으로 구분된 것들이 많은데, 실제 실무에서는 어떤식으로 사용되고 있나였다. A : 실습에서는 default vpc와 subnet에서 AWS resource가 생성이 되는데, 실무에서 default vpc와 subnet을 사용하지 않는 것이 권장된다고 한다. 실제 업무에서는 Production에서 각 각의 서비스별로나 네트워크 보안 정책에 따라서 VPC를 나눠서 관리를 하고, 세팅을 하는 것이 옳은 방법이라고 한다. AWS 서비스 중에서 완전 관리형 서비스가 아닌 서비스들은 VPC을 타지만, 완전 관리형 서비스의 경우에는 VPC가 아닌 별도의 AWS Zone(AWS전용 네트워크 라인)을 탄다.","link":"/2022/06/19/202206/220619_datapipeline_study/"},{"title":"220623 Terraform study","text":"이번 포스팅에서는 실제 프로젝트에서 AWS Topology를 Terraform 코드로 작성(IaC)해보면서 전체적인 프로젝트의 공통적인 구조를 어떻게 작성을 할 것인가에 대해 방향을 잡아가면서 정리한 내용을 작성해보려고 한다. 데이터 파이프라인 구축에 있어, AWS network topology 구성 우선 AWS의 서비스를 활용해서 데이터 파이프라인을 구축함에 있어, 전체적인 AWS network topology의 구성은 중요하다. 실습을 했을때는 default VPC, Subnet에 각 각의 AWS 서비스를 활용해서 network topology를 구성을 했지만, 실무에서는 default vpc와 subnet을 사용하지 않는 것을 권장한다고 한다. 실제 업무에서는 Production에서 각 각의 서비스별로나 네트워크 보안 정책에 따라서 VPC를 나눠서 관리하고, 세팅하는 게 일반적이라고 한다. (DE 현직자 오프라인 수업을 통해 배웠다.) AWS 서비스 중에서 완전 관리형 서비스가 아닌 서비스들은 VPC를 타지만, 완전 관리형 서비스의 경우에는 VPC가 아닌 별도의 AWS Zone(AWS 전용 네트워크 라인)을 탄다고 한다. 그래서 이번 프로젝트 진행에 있어, 각 프로젝트별로 하나의 AWS Network Topology가 나오기 때문에 위의 구성과 같이 전체적으로 구성을 해보려고 한다.","link":"/2022/06/23/202206/220623-terraform-study/"},{"title":"220626 데이터 파이프라인 구축 오프라인 수업 &#x2F; 5주차","text":"이번 포스팅에서는 다섯 번째 데이터 파이프라인 구축 오프라인 수업시간에서 배운 내용을 정리하려고 한다. Flink우선 실습에 앞서 간단하게 Flink에 대해서 정리를 해보자. Flink는 분산 데이터 처리 프레임워크 (Processing unbounded data)로, Kafka와 같은 MQ에 Flink를 붙여서 처리를 할 수 있다. 이외에도 MQ에 Kinesis Firehose를 붙이고, Lambda를 붙여서 custom한 형태의 데이터로 추출을 할 수도 있고, 데이터를 암호화하거나 특정 format(Parquet, ORC)으로 변환을 해서 추출을 할 수도 있다.또 Logstash를 붙여서 데이터를 간단하게 처리해서 넘겨줄 수 있다. (whitelist / filter plugin을 통해 처리) 커스텀하게 데이터를 모아주거나 분석을 하는 경우, 예를들어 GPS 신호를 계속 보내서 사용자들이 이 데이터를 1분동안 aggregation해서 어느 지역에 사람이 많은지 분석하는 작업은 logstash로 분석을 하는 것이 불가능하다. 그리고 각 각의 logstash로 나눠서 분산처리를 하는 것은 가능하지만, 각 각의 logstash가 서로 데이터를 공유하지는 못하기 때문에(클러스터 내의 노드로써 존재하지 않기 때문에) 복잡한 스트림 처리나 프로세싱, 분석이 필요할때 Flink를 사용한다. 그리고 Flink, Storm을 개발할때에는 DAG를 많이 사용한다. Kafka에서 받아온 실시간 데이터를 키별로 분류를 하는 작업에서도 사용이 될 수 있는데, 게임 데이터를 분석한다고 가정했을때 각 캐릭터의 직업에 따라 분류를 해서 각 직업별로 행동 패턴을 분석하고자 할때 각 각의 Dataflow를 머릿속으로 구조화시킨 다음에 붙여주는 작업을 해야한다.source operator(Kafka) -&gt; keyBy operator -&gt; aggregation operator -&gt; HDFS sink Flink와 같은 데이터 플로우 프로그래밍이 필요한 어플리케이션은 operator와 같은 요소를 하나 하나 개발을 한 다음에 연결을 해주는 방식으로 개발을 하게 된다. DB ETL작업을 할때 DB에서 주기적으로 데이터를 select해서 Flink내부에서 처리를 하고자 할 때에도 source operator를 사용한다. (Flink 외부에서 데이터를 긁어오는 부분을 source operator) 중간에서 데이터를 변환해주는 부분을 Transformation Operator라고 한다. 그리고 처리 결과를 밖으로 빼내는 부분을 Sink Operator라고 한다. Flink parallelism실제 flink로 어플리케이션을 개발하게 되면, 수십개의 TM가 생성이 된다.개발을 할때 각 각의 operator의 갯수를 parallism을 통해 정의할 수 있다.Kafka(3) -&gt; Map(10) -&gt; Sink(3) =&gt; 16개의 operator가 TM의 각 각의 노드에 분산이 되어 처리된다. 이와같은 특징으로, Fluentd와 logstash와 같은 agent 기반의 데이터 파이프라인과 비교되어 사용된다. Code 구현1234567891011// Source operator 구현DataStream&lt;String&gt; lines = env.addSource(new FlinkKafkaConsumer&lt;&gt;());// Transformation operator 구현DataStream&lt;Log&gt; logs = lines.map((line) -&gt; parse(line));// Transformation operator 구현 (위의 Transformation 결과 변수를 또 다시 변형)DataStream&lt;Statistics&gt; stats = logs.keyBy(&quot;id&quot;) // 10초마다 데이터를 끊어서 처리 .timeWindow(Time.second(10)) .apply(new MyWindowAggregationFunc());stats.addSink(new CustomSink(path)); Operator Chaining각 각의 노드들에 분산되서 처리되서 처리되는 것 보다는 서로 연관있는 operator나 데이터를 주고 받지 않고 바로 바로 처리해도 되는 요소의 경우에는 operator chain을 해서 효율성을 높일 수 있다. https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/ [실습] Flink cluster 이번 Flink 실습에서는 EC2 instance에서 Flink를 설치하고, 웹 상에서 Flink cluster를 관리할 수 있다. 따라서 default로 설정이 되어있는 SSH(21) 포트 외에 추가적으로 8081 port를 열어줘야한다. (Inbound rule 수정) Flink 설치https://www.apache.org/dyn/closer.lua/flink/flink-1.14.5/flink-1.14.5-bin-scala_2.12.tgz 1234567$wget https://dlcdn.apache.org/flink/flink-1.14.5/flink-1.14.5-bin-scala_2.12.tgz$tar -zxvf flink-1.14.5-bin-scala_2.12.tgz$cd flink-1.14.5-bin-scala_2.12$sudo apt install openjdk-11-jdk 실습에서는 하나의 EC2 instance 내에 JM, TM을 각 각 하나씩 두지만, 실제로는 (EC2, Kubernetes, YARN과 같은 컨테이너 환경에서 개별로 존재)JM가 따로, TM도 서로 다른 컨테이너안에서 개별 노드로 존재한다. 12# 설치한 flink 디렉토리 안의 bin 폴더$./bin/start-cluster.sh Flink에 실행할 Application importJava로 코드를 작성한 다음에 *.jar 파일로 추출을 해서 브라우저 UI 상에서 Job을 제출하거나 터미널상에서 실행을 할 수도 있다. Browser UI상에서 Job submit 123Entry Class : org.apache.flink.streaming.examples.multiple.SocketMultipleParallelism: 1 #TM를 1개Program Arguments: --hostname localhost --port 9090 Terminal 상에서 Job submit 123$./flink-1.14.5/bin/flink run [실행할 job의 *.jar 파일] --hostname localhost --port 9090# 또는 원격지 (PC)에서 생성한 EC2 인스턴스의 주소를 통해 Flink로 코드를 실행할 수도 있다. EC2 Instance에서는 flink에서 Listen할 data source operator의 역할을 해주기 위해서 9090 포트를 열어서 데이터를 입력해줘야 한다. 1$nc -l 9090 EC2 instance에서 9090 포트를 열어준 다음에 문자를 입력해주게 되면, 처리결과를 아래와 같이 터미널상에서 log 파일을 직접 확인하거나, Apache Flink Dashboard의 TM의 항목을 통해 처리 결과를 확인해줄 수 있다. (Stdout) 다시 Flink에서의 데이터 처리 흐름을 살펴보면, Socket에 붙어서 텍스트를 입력을 해주게 되면 Flink Application에 source로 데이터가 들어가게 되고, Transformation operator(Map function)을 통해 데이터가 정제되서 최종적으로 Sink operator(Stdout)을 통해 출력을 해주게 된다. Apache Flink DashboardApache Flink Dashboard에서 Overview 메뉴를 통해 현재 실행중인 Job이 몇 개인지 확인할 수 있고, JM, TM의 상세 정보(Total Process Memory/Total Flink Memory/TM의 heap 메모리 영역)를 확인할 수 있다.JM의 Running job 항목을 클릭해보면, operator에 대한 정보를 확인할 수 있는데, 클릭하면 BackPressure/Idle/Busy 정보(%)를 확인할 수 있는데 이 부분에 대한 확인이 중요하다. Flink와 관련된 서비스를 제공하는 third party 서비스사에서 제공하는 샘플 코드도 한 번 보기(코드 참고해서 직접 커스텀 어플리케이션 만들어보기) 시각화 및 분석 툴 이번 파트는 많이 유용할 것 같다. 지금 프로젝트로 정리하고 있는 데이터 파이프라인 중에 Amazon EMR로 데이터를 정제하고, DW, DM까지만 데이터를 적재하는 부분까지만 했었는데, 이번 5주차 수업을 통해서 그 다음 파이프라인에서는 어떤식으로 데이터를 분석하고 시각화할지에 대해서 아이디어를 얻을 수 있었던 것 같다. Elasticsearch Apache Lucene를 기반으로 한 오픈소스 분산 검색 엔진이다. 루씬(Lucene)Lucene이란 자바로 만들어진 고성능 정보 검색(IR: Information Retrieval)라이브러리로, 여기서 IR이란 문서를 검색하거나, 문서의 내용을 검색하거나 문서와 관련된 메타 정보를 검색하는 과정을 의미한다.그리고 Lucene은 파일 검색이나 웹 문서 수집, 웹 검색 등에 바로 사용할 수 있는 어플리케이션이 아닌, 검색 기능을 갖고 있는 어플리케이션을 개발할 때 사용할 수 있는 라이브러리이다. JSON 기반의 문서를 저장하고 검색할 수 있다. (이러한 이유로 로그와 같은 JSON형태의 데이터를 쉽게 저장할 수 있는 데이터베이스, NoSQL와 같이 사용이 되고 있다) 로그를 수집하고 조회하는 용도(로깅 &amp; 모니터링)/ 통합 로그 분석 및 보안관련 이벤트 분석에 사용되고 있다. 실시간 집계 및 데이터 분석 용도로도 사용되고 있다. ML 기능도 추가가 되어, ML 엔진으로도 활용되고 있다. Elasticsearch는 온프레미스 환경에서 로컬로 서버를 구축할 수도 있고, AWS상에서만 효율적으로 활용할때에는 QuickSight를 사용하기도 한다.그리고 Elasticsearch는 데이터 검색 엔진으로 나왔지만, 최근에는 short term 데이터 저장소로도 활용이 되고 있다. 시각화 툴로써도 기능이 좋지만, 전문 BI 툴에 비하면 부족한 면이 있지만, 엔지니어에게 있어서 기능적인 면은 충분하다.(분석가, 데이터 사이언티스트에게 있어서는 부족한 부분이 있다)그 이유는 엔지니어에게 있어서는 로그 데이터를 뽑아서 관련된 정보를 보거나, 그래프로 그리는 툴정도로만 활용이 되기 때문이다.그리고 ElasticSearch는 Server side 어플리케이션으로, UI가 없기 때문에 Kibana가 붙어서 시각화 처리를 해준다. 데이터 베이스에서 Query를 통한 처리와 Elasticsearch에서의 처리의 차이실제 DB에 일일이 Query를 작성해서 원하는 데이터를 추출 할 수는 있지만, 그에 대한 한계점이 명확하기 때문에 ES를 사용한다. 대표적인 한계점으로는 데이터베이스의 query 연산은 느리고 부하가 높으며, 공백이나 띄어쓰기를 포함한 exact 연산을 기반으로 한다는 단점이 있다. ES의 특징 준실시간 검색 엔진(Full text Search) 클러스터링을 통한 안정성 강화와 부하 분산 다양한 형태의 문서도 동적으로 인데싱되어 검색이 가능하다.(Schemaless)-&gt; JSON 기반으로 문서를 제공하기 때문에 NoSQL로 분류를 하기도 한다. REST API로 검색을 할 수 있기 때문에 별도의 드라이버나 라이브러리 없이 연동이 가능하다. 역색인(Inverted Index) : 검색어가 포함된 모든 문서를 조회하는 것이 아닌, 해당 검색어가 포함된 모든 문서의 위치를 알 수 있다. 미리 검색을 통한 indexing을 해주기 때문에 검색에 대한 성능이 좋다. 확장성과 가용성이 좋다. RDBMS와 Elasticsearch의 차이점Elasticsearch는 단기 저장소로도 사용이 된다고 했는데, RDBMS와는 아래와같은 차이점이 있다. 아래의 도표에서 Elasticsearch의 Type은 최신 버전에서 개념적으로 사라지고 있다.Index는 데이터 저장 공간으로, 검색의 범위가 될 수 있다.(Query에서 테이블과 같은 역할, server-log-2022.06.27 또는 server.log-2022.06.27로 날짜 또는 앞의 데이터 타입에 따라 데이터를 구분하여 읽을 수 있게 해준다)그리고 Shard는 primary와 replica로 구분이 되는데, 문서를 저장하는 단위 중 하나로, 하나의 인덱스가 여러 물리적인 단위인 샤드로 나뉘어서 분산되어 저장이 된다. 그리고 이 Shard 단위로 replica가 된다.Document는 데이터의 최소 단위(row)로, 다수의 필드로 구성이 되어 있으며, 기본적으로 JSON format으로 제공된다.Field는 문서를 구성하는 column으로, 동적 데이터 타입이다. (성능을 높이기 위해서는 타입을 static하게 assign해주는 것이 좋다)하나의 Document는 특정 Index의 특정 Shard에 속한 데이터 조각으로, JSON의 Key-value 형태로 Mapping이 되어있다. ES node의 역할Elasticsearch에는 여러 종류의 노드들이 존재한다. [Master node] 마스터 노드는 클러스터의 상태와 각종 메타데이터를 관리하는 역할을 한다. [Data node] Document를 저장하고 조회하는 역할을 한다. 이 Data 노드에 여러가지 role을 적용할 수 있다. (한 노드가 다양한 노드의 역할을 수행할 수도 있다.)데이터 저장 [Ingest node] 실제로는 Master node와 Data node만 존재해도 되지만, Document에 저장이 되기 전에 전처리 작업이 필요한 경우에는 Ingest node가 필요하다. (Data node에 Ingest node의 역할을 부여) [Coordinate node] 요청을 데이터 노드로 전달하고, 다시 데이터 노드로부터 결과를 취합하는 역할을 하는 노드이다. [이외의 다양한 노드의 roles] data_hot, data_warm, data_cold, ml(GPU가 붙어있는 노드), etc… [이외의 다양한 노드의 roles] Elasticsearch를 운영/관리할때에는 JVM의 heap을 얼마나 설정하느냐와 Shard 수 및 Shard 당 용량 그리고 Replica 수, 클러스터 노드 수, OS 메모리 용량 등 고려되어야할 부분이 많다. 구체적인 내용은 아래의 노트 필기를 확인해보고, Elasticsearch를 직접 서버에 올려서 구성할때 아래의 조건들을 직접 적용시켜가면서 운영해봐야겠다. [상황예시] 데이터 노드를 구성할때에는 데이터 노드의 종류인 hot, warm, cold 중에서 최신/조회 수가 많은 데이터들을 담아두는 hot node를 구성한다.(SSD) 1주일내에 들어온 데이터가 조회 수가 많은 데이터라면, hot node에 담아두고, 1주일이 지나면 자동으로 warm data node(HDD)로 migration할 수 있도록 해줄 수 있다.이와같이 장비와의 호환성을 맞춰서 각 각의 데이터 노드를 구성할 수 있다. ELK StackELK stack은 Elasticsearch(데이터 저장 및 검색 엔진 / Short term 데이터 저장소로도 활용), Logstash(데이터 수집 및 전처리), Kibana(데이터 시각화)로 컴포넌트들이 구성이 되어있다. 그리고 ELK를 구성하는 각 컴포넌트들은 Elastic 기업에서 진행하고 있는 In-house 프로젝트들로써 각 컴포넌트들 간의 호환성이 좋다. ref. EFK(Elasticsearch Fluentd Kibana) 스택을 사용할 수도 있다. KibanaKibana는 Elasticsearch에 붙여서 데이터를 검색하고, 시각화해주는 UI dashboard이다.일일이 사용자가 API 요청을 Elasticsearch의 데이터 노드들에 날려서 데이터를 검색할 수 없기 때문에 Kibana가 이러한 번거로움을 개선하여, ES와 사용자 간의 인터페이스 역할을 해준다.시각화는 다양한 시각화 도구를 제공(막대, 원형, 표, 히스토그램, 히트맵 등)해준다. Amazon OpenSearch ServiceAmazon OpenSearch Service는 AWS의 Elasticsearch 관리형 서비스로, 오픈소스 라이센스의 문제로 인해 AWS 측에서는 Elasticsearch를 fork하여 별도의 프로젝트로 관리 및 서비스하고 있다고 한다. v7.10까지는 기능이 모두 동일하고, OpenSearch 초기 버전은 Elasticsearch와 거의 동일하다. Elasticsearch &amp; Kibana 설치/운영Window, mac, Linux 로컬 환경에서 설치를 하고 테스트를 할 수도 있지만, docker container 기반으로 Elasticsearch와 Kibana를 설치해서 테스트를 할 수도 있다. [Elasticsearch 설치 in local]https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html [Elasticsearch와 Kibana를 docker에 설치]https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html [실습] Amazon Opensearch 서비스 [Create domain]으로 새로운 도메인을 생성해준다. 배포 유형은 실습이기 때문에 개발 및 테스트 유형을 선택해준다. 그리고 버전은 Elasticsearch와 Kibana의 환경을 실습하기 위해서 Elasticsearch 7.10 버전을 선택하도록 한다. 가용영역에 대한 설정은 가용성을 높여서 사용할 필요가 없기 때문에 1-AZ로 설정하고, 인스턴스 유형은 t3.small.search, 노드 수는 3, EBS/SSD/10GB로 선택을 한다. 또한 추가적으로 네트워크는 public access, Advanced cluster settings에서는 Max clause count의 값을 default로 1024로 설정해준다. (Master user에 대해서도 IAM 옵션이 아닌 사용자 계정을 만들고, 접근 정책도 Only use fine-grained access control로 설정하도록 한다) ElasticsearchElasticsearch의 장점인 JDBC나 ODBC 별도의 client library 상관없이 Kibana 자체적으로 제공하는 기능을 사용해서 web 상에서 직접 query를 보내서 API 테스트해 볼 수 있다. 인덱스 생성 데이터 베이스 생성과 같이 분산된 document를 쌓을 공간을 만드는 것을 말한다. (앞에서는 쌓을 공간을 만드는 것을 index, 데이터를 ES에 넣는 행위를 indexing이라고 한다) [실습] Kibana link를 통해 Elasticsearch에 들어가서 sample로 제공해주는 로그와 eCommerce 데이터를 선택해주면 생성한 Elasticsearch로 데이터를 밀어넣어준다. Dashboard 메뉴에서는 밀어넣은 데이터에 대한 dashboard 하위 메뉴가 생성이 되고, 클릭하면 미리 생성되어있는 dashboard graph를 볼 수 있다.이러한 시각화 그래프는 Discovery에서 해당 그래프의 데이터가 json 형태로 쌓여있기 때문에 이 데이터를 기반으로 그래프를 그려준다. 그리고 이 Discovery 메뉴에서는 column 이름을 기준으로 해서 데이터를 필터할 수도 있다.추가적으로 그래프를 그릴때는 Visualize메뉴를 통해 시각화하고자하는 데이터 dashboard를 선택해서 x-axis, y-axis를 선택해서 그리면 된다.좌측 메뉴중에 Management 하위의 Dev Tool을 활용하면, query를 보내서 데이터를 검색할 수 있다. Index 생성 Index를 생성한다는 것은 앞서 배웠듯이 데이터를 검색하는 범위를 새롭게 생성한다는 의미이다. 개발자의 Kibana 활용 개발자들은 주로 Pi chart나 추세를 보여주는 라인 그래프를 주로 활용한다. API 테스트 Dev Tools에서 RestAPI를 지원하기 때문에 다른 서비스와 연계시켜서 검색하는 서비스를 만들어낼 수 있다. 사용자 정의 인덱스 생성 Dev Tools에서 아래와 같이 인덱스를 생성 할 수 있다. 123PUT /my-index-1PUT /my-index-2PUT /my-index-3 Stack Management / Index pattern / Create index pattern을 통해 새롭게 생성한 인덱스(Document가 쌓이는 저장 공간)를 확인할 수 있다. 이 인덱스를 my-index*와 같은 패턴을 정의해서 생성할 수 있다서비스별로 로그를 분류할때에도 위의 방법이 활용되는데, service-2022.07.01과 같이 날짜별로 인덱스를 만들어서 데이터를 쌓을 수 있다. 각 각의 날짜별로 분류된 서비스의 로그 데이터는 service-*와 같은 특정 패턴으로 정의를 해서 생성을 할 수 있으며, 이렇게 정의된 패턴을 통해 Discover 메뉴에서 전체 서비스 로그에 대해 조건 검색을 할 수도 있다.schemaless이기 때문에 좀 더 유연하게 데이터를 검색할 수 있다. Logstash / Flink —&gt; ES Flink나 Logstash를 활용해서 ES에 데이터를 넣어주면, 데이터가 계속해서 쌓이게 되고, 쌓인 데이터로 ES에서 데이터 분석을 하거나 시각화를 할 수 있다. BI tool Amazon Quicksight는 일종의 BI(Business Intelligence) 툴로서, Kibana보다는 좀 더 계산된 기능을 지원하고, 서버 분석을 해서 데이터를 긁어오는 서버와 UI가 합쳐진 형태를 BI툴이라고 한다. BI 툴은 Data-Driven 형태로 비즈니스 관점에서 시간대별로 데이터를 보여주고, 다양한 관점의 자료를 시각화하여 더 나은 비즈니스 의사 결정을 내릴 수 있도록 도와주는 tool이다. Kibana는 BI 툴이라고 하지 않으며, 단지 데이터를 시각화해주는 기능적인 면에서는 비슷하다고 볼 수 있다. 대표적인 BI 솔루션으로는 Tableau, MS Power BI, MicroStrategy, Redash 등이 있다. Kibana를 들어가보면, Opensearch에서 샘플로 제공해주는 데이터들을 넣어서 dashboard를 생성해볼 수 있다. 사용예로는 보고서, 온라인 분석 처리, 데이터 마이닝, 시각화등에 사용되며, SQL, Hadoop, Hive를 통해서 SQL로 쿼리를 해보는 것과 달리 쉽게 UI상에서의 조작을 통해 데이터를 손쉽게 분석할 수 있게 도와준다. Ref. Data mining : 통계적으로 전처리를 하여 데이터 간의 아직 알려지지 않은 관련성이나 경향 등을 분석하는 방법이다. 서로다른 데이터 소스의 데이터를 결합해서 새로운 경향을 도출해내는 방법으로 이해할 수 있다. Amazon Quicksight Cloud native serverless BI 서비스이다. 데이터를 기반으로 시각화 dashboard를 생성하고 공유할 수 있다. (AWS 계정 불필요) 별도의 인프라 관리 없이 S3에 있는 페타바이트 규모의 데이터에 쿼리를 하고 시각화 할 수 있다. 다양한 데이터 소스(AWS/Third party cloud/On-premise 데이터)를 연결할 수 있다. AWS 계정없이 사용할 수 있기 때문에 데이터 분석/시각화를 하는 타 부서에서 별도의 서비스로써 활용을 할 수 있다. SPICE(Super-fast, Parallel, In-memory, Calculation, Engine)을 이용한 캐시로 조회 성능이 향상 되었다. ML Insight 자체를 제공하기 때문에 ML 기반의 이상 탐지와 ML 기반의 예측을 할 수도 있다. 또한 자동 서술 기능으로 dashboard의 결과를 텍스트로 작성해준다고 한다. Amazon SageMaker 통합을 통해서 복잡한 데이터 파이프라인 없이 ML 모델을 통합할 수 있다. 다양한 암호화 기능을 제공한다.(다양한 데이터 표준에 대한 Compliance / HIPAA, HITRUST CSF, GDPR, SOC, PCI, ISO 27001, FedRAMP(High)) 다양한 데이터 소스 제공Quicksight에서는 아래의 다양한 서비스로부터 데이터 소스를 받아서 활용을 할 수도 있다. File S3 Athena Redshift RDS Aurora MySQL PostgreSQL ORACLE Salesforce Presto Snowflake 5주차 수업 후기 이번 5주차 수업에서는 Flink에 대한 실습과 Elasticsearch의 운용과 관리에 대한 내용을 포함한 전반적인 내용들에 대해서 학습을 하였다. 사실 이전에 온라인 강의로 데이터 파이프라인 구축에 대한 수업을 들었을때에는 각 각의 비슷 비슷한 기능을 하는 구성요소가 왜 존재를 하는지, 그리고 단순히 AWS OpenSearch 서비스를 띄워서 Elasticsearch의 기능을 사용만 했었기 때문에 만약에 ElasticSearch를 별도의 서버에 구축해서 운영 및 관리를 할때에는 어떤식으로 해야되는지에 대해서는 전혀 알지 못했었다. 그런데 이번 수업을 통해서 Elasticsearch 클러스터 내의 각 노드들은 어떤식으로 구성이 되는지 구체적으로 알 수 있었다.(내용에 대해 반복학습이 필요할 것 같다) 6주차 수업 내용 Amazon QuickSight에 대한 이론 및 실습 마무리 통합 파이프라인 구축 기업사례에 대한 공유","link":"/2022/06/26/202206/220626_datapipeline_study/"},{"title":"221008 Apache NiFi 스터디 (작성중...)","text":"이번 포스팅에서는 최근에 새롭게 접하게 된 ETL 툴인 Apache NiFi에 대해서 정리해보려고 한다. NiFi의 기술적 배경NiFi는 NAS라는 미국의 국가 안보국에서 Apache에 기부한 Dataflow 엔진으로, ETL툴의 일종이다.NiFi는 과거에 NSA에 의해 개발되었다가 2014년 기술 전송 프로그램의 일부로, 오픈소스화된 나이아가라파일즈(NiagaraFiles)에 기반을 두고 있다. NiFi는 분산환경에서 대량의 데이터를 수집, 처리한다. =&gt; FBP개념을 구현한 오픈소스 프로젝트여기서 FBP란 사전에 Data Flow를 정의하고, 이를 지속적으로 유지하면서 데이터를 교환하는 프로그래밍 패러다임이다.(마치 Apache Airflow에서 DAG를 작성해서 전체적인 Task flow를 구성한 뒤에 반복적인 데이터 처리를 자동화 시키는 것과 같은 맥락이다) NiFi의 필요성그래서 왜 NiFi가 필요할까? 앞서 이미 정의했듯이 NiFi는 ETL툴의 일종으로, 클러스터로 구성이 되어있기 때문에 대량의 데이터를 분산시켜서 처리할 수 있다.NiFi는 A 시스템에서 B 시스템으로 데이터를 이관하는 것을 손쉽게 할 수 있도록 도와주는 서비스 툴로, 데이터를 이관하는 중간에 데이터를 변형(정제)할 수 있다. 그 외에도 관리 및 모니터링이 가능하다. NiFi의 구성NiFi는 FlowFile, Processor, Connection, 이 세 가지로 구성이 되어있다. [FlowFile] FlowFile은 NiFi가 인식하는 데이터 단위로, 속성과 내용이 Key/Value 혀태로 구성이 되어있다. Processor마다 이동시 복사본이 생성되어, 추적이 용이하다. ref. query를 작성할때 [table명] 대신에 FLOWFILE을 작성하게 되면, 파이프라인 상에서 흘러가는 파일 정보를 읽을 수 있다. [Processor] FlowFile을 수집, 변형 및 저장하는 기능이 있으며, 자주 사용되는 프로세서로는 http, kafka, db, ftp와 관련된 프로세서, 속성을 변경하는 updateattribute, 데이터를 합치는 mergecontent, 데이터를 분할하는 split, 데이터 타입을 변경하는 convert등이 있다. [Connection] 각 Processor별로 연결해서 FLOWFILE을 전달하는 역할을 담당하고, FlowFile의 우선순위, 만료, 부하조절 기능도 제한하고 있다. NiFi 아키텍쳐 [Web Server] 발생하는 이벤트를 모니터링, 소프트웨어를 시각적으로 제어하기 위해서 사용된다. (HTTP기반 구성요소) [Flow controller] NiFi 동작의 뇌 역할을 담당한다. NiFi 확장기능의 실행을 통제하고, 이를 위한 자원 할당을 스케줄링한다. [Extensions] NiFi가 다양한 종류의 시스템과 통신할 수 있게 하는 다양한 플러그인이다. [FlowFile Repository] NiFi가 현재 실행중인 FlowFile의 상태를 추적하고, 정비하기 위해 사용된다. [Content Repository] 전송 대상의 데이터가 관리된다.","link":"/2022/10/08/202210/221008_apache_nifi_study/"},{"title":"221016 Mapper class Bean 등록 Issue","text":"우선 본 포스팅에서 다루고자 하는 에러가 어떤 상황에서 발생하게 되었는지에 대해서 간략하게 설명을 하고, 어떤 접근으로 해결하려고 했는지에 대해서 기술해보려고 한다. 프로젝트 진행 회사에서 진행중인 프로젝트에서 백엔드를 구성하면서 간단한 CRUD 처리의 경우에는 JPA를 사용하고, 복잡한 쿼리 사용의 경우에는 MyBatis를 사용하기로 했다. 여기서 말하는 복잡한 쿼리란 간단한 CRUD 이외의 복잡한 쿼리를 말한다.그냥 이렇게 하라고 하니깐 하는 게 아니라 이렇게 구성을 하면 어떤 이점이 있는지에 대해서 간단하게 짚고 넘어가도록 하자. Hibernate vs MyBatis 과거 EJB2로 개발을 하던 당시 Gavin king이라는 사람이 사용자 친화적이지 않은 자바 애플리케이션 개발 방식을 좀 더 사용자 친화적이게 만들고자 개발을 하게 된 것이 바로 그 유명한 Hibernate이다. 이 Hibernate가 점차 인기가 많아지자 자바 진영에서 Gavin king을 영입해서 자바 ORM(Object Relational Mapping) 기술에 대한 표준 명세를 개발하도록 하였는데, 그것이 바로 JPA(Java Persistent API)이다. JPA는 ORM을 사용하기 위한 인터페이스를 모아둔 것으로, 이를 구현한 프레임워크 중 대표적으로 Hibernate가 있고, Spring에서는 대부분 Hibernate를 사용하고 있다.Spring에서는 JPA를 사용할 때 구현체들을 직접 다루지 않고, 구현체들을 좀 더 쉽게 사용하고자 추상화 시킨 Spring Data JPA라는 모듈을 시용하여 JPA 기술을 다룬다. 그렇다면, JPA 하나만 사용해서 개발하면 만능일까? 찾아보니 대부분 단순한 CRUD의 처리를 하는 경우에는 JPA만을 사용해도 괜찮다고 한다. 하지만, 복잡한 통계나 정산관련 조회 쿼리가 포함된 경우, MyBatis로 처리하면 좀 더 개발자에게 편하다. 물론 JPA로도 복잡한 집계성 쿼리를 처리하는 것도 가능은 하지만 구현이 쉽지 않기 때문에 MyBatis를 사용하는 것이 낫다고 한다. 따라서 JPA 또는 MyBatis만 사용하는 것이 아닌, 두 개를 적절히 조합해서 프로젝트를 진행하면 업무적으로 효율이 높아질 수 있다는 결론이 나온다. MyBatis의 사용 MyBatis는 Mapper를 별도로 구성하며, SqlSession을 직접 사용하는 형태가 아닌 Mapper를 통해 처리한다. 기본적으로 Mapper를 사용하게 되면, Mapping 파일이 자동으로 Mapper의 단위가 되기 때문에 유지보수 및 관리에 용이하다. 문제상황 JPA와 MyBatis를 하나의 프로젝트에서 구성을 하면서 직면한 문제는 @Mapper annotation을 붙여서 정의한 Mapper class의 Bean 객체가 등록이 되지 않아 Service 클래스에서 해당 Mapper class의 Bean 객체의 의존성 주입시에 아래의 에러가 발생하였다. &quot;Consider defining a bean of type '{Mapper class package detail}' in your configuration.&quot; 해결책 Mapper class에 달아주는 @Mapper annotation은 기본적으로 @Component annotation 없이 클래스를 generate해준다. @Component annotation은 기본적으로 Spring container에 Bean 객체를 등록해주기 위해 스캔해주는 과정을 위한 annotation이다. @Service와 @Repository, @Controller의 Custom annotation 정의를 보면, 모두 @Component annotation을 상속하고 있음을 알 수 있다. solution1) *.yml 파일에 mybatis에 대한 설정 추가하기 123mybatis: mapper-locations: classpath:{구체 경로 지정}/mapper/*.xml type-aliases-package: ...(생략)... solution2) pom.xml 파일에서 dependency의 버전 간 호환성 문제 mybatis-spring-boot-starter는 1.3.2, mybatis-spring을 1.3.2로 버전을 설정한다.이 부분은 기존에 작업하고 있는 백엔드 프로젝트의 pom.xml의 버전 호환을 확인하여 수정할 필요가 있다. (현재 별도로 백엔드 프로젝트를 작업중...) 일단 문제상황 재현을 위해 생성한 프로젝트의 pom.xml에서 아래와 같이 mybatis-spring-boot-starter(1.3.2), mybatis(3.4.6), mybatis-spring(1.3.2)로 버전을 맞춰서 다시 Maven clean 및 install을 해주니 정상적으로 Mapper class의 Bean이 등록되어 에러 메시지가 더 이상 나오지 않았다. 123456789101112131415 &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;","link":"/2022/10/16/202210/221016_spring_error/"},{"title":"230502 Vue","text":"이번 포스팅에서는","link":"/2022/05/02/202305/230502_vue_study/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"TIL","slug":"TIL","link":"/tags/TIL/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Vim","slug":"Vim","link":"/tags/Vim/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"PseudoCode","slug":"PseudoCode","link":"/tags/PseudoCode/"},{"name":"repository","slug":"repository","link":"/tags/repository/"},{"name":"BaekjoonOnlineJudge","slug":"BaekjoonOnlineJudge","link":"/tags/BaekjoonOnlineJudge/"},{"name":"Self-Development","slug":"Self-Development","link":"/tags/Self-Development/"},{"name":"Assignment","slug":"Assignment","link":"/tags/Assignment/"},{"name":"ProbabilityPuzzle","slug":"ProbabilityPuzzle","link":"/tags/ProbabilityPuzzle/"},{"name":"TypeScript","slug":"TypeScript","link":"/tags/TypeScript/"},{"name":"self-development","slug":"self-development","link":"/tags/self-development/"},{"name":"List","slug":"List","link":"/tags/List/"},{"name":"Tuple","slug":"Tuple","link":"/tags/Tuple/"},{"name":"Sorting","slug":"Sorting","link":"/tags/Sorting/"},{"name":"Counting-sort","slug":"Counting-sort","link":"/tags/Counting-sort/"},{"name":"Data-structure-Theory","slug":"Data-structure-Theory","link":"/tags/Data-structure-Theory/"},{"name":"Array","slug":"Array","link":"/tags/Array/"},{"name":"Linked-list","slug":"Linked-list","link":"/tags/Linked-list/"},{"name":"Incomplete","slug":"Incomplete","link":"/tags/Incomplete/"},{"name":"Stack","slug":"Stack","link":"/tags/Stack/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"Brute-force-search-Algorithm","slug":"Brute-force-search-Algorithm","link":"/tags/Brute-force-search-Algorithm/"},{"name":"String","slug":"String","link":"/tags/String/"},{"name":"Greedy-Algorithm","slug":"Greedy-Algorithm","link":"/tags/Greedy-Algorithm/"},{"name":"Sort","slug":"Sort","link":"/tags/Sort/"},{"name":"Pull-Request","slug":"Pull-Request","link":"/tags/Pull-Request/"},{"name":"List-comprehension","slug":"List-comprehension","link":"/tags/List-comprehension/"},{"name":"Dictionary-comprehension","slug":"Dictionary-comprehension","link":"/tags/Dictionary-comprehension/"},{"name":"NodeJS","slug":"NodeJS","link":"/tags/NodeJS/"},{"name":"Express Framework","slug":"Express-Framework","link":"/tags/Express-Framework/"},{"name":"Express-Framework","slug":"Express-Framework","link":"/tags/Express-Framework/"},{"name":"Plotly","slug":"Plotly","link":"/tags/Plotly/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"Git-Stash","slug":"Git-Stash","link":"/tags/Git-Stash/"},{"name":"Practice","slug":"Practice","link":"/tags/Practice/"},{"name":"Resolved-Error","slug":"Resolved-Error","link":"/tags/Resolved-Error/"},{"name":"HTTP-Request-Response","slug":"HTTP-Request-Response","link":"/tags/HTTP-Request-Response/"},{"name":"HackerRank","slug":"HackerRank","link":"/tags/HackerRank/"},{"name":"ReactJS","slug":"ReactJS","link":"/tags/ReactJS/"},{"name":"Framework","slug":"Framework","link":"/tags/Framework/"},{"name":"Library","slug":"Library","link":"/tags/Library/"},{"name":"ReactJS-Axios","slug":"ReactJS-Axios","link":"/tags/ReactJS-Axios/"},{"name":"Jest","slug":"Jest","link":"/tags/Jest/"},{"name":"Jest-matcher","slug":"Jest-matcher","link":"/tags/Jest-matcher/"},{"name":"JavaScript-unit-test","slug":"JavaScript-unit-test","link":"/tags/JavaScript-unit-test/"},{"name":"Class-design","slug":"Class-design","link":"/tags/Class-design/"},{"name":"Hexo-Image-Optimization","slug":"Hexo-Image-Optimization","link":"/tags/Hexo-Image-Optimization/"},{"name":"Python-Assignment","slug":"Python-Assignment","link":"/tags/Python-Assignment/"},{"name":"React-Jest","slug":"React-Jest","link":"/tags/React-Jest/"},{"name":"React-router-test","slug":"React-router-test","link":"/tags/React-router-test/"},{"name":"React-testing-library","slug":"React-testing-library","link":"/tags/React-testing-library/"},{"name":"Unit-testing","slug":"Unit-testing","link":"/tags/Unit-testing/"},{"name":"React-router","slug":"React-router","link":"/tags/React-router/"},{"name":"Bundle-size","slug":"Bundle-size","link":"/tags/Bundle-size/"},{"name":"Lazy","slug":"Lazy","link":"/tags/Lazy/"},{"name":"Suspense","slug":"Suspense","link":"/tags/Suspense/"},{"name":"Pyenv","slug":"Pyenv","link":"/tags/Pyenv/"},{"name":"Virtualenv","slug":"Virtualenv","link":"/tags/Virtualenv/"},{"name":"Autoenv","slug":"Autoenv","link":"/tags/Autoenv/"},{"name":"TDD","slug":"TDD","link":"/tags/TDD/"},{"name":"Installation","slug":"Installation","link":"/tags/Installation/"},{"name":"Memoirs","slug":"Memoirs","link":"/tags/Memoirs/"},{"name":"HTML","slug":"HTML","link":"/tags/HTML/"},{"name":"CSS","slug":"CSS","link":"/tags/CSS/"},{"name":"Float","slug":"Float","link":"/tags/Float/"},{"name":"Assignment-feedback","slug":"Assignment-feedback","link":"/tags/Assignment-feedback/"},{"name":"CSS-Sprites","slug":"CSS-Sprites","link":"/tags/CSS-Sprites/"},{"name":"Sass","slug":"Sass","link":"/tags/Sass/"},{"name":"IR","slug":"IR","link":"/tags/IR/"},{"name":"Aria-label","slug":"Aria-label","link":"/tags/Aria-label/"},{"name":"HTML-CSS","slug":"HTML-CSS","link":"/tags/HTML-CSS/"},{"name":"Grid","slug":"Grid","link":"/tags/Grid/"},{"name":"Flex","slug":"Flex","link":"/tags/Flex/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"JavaScript-Basic","slug":"JavaScript-Basic","link":"/tags/JavaScript-Basic/"},{"name":"Web-basic","slug":"Web-basic","link":"/tags/Web-basic/"},{"name":"basic-term","slug":"basic-term","link":"/tags/basic-term/"},{"name":"React-Framework","slug":"React-Framework","link":"/tags/React-Framework/"},{"name":"Basic-term","slug":"Basic-term","link":"/tags/Basic-term/"},{"name":"Web-browser","slug":"Web-browser","link":"/tags/Web-browser/"},{"name":"Brute-force-problem","slug":"Brute-force-problem","link":"/tags/Brute-force-problem/"},{"name":"Map","slug":"Map","link":"/tags/Map/"},{"name":"Set","slug":"Set","link":"/tags/Set/"},{"name":"Unit-Test","slug":"Unit-Test","link":"/tags/Unit-Test/"},{"name":"Consecutive-number-subsequence","slug":"Consecutive-number-subsequence","link":"/tags/Consecutive-number-subsequence/"},{"name":"Number-subsequence","slug":"Number-subsequence","link":"/tags/Number-subsequence/"},{"name":"JavaScript-module-pattern","slug":"JavaScript-module-pattern","link":"/tags/JavaScript-module-pattern/"},{"name":"Jasmine","slug":"Jasmine","link":"/tags/Jasmine/"},{"name":"Sliding-window","slug":"Sliding-window","link":"/tags/Sliding-window/"},{"name":"JavaScript-Map","slug":"JavaScript-Map","link":"/tags/JavaScript-Map/"},{"name":"Two-pointer-algorithm","slug":"Two-pointer-algorithm","link":"/tags/Two-pointer-algorithm/"},{"name":"incomplete","slug":"incomplete","link":"/tags/incomplete/"},{"name":"Webpack","slug":"Webpack","link":"/tags/Webpack/"},{"name":"this-binding","slug":"this-binding","link":"/tags/this-binding/"},{"name":"queue","slug":"queue","link":"/tags/queue/"},{"name":"sorting","slug":"sorting","link":"/tags/sorting/"},{"name":"Mobile-first-method","slug":"Mobile-first-method","link":"/tags/Mobile-first-method/"},{"name":"Decision-algorithm","slug":"Decision-algorithm","link":"/tags/Decision-algorithm/"},{"name":"Stack-frame","slug":"Stack-frame","link":"/tags/Stack-frame/"},{"name":"Recursive-function","slug":"Recursive-function","link":"/tags/Recursive-function/"},{"name":"Tree-traversal","slug":"Tree-traversal","link":"/tags/Tree-traversal/"},{"name":"Execution-context","slug":"Execution-context","link":"/tags/Execution-context/"},{"name":"Ant-design","slug":"Ant-design","link":"/tags/Ant-design/"},{"name":"babel","slug":"babel","link":"/tags/babel/"},{"name":"webpack","slug":"webpack","link":"/tags/webpack/"},{"name":"react-script","slug":"react-script","link":"/tags/react-script/"},{"name":"Unit-test","slug":"Unit-test","link":"/tags/Unit-test/"},{"name":"API-Testing","slug":"API-Testing","link":"/tags/API-Testing/"},{"name":"Node","slug":"Node","link":"/tags/Node/"},{"name":"JavaScript-Engine","slug":"JavaScript-Engine","link":"/tags/JavaScript-Engine/"},{"name":"Web-API","slug":"Web-API","link":"/tags/Web-API/"},{"name":"SSG","slug":"SSG","link":"/tags/SSG/"},{"name":"TTV","slug":"TTV","link":"/tags/TTV/"},{"name":"TTI","slug":"TTI","link":"/tags/TTI/"},{"name":"Anchor-tag-security-issue","slug":"Anchor-tag-security-issue","link":"/tags/Anchor-tag-security-issue/"},{"name":"Integration-Test","slug":"Integration-Test","link":"/tags/Integration-Test/"},{"name":"Redux","slug":"Redux","link":"/tags/Redux/"},{"name":"Mobx","slug":"Mobx","link":"/tags/Mobx/"},{"name":"ContextAPI","slug":"ContextAPI","link":"/tags/ContextAPI/"},{"name":"eslint","slug":"eslint","link":"/tags/eslint/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"hotfix-branch","slug":"hotfix-branch","link":"/tags/hotfix-branch/"},{"name":"Angular","slug":"Angular","link":"/tags/Angular/"},{"name":"ZoneJS","slug":"ZoneJS","link":"/tags/ZoneJS/"},{"name":"Browser","slug":"Browser","link":"/tags/Browser/"},{"name":"faker","slug":"faker","link":"/tags/faker/"},{"name":"dummy-data","slug":"dummy-data","link":"/tags/dummy-data/"},{"name":"CORS","slug":"CORS","link":"/tags/CORS/"},{"name":"Server-scaling","slug":"Server-scaling","link":"/tags/Server-scaling/"},{"name":"Flow","slug":"Flow","link":"/tags/Flow/"},{"name":"Cookie","slug":"Cookie","link":"/tags/Cookie/"},{"name":"Session","slug":"Session","link":"/tags/Session/"},{"name":"JWT","slug":"JWT","link":"/tags/JWT/"},{"name":"Passport.js","slug":"Passport-js","link":"/tags/Passport-js/"},{"name":"RxJS","slug":"RxJS","link":"/tags/RxJS/"},{"name":"Work-life-balance","slug":"Work-life-balance","link":"/tags/Work-life-balance/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"Hive","slug":"Hive","link":"/tags/Hive/"},{"name":"BigData","slug":"BigData","link":"/tags/BigData/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"Data-Pipeline","slug":"Data-Pipeline","link":"/tags/Data-Pipeline/"},{"name":"HDFS","slug":"HDFS","link":"/tags/HDFS/"},{"name":"MapReduce","slug":"MapReduce","link":"/tags/MapReduce/"},{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"HDP","slug":"HDP","link":"/tags/HDP/"},{"name":"EMR","slug":"EMR","link":"/tags/EMR/"},{"name":"Pig","slug":"Pig","link":"/tags/Pig/"},{"name":"RDS","slug":"RDS","link":"/tags/RDS/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Engineer","slug":"Engineer","link":"/tags/Engineer/"},{"name":"Study","slug":"Study","link":"/tags/Study/"},{"name":"Visualization","slug":"Visualization","link":"/tags/Visualization/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","link":"/tags/Kibana/"},{"name":"Tableau","slug":"Tableau","link":"/tags/Tableau/"},{"name":"Dashboard","slug":"Dashboard","link":"/tags/Dashboard/"},{"name":"Selenium","slug":"Selenium","link":"/tags/Selenium/"},{"name":"Excel","slug":"Excel","link":"/tags/Excel/"},{"name":"Certificates","slug":"Certificates","link":"/tags/Certificates/"},{"name":"Project","slug":"Project","link":"/tags/Project/"},{"name":"Portfolio","slug":"Portfolio","link":"/tags/Portfolio/"},{"name":"SQLD","slug":"SQLD","link":"/tags/SQLD/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"DataWarehouse","slug":"DataWarehouse","link":"/tags/DataWarehouse/"},{"name":"DataLake","slug":"DataLake","link":"/tags/DataLake/"},{"name":"DataFrames","slug":"DataFrames","link":"/tags/DataFrames/"},{"name":"Numpy","slug":"Numpy","link":"/tags/Numpy/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"Seaborn","slug":"Seaborn","link":"/tags/Seaborn/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"Lag","slug":"Lag","link":"/tags/Lag/"},{"name":"Airflow","slug":"Airflow","link":"/tags/Airflow/"},{"name":"Bigdata","slug":"Bigdata","link":"/tags/Bigdata/"},{"name":"Data-Engineer-Projects","slug":"Data-Engineer-Projects","link":"/tags/Data-Engineer-Projects/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"Terraform","slug":"Terraform","link":"/tags/Terraform/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"NiFi","slug":"NiFi","link":"/tags/NiFi/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Hexo","slug":"Hexo","link":"/categories/Hexo/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"Algorithm-problem-solving","slug":"Algorithm-problem-solving","link":"/categories/Algorithm-problem-solving/"},{"name":"TypeScript","slug":"TypeScript","link":"/categories/TypeScript/"},{"name":"Data-Structure-&amp;-Algorithm","slug":"Data-Structure-Algorithm","link":"/categories/Data-Structure-Algorithm/"},{"name":"JavaScript","slug":"JavaScript","link":"/categories/JavaScript/"},{"name":"Pandas","slug":"Pandas","link":"/categories/Pandas/"},{"name":"NodeJS","slug":"NodeJS","link":"/categories/NodeJS/"},{"name":"Resolved-Error","slug":"Resolved-Error","link":"/categories/Resolved-Error/"},{"name":"ReactJS","slug":"ReactJS","link":"/categories/ReactJS/"},{"name":"JavaScript-Test","slug":"JavaScript-Test","link":"/categories/JavaScript-Test/"},{"name":"React-Test","slug":"React-Test","link":"/categories/React-Test/"},{"name":"Dev-Environment","slug":"Dev-Environment","link":"/categories/Dev-Environment/"},{"name":"TDD","slug":"TDD","link":"/categories/TDD/"},{"name":"Memoirs","slug":"Memoirs","link":"/categories/Memoirs/"},{"name":"HTML&#x2F;CSS","slug":"HTML-CSS","link":"/categories/HTML-CSS/"},{"name":"To-Be-Good-Front-end-Developer","slug":"To-Be-Good-Front-end-Developer","link":"/categories/To-Be-Good-Front-end-Developer/"},{"name":"Web-browser","slug":"Web-browser","link":"/categories/Web-browser/"},{"name":"Front-end-interview","slug":"Front-end-interview","link":"/categories/Front-end-interview/"},{"name":"NextJS","slug":"NextJS","link":"/categories/NextJS/"},{"name":"ReactJS-TypeScript","slug":"ReactJS-TypeScript","link":"/categories/ReactJS-TypeScript/"},{"name":"Ant-Design","slug":"Ant-Design","link":"/categories/Ant-Design/"},{"name":"Angular","slug":"Angular","link":"/categories/Angular/"},{"name":"Work","slug":"Work","link":"/categories/Work/"},{"name":"Hadoop","slug":"Hadoop","link":"/categories/Hadoop/"},{"name":"SQL","slug":"SQL","link":"/categories/SQL/"},{"name":"Data-Pipeline","slug":"Data-Pipeline","link":"/categories/Data-Pipeline/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"},{"name":"Docker&amp;K8s","slug":"Docker-K8s","link":"/categories/Docker-K8s/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"DataStorage","slug":"DataStorage","link":"/categories/DataStorage/"},{"name":"Numpy","slug":"Numpy","link":"/categories/Numpy/"},{"name":"Data-Visualization","slug":"Data-Visualization","link":"/categories/Data-Visualization/"},{"name":"Airflow","slug":"Airflow","link":"/categories/Airflow/"},{"name":"Side-project","slug":"Side-project","link":"/categories/Side-project/"},{"name":"Terraform","slug":"Terraform","link":"/categories/Terraform/"},{"name":"NiFi","slug":"NiFi","link":"/categories/NiFi/"},{"name":"Vue","slug":"Vue","link":"/categories/Vue/"}],"pages":[{"title":"210201 Open Source Contribution","text":"Open Source Contribution 한 번쯤 오픈 소스 프로젝트에 참가하고 싶었는데, 이번 기회에 한 번 참여해보려고 한다. 조사단계 : Open source contribution reference : reference https://github.com/MunGell/awesome-for-beginners (1) 오픈소스 프로젝트를 찾고 있다면, 오픈소스 프로젝트를 찾아서 해당 Github repository의 issue에서 first-timers-only labeling 되어있는 Issue를 찾아서 Open하고, @first_tmrs_only 를 Twitter에서 follow해서 새로운 first-timers-only issue가 새로 생성이 되었을때 알람을 받는다. (2)","link":"/about/210201-Open-source-contribution.html"},{"title":"about me","text":"안녕하십니까제 이름은 이현기 입니다. 제 블로그는 개발자로 취업준비를 하면서 공부했던 내용들과 개발자로 성장해나가는 저의 이야기를 담고 있습니다. 저는 누구보다도 제대로된 개발자로 성장하기를 바라며, 그러기 위해서 끊임없이 노력을 하고 있습니다. 말로만 열심히 하는 개발자가 아닌 진짜 열심히 한다는 것을 실력으로 증명할 수 있는 그런 개발자가 되겠습니다. 보유 자격증 정보처리 기사(Engineer Information Processing) AWS Certified Cloud Practitioner(CLF-C01) SQLD 리눅스 마스터 2급(Linux master level2) Let's talk Email : hyungi.lee.622@gmail.com GitHub : github.com/LeeHyungi0622","link":"/about/index.html"}]}